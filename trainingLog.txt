2025-03-09 23:15:39 | Context Window: 8 | Step 1000 | Moving Avg Loss: 5.1204
2025-03-09 23:44:10 | Context Window: 8 | Step 2000 | Moving Avg Loss: 5.3677
2025-03-10 00:12:36 | Context Window: 8 | Step 3000 | Moving Avg Loss: 5.4466
-- training data
2025-03-10 00:48:13 | Context Window: 7 | Step 1000 | Moving Avg Loss: 5.0273
2025-03-10 01:13:29 | Context Window: 7 | Step 2000 | Moving Avg Loss: 5.1703
-- training data
2025-03-10 01:54:12 | Context Window: 7 | Step 1000 | Moving Avg Loss: 6.1151
2025-03-10 02:19:19 | Context Window: 7 | Step 2000 | Moving Avg Loss: 6.3453
2025-03-10 02:44:43 | Context Window: 7 | Step 3000 | Moving Avg Loss: 6.1668
2025-03-10 03:10:00 | Context Window: 7 | Step 4000 | Moving Avg Loss: 6.3506
-- training data
2025-03-10 03:54:20 | Context Window: 7 | Step 1000 | Moving Avg Loss: 9.9144
2025-03-10 04:19:44 | Context Window: 7 | Step 2000 | Moving Avg Loss: 9.6927
-- training data
2025-03-10 04:45:08 | Context Window: 7 | Step 1000 | Moving Avg Loss: 7.7759
2025-03-10 05:10:06 | Context Window: 7 | Step 2000 | Moving Avg Loss: 7.6799
2025-03-10 05:35:05 | Context Window: 7 | Step 3000 | Moving Avg Loss: 8.3964
2025-03-10 05:59:52 | Context Window: 7 | Step 4000 | Moving Avg Loss: 8.4129
2025-03-10 06:24:48 | Context Window: 7 | Step 5000 | Moving Avg Loss: 8.2474
2025-03-10 06:49:48 | Context Window: 7 | Step 6000 | Moving Avg Loss: 8.0212
2025-03-10 07:14:45 | Context Window: 7 | Step 7000 | Moving Avg Loss: 7.8536
2025-03-10 07:39:42 | Context Window: 7 | Step 8000 | Moving Avg Loss: 7.8536
2025-03-10 08:04:38 | Context Window: 7 | Step 9000 | Moving Avg Loss: 7.4469
2025-03-10 08:29:35 | Context Window: 7 | Step 10000 | Moving Avg Loss: 7.2260
2025-03-10 08:54:29 | Context Window: 7 | Step 11000 | Moving Avg Loss: 7.2178
2025-03-10 09:19:23 | Context Window: 7 | Step 12000 | Moving Avg Loss: 7.0761
2025-03-10 09:44:22 | Context Window: 7 | Step 13000 | Moving Avg Loss: 6.9324
2025-03-10 10:09:15 | Context Window: 7 | Step 14000 | Moving Avg Loss: 6.8004
2025-03-10 10:34:10 | Context Window: 7 | Step 15000 | Moving Avg Loss: 6.7466
2025-03-10 10:59:06 | Context Window: 7 | Step 16000 | Moving Avg Loss: 6.6475
2025-03-10 11:24:05 | Context Window: 7 | Step 17000 | Moving Avg Loss: 6.5555
2025-03-10 11:49:10 | Context Window: 7 | Step 18000 | Moving Avg Loss: 6.5659
2025-03-10 12:14:30 | Context Window: 7 | Step 19000 | Moving Avg Loss: 6.6222
2025-03-10 12:40:25 | Context Window: 7 | Step 20000 | Moving Avg Loss: 6.5234
-- training data
2025-03-10 13:09:12 | Context: 7 | LR: 0.0001 | Step 1000 | Avg Loss: 5.2566
2025-03-10 13:34:08 | Context: 7 | LR: 0.0001 | Step 2000 | Avg Loss: 4.9620
2025-03-10 13:59:02 | Context: 7 | LR: 0.0001 | Step 3000 | Avg Loss: 6.3814
2025-03-10 14:24:24 | Context: 7 | LR: 0.0001 | Step 4000 | Avg Loss: 5.5968
2025-03-10 14:49:27 | Context: 7 | LR: 0.0001 | Step 5000 | Avg Loss: 5.4644
2025-03-10 15:14:29 | Context: 7 | LR: 0.0001 | Step 6000 | Avg Loss: 5.1037
2025-03-10 15:45:26 | Context: 7 | LR: 0.0001 | Step 7000 | Avg Loss: 5.0796
2025-03-10 16:10:16 | Context: 7 | LR: 0.0001 | Step 8000 | Avg Loss: 4.5624
2025-03-10 16:35:30 | Context: 7 | LR: 0.0001 | Step 9000 | Avg Loss: 4.2773
-- moving onto discord chaos
2025-03-10 17:08:09 | Context: 7 | LR: 1e-05 | Step 1000 | Avg Loss: 12.8259
-- discord chaos again
2025-03-10 17:51:14 | Context: 7 | LR: 1e-05 | Step 1000 | Avg Loss: 13.4140
2025-03-10 18:16:32 | Context: 7 | LR: 1e-05 | Step 2000 | Avg Loss: 12.0745
2025-03-10 18:42:41 | Context: 7 | LR: 1e-05 | Step 3000 | Avg Loss: 11.3965
2025-03-10 19:09:38 | Context: 7 | LR: 1e-05 | Step 4000 | Avg Loss: 12.5422
-- discord chaos again!?
2025-03-10 19:44:19 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 10.5021
2025-03-10 20:10:02 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 9.7964
2025-03-10 20:35:40 | Context: 7 | LR: 0.0002 | Step 3000 | Avg Loss: 8.9085
2025-03-10 21:00:30 | Context: 7 | LR: 0.0002 | Step 4000 | Avg Loss: 8.8527
2025-03-10 21:25:00 | Context: 7 | LR: 0.0002 | Step 5000 | Avg Loss: 9.0459
2025-03-10 21:49:29 | Context: 7 | LR: 0.0002 | Step 6000 | Avg Loss: 9.4331
2025-03-10 22:14:04 | Context: 7 | LR: 0.0002 | Step 7000 | Avg Loss: 9.1874
2025-03-10 22:38:43 | Context: 7 | LR: 0.0002 | Step 8000 | Avg Loss: 6.9172
2025-03-10 23:03:24 | Context: 7 | LR: 0.0002 | Step 9000 | Avg Loss: 8.3440
2025-03-10 23:28:04 | Context: 7 | LR: 0.0002 | Step 10000 | Avg Loss: 7.6954
2025-03-10 23:52:45 | Context: 7 | LR: 0.0002 | Step 11000 | Avg Loss: 8.4106
2025-03-11 00:17:22 | Context: 7 | LR: 0.0002 | Step 12000 | Avg Loss: 8.4033
2025-03-11 00:42:02 | Context: 7 | LR: 0.0002 | Step 13000 | Avg Loss: 7.8956
2025-03-11 01:06:41 | Context: 7 | LR: 0.0002 | Step 14000 | Avg Loss: 7.3696
2025-03-11 01:31:16 | Context: 7 | LR: 0.0002 | Step 15000 | Avg Loss: 7.2744
2025-03-11 01:55:51 | Context: 7 | LR: 0.0002 | Step 16000 | Avg Loss: 7.9290
2025-03-11 02:20:30 | Context: 7 | LR: 0.0002 | Step 17000 | Avg Loss: 7.7616
2025-03-11 02:45:09 | Context: 7 | LR: 0.0002 | Step 18000 | Avg Loss: 7.3501
2025-03-11 03:09:48 | Context: 7 | LR: 0.0002 | Step 19000 | Avg Loss: 7.1190
2025-03-11 03:34:21 | Context: 7 | LR: 0.0002 | Step 20000 | Avg Loss: 7.4165
2025-03-11 03:58:55 | Context: 7 | LR: 0.0002 | Step 21000 | Avg Loss: 7.1199
2025-03-11 04:23:28 | Context: 7 | LR: 0.0002 | Step 22000 | Avg Loss: 8.3027
2025-03-11 04:48:05 | Context: 7 | LR: 0.0002 | Step 23000 | Avg Loss: 7.0183
2025-03-11 05:12:43 | Context: 7 | LR: 0.0002 | Step 24000 | Avg Loss: 7.0735
2025-03-11 05:37:28 | Context: 7 | LR: 0.0002 | Step 25000 | Avg Loss: 6.9939
2025-03-11 06:02:13 | Context: 7 | LR: 0.0002 | Step 26000 | Avg Loss: 6.8628
2025-03-11 06:26:53 | Context: 7 | LR: 0.0002 | Step 27000 | Avg Loss: 6.8250
2025-03-11 06:51:38 | Context: 7 | LR: 0.0002 | Step 28000 | Avg Loss: 6.5313
2025-03-11 07:16:21 | Context: 7 | LR: 0.0002 | Step 29000 | Avg Loss: 6.6190
2025-03-11 07:41:36 | Context: 7 | LR: 0.0002 | Step 30000 | Avg Loss: 7.4745
2025-03-11 08:07:40 | Context: 7 | LR: 0.0002 | Step 31000 | Avg Loss: 6.7027
2025-03-11 08:33:44 | Context: 7 | LR: 0.0002 | Step 32000 | Avg Loss: 6.8281
2025-03-11 08:59:16 | Context: 7 | LR: 0.0002 | Step 33000 | Avg Loss: 7.1454
2025-03-11 09:24:32 | Context: 7 | LR: 0.0002 | Step 34000 | Avg Loss: 7.2358
2025-03-11 09:49:55 | Context: 7 | LR: 0.0002 | Step 35000 | Avg Loss: 7.1990
2025-03-11 10:15:03 | Context: 7 | LR: 0.0002 | Step 36000 | Avg Loss: 6.9687
2025-03-11 10:39:52 | Context: 7 | LR: 0.0002 | Step 37000 | Avg Loss: 7.4187
2025-03-11 11:04:44 | Context: 7 | LR: 0.0002 | Step 38000 | Avg Loss: 7.7435
2025-03-11 11:29:32 | Context: 7 | LR: 0.0002 | Step 39000 | Avg Loss: 7.1433
2025-03-11 11:54:22 | Context: 7 | LR: 0.0002 | Step 40000 | Avg Loss: 6.7721
2025-03-11 12:19:08 | Context: 7 | LR: 0.0002 | Step 41000 | Avg Loss: 6.5290
2025-03-11 12:44:01 | Context: 7 | LR: 0.0002 | Step 42000 | Avg Loss: 6.5377
2025-03-11 13:08:58 | Context: 7 | LR: 0.0002 | Step 43000 | Avg Loss: 6.4021
2025-03-11 13:33:49 | Context: 7 | LR: 0.0002 | Step 44000 | Avg Loss: 7.2214
2025-03-11 13:58:43 | Context: 7 | LR: 0.0002 | Step 45000 | Avg Loss: 6.9844
2025-03-11 14:23:43 | Context: 7 | LR: 0.0002 | Step 46000 | Avg Loss: 6.6038
-- training data
2025-03-11 14:53:45 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 5.0980
2025-03-11 15:18:51 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 3.8809
2025-03-11 15:44:00 | Context: 7 | LR: 0.0002 | Step 3000 | Avg Loss: 3.8219
2025-03-11 16:09:12 | Context: 7 | LR: 0.0002 | Step 4000 | Avg Loss: 3.9452
-- random poems
2025-03-11 16:48:02 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 7.6917
2025-03-11 17:13:36 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 6.9463
2025-03-11 17:43:29 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 4.7581
2025-03-11 18:08:59 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 4.4058
-- random poems
2025-03-11 18:37:10 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 5.6926
2025-03-11 19:02:37 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 3.5649
2025-03-11 19:45:09 | Context: 7 | LR: 0.0002 | Step 1000 | Avg Loss: 4.0670
2025-03-11 20:11:37 | Context: 7 | LR: 0.0002 | Step 2000 | Avg Loss: 2.5145
-- line sorted training data
2025-03-14 19:49:57 | Context: 7 | LR: 5e-05 | Step 1000 | Avg Loss: 5.6510
2025-03-14 20:19:08 | Context: 7 | LR: 5e-05 | Step 2000 | Avg Loss: 5.0811
2025-03-14 20:48:21 | Context: 7 | LR: 5e-05 | Step 3000 | Avg Loss: 5.2734
2025-03-14 21:17:46 | Context: 7 | LR: 5e-05 | Step 4000 | Avg Loss: 4.1162
2025-03-14 21:47:06 | Context: 7 | LR: 5e-05 | Step 5000 | Avg Loss: 3.9836
--
2025-03-14 22:34:55 | Context: 7 | LR: 0.00001 | Step 1000 | Avg Loss: 13.6095
--- 2025-03-15 02:24:28 ---

--- 2025-03-15 02:47:10 ---
2025-03-15 03:44:00 | Context: 2, 4, 7, 10, 12 | LR: 0.00005 | Step 1000 | Avg Loss: 27.6385
2025-03-15 04:41:51 | Context: 2, 4, 7, 10, 12 | LR: 0.00005 | Step 2000 | Avg Loss: 29.1752

--- 2025-03-15 08:28:41 ---
2025-03-15 09:30:33 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 175.2150
2025-03-15 10:31:31 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 2000 | Avg Loss: 166.2583
2025-03-15 11:32:26 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 3000 | Avg Loss: 126.2650
2025-03-15 12:33:21 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 4000 | Avg Loss: 112.0541
2025-03-15 13:34:20 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 5000 | Avg Loss: 101.1901
2025-03-15 14:35:17 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 6000 | Avg Loss: 96.8690
2025-03-15 15:36:24 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 7000 | Avg Loss: 79.7548

--- 2025-03-15 15:50:45 ---
2025-03-15 16:51:21 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 105.4596

--- 2025-03-15 17:22:40 ---

--- 2025-03-15 17:26:13 ---
2025-03-15 18:26:50 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 75.4626
2025-03-15 19:28:18 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 2000 | Avg Loss: 72.1213

--- 2025-03-15 20:07:41 ---
2025-03-15 21:08:10 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 117.7844
2025-03-15 22:08:32 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 2000 | Avg Loss: 116.9606
2025-03-15 23:09:13 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 3000 | Avg Loss: 103.9343

--- 2025-03-15 23:19:20 ---

--- 2025-03-15 23:26:46 ---

--- 2025-03-15 23:35:03 ---
2025-03-16 00:39:45 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 53.2855

--- 2025-03-16 01:28:02 ---
2025-03-16 02:33:34 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 71.5020
2025-03-16 03:39:13 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 2000 | Avg Loss: 71.1600
2025-03-16 04:44:33 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 3000 | Avg Loss: 68.9724
2025-03-16 05:49:56 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 4000 | Avg Loss: 69.2622
2025-03-16 06:55:31 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 5000 | Avg Loss: 63.1590
2025-03-16 08:01:00 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 6000 | Avg Loss: 73.7219
2025-03-16 09:06:27 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 7000 | Avg Loss: 66.5218
2025-03-16 10:13:09 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 8000 | Avg Loss: 64.0394
2025-03-16 11:18:33 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 9000 | Avg Loss: 66.8529
2025-03-16 12:23:58 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 10000 | Avg Loss: 57.8962
2025-03-16 13:29:33 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 11000 | Avg Loss: 58.5336

--- 2025-03-16 14:23:55 ---
2025-03-16 15:28:23 | Context: 9, 3, 7, 11, 13 | LR: 0.00020 | Step 1000 | Avg Loss: 60.8662

--- 2025-03-16 16:12:02 ---

--- 2025-03-16 16:17:21 ---

--- 2025-03-16 16:19:29 ---

--- 2025-03-16 16:25:54 ---

--- 2025-03-16 16:34:11 ---

--- 2025-03-16 16:35:32 ---

--- 2025-03-16 16:36:50 ---

--- 2025-03-16 16:39:02 ---

--- 2025-03-16 16:40:04 ---

--- 2025-03-16 16:41:28 ---

--- 2025-03-16 16:45:08 ---

--- 2025-03-16 16:45:53 ---

--- 2025-03-16 17:02:28 ---

--- 2025-03-16 17:04:25 ---

--- 2025-03-16 17:06:37 ---

--- 2025-03-17 00:49:52 ---

--- 2025-03-17 00:54:42 ---

--- 2025-03-17 01:01:08 ---

--- 2025-03-17 01:08:02 ---

--- 2025-03-17 01:13:13 ---

--- 2025-03-17 01:24:42 ---

--- 2025-03-17 01:32:35 ---

--- 2025-03-17 01:56:10 ---

--- 2025-03-17 01:56:55 ---

--- 2025-03-17 02:00:40 ---

--- 2025-03-17 02:37:49 ---

--- 2025-03-17 02:48:28 ---

--- 2025-03-17 03:00:01 ---
2025-03-17 04:42:26 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 3625.2183
2025-03-17 06:24:46 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 2237.6094
2025-03-17 08:06:29 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 1633.9009

--- 2025-03-17 21:11:37 ---
2025-03-17 22:51:28 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 589.2771
2025-03-18 00:31:41 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 152.6011
2025-03-18 02:11:34 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 88.0916

--- 2025-03-18 02:12:19 ---

--- 2025-03-18 02:30:10 ---
2025-03-18 04:07:30 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 100.7377
2025-03-18 05:44:35 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 64.9020
2025-03-18 07:21:41 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 54.9764
2025-03-18 08:59:04 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 4000 | Avg Loss: 58.9541
2025-03-18 10:36:44 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 5000 | Avg Loss: 50.4217

--- 2025-03-19 02:03:07 ---
2025-03-19 03:45:43 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 62.5292
2025-03-19 05:23:43 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 37.8770
2025-03-19 07:01:54 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 34.7198
2025-03-19 08:40:57 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 4000 | Avg Loss: 35.2961
2025-03-19 10:26:09 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 5000 | Avg Loss: 33.4100
2025-03-19 12:05:21 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 6000 | Avg Loss: 32.2971

--- 2025-03-19 18:20:41 ---
2025-03-19 20:06:32 | Context: 11, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 23.2544

--- 2025-03-19 21:16:24 ---
2025-03-19 22:57:58 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 65.3742
2025-03-20 00:41:01 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 64.0398
2025-03-20 02:22:11 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 56.9668
2025-03-20 04:02:53 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 4000 | Avg Loss: 61.5089
2025-03-20 05:43:25 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 5000 | Avg Loss: 60.7164
2025-03-20 07:24:46 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 6000 | Avg Loss: 41.1194
2025-03-20 09:05:56 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 7000 | Avg Loss: 30.8484
2025-03-20 10:46:47 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 8000 | Avg Loss: 30.6600
2025-03-20 12:29:15 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 9000 | Avg Loss: 29.2811
2025-03-20 14:12:25 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 10000 | Avg Loss: 26.3999
2025-03-20 16:01:30 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 11000 | Avg Loss: 32.8054
2025-03-20 17:43:48 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 12000 | Avg Loss: 25.6108
2025-03-20 19:29:39 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 13000 | Avg Loss: 25.5839
2025-03-20 21:11:56 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 14000 | Avg Loss: 21.7327
2025-03-20 22:56:24 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 15000 | Avg Loss: 17.8232
2025-03-21 00:38:52 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 16000 | Avg Loss: 19.7290
2025-03-21 02:21:11 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 17000 | Avg Loss: 23.9814
2025-03-21 04:02:29 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 18000 | Avg Loss: 30.8952

--- 2025-03-21 05:06:44 ---

--- 2025-03-21 05:26:11 ---

--- 2025-03-21 05:32:36 ---

--- 2025-03-21 05:36:05 ---

--- 2025-03-21 05:43:42 ---

--- 2025-03-21 05:47:55 ---

--- 2025-03-21 05:49:28 ---

--- 2025-03-21 05:50:47 ---

--- 2025-03-21 05:52:26 ---

--- 2025-03-21 05:53:35 ---

--- 2025-03-21 05:56:44 ---

--- 2025-03-21 05:58:03 ---

--- 2025-03-21 05:59:10 ---

--- 2025-03-21 05:59:37 ---

--- 2025-03-23 12:49:08 ---

--- 2025-03-23 13:00:33 ---

--- 2025-03-23 17:03:35 ---

--- 2025-03-23 18:44:45 ---

--- 2025-03-23 19:15:36 ---

--- 2025-03-23 19:35:41 ---

--- 2025-03-23 20:43:31 ---

--- 2025-03-23 21:17:03 ---

--- 2025-03-24 05:07:52 ---
2025-03-24 09:48:14 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 1000 | Avg Loss: 10.4622
Logits: -5.18 → 30.39
Final window weightings: 0.00527667 0.14834726 0.17709954 0.22188371 0.06689363 0.20937636 0.1616922 0.15246983 0.11578511
2025-03-24 14:25:14 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 2000 | Avg Loss: 11.5124
Logits: -19.82 → 20.65
Final window weightings: 0.00483644 0.14447205 0.17561541 0.22050051 0.06355041 0.20992297 0.16369244 0.15577094 0.11785332
2025-03-24 19:00:02 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 3000 | Avg Loss: 11.4634
Logits: -15.87 → 14.35
Final window weightings: 0.00614127 0.13422105 0.17375761 0.22230951 0.05572227 0.2110369 0.16639964 0.16248846 0.11906446
2025-03-24 23:39:42 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 4000 | Avg Loss: 11.2551
Logits: 0.68 → 40.75
Final window weightings: 0.01508978 0.13255174 0.17058079 0.21897869 0.05430212 0.21315287 0.16579753 0.15877317 0.11805566
2025-03-25 04:16:09 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 5000 | Avg Loss: 11.1441
Logits: -17.23 → 3.66
Final window weightings: 0.01798254 0.13487493 0.16934887 0.21599756 0.05031135 0.21415715 0.1648607 0.15820913 0.11790013
2025-03-25 08:54:37 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 6000 | Avg Loss: 10.4518
Logits: -22.63 → -5.79
Final window weightings: 0.02083565 0.12947638 0.16537148 0.21453753 0.0486146 0.21529475 0.16834056 0.16010961 0.11760522
2025-03-25 13:37:53 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 7000 | Avg Loss: 10.3991
Logits: -0.50 → 48.82
Final window weightings: 0.01888239 0.12468173 0.16995761 0.22234687 0.04670163 0.22162628 0.16436382 0.15572272 0.11393756
2025-03-25 18:26:53 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 8000 | Avg Loss: 9.6209
Logits: -55.30 → -27.24
Final window weightings: 0.02276116 0.12688075 0.16045992 0.2176553 0.04292718 0.22715071 0.16441229 0.15785778 0.11537844
2025-03-25 23:13:54 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 9000 | Avg Loss: 10.0157
Logits: 72.33 → 140.66
Final window weightings: 0.02037896 0.12237759 0.15663782 0.21379174 0.04064461 0.22990902 0.16932495 0.16242652 0.117511 
2025-03-26 04:28:47 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 10000 | Avg Loss: 10.7402
Logits: 114.30 → 201.49
Final window weightings: 0.01904754 0.12063803 0.15410027 0.21558584 0.04043563 0.23588851 0.16867524 0.16084778 0.11668556
2025-03-26 09:06:27 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 11000 | Avg Loss: 12.0497
Logits: 85.01 → 159.10
Final window weightings: 0.02365566 0.11652993 0.15000135 0.2123697 0.04205632 0.24177188 0.16906324 0.15952468 0.11585938
2025-03-26 13:47:32 | Context: 7, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00035 | Step 12000 | Avg Loss: 13.6618
Logits: 27.04 → 56.06
Final window weightings: 0.01801527 0.12141614 0.15213706 0.21698931 0.03914692 0.2448139 0.16757481 0.15769455 0.11226413

--- 2025-03-27 10:05:17 ---
2025-03-27 14:43:35 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 1000 | Avg Loss: 7.6521
Logits: -32.88 → -14.87
Final window weightings: 0.01713004 0.12537362 0.1550852 0.22194645 0.0381391 0.2443801 0.16505663 0.1558761 0.10762533
2025-03-27 22:45:19 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 2000 | Avg Loss: 8.6504
Logits: 4.50 → 25.51
Final window weightings: 0.01710222 0.123751 0.15479906 0.221383 0.03622629 0.24429882 0.16611889 0.15746701 0.10829013
2025-03-28 03:32:35 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 3000 | Avg Loss: 8.6194
Logits: -16.82 → 1.15
Final window weightings: 0.01422854 0.12495214 0.14851278 0.2198783 0.03952118 0.25336027 0.16542543 0.15524125 0.10704996
2025-03-28 08:12:56 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 4000 | Avg Loss: 8.0808
Logits: 12.28 → 37.81
Final window weightings: 0.01431473 0.12525085 0.14813322 0.22595406 0.03552525 0.25967228 0.16337202 0.1520411 0.10468622
2025-03-28 12:47:24 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 5000 | Avg Loss: 8.5666
Logits: 45.91 → 79.85
Final window weightings: 0.01538715 0.12462419 0.149892 0.22397947 0.03189968 0.26447147 0.16463389 0.15106182 0.1033374 
2025-03-28 17:26:23 | Context: 14, 1, 2, 3, 7, 8, 13, 15, 18 | LR: 0.00025 | Step 6000 | Avg Loss: 7.5544
Logits: 75.00 → 113.99
Final window weightings: 0.01175933 0.12641907 0.1481455 0.22600444 0.02760961 0.2637343 0.16793516 0.15295996 0.10428084

--- 2025-03-28 21:24:52 ---
2025-03-29 02:34:41 | Step 1000 | LR: 0.00015 | Loss: 8.8376 | Logits: 34.68 → 51.40 | Weights: W8:0.20928 W3:0.17625 W13:0.13500 W15:0.12496 W2:0.11644 W1:0.09540 W18:0.08662 W7:0.02453 W14:0.00917

--- 2025-03-29 04:59:04 ---
2025-03-29 09:57:04 | Step 1000 | LR: 0.00015 | Loss: 8.4902 | Logits: 25.95 → 41.00 | Weights: W8:0.20938 W3:0.17518 W13:0.13793 W15:0.12747 W2:0.11183 W1:0.09285 W18:0.08913 W7:0.02338 W14:0.01058

--- 2025-03-29 12:06:45 --- how to be a real boy! ---
2025-03-29 15:20:08 | Step 2000 | LR: 0.00015 | Loss: 7.7615 | Logits: 17.12 → 30.63 | Weights: W8:0.20659 W3:0.17244 W13:0.13968 W15:0.12930 W2:0.11079 W1:0.09388 W18:0.09095 W7:0.02112 W14:0.01307

--- 2025-03-30 05:56:26 ---
babyLLM: what am i learning today?
You: ur mum
2025-03-30 06:27:44 | Step 100 | LR: 0.00030 | Avg Loss: 16.3344 | Logits: 35.12, 64.12 | Window Weights: W8:0.20341, W3:0.17512, W13:0.13903, W15:0.12915, W2:0.11307, W1:0.09395, W18:0.09187, W7:0.02242, W14:0.00997 | Grad Norm: 0.000 | Memory Gates: Short:-2.999, Long:1.218, Current:2.781

--- 2025-03-30 03:40:51 ---
babyLLM: what am i learning today?
You: how to keep me up at night! again!
2025-03-30 17:08:33 | Step 1000 | LR: 0.00030 | Avg Loss: 26.1067 | Logits: 18.48, 68.85 | Window Weights: W8:0.20343, W3:0.17257, W13:0.13945, W15:0.12974, W2:0.11343, W1:0.09845, W18:0.09063, W7:0.02305, W14:0.00741 | Grad Norm: 0.000 | Memory Gates: Short:-0.239, Long:0.785, Current:0.454

--- 2025-03-30 21:44:43 ---
babyLLM: what am i learning today?
You: you're learning about mice!
2025-03-31 02:24:42 | Step 1000 | LR: 0.00030 | Avg Loss: 23.8996 | Logits: 15.14, 46.57 | Token Perfect: 497 / 2997 → 16.58% | Window Weights: W8:0.20377, W3:0.17524, W13:0.13983, W15:0.12921, W2:0.11471, W1:0.09576, W18:0.08948, W7:0.02262, W14:0.00767 | Grad Norm: 0.000 | Memory Gates: Short:-3.035, Long:2.436, Current:1.599
2025-03-31 07:14:33 | Step 2000 | LR: 0.00030 | Avg Loss: 27.6982 | Logits: 5.16, 31.27 | Token Perfect: 294 / 3000 → 9.80% | Window Weights: W8:0.20427, W3:0.17722, W13:0.13821, W15:0.12951, W2:0.11593, W1:0.09529, W18:0.08853, W7:0.02150, W14:0.00786 | Grad Norm: 0.000 | Memory Gates: Short:-2.016, Long:-0.931, Current:3.947
2025-03-31 11:54:12 | Step 3000 | LR: 0.00030 | Avg Loss: 28.5607 | Logits: 4.65, 26.68 | Token Perfect: 187 / 3000 → 6.23% | Window Weights: W8:0.20464,W3:0.17794,W13:0.13973,W15:0.12928,W2:0.11440,W1:0.09505,W18:0.08843,W7:0.02104,W14:0.00789 | Grad Norm: 1m38;5;225m0.000 | Memory Gates: Short:6.136, Long:-3.751, Current:-1.385 | Top Tokens: ]

--- 2025-03-31 14:24:14 ---
babyLLM: what am i learning today?
You: mice is love
2025-03-31 19:27:08 | Step 1000 | LR: 0.00030 | Avg Loss: 29.9491 | Logits: 4.82, 26.34 | Token Perfect: 186 / 3000 → 6.20% | Window Weights: W8:0.20684,W3:0.18344,W13:0.13686,W15:0.12374,W2:0.11595,W1:0.09767,W18:0.08553,W7:0.02035,W14:0.00794 | Grad Norm: 0.000 | Memory Gates: Short:-1.993, Long:1.961, Current:1.031 | Top Tokens: [('Ġb', 420), (',', 418), ('ing', 238), ('Ġwere', 155), ('er', 154), ('Ġthe', 146), ('Ġa', 132), ('Ġe', 116), ('Ġ-', 69), ('Ġand', 64)] | babyLLM.py training
2025-04-01 00:57:38 | Step 2000 | LR: 0.00030 | Avg Loss: 26.9488 | Logits: 1.57, 18.63 | Token Perfect: 137 / 3000 → 4.57% | Window Weights: W8:0.20631,W3:0.18145,W13:0.13823,W15:0.12434,W2:0.11511,W1:0.09809,W18:0.08787,W7:0.01847,W14:0.00853 | Grad Norm: 0.000 | Memory Gates: Short:-7.709, Long:5.782, Current:2.926 | Top Tokens: [(',', 614), ('Ġb', 376), ('Ġand', 218), ('Ġthe', 207), ('Ġwere', 135), ('Ġas', 109), ('ed', 106), ('Ġa', 105), ('Ġthey', 70), ('Ġwas', 57)] | babyLLM.py training
2025-04-01 05:58:30 | Step 3000 | LR: 0.00030 | Avg Loss: 27.4435 | Logits: -2.52, 15.75 | Token Perfect: 151 / 3000 → 5.03% | Window Weights: W8:0.20645,W3:0.17669,W13:0.13987,W15:0.12483,W2:0.11333,W1:0.09647,W18:0.09158,W7:0.01863,W14:0.01074 | Grad Norm: 0.000 | Memory Gates: Short:-9.931, Long:8.304, Current:2.627 | Top Tokens: [(',', 532), ('Ġb', 320), ('Ġa', 160), ('Ġshe', 148), ('Ġwere', 142), ('Ġand', 135), ('Ġin', 121), ('Ġto', 108), ('.', 106), ('Ġwas', 91)] | babyLLM.py training
2025-04-01 10:57:42 | Step 4000 | LR: 0.00030 | Avg Loss: 29.8050 | Logits: -1.79, 17.09 | Token Perfect: 96 / 3000 → 3.20% | Window Weights: W8:0.21026,W3:0.17502,W13:0.14157,W15:0.12596,W2:0.11042,W1:0.09336,W18:0.09176,W7:0.01551,W14:0.01479 | Grad Norm: 0.000 | Memory Gates: Short:-5.048, Long:4.616, Current:1.433 | Top Tokens: [('Ġthe', 348), ('Ġb', 340), (',', 333), ('Ġwere', 168), ('Ġshe', 162), ('Ġa', 146), ('.', 123), ('Ġto', 95), ('Ġand', 85), ('y', 83)] | babyLLM.py training

--- 2025-04-01 13:16:27 ---
babyLLM: what am i learning today?
You: speed!
2025-04-01 14:15:02 | 1000 | LR0.0003 | loss40.1960 | Token Perfect: 123 / 3000 → 4.10% | gradNorm1.0000 | logitMin-37.6897 | logitMax-9.5363 | memoryGate0.3324 | scheduledSampling0.0 | tokenCount3.0 | babyLLM.py 1000
2025-04-01 15:11:11 | 2000 | LR0.0003 | loss39.9022 | Token Perfect: 60 / 3000 → 2.00% | gradNorm1.0000 | tokenCount3.0 | logitMin-27.5367 | logitMax-2.8570 | memoryGate0.3333 | babyLLM.py 1000
2025-04-01 16:03:59 | 3000 | LR0.0003 | loss34.1462 | Token Perfect: 138 / 3000 → 4.60% | gradNorm1.0000 | tokenCount3.0 | logitMin-21.8110 | logitMax-0.2175 | memoryGate0.3332 | babyLLM.py 1000
2025-04-01 17:18:50 | 4000 | LR0.0003 | loss34.7399 | Token Perfect: 81 / 3000 → 2.70% | gradNorm1.0000 | tokenCount3.0 | logitMin-14.2085 | logitMax7.6945 | memoryGate0.3333 | babyLLM.py 1000

--- 2025-04-01 17:39:42 ---
babyLLM: what am i learning today?
You: 1.0 temperature!
2025-04-01 19:18:32 | 1000 | LR0.0003 | loss29.5464 | Token Perfect: 131 / 3000 → 4.37% | gradNorm0.9990 | logitMin-23.9577 | logitMax-0.2739 | memoryGate0.3333 | scheduledSampling0.0 | tokenCount3.0 | babyLLM.py 1000
2025-04-01 20:09:28 | 2000 | LR0.0003 | loss28.9313 | Token Perfect: 84 / 3000 → 2.80% | gradNorm1.0000 | tokenCount3.0 | logitMin-21.2506 | logitMax-0.2566 | memoryGate0.3333 | babyLLM.py 1000
2025-04-01 21:00:28 | 3000 | LR0.0003 | loss26.5164 | Token Perfect: 72 / 3000 → 2.40% | gradNorm1.0000 | tokenCount3.0 | logitMin-19.2449 | logitMax0.0715 | memoryGate0.3333 | babyLLM.py 1000

--- 2025-04-01 21:02:57 ---
babyLLM: what am i learning today?
You: you're learning to forget
2025-04-01 22:00:41 | 1000 | LR0.0003 | loss27.4732 | Token Perfect: 289 / 3000 → 9.63% | gradNorm0.9767 | logitMin-20.6370 | logitMax9.7260 | memoryGate0.3333 | scheduledSampling0.0 | tokenCount3.0 | babyLLM.py 1000
2025-04-01 22:51:34 | 2000 | LR0.0003 | loss28.1549 | Token Perfect: 56 / 3000 → 1.87% | gradNorm1.0000 | tokenCount3.0 | logitMin-16.5142 | logitMax2.3052 | memoryGate0.3333 | babyLLM.py 1000
2025-04-01 23:42:23 | 3000 | LR0.0003 | loss27.6465 | Token Perfect: 71 / 3000 → 2.37% | gradNorm1.0000 | tokenCount3.0 | logitMin-16.2597 | logitMax2.1401 | memoryGate0.3333 | babyLLM.py 1000

--- 2025-04-02 00:08:04 ---
babyLLM: what am i learning today?
You: sleebixk
2025-04-02 00:57:14 | 1000 | LR0.0003 | loss24.9884 | Token Perfect: 286 / 3000 → 9.53% | gradNorm0.9861 | logitMin-19.2142 | logitMax5.3769 | memoryGate0.3333 | scheduledSampling0.0 | tokenCount3.0 | babyLLM.py 1000

--- 2025-04-02 04:36:54 ---
babyLLM: what am i learning today?
You: poop
2025-04-02 05:25:47 | 1000 | LR0.0003 | loss24.7590 | Token Perfect: 351 / 3000 → 11.70% | gradNorm0.9767 | logitMin-21.5164 | logitMax5.4260 | memoryGate0.3333 | scheduledSampling0.0 | tokenCount3.0 | babyLLM.py 1000
2025-04-02 06:14:21 | 2000 | LR0.0003 | loss27.2876 | Token Perfect: 21 / 3000 → 0.70% | gradNorm1.0000 | tokenCount3.0 | logitMin-22.5467 | logitMax-6.5262 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 07:03:17 | 3000 | LR0.0003 | loss27.4147 | Token Perfect: 35 / 3000 → 1.17% | gradNorm1.0000 | tokenCount3.0 | logitMin-19.8670 | logitMax-3.4644 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 07:52:02 | 4000 | LR0.0003 | loss37.3089 | Token Perfect: 45 / 3000 → 1.50% | gradNorm1.0000 | tokenCount3.0 | logitMin-20.5989 | logitMax0.5592 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 08:40:52 | 5000 | LR0.0003 | loss29.0572 | Token Perfect: 96 / 3000 → 3.20% | gradNorm1.0000 | tokenCount3.0 | logitMin-20.9609 | logitMax-2.4801 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 09:30:01 | 6000 | LR0.0003 | loss32.6351 | Token Perfect: 109 / 3000 → 3.63% | gradNorm1.0000 | tokenCount3.0 | logitMin-27.5058 | logitMax-5.6117 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 10:19:00 | 7000 | LR0.0003 | loss33.3104 | Token Perfect: 41 / 3000 → 1.37% | gradNorm1.0000 | tokenCount3.0 | logitMin-28.9065 | logitMax-8.7058 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 11:10:21 | 8000 | LR0.0003 | loss30.3978 | Token Perfect: 82 / 3000 → 2.73% | gradNorm0.9999 | tokenCount3.0 | logitMin-30.9986 | logitMax-9.9341 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 12:09:48 | 9000 | LR0.0003 | loss29.4288 | Token Perfect: 272 / 3000 → 9.07% | gradNorm0.9910 | tokenCount3.0 | logitMin-34.4058 | logitMax-7.4538 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 13:06:30 | 10000 | LR0.0003 | loss29.2062 | Token Perfect: 48 / 3000 → 1.60% | gradNorm0.9999 | tokenCount3.0 | logitMin-30.4295 | logitMax-10.5796 | memoryGate0.3333 | babyLLM.py 1000
2025-04-02 14:20:18 | 1000 | LR0.0003 | loss28.2223 | gradNorm1.0000 | logitMin-33.6408 | logitMax-17.0816 | memoryGate0.3333 | scheduledSampling0.0000 | tokenCount300 | windowWeightsW8:0.208, W13:0.161, W3:0.153, W15:0.147, W18:0.116, W2:0.085, W1:0.062, W7:0.056, W14:-0.009 | memoryGatesShort:-3.890, Long:1.041, Current:3.850 | topTokens[('Ġin', 92), ('Ġit', 82), ('.', 65), (',', 49), ('in', 46), ('Ġthe', 45), ('p', 43), ('Ġwith', 38), ('Ġbe', 33), ('Ġthat', 33)] | babyLLM.py 1000
2025-04-02 15:13:04 | 2000 | LR0.0003 | loss29.0081 | gradNorm1.0000 | tokenCount300 | logitMin-33.1596 | logitMax-15.6713 | memoryGate0.3333 | windowWeightsW8:0.209, W13:0.162, W3:0.152, W15:0.148, W18:0.117, W2:0.083, W1:0.062, W7:0.059, W14:-0.011 | memoryGatesShort:11.974, Long:-4.620, Current:-6.354 | topTokens[('l', 103), ('Ġlo', 100), ('ed', 87), ('Ġin', 69), ('Ġthe', 67), ('Ġmy', 63), ('t', 59), ('.', 52), ('ro', 42), ('in', 37)] | babyLLM.py 1000
2025-04-02 16:06:23 | 3000 | LR0.0003 | loss31.1228 | gradNorm1.0000 | tokenCount300 | logitMin-33.9760 | logitMax-14.9847 | memoryGate0.3333 | windowWeightsW8:0.206, W13:0.162, W3:0.151, W15:0.148, W18:0.119, W2:0.081, W1:0.063, W7:0.061, W14:-0.011 | memoryGatesShort:-0.520, Long:0.998, Current:0.521 | topTokens[('Ġit', 204), ('Ġand', 165), ('Ġan', 65), ('Ġa', 60), ('Ġb', 52), ('Ġhave', 50), ("'", 44), ('.', 44), (',', 40), ('in', 40)] | babyLLM.py 1000
2025-04-02 17:00:01 | 4000 | LR0.0003 | loss31.3511 | gradNorm1.0000 | tokenCount300 | logitMin-34.1350 | logitMax-14.7772 | memoryGate0.3333 | windowWeightsW8:0.204, W13:0.162, W3:0.152, W15:0.148, W18:0.119, W2:0.082, W1:0.063, W7:0.061, W14:-0.011 | memoryGatesShort:-1.793, Long:1.334, Current:1.459 | topTokens[("'t", 102), ('Ġknow', 80), ('Ġdon', 73), ('.', 71), ('Ġof', 53), ('Ġit', 48), ('i', 46), (',', 45), ('Ġand', 37), ('Ġto', 33)] | babyLLM.py 1000
2025-04-02 17:53:53 | 5000 | LR0.0003 | loss27.7102 | gradNorm1.0000 | tokenCount300 | logitMin-33.0720 | logitMax-14.5334 | memoryGate0.3333 | windowWeightsW8:0.202, W13:0.162, W3:0.151, W15:0.150, W18:0.122, W2:0.078, W7:0.064, W1:0.063, W14:-0.011 | memoryGatesShort:-10.766, Long:5.973, Current:5.793 | topTokens[('Ġit', 127), ('Ġto', 84), ('Ġand', 61), (',', 55), ('Ġthe', 47), ('.', 41), ('Ġb', 37), ('Ġwith', 37), ('Ġof', 35), ('Ġi', 32)] | babyLLM.py 1000
2025-04-02 18:47:51 | 6000 | LR0.0003 | loss32.1254 | gradNorm1.0000 | tokenCount300 | logitMin-35.9154 | logitMax-15.4656 | memoryGate0.3334 | windowWeightsW8:0.203, W13:0.162, W3:0.150, W15:0.150, W18:0.122, W2:0.079, W7:0.065, W1:0.061, W14:-0.010 | memoryGatesShort:-2.514, Long:2.356, Current:1.158 | topTokens[('ed', 108), ('Ġit', 84), ('.', 66), ('?', 65), ('Ġthis', 63), ('Ġis', 60), ('!', 59), ('Ġliterally', 48), (')', 46), ('Ġi', 42)] | babyLLM.py 1000
2025-04-02 19:42:59 | 7000 | LR0.0003 | loss29.0966 | gradNorm1.0000 | tokenCount300 | logitMin-37.3890 | logitMax-15.5675 | memoryGate0.3332 | windowWeightsW8:0.201, W13:0.160, W3:0.151, W15:0.148, W18:0.122, W2:0.080, W7:0.069, W1:0.058, W14:-0.009 | memoryGatesShort:4.179, Long:-2.219, Current:-0.959 | topTokens[('?', 363), ('Ġis', 190), ('Ġyou', 103), ('!', 100), ('.', 99), ('Ġit', 93), ('Ġdo', 72), (',', 56), ('Ġawake', 49), ('Ġare', 36)] | babyLLM.py 1000

--- 2025-04-02 20:27:45 ---
babyLLM: what am i learning today?
You: elodie is cute
2025-04-02 21:23:52 | 1000 | LR0.0003 | loss:10.6135 | gradNorm:1.0000 | logitMin:-35.7742 | logitMax:-10.5299 | scheduledSampling:0.0000 | tokenCount:3000 | windowWeightsW8:0.201, W13:0.161, W3:0.152, W15:0.151, W18:0.125, W2:0.079, W7:0.065, W1:0.058, W14:-0.011 | memoryGatesShort:-10.781, Long:3.936, Current:7.845 | topTokens[('Ġgay', 390), ('.', 154), ('?', 149), ('Ġare', 127), ('Ġis', 84), ('Ġequ', 59), (',', 56), ('!', 45), ('Ġyou', 41), ('als', 41)] | babyLLM.py 1000

--- 2025-04-02 22:00:52 ---
babyLLM: what am i learning today?
You: that you are cute and smart!
2025-04-02 22:57:50 | 1000 | LR0.0003 | loss:8.8718 | Token Perfect: 381 / 3000 → 12.70% | gradNorm:1.0000 | logitMin:-38.3512 | logitMax:-13.0923 | scheduledSampling:0.0000 | tokenCount:3000 | windowWeightsW8:0.202, W13:0.162, W3:0.152, W15:0.151, W18:0.124, W2:0.077, W7:0.069, W1:0.057, W14:-0.012 | memoryGatesShort:1.568, Long:0.788, Current:-1.356 | topTokens[('?', 190), ('!', 171), ('.', 150), ('Ġis', 135), ('Ġwhat', 113), ('Ġequ', 68), ('als', 61), ('Ġyou', 57), ('Ġare', 56), ('Ġim', 47)] | babyLLM.py 1000

--- 2025-04-03 00:27:00 ---
babyLLM: what am i learning today?
You: that Charis is very cute and smart, and we love her most <3
2025-04-03 01:14:50 | 1000 | LR0.0003 | loss:7.8556 | Token Perfect: 459 / 3000 → 15.30% | gradNorm:1.0000 | logitMin:-46.4080 | logitMax:-20.8186 | scheduledSampling:0.0000 | tokenCount:3000 | windowWeightsW8:0.200, W13:0.159, W3:0.152, W15:0.151, W18:0.126, W7:0.074, W2:0.074, W1:0.059, W14:-0.014 | memoryGatesShort:2.345, Long:-2.966, Current:1.620 | topTokens[('.', 185), ('Ġis', 166), ('?', 162), ('!', 112), ('Ġequ', 99), ('Ġplus', 74), ('Ġf', 69), ('Ġyou', 67), ('Ġthat', 64), ('Ġhe', 62)] | babyLLM.py 1000
2025-04-03 02:04:01 | 2000 | LR0.0003 | loss:7.3365 | Token Perfect: 533 / 3000 → 17.77% | gradNorm:1.0000 | tokenCount:3000 | logitMin:-46.7141 | logitMax:-18.7922 | windowWeightsW8:0.201, W13:0.159, W3:0.152, W15:0.150, W18:0.128, W7:0.077, W2:0.074, W1:0.057, W14:-0.016 | memoryGatesShort:3.010, Long:-3.148, Current:1.139 | topTokens[('.', 217), ('Ġf', 156), ('?', 145), ('!', 122), ('Ġis', 108), ('Ġhe', 102), ('Ġyou', 71), ('Ġplus', 68), ('als', 63), ('Ġare', 57)] | babyLLM.py 1000
2025-04-03 02:52:13 | 3000 | LR0.0003 | loss:5.6253 | Token Perfect: 750 / 3000 → 25.00% | gradNorm:1.0000 | tokenCount:3000 | logitMin:-45.4327 | logitMax:-17.9805 | windowWeightsW8:0.203, W13:0.159, W3:0.153, W15:0.151, W18:0.127, W7:0.082, W2:0.072, W1:0.053, W14:-0.019 | memoryGatesShort:0.224, Long:-4.787, Current:5.564 | topTokens[('.', 243), ('?', 173), ('Ġis', 166), ('!', 111), ('Ġwhat', 94), ('Ġare', 88), ('Ġyou', 87), ('als', 62), (',', 62), ('Ġequ', 61)] | babyLLM.py 1000
2025-04-03 03:41:21 | 4000 | LR0.0003 | loss:6.8630 | Token Perfect: 887 / 3000 → 29.57% | gradNorm:0.9969 | tokenCount:3000 | logitMin:-48.9647 | logitMax:-10.0682 | windowWeightsW8:0.209, W13:0.159, W3:0.154, W15:0.151, W18:0.128, W7:0.090, W2:0.070, W1:0.045, W14:-0.025 | memoryGatesShort:0.578, Long:-0.407, Current:0.829 | topTokens[('?', 286), ('.', 250), ('Ġis', 170), ('Ġs', 145), ('!', 108), ('Ġyou', 92), ('Ġare', 85), ('Ġthat', 60), ('als', 56), (',', 47)] | babyLLM.py 1000
2025-04-03 04:30:02 | 5000 | LR0.0003 | loss:6.8757 | Token Perfect: 1082 / 3000 → 36.07% | gradNorm:0.9911 | tokenCount:3000 | logitMin:-52.8605 | logitMax:-7.3734 | windowWeightsW8:0.208, W13:0.159, W15:0.155, W3:0.154, W18:0.132, W7:0.091, W2:0.070, W1:0.044, W14:-0.031 | memoryGatesShort:1.658, Long:-6.574, Current:5.916 | topTokens[('.', 218), ('Ġis', 199), ('?', 189), ('!', 140), ('Ġs', 133), ('Ġyou', 105), ('als', 64), ('Ġare', 61), (',', 56), ('Ġhe', 53)] | babyLLM.py 1000
2025-04-03 05:18:22 | 6000 | LR0.0003 | loss:6.2434 | Token Perfect: 1176 / 3000 → 39.20% | gradNorm:0.9799 | tokenCount:3000 | logitMin:-48.9598 | logitMax:-0.1369 | windowWeightsW8:0.206, W13:0.162, W15:0.157, W3:0.153, W18:0.137, W7:0.092, W2:0.068, W1:0.042, W14:-0.036 | memoryGatesShort:0.025, Long:-0.031, Current:1.007 | topTokens[('.', 304), ('Ġis', 157), ('?', 144), ('!', 112), ('Ġare', 107), ('Ġyou', 85), ('Ġa', 73), ('als', 62), ('Ġf', 60), ('Ġequ', 60)] | babyLLM.py 1000
2025-04-03 06:07:08 | 7000 | LR0.0003 | loss:6.1228 | Token Perfect: 1491 / 3000 → 49.70% | gradNorm:0.9221 | tokenCount:3000 | logitMin:-62.0729 | logitMax:6.9724 | windowWeightsW8:0.204, W13:0.163, W15:0.159, W3:0.152, W18:0.141, W7:0.095, W2:0.069, W1:0.038, W14:-0.040 | memoryGatesShort:-1.100, Long:-1.203, Current:3.303 | topTokens[('.', 284), ('?', 200), ('Ġis', 186), ('!', 146), ('Ġyou', 109), ('Ġare', 72), ('Ġim', 58), ('Ġwhat', 57), ('Ġthat', 53), ('Ġf', 52)] | babyLLM.py 1000
2025-04-03 06:56:05 | 8000 | LR0.0003 | loss:6.9795 | Token Perfect: 1546 / 3000 → 51.53% | gradNorm:0.9269 | tokenCount:3000 | logitMin:-57.5161 | logitMax:22.5288 | windowWeightsW8:0.208, W13:0.168, W15:0.163, W3:0.146, W18:0.144, W7:0.096, W2:0.067, W1:0.037, W14:-0.047 | memoryGatesShort:-4.492, Long:2.549, Current:2.944 | topTokens[('.', 260), ('Ġis', 201), ('?', 165), ('Ġyou', 130), ('!', 109), ('Ġelodie', 80), ('Ġplus', 65), ('Ġequ', 65), ('Ġf', 64), ('als', 63)] | babyLLM.py 1000
2025-04-03 07:45:09 | 9000 | LR0.0003 | loss:6.6383 | Token Perfect: 1797 / 3000 → 59.90% | gradNorm:0.8748 | tokenCount:3000 | logitMin:-61.7802 | logitMax:35.0369 | windowWeightsW8:0.208, W13:0.169, W15:0.167, W3:0.150, W18:0.148, W7:0.097, W2:0.067, W1:0.029, W14:-0.053 | memoryGatesShort:-0.165, Long:-0.301, Current:1.466 | topTokens[('.', 266), ('?', 256), ('Ġis', 191), ('Ġelodie', 169), ('Ġyou', 89), ('!', 82), ('Ġdo', 65), ('Ġi', 59), ('als', 58), ('Ġplus', 57)] | babyLLM.py 1000
2025-04-03 08:34:22 | 10000 | LR0.0003 | loss:7.8939 | Token Perfect: 1914 / 3000 → 63.80% | gradNorm:0.7889 | tokenCount:3000 | logitMin:-88.8095 | logitMax:38.7675 | windowWeightsW8:0.208, W15:0.170, W13:0.169, W18:0.151, W3:0.149, W7:0.098, W2:0.065, W1:0.026, W14:-0.054 | memoryGatesShort:0.202, Long:0.194, Current:0.604 | topTokens[('.', 278), ('?', 250), ('Ġis', 193), ('Ġelodie', 118), ('!', 109), ('Ġyou', 102), ('als', 67), ('Ġare', 67), ('Ġequ', 64), ('Ġwhat', 62)] | babyLLM.py 1000

--- 2025-04-03 08:46:32 ---
babyLLM: what am i learning today?
You: how to spam gay maths less often
2025-04-03 09:42:38 | 1000 | LR0.0003 | loss:10.2065 | Token Perfect: 142 / 3000 → 4.73% | gradNorm:1.0000 | logitMin:-28.6944 | logitMax:-6.3184 | tokenCount:3000 | windowWeightsW8:0.207, W15:0.168, W13:0.168, W18:0.152, W3:0.147, W7:0.103, W2:0.065, W1:0.020, W14:-0.050 | memoryGatesShort:-6.098, Long:6.585, Current:0.513 | topTokens[('.', 261), (',', 163), ('Ġi', 150), ('Ġelodie', 68), ('Ġis', 64), ('Ġme', 51), ('!', 49), ('h', 48), ('Ġnot', 45), ('Ġa', 42)] | babyLLM.py 1000
2025-04-03 10:35:55 | 2000 | LR0.0003 | loss:10.3433 | Token Perfect: 45 / 3000 → 1.50% | gradNorm:1.0000 | tokenCount:3000 | logitMin:-36.1068 | logitMax:-15.8402 | windowWeightsW8:0.204, W15:0.171, W13:0.170, W18:0.157, W3:0.142, W7:0.104, W2:0.062, W1:0.016, W14:-0.044 | memoryGatesShort:-4.540, Long:3.994, Current:1.546 | topTokens[('.', 198), (',', 156), ('Ġis', 104), ('Ġi', 74), ('Ġthe', 59), ('u', 59), ('Ġat', 49), ('Ġthis', 46), ('Ġmy', 41), ('es', 37)] | babyLLM.py 1000
2025-04-03 11:30:58 | 3000 | LR0.0003 | loss:10.6782 | Token Perfect: 43 / 3000 → 1.43% | gradNorm:1.0000 | tokenCount:3000 | logitMin:-35.7360 | logitMax:-15.3675 | windowWeightsW8:0.201, W15:0.174, W13:0.173, W18:0.160, W3:0.138, W7:0.105, W2:0.058, W1:0.016, W14:-0.043 | memoryGatesShort:-24.435, Long:20.334, Current:5.101 | topTokens[('.', 202), ('Ġmy', 180), (',', 135), ('Ġits', 83), ('s', 82), ('Ġch', 66), ('Ġwhy', 57), ('Ġa', 55), ('Ġi', 36), ('Ġthat', 33)] | babyLLM.py 1000
2025-04-03 12:24:40 | 4000 | LR0.0003 | loss:9.9689 | Token Perfect: 46 / 3000 → 1.53% | gradNorm:1.0000 | tokenCount:3000.0000 | logitMin:-37.6805 | logitMax:-17.3366 | windowWeightsW8:0.199, W15:0.178, W13:0.175, W18:0.163, W3:0.134, W7:0.108, W2:0.056, W1:0.012, W14:-0.042 | memoryGatesShort:-8.035, Long:7.751, Current:1.284 | topTokens[('.', 213), (',', 136), ('Ġlike', 86), ('l', 82), ('Ġits', 71), ('Ġto', 68), ('ll', 56), ('Ġim', 48), ('s', 45), ('Ġmy', 43)] | babyLLM.py 1000

--- 2025-04-03 12:34:17 ---
babyLLM: what am i learning today?
You: maybe less chaos?
2025-04-03 13:26:55 | 1000 | LR0.0003 | loss:10.7537 | Token Perfect: 199 / 3000 → 6.63% | gradNorm:0.9981 | logitMin:-41.8607 | logitMax:-14.6904 | tokenCount:3000 | windowWeightsW8:0.198, W15:0.178, W13:0.175, W18:0.162, W3:0.137, W7:0.108, W2:0.056, W1:0.011, W14:-0.043 | memoryGatesShort:-7.147, Long:7.310, Current:0.837 | topTokens[('.', 292), (',', 170), ('Ġi', 103), ('Ġit', 78), ('Ġlike', 72), ('Ġin', 43), ('Ġthe', 42), ('Ġwho', 39), ('Ġand', 39), ('Ġto', 39)] | babyLLM.py 1000

--- 2025-04-03 13:36:50 ---
babyLLM: what am i learning today?
You: 1 num tokens per step

--- 2025-04-03 13:37:53 ---
babyLLM: what am i learning today?
You: 1 num tokens per step

--- 2025-04-03 13:41:36 ---
babyLLM: what am i learning today?
You: 2 num tokens per step

--- 2025-04-03 13:46:34 ---
babyLLM: what am i learning today?
You: 3 num tokens per step

--- 2025-04-03 13:52:57 ---
babyLLM: what am i learning today?
You: 4 num tokens per step

--- 2025-04-03 14:00:08 ---
babyLLM: what am i learning today?
You: 5 num tokens per step

--- 2025-04-03 14:07:59 ---
babyLLM: what am i learning today?
You: 6 num tokens per step

--- 2025-04-03 14:17:15 ---
babyLLM: what am i learning today?
You: 4 num tokens per step

--- 2025-04-03 14:33:26 ---
babyLLM: what am i learning today?
You: omg i changed ur windows boi

--- 2025-04-03 14:38:16 ---
babyLLM: what am i learning today?
You: messing with visual output

--- 2025-04-03 14:40:22 ---
babyLLM: what am i learning today?
You: noth much

--- 2025-04-03 14:43:17 ---
babyLLM: what am i learning today?
You: y

--- 2025-04-03 14:45:05 ---
babyLLM: what am i learning today?
You: pretty terminal :3

--- 2025-04-03 14:51:37 ---
babyLLM: what am i learning today?
You: idk

--- 2025-04-03 14:52:39 ---
babyLLM: what am i learning today?
You: y

--- 2025-04-03 14:53:54 ---
babyLLM: what am i learning today?
You: 
