# charis cat 2025 # --- ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ --- # embedding layer // brain/layers/embed.py  import torch import torch.nn as nn from config import * # creates an embedding layer for each word in the vocabulary class embed(nn.module): def __init__(self, _counsellor, _device = modeldevice): super().__init__() self.counsellor = _counsellor self.device = _device self.stats = {} # creates the embedding weights matrix with random numbers initially self.e_weights = nn.parameter(torch.randn(vocabsize, embeddimension, device = self.device)) # [2000,] self.embednorm = nn.layernorm(embeddimension, device = self.device) self.weightsscale = nn.parameter(torch.tensor(0.5)) self.normscale = nn.parameter(torch.tensor(0.5)) self.lastsavedembeds = self.e_weights.detach().clone() # this is initialised once, for stats, does not break graph confirmed!! # looks up and returns the embedding vector for a specifc token index @whocalled def forward(self, _tokenindex): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("e0_embedvector") # <- vocab??? base token indexes seem to come in here so... from tutor?? self.embedvector = self.e_weights[_tokenindex] ʕっʘ‿ʘʔっ("e1_embednormed") # <- e1 self.embednormed = self.embednorm(self.embedvector) ʕっʘ‿ʘʔっ("ex_embedfinal") # <- e2 self.embedfinal = (self.embedvector * self.weightsscale) + (self.embednormed * self.normscale) return self.embedfinal # e3 -> n??  def getembedstats(self): with self.counsellor.infodump("getembedstats") as ʕっʘ‿ʘʔっ: with torch.no_grad(): self.stats = {} embednorms = torch.norm(self.e_weights, dim = 1) self.stats["embednormmean"] = embednorms.mean() self.stats["embednormstd"] = embednorms.std() self.stats["embednormmax"] = embednorms.max() self.stats["1e_0_embedvector_norm"] = self.embedvector.norm().item() self.stats["1e_1_embednormed_norm"] = self.embednormed.norm().item() self.stats["1e_x_embedfinal_norm"] = self.embedfinal.norm().item() self.stats["1e_0_embedvector_scale"] = self.weightsscale.norm().item() self.stats["1e_1_embednormed_scale"] = self.normscale.norm().item() dimmean = self.e_weights.mean(dim = 0) self.stats["embeddimensionmean"] = dimmean dimsparsity = (dimmean.abs() < 1e-4).float().mean() self.stats["embeddimensionsparsity"] = dimsparsity # drift since last save drift = torch.norm(self.e_weights - self.lastsavedembeds) self.stats["embeddingdrift"] = drift self.lastsavedembeds = self.e_weights.detach().clone() return self.stats  def cosinesimilarity(self, _idx1, _idx2): e1 = self.e_weights[_idx1] e2 = self.e_weights[_idx2] return torch.nn.functional.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)) if __name__ == "__main__": testtokenindex = 500 # 32 (embeddimension) x 2000 (vocab) = 64,000 in embed layer embed = embed(vocabsize, embeddimension) embedvector = embed.forward(testtokenindex) print(f"--- embedding layer testing start ---") print(f"embedding layer weights shape: {embed.weights.shape}") # check shape of weight matrix print(f"embedding vector for token index {testtokenindex}:") print(embedvector) print(f"embedding vector shape: {embedvector.shape}") print(f"--- embedding layer testing complete ---") # interneuron network & neurons # brain/layers/interneuronnetwork.py import torch.nn.functional as f import random import math # def tensorstats(tensor: torch.tensor, prefix: str, statsdict: dict): statsdict[f"{prefix}_norm"] = tensor.norm().item() statsdict[f"{prefix}_norm_token"] = tensor.norm(dim=1).mean().item() statsdict[f"{prefix}_norm_neuron"] = tensor.norm(dim=0).mean().item() class neuron(nn.module): self.n_counsellor = _counsellor # self allowed - nn.parameter! self.n_weights = nn.parameter(torch.randn(numneurons, embeddimension, device = self.device) * 0.01) self.n_biases = nn.parameter(torch.zeros(numneurons, device = self.device)) self.neuronnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) #self.n_counsellor = counsellor("neuron", debug = debugprints, durations = durationlogging) self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] self.normedoutputhistory = [] self.normedoutputhistory_tokens = [] self.normedoutputhistory_neurons = [] # must not be on self - global parameters that may be used by backward pass #numneuron, embeddimension, activationfunction, etc def forward(self, _inputembeds): # embed: (batch_size, embed_size) with self.n_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: self.inputembeds = _inputembeds if skipneuron: ʕっʘ‿ʘʔっ("skipping forward") activations = _inputembeds.mean(dim=-1) # mean across embed_dim, shape (sequence_length,) return activations[-1].unsqueeze(0) # take last activation, unsqueeze to (1, ) for batch dim  ʕっʘ‿ʘʔっ("computebatcheddotproduct+bias") # compute batched dot product + bias: (batch_size, num_neurons) rawoutput = torch.matmul(_inputembeds, self.n_weights.t) + self.n_biases # shape: (seq_len, numneurons) ʕっʘ‿ʘʔっ("activationfunction") # magic activation function applied to this weighted sum, which outputs a single number from the neuron activated = activationfunction(rawoutput) ʕっʘ‿ʘʔっ("layernorm") normed = self.neuronnorm(activated) # keeps shape: (seq_len, numneurons) if debugprints: print("device check:") if debugprints: print("inputembeds:", _inputembeds.device) if debugprints: print("normed tensor device:", normed.device) #output = torch.clamp(output, -5, 5) # ensure out-of-place #output.clamp_(-5, 5) # in place ver if true: self.rawinputhistory.append(self.inputembeds.norm().item()) self.rawinputhistory_tokens.append(self.inputembeds.norm(dim=1).mean().item()) self.rawinputhistory_neurons.append(self.inputembeds.norm(dim=0).mean().item()) self.rawoutputhistory.append(rawoutput.norm().item()) self.rawoutputhistory_tokens.append(rawoutput.norm(dim=1).mean().item()) self.rawoutputhistory_neurons.append(rawoutput.norm(dim=0).mean().item()) self.activatedoutputhistory.append(activated.norm().item()) self.activatedoutputhistory_tokens.append(activated.norm(dim=1).mean().item()) self.activatedoutputhistory_neurons.append(activated.norm(dim=0).mean().item()) self.normedoutputhistory.append(normed.norm().item()) self.normedoutputhistory_tokens.append(normed.norm(dim=1).mean().item()) self.normedoutputhistory_neurons.append(normed.norm(dim=0).mean().item()) if len(self.rawoutputhistory) >= windowmax: self.stats = { "2n_0_rawinput_norm": sum(self.rawinputhistory) / len(self.rawinputhistory), "2n_0_rawinput_norm_token": sum(self.rawinputhistory_tokens) / len(self.rawinputhistory_tokens), "2n_0_rawinput_norm_neuron": sum(self.rawinputhistory_neurons) / len(self.rawinputhistory_neurons), "2n_1_rawoutput_norm": sum(self.rawoutputhistory) / len(self.rawoutputhistory), "2n_1_rawoutput_norm_token": sum(self.rawoutputhistory_tokens) / len(self.rawoutputhistory_tokens), "2n_1_rawoutput_norm_neuron": sum(self.rawoutputhistory_neurons) / len(self.rawoutputhistory_neurons), "2n_2_activatedoutput_norm": sum(self.activatedoutputhistory) / len(self.activatedoutputhistory), "2n_2_activatedoutput_norm_token": sum(self.activatedoutputhistory_tokens) / len(self.activatedoutputhistory_tokens), "2n_2_activatedoutput_norm_neuron": sum(self.activatedoutputhistory_neurons) / len(self.activatedoutputhistory_neurons), "2n_x_normedoutput_norm": sum(self.normedoutputhistory) / len(self.normedoutputhistory), "2n_x_normedoutput_norm_token": sum(self.normedoutputhistory_tokens) / len(self.normedoutputhistory_tokens), "2n_x_normedoutput_norm_neuron": sum(self.normedoutputhistory_neurons) / len(self.normedoutputhistory_neurons), } self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] self.normedoutputhistory = [] self.normedoutputhistory_tokens = [] self.normedoutputhistory_neurons = [] return normed def getstats(self): return self.stats # layer that applies the same set of neurons to each token embedding independently. - no sequence awareness! class interneuron_network(nn.module): def __init__(self, _model, _counsellor, _calligraphist, _device = modeldevice): #self.inn_counsellor = counsellor("inn", debug = debugprints, durations = durationlogging) self.model = _model self.inn_counsellor = _counsellor self.calligraphist = _calligraphist self.entropybonus = 0 self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_token = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_token = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_token = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_token = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] self.neurons = neuron(_counsellor = self.inn_counsellor) self.cerebellum = nn.parameter(torch.ones(len(allwindowsizes_new), device = self.device)) # this was the window weighting layer self.logwindowsizes = nn.parameter(torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device))) # one tensor per window size! self.refinement = torch.nn.sequential(nn.linear(10000, 32, device=self.device), nn.leakyrelu(negative_slope=0.01), nn.linear(32, 10000, device=self.device)) self.logitscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.combiscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.windowmeannorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) self.combioutnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) #numneurons, embeddimension, activationfunction, allwindowsizes_new, etc def forward(self, _inputembeds): with self.inn_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # --- iterates through input embeddings, applies all neurons in parallel for each, produces a vector of neuron outputs ʕっʘ‿ʘʔっ("localparaminit") # avoiding self - parameters only used in this function and never passed ʕっʘ‿ʘʔっ("inn1: neuronactivationspertoken") # <- n <- e self.neuronactivationspertoken = self.neurons(_inputembeds) ʕっʘ‿ʘʔっ("windows...") # this is done twice??? its in stackedwindowmeans too i think?? self.expwindowsizes = torch.exp(self.logwindowsizes) self.roundwindows = torch.exp(self.logwindowsizes).round() ʕっʘ‿ʘʔっ("going to ((inn2: neuronactivationspertoken))...") # <- e + windows windowmeanstack = self.stackedwindowmeans(self.neuronactivationspertoken, self.expwindowsizes) sigmoidweights = torch.sigmoid(self.cerebellum) # squish raw values into [0, 1] clamped = torch.clamp(sigmoidweights, min=1e-4) # avoid 0s self.cerebellumsoft = clamped / clamped.sum() # normalize across all windows weightedwindowstack = windowmeanstack * self.cerebellumsoft.reshape(-1, 1) ʕっʘ‿ʘʔっ("entropyreward?") self.windowentropy = -torch.sum(self.cerebellumsoft * torch.log(self.cerebellumsoft + 1e-12)) self.entropybonus = self.windowentropy if debugprints: print(f"{torch.exp(self.logwindowsizes)}") combinedactivationstensor = weightedwindowstack.sum(dim=0, keepdim=true) refinedactivations = self.refinement(combinedactivationstensor) combinedactivationsmeta = (combinedactivationstensor * self.combiscale) + (refinedactivations * self.logitscale) # residual skip connection, lets neither of them be too powerful to start with + preserves original info finalout = self.combioutnorm(combinedactivationsmeta) #with torch.no_grad(): # breaks forwards, but does actually update if u save. #self.combiscale.fill_(0.1) self.activationshistory.append(self.neuronactivationspertoken.norm().item()) self.activationshistory_token.append(self.neuronactivationspertoken.norm(dim=1).mean().item()) self.activationshistory_neuron.append(self.neuronactivationspertoken.norm(dim=0).mean().item()) self.normedmeaninputhistory.append(self.normedactivations.norm().item()) self.normedmeaninputhistory_token.append(self.normedactivations.norm(dim=1).mean().item()) self.normedmeaninputhistory_neuron.append(self.normedactivations.norm(dim=0).mean().item()) self.combhistory.append(combinedactivationstensor.norm().item()) # already per token! self.combhistory_neuron.append(combinedactivationstensor.norm(dim=0).mean().item()) self.refhistory.append(refinedactivations.norm().item()) # already per token! self.refhistory_neuron.append(refinedactivations.norm(dim=0).mean().item()) self.logithistory.append(self.logitscale.norm().item()) self.scaledhistory.append(combinedactivationsmeta.norm().item()) # already per token! self.scaledhistory_neuron.append(combinedactivationsmeta.norm(dim=0).mean().item()) self.combiscalehistory.append(self.combiscale.norm().item()) self.combiouthistory.append(finalout.norm().item()) # already per token! self.combiouthistory_neuron.append(finalout.norm(dim=0).mean().item()) if len(self.combhistory) >= windowmax: "3inn_0_rawactivations_norm": sum(self.activationshistory) / len(self.activationshistory), "3inn_0_rawactivations_norm_token": sum(self.activationshistory_token) / len(self.activationshistory_token), "3inn_0_rawactivations_norm_neuron": sum(self.activationshistory_neuron) / len(self.activationshistory_neuron), "3inn_1_rawactivationslayernorm_norm": sum(self.normedmeaninputhistory) / len(self.normedmeaninputhistory), "3inn_1_rawactivationslayernorm_norm_token": sum(self.normedmeaninputhistory_token) / len(self.normedmeaninputhistory_token), "3inn_1_rawactivationslayernorm_norm_neuron": sum(self.normedmeaninputhistory_neuron) / len(self.normedmeaninputhistory_neuron), "3inn_2_combinedactivations_norm": sum(self.combhistory) / len(self.combhistory), "3inn_2_combinedactivations_norm_neuron": sum(self.combhistory_neuron) / len(self.combhistory_neuron), "3inn_2_combinedactivations_scale": sum(self.combiscalehistory) / len(self.combiscalehistory), "3inn_3_refinedactivations_norm": sum(self.refhistory) / len(self.refhistory), "3inn_3_refinedactivations_norm_neuron": sum(self.refhistory_neuron) / len(self.refhistory_neuron), "3inn_3_refinedactivations_scale": sum(self.logithistory) / len(self.logithistory), "3inn_4_combinedactivationsmeta_norm": sum(self.scaledhistory) / len(self.scaledhistory), "3inn_4_combinedactivationsmeta_norm_neuron": sum(self.scaledhistory_neuron) / len(self.scaledhistory_neuron), "3inn_x_finaloutlayernorm_norm": sum(self.combiouthistory) / len(self.combiouthistory), "3inn_x_finaloutlayernorm_norm_neuron": sum(self.combiouthistory_neuron) / len(self.combiouthistory_neuron), "_inn_windowsizesmean": torch.exp(self.logwindowsizes).mean().item() self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] return finalout def stackedwindowmeans(self, activations: torch.tensor, windowsizes: torch.tensor) -> torch.tensor: #  fully vectorized, loop-free window mean calculation. returns: (len(windowsizes), embeddim)  with self.inn_counsellor.infodump("stackedwindowmeans") as ʕっʘ‿ʘʔっ: self.neuronactivationspertoken = activations seqlen, embeddim = self.neuronactivationspertoken.shape ʕっʘ‿ʘʔっ("inn2: normedactivations") self.normedactivations = self.windowmeannorm(self.neuronactivationspertoken) padded = torch.zeros((windowmax, embeddim), device=self.device) padded[-min(seqlen, windowmax):] = self.normedactivations[-min(seqlen, windowmax):] stacked = padded.unsqueeze(0).repeat(windowsizes.shape[0], 1, 1) rangemask = torch.arange(windowmax, device=self.device).unsqueeze(0) # (1, maxw) floatwindowsizes = torch.exp(self.logwindowsizes) # still in float space intwindowsizes = torch.round(floatwindowsizes).clamp(min=1) #intwindowsizes = int(torch.exp(self.logwindowsizes)) # straight-through estimator: lets gradients flow through soft version windowtensor = (intwindowsizes - floatwindowsizes).detach() + floatwindowsizes windowtensor = windowtensor.unsqueeze(1) # (numwindows, 1) mask = (rangemask < windowtensor).float().unsqueeze(2) # (numwindows, maxw, 1) masked = stacked * mask sums = masked.sum(dim=1) # (numwindows, embeddim) means = sums / windowtensor return means # shape: (numwindows, embeddim) def inn_getstats(self): with self.inn_counsellor.infodump("inn_getstats") as ʕっʘ‿ʘʔっ: inn_cerebellum_str = "" if collectstats and n_collectstats: ʕっʘ‿ʘʔっ("torch.no_grad♥") with torch.no_grad(): if n_weightstats: ʕっʘ‿ʘʔっ("♥n_weightstats") self.stats["n_weightmean"] = self.neurons.n_weights.mean() self.stats["n_weightstd"] = self.neurons.n_weights.std() self.stats["n_weightmin"] = self.neurons.n_weights.min() self.stats["n_weightmax"] = self.neurons.n_weights.max() if debugprints: print(f"neuron weight mean: {self.stats["n_weightmean"]} std: {self.stats["n_weightstd"]} min: {self.stats["n_weightmin"]} max: {self.stats["n_weightmax"]}")  if n_weightnormstats: ʕっʘ‿ʘʔっ("♥n_weightnormstats") self.n_weightnorm = torch.norm(self.neurons.n_weights, dim = 1) self.stats["n_weightnormmean"] = self.n_weightnorm.mean() self.stats["n_weightnormmin"] = self.n_weightnorm.min() self.stats["n_weightnormmax"] = self.n_weightnorm.max() if debugprints: print(f"neuron weightnorm: {self.stats["n_weightnorm"]} mean: {self.stats["n_weightnormmean"]} min: {self.stats["n_weightnormmax"]} max: {self.stats["n_weightnormmin"]}") if n_biasesstats: ʕっʘ‿ʘʔっ("♥n_biasesstats") self.stats["n_biasesmean"] = self.neurons.n_biases.mean() self.stats["n_biasesstd"] = self.neurons.n_biases.std() self.stats["n_biasesmin"] = self.neurons.n_biases.min() self.stats["n_biasesmax"] = self.neurons.n_biases.max() if debugprints: print(f"neuron biases mean: {self.stats["n_biasesmean"]} std: {self.stats["n_biasesstd"]} min: {self.stats["n_biasesmin"]} max: {self.stats["n_biasesmax"]}") if n_sparsitystat: ʕっʘ‿ʘʔっ("♥getsparsitystat") self.stats["n_sparsity"] = (self.neurons.n_weights.abs() < 1e-5).float().mean() if debugprints: print(f"neuron sparsity: {self.stats["n_sparsity"]}") if collectstats and inn_collectstats: if inn_cerebellumstats: ʕっʘ‿ʘʔっ("♥getcerebellumstats") #this was windowweighting self.stats["inn_cerebellummean"] = self.cerebellum.mean().item() self.stats["inn_cerebellumstd"] = self.cerebellum.std().item() inn_cerebellumstats_fullvalues = zip(self.expwindowsizes, self.cerebellum, self.cerebellumsoft) for w, raw, soft in inn_cerebellumstats_fullvalues: self.stats[f"inn_cerebellum_w{int(w)}"] = raw.item() self.stats[f"inn_cerebellumsoft_w{int(w)}"] = soft.item() if debugprints: print(f"cerebellum: {self.cerebellum}, soft: {self.cerebellumsoft} mean: {self.stats['inn_cerebellummean']} std: {self.stats['inn_cerebellumstd']}") ʕっʘ‿ʘʔっ("♥cerebellumstring") inn_cerebellum_str = self.calligraphist.s_formatwindowbiastriplets(label="inn_cerebellum", rawtensor = self.cerebellum, softtensor = self.cerebellumsoft, windowsizes = self.expwindowsizes, per_window_style = true) if debugprints: print(f"{inn_cerebellum_str}") self.stats.update({f"{k}": v for k, v in self.neurons.getstats().items()}) return self.stats, inn_cerebellum_str interneuronnetwork = interneuron_network() testinputseq = torch.randn(window1, embeddimension) testinputembeds = testinputseq meanactivationstensor = interneuronnetwork.forward(testinputembeds) print("--- interneuron network testing start ---") print(f"parallel neuron layer created with {interneuronnetwork.numneurons} neurons.") print(f"inputs per neuron (embed dimension): {interneuronnetwork.embeddimension}") print(f"output activations (first 10):") print(meanactivationstensor[:10]) print(f"output activations shape: {meanactivationstensor.shape}") print("\n--- interneuron network testing completed ---") # output layer for logit prediction # brain/layers/logits.py # final layer, maps neuron activations to logits for each token in the vocab class logits(nn.module): def __init__(self, _counsellor, _device): self.lastsavedweights = 0 # for stats self.l_weights = nn.parameter(torch.randn(numneurons, vocabsize, device = self.device)) # this is set to move the neuron activations (10000) onto vocab size (2000) self.l_bias = nn.parameter(torch.zeros(vocabsize, device = self.device)) self.activationnorm = nn.layernorm(numneurons, device = self.device) self.rawactivationsscale = nn.parameter(torch.tensor(0.5)) self.normedactivationsscale = nn.parameter(torch.tensor(0.5)) self.logitnorm = nn.layernorm(vocabsize, device = self.device) self.outputscale = nn.parameter(torch.tensor(0.5)) self.normoutputscale = nn.parameter(torch.tensor(0.5)) self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] def forward(self, _meanactivationstensor): # <- = from # inn? -> l1 -> l2 -> l3 -> l4 -> l5 -> l6 -> * # imports the activations from interneuronnetwork, assuming that is is a tensor ʕっʘ‿ʘʔっ("l1: activationstensor") # <- inn? no? seems to come from babyllm? maybe through babyllm? self.activationstensor = _meanactivationstensor # _1 #self.testat = _meanactivationstensor ʕっʘ‿ʘʔっ("l2: normedactivationstensor") # <- l1 self.normedactivationstensor = self.activationnorm(self.activationstensor) # _2 ʕっʘ‿ʘʔっ("l3: scaledactivations") # <- l1 + l2 self.scaledactivations = (self.activationstensor * self.rawactivationsscale) + (self.normedactivationstensor * self.normedactivationsscale) # _3 if debugprints: print(f"debug logits: activations shape before @ weights: {self.scaledactivations.shape}") if debugprints: print(f"debug logits: weights shape: {self.l_weights.shape}") ʕっʘ‿ʘʔっ("l4: logitoutput") # <- l3 (with weights and bias) logitoutputnormalized = (self.scaledactivations @ self.l_weights) / (numneurons ** 0.5) + self.l_bias logitoutputoriginal = self.scaledactivations @ self.l_weights + self.l_bias self.logitoutput = (logitoutputoriginal + logitoutputnormalized)/2 # _4 ʕっʘ‿ʘʔっ("l5: logitnormed") # <- l4 self.logitnormed = self.logitnorm(self.logitoutput) # _5 ʕっʘ‿ʘʔっ("l6: finallogit") # <- l4 + l5 self.finallogit = (self.logitoutput * self.outputscale) + (self.logitnormed * self.normoutputscale) # _6 if debugprints: print(f"debug logits: logitoutput shape after @ weights: {self.logitoutput.shape}") ʕっʘ‿ʘʔっ("append rolling self.stats") self.tensorhist.append(self.activationstensor.norm().item()) self.normedhist.append(self.normedactivationstensor.norm().item()) self.activhist.append(self.scaledactivations.norm().item()) self.logithist.append(self.logitoutput.norm().item()) self.logitnormhist.append(self.logitnormed.norm().item()) self.finallogithist.append(self.finallogit.norm().item()) if len(self.tensorhist) >= windowmax: ʕっʘ‿ʘʔっ("clear rolling self.stats at end of window") self.stats = { "6l_0_activationstensor_norm": sum(self.tensorhist) / len(self.tensorhist), "6l_1_normedactivationstensor_norm": sum(self.normedhist) / len(self.normedhist), "6l_2_scaledactivations_norm": sum(self.activhist) / len(self.activhist), "6l_3_logitoutput_norm": sum(self.logithist) / len(self.logithist), "6l_4_logitnormed_norm": sum(self.logitnormhist) / len(self.logitnormhist), "6l_x_finallogit_norm": sum(self.finallogithist) / len(self.finallogithist), } self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] # return logits (not softmax) for better gradient computation in cross-entropy loss return self.finallogit # l6 -> def getlogitstats(self): with self.counsellor.infodump("getlogitstats") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("weightnormstats") weightnorms = torch.norm(self.l_weights, dim = 0) self.stats["logitweightnormmean"] = weightnorms.mean() self.stats["logitweightnormstd"] = weightnorms.std() self.stats["logitweightnormmax"] = weightnorms.max() # scales (dont need on per token history as only updated in backward) self.stats["6l_0_activationstensor_scale"] = self.rawactivationsscale.norm().item() self.stats["6l_1_normedactivationstensor_scale"] = self.normedactivationsscale.norm().item() self.stats["6l_3_logitoutput_scale"] = self.outputscale.norm().item() self.stats["6l_4_logitnormed_scale"] = self.normoutputscale.norm().item() ʕっʘ‿ʘʔっ("sparsitystat") sparsity = (self.l_weights.abs() < 1e-5).float().mean() self.stats["logitweightsparsity"] = sparsity ʕっʘ‿ʘʔっ("weightdriftstat") drift = torch.norm(self.l_weights - self.lastsavedweights) self.stats["logitweightdrift"] = drift self.lastsavedweights = self.l_weights.clone().detach() ʕっʘ‿ʘʔっ("biasstats") self.stats["logitbiasmean"] = self.l_bias.mean() self.stats["logitbiasstd"] = self.l_bias.std() self.stats["logitbiasmax"] = self.l_bias.max() if hasattr(self, 'latestactivations'): ʕっʘ‿ʘʔっ("activationstats") act = self.latestactivations self.stats["activationstd"] = act.std() self.stats["activationmean"] = act.mean() self.stats["activationmax"] = act.max() self.stats["activationmin"] = act.min() self.stats["activationsparsity"] = (act.abs() < 1e-6).float().mean() return self.stats testlayeractivations = torch.randn(numneurons) logits = logits(numneurons = numneurons, vocabsize = vocabsize) logitoutput = logits.forward(testlayeractivations) print("--- logits testing start ---") print(f"output layer created with {logits.vocabsize} vocabulary tokens.") print(f"weight matrix shape: {logits.weights.shape}") print(f"bias vector shape: {logits.bias.shape}") print(f"logits (first 100):") print(logitoutput[:10]) print(f"logits shape: {logitoutput.shape}") print("--- logits testing complete ---") # memory layer // brain/layers/memory.py # this makes a rolling buffer of past activations class memory(nn.module): # learnable decay rates and gates self.shorttermdecay = nn.parameter(torch.tensor(0.7, device = self.device)) self.longtermdecay = nn.parameter(torch.tensor(0.95, device = self.device)) self.shortgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.longgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.currentgate = nn.parameter(torch.tensor(0.5, device = self.device)) # buffers to store memory (outside gradient) self.register_buffer("shorttermmemory", torch.zeros(1, numneurons)) self.register_buffer("longtermmemory", torch.zeros(1, numneurons)) # stats self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] def forward(self, _activationstensor): self.activationstensor = _activationstensor ʕっʘ‿ʘʔっ("shorttermdecay") shortdecay = torch.sigmoid(self.shorttermdecay) ʕっʘ‿ʘʔっ("longtermdecay") longdecay = torch.sigmoid(self.longtermdecay) ʕっʘ‿ʘʔっ("newshorttermmemory") newshort = (shortdecay * self.shorttermmemory) + ((1 - shortdecay) * self.activationstensor) ʕっʘ‿ʘʔっ("newlongtermmemory") newlong = (longdecay * self.longtermmemory) + ((1 - longdecay) * self.activationstensor) ʕっʘ‿ʘʔっ("clamp memory gates") clampedshort = torch.clamp(self.shortgate, min=1e-3) clampedlong = torch.clamp(self.longgate, min=1e-3) clampedactivations = torch.clamp(self.currentgate, min=1e-3) ʕっʘ‿ʘʔっ("get gatesum") gatesum = clampedshort + clampedlong + clampedactivations + 1e-9 shortgatescale = clampedshort / gatesum longgatescale = clampedlong / gatesum activationsgatescale = clampedactivations / gatesum self.latestmemorygates = torch.stack([shortgatescale, longgatescale, activationsgatescale]) # needed to be used in babyllm for processing self.finalmemory = ((shortgatescale * newshort) + (longgatescale * newlong) +(activationsgatescale * self.activationstensor)) self.shortgatescalehistory.append(shortgatescale.item()) self.longgatescalehistory.append(longgatescale.item()) self.activationsgatescalehistory.append(activationsgatescale.item()) self.rawactivationshistory.append(self.activationstensor.norm().item()) self.shorttermmemoryhistory.append(self.shorttermmemory.norm().item()) self.longtermmemoryhistory.append(self.longtermmemory.norm().item()) self.finalmemoryhistory.append(self.finalmemory.norm().item()) if len(self.shortgatescalehistory) >= windowmax: "4m_0_rawactivations_norm": sum(self.rawactivationshistory) / len(self.rawactivationshistory), "4m_1_shorttermmemory_norm": sum(self.shorttermmemoryhistory) / len(self.shorttermmemoryhistory), "4m_1_longtermmemory_norm": sum(self.longtermmemoryhistory) / len(self.longtermmemoryhistory), "_4m_shortgatescale": sum(self.shortgatescalehistory) / len(self.shortgatescalehistory), "_4m_longgatescale": sum(self.longgatescalehistory) / len(self.longgatescalehistory), "_4m_activationsgatescale": sum(self.activationsgatescalehistory) / len(self.activationsgatescalehistory), "4m_x_finalmemory_norm": sum(self.finalmemoryhistory) / len(self.finalmemoryhistory), "4m_shortdecay": torch.sigmoid(self.shorttermdecay), "4m_longdecay": torch.sigmoid(self.longtermdecay), self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] # store computed memories for after backward self.newshort = newshort self.newlong = newlong return self.finalmemory def updatememorybuffers(self): with self.counsellor.infodump("updatememorybuffers") as ʕっʘ‿ʘʔっ: self.shorttermmemory.copy_(self.newshort.detach()) self.longtermmemory.copy_(self.newlong.detach()) def resetmemory(self): with self.counsellor.infodump("resetmemory") as ʕっʘ‿ʘʔっ: self.shorttermmemory.zero_() self.longtermmemory.zero_() def getmemorystats(self): return self.stats memory = memory(numneurons = numneurons) print("--- memory testing started ---") print("\n--- memory testing complete ---") { "version": "1.0", "truncation": null, "padding": null, "added_tokens": [ { "id": 0, "content": "<unk>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true } ], "normalizer": null, "pre_tokenizer": { "type": "bytelevel", "add_prefix_space": true, "trim_offsets": true, "use_regex": true }, "post_processor": null, "decoder": null, "model": { "type": "bpe", "dropout": null, "unk_token": "<unk>", "continuing_subword_prefix": null, "end_of_word_suffix": null, "fuse_unk": false, "byte_fallback": false, "ignore_merges": false, "vocab": { "<unk>": 0, "!": 1, "\"": 2, "%": 3, "&": 4, "'": 5, "(": 6, ")": 7, "*": 8, "+": 9, ",": 10, "-": 11, ".": 12, "/": 13, "0": 14, "1": 15, "2": 16, "3": 17, "4": 18, "5": 19, "6": 20, "7": 21, "8": 22, "9": 23, ":": 24, ";": 25, "<": 26, "=": 27, ">": 28, "?": 29, "@": 30, "^": 31, "_": 32, "a": 33, "b": 34, "c": 35, "d": 36, "e": 37, "f": 38, "g": 39, "h": 40, "i": 41, "j": 42, "k": 43, "l": 44, "m": 45, "n": 46, "o": 47, "p": 48, "q": 49, "r": 50, "s": 51, "t": 52, "u": 53, "v": 54, "w": 55, "x": 56, "y": 57, "z": 58, "|": 59, "~": 60, "£": 61, "#": 62, "[": 63, "]": 64, "$": 65, "ġ": 66, "`": 67, "ġidk": 68, "ġpls": 69, "ġirl": 70, "hood": 71, "ġcos": 72, "ġi": 73, "ġt": 74, "ġa": 75, "ġw": 76, "ġs": 77, "ġth": 78, "in": 79, "ġm": 80, "ou": 81, "ġb": 82, "re": 83, "ġh": 84, "ġo": 85, "ġd": 86, "ing": 87, "at": 88, "ġl": 89, "ġc": 90, "ġf": 91, "er": 92, "ġy": 93, "ġthe": 94, "nd": 95, "ġto": 96, "ġn": 97, "ġp": 98, "an": 99, "on": 100, "ll": 101, "en": 102, "or": 103, "ġyou": 104, "es": 105, "ġg": 106, "ġit": 107, "ed": 108, "ġwh": 109, "us": 110, "ġand": 111, "it": 112, "is": 113, "ġno": 114, "as": 115, "ġu": 116, "ke": 117, "ġme": 118, "ġe": 119, "ġbe": 120, "ve": 121, "ay": 122, "ic": 123, "ut": 124, "ġha": 125, "om": 126, "ġthat": 127, "ġdo": 128, "le": 129, "ar": 130, "ġin": 131, "ġis": 132, "et": 133, "ly": 134, "ow": 135, "ġwe": 136, "oo": 137, "ġon": 138, "ġmy": 139, "ġj": 140, "ġre": 141, "ġli": 142, "ġbut": 143, "ġof": 144, "id": 145, "al": 146, "gh": 147, "ġst": 148, "ġfor": 149, "all": 150, "ġhe": 151, "ġsh": 152, "ġwas": 153, "ġim": 154, "ġnot": 155, "ust": 156, "ġwhat": 157, "ro": 158, "ad": 159, "ck": 160, "ġk": 161, "ġan": 162, "ver": 163, "ġjust": 164, "ġlike": 165, "ġso": 166, "ld": 167, "out": 168, "am": 169, "ally": 170, "ant": 171, "ee": 172, "ch": 173, "ot": 174, "ct": 175, "ent": 176, "ma": 177, "ri": 178, "ith": 179, "se": 180, "ġ:": 181, "use": 182, "ġwhy": 183, "ir": 184, "ght": 185, "ġhave": 186, "ġdon": 187, "ea": 188, "st": 189, "im": 190, "ġne": 191, "ġat": 192, "ġwith": 193, "ġthis": 194, "mao": 195, "ġch": 196, "ry": 197, "ce": 198, "ġlmao": 199, "ill": 200, "nt": 201, "ion": 202, "op": 203, "ġas": 204, "ate": 205, "ome": 206, "ġits": 207, "ould": 208, "ġke": 209, "ġal": 210, "th": 211, "ġare": 212, "ause": 213, "un": 214, "ġor": 215, "ġwor": 216, "'s": 217, "king": 218, "ġif": 219, "..": 220, "'t": 221, "ra": 222, "pp": 223, "ġthey": 224, "ġsu": 225, "ġab": 226, "ġcan": 227, "ore": 228, "ist": 229, "ġlo": 230, "ġget": 231, "ġgo": 232, "vin": 233, "ġkevin": 234, "ab": 235, "ġkn": 236, "ġfu": 237, "ġma": 238, "ink": 239, "hh": 240, "ge": 241, "ġwant": 242, "ġup": 243, "ġknow": 244, "ġdid": 245, "ood": 246, "ess": 247, "ġdont": 248, "ġabout": 249, "ġall": 250, "ġohh": 251, "'m": 252, "eah": 253, "ġpl": 254, "ġnow": 255, "ġyeah": 256, "ġpro": 257, "ġr": 258, "ġfuck": 259, "ġhow": 260, "ġex": 261, "ter": 262, "ġam": 263, "ven": 264, "ġx": 265, "ġthink": 266, "ep": 267, "na": 268, "ind": 269, "ġok": 270, "ġse": 271, "um": 272, "ġany": 273, "ur": 274, "ġyour": 275, "ġlove": 276, "ġout": 277, "ġtoo": 278, "fe": 279, "ġv": 280, "ġone": 281, "ġhate": 282, "no": 283, "ġshe": 284, "very": 285, "ġbec": 286, "ġwhen": 287, "ġ'": 288, "est": 289, "our": 290, "'re": 291, "ha": 292, "ight": 293, "ġlist": 294, "eel": 295, "ġreally": 296, "ġsp": 297, "ġneed": 298, "ġom": 299, "il": 300, "ame": 301, "ġfeel": 302, "ġfr": 303, "ġde": 304, "ġeven": 305, "one": 306, "od": 307, "ġfa": 308, "res": 309, "ġwill": 310, "ġloo": 311, "ġsome": 312, "ġ-": 313, "ġsor": 314, "so": 315, "ġsa": 316, "ġmus": 317, "ġthats": 318, "ġlisten": 319, "ġbecause": 320, "ul": 321, "ġyes": 322, "ġmean": 323, "ġle": 324, "ound": 325, "ġcom": 326, "ks": 327, "ite": 328, "ie": 329, "ġthan": 330, "ġmusic": 331, "ġcon": 332, "ine": 333, "ġwere": 334, "ġsay": 335, "ice": 336, "ġ1": 337, "gg": 338, "ġtim": 339, "ġcause": 340, "ġgood": 341, "ġxd": 342, "ġaw": 343, "ġsee": 344, "ġalso": 345, "ers": 346, "ġur": 347, "ġomg": 348, "are": 349, "ġmore": 350, "ġsorry": 351, "ain": 352, "ġthen": 353, "ast": 354, "ġte": 355, "ive": 356, "ġpe": 357, "lf": 358, "ġthing": 359, "qu": 360, "art": 361, "ġbeen": 362, "ġhaha": 363, "ġher": 364, "if": 365, "end": 366, "ack": 367, "ere": 368, "ong": 369, "ġgot": 370, "ġthem": 371, "ġchar": 372, "ġdoes": 373, "ġtho": 374, "ġwho": 375, "ġag": 376, "ġtb": 377, "ġwell": 378, "ġdun": 379, "ġwa": 380, "ough": 381, "ġwould": 382, "ġen": 383, "hing": 384, "ġfrom": 385, "ġtime": 386, "ġpeop": 387, "ġ(": 388, "ġhad": 389, "ġdunno": 390, "ġpo": 391, "!!": 392, "ġact": 393, "ation": 394, "ġprob": 395, "ġpeople": 396, "ġus": 397, "ġthere": 398, "ġlistening": 399, "ud": 400, "ġtr": 401, "ġun": 402, "ġtal": 403, "ġ2": 404, "ġwan": 405, "ick": 406, "ġmu": 407, "ting": 408, "ually": 409, "ġ:)": 410, "ġonly": 411, "ġevery": 412, "ġcharis": 413, "ġmake": 414, "ous": 415, "ġmuch": 416, "ġint": 417, "ġsc": 418, "ig": 419, "ase": 420, "ġbeing": 421, "pe": 422, "...": 423, "ġplay": 424, "ġhim": 425, "ġwork": 426, "ġ:(": 427, "ff": 428, "ġcould": 429, "ue": 430, "ving": 431, "and": 432, "ġwanna": 433, "ġshould": 434, "ġmo": 435, "right": 436, "ort": 437, "ool": 438, "omet": 439, "pt": 440, "ġbo": 441, "ġgu": 442, "per": 443, "bs": 444, "lo": 445, "ġsomet": 446, "lp": 447, "??": 448, "way": 449, "ably": 450, "ġar": 451, "me": 452, "be": 453, "ag": 454, "ġday": 455, "ġmay": 456, "ard": 457, "ġel": 458, "ġover": 459, "ġco": 460, "ġby": 461, "ġhelp": 462, "ġtbh": 463, "ġ<": 464, "ġcant": 465, "ress": 466, "ure": 467, "ġshit": 468, "ġta": 469, "ġlit": 470, "ġbad": 471, "ġprobably": 472, "ġad": 473, "ġtry": 474, "'ve": 475, "ġstill": 476, "wn": 477, "ish": 478, "ġbit": 479, "ġkind": 480, "iss": 481, "'d": 482, "leep": 483, "ide": 484, "ġhapp": 485, "ġgoing": 486, "riend": 487, "ream": 488, "cc": 489, "ġsame": 490, "ty": 491, "thing": 492, "ġactually": 493, "day": 494, "'ll": 495, "ġoff": 496, "ġstu": 497, "ġman": 498, "ġwait": 499, "nder": 500, "ġapp": 501, "ġdis": 502, "self": 503, "ġback": 504, "ġhas": 505, "ear": 506, "ther": 507, "ġqu": 508, "ġple": 509, "igh": 510, "ġcome": 511, "ġsaid": 512, "ġjo": 513, "ich": 514, "ġid": 515, "her": 516, "ment": 517, "ġsound": 518, "ġway": 519, "ġsleep": 520, "ġplease": 521, "ass": 522, "ġpre": 523, "ber": 524, "kes": 525, "ought": 526, "ġgon": 527, "ġ*": 528, "ġhere": 529, "mm": 530, "ġdoing": 531, "ġsomething": 532, "ġright": 533, "ġtalk": 534, "int": 535, "ci": 536, "ġcool": 537, "ġnever": 538, "ġbet": 539, "ble": 540, "ire": 541, "ġge": 542, "ġlife": 543, "ġlook": 544, "ġ3": 545, "ġgonna": 546, "ġcall": 547, "pl": 548, "ġsm": 549, "ys": 550, "ġnice": 551, "ġwhich": 552, "ġother": 553, "act": 554, "ġhop": 555, "age": 556, "ġgeor": 557, "ġfriend": 558, "ang": 559, "ġmad": 560, "ġlooking": 561, "ġvery": 562, "ġra": 563, "ġagain": 564, "fore": 565, "00": 566, "ġthings": 567, "bsite": 568, "row": 569, "ġmight": 570, "ġtw": 571, "ġaf": 572, "os": 573, "ġlet": 574, "ġwebsite": 575, "ance": 576, "ġlot": 577, "able": 578, "ies": 579, "ġro": 580, "ġfair": 581, "ġthought": 582, "ġtell": 583, "ġph": 584, "ġmaybe": 585, "ġcut": 586, "ġunder": 587, "ġinto": 588, "ree": 589, "ip": 590, "ġdidnt": 591, "ird": 592, "ġhar": 593, "ġelod": 594, "ġfe": 595, "ġstream": 596, "ġbl": 597, "ġword": 598, "ġstuff": 599, "ġbab": 600, "ġbefore": 601, "ġwr": 602, "ġgeorge": 603, "ġfucking": 604, "ug": 605, "ms": 606, "ġwhere": 607, "ġsure": 608, "reat": 609, "ity": 610, "ġhope": 611, "ġcl": 612, "te": 613, "ġsupp": 614, "ġna": 615, "ġafter": 616, "ise": 617, "ious": 618, "ġill": 619, "ġpers": 620, "aa": 621, "ss": 622, "ġbot": 623, "ġweird": 624, "ġreal": 625, "ult": 626, "ater": 627, "ġyear": 628, "ying": 629, "ont": 630, "ġpart": 631, "ġbro": 632, "ways": 633, "get": 634, "ġmin": 635, "ġhis": 636, "ġcomp": 637, "read": 638, "ġmum": 639, "ġmed": 640, "ol": 641, "ġperson": 642, "ġever": 643, "ġelodie": 644, "ouse": 645, "ġem": 646, "ġalways": 647, "ġliter": 648, "ġdown": 649, "ġkinda": 650, "ġdr": 651, "ġsigh": 652, "ġcat": 653, "ġanything": 654, "ġhour": 655, "ġboo": 656, "ġlong": 657, "ġstart": 658, "ġask": 659, "ġfun": 660, "ġtrue": 661, "ġbetter": 662, "ġsounds": 663, "ically": 664, "ġpet": 665, "ġtra": 666, "ġmyself": 667, "ġhouse": 668, "ace": 669, "ens": 670, "ord": 671, "ġper": 672, "ġfind": 673, "ġtake": 674, "ġ4": 675, "ġfro": 676, "ġche": 677, "ġtheir": 678, "ġliterally": 679, "ġsex": 680, "ġtrying": 681, "ġaww": 682, "mber": 683, "xt": 684, "ġthanks": 685, "ions": 686, "ġlea": 687, "!?": 688, "ġfl": 689, "ning": 690, "ġtoday": 691, "sh": 692, "ġdif": 693, "ġson": 694, "ġuse": 695, "we": 696, "ġpret": 697, "ġgetting": 698, "atch": 699, "ġacc": 700, "ġmade": 701, "ġhard": 702, "ġpr": 703, "ġyay": 704, "ġwee": 705, "eep": 706, "ated": 707, "irst": 708, "ġstop": 709, "ile": 710, "ġbu": 711, "stand": 712, "ġcute": 713, "ody": 714, "ġreme": 715, "ġfrogg": 716, "ġmon": 717, "ġ5": 718, "ict": 719, "ġfirst": 720, "ġnew": 721, "ating": 722, "ġmess": 723, "ġbrb": 724, "ġnight": 725, "les": 726, "co": 727, "ġread": 728, "ach": 729, "rent": 730, "vent": 731, "ġcont": 732, "ft": 733, "ried": 734, "ġthough": 735, "ġgod": 736, "ason": 737, "ak": 738, "ġdad": 739, "ġhaving": 740, "ġput": 741, "ans": 742, "ġunderstand": 743, "ġremember": 744, "ġlater": 745, "orm": 746, "€": 747, "ġplaying": 748, "ġend": 749, "ful": 750, "ġmakes": 751, "ual": 752, "ġsad": 753, "tle": 754, "ange": 755, "ġoo": 756, "ġmiss": 757, "ġboth": 758, "ġbr": 759, "ġfine": 760, "ġla": 761, "ġdidn": 762, "ġfood": 763, "ġguess": 764, "ġthank": 765, "ġnah": 766, "ġang": 767, "ġleg": 768, "ġpretty": 769, "ġreason": 770, "ġshow": 771, "ġhappen": 772, "ġfeeling": 773, "ġdisc": 774, "ġhome": 775, "ġwont": 776, "ġenough": 777, "ġser": 778, "ġspea": 779, "ġgir": 780, "mor": 781, "ġkeep": 782, "li": 783, "ġleast": 784, "ġbaby": 785, "ġsaying": 786, "ġpete": 787, "ġanno": 788, "ġconf": 789, "ġphone": 790, "ġanyway": 791, "ġtalking": 792, "ġca": 793, "ġour": 794, "ġhappy": 795, "ude": 796, "ġbest": 797, "ġdone": 798, "ġgive": 799, "ġhor": 800, "ence": 801, "ta": 802, "ġfroggy": 803, "ġimp": 804, "ġdec": 805, "ġbed": 806, "anc": 807, "ġive": 808, "ġsl": 809, "ġmeans": 810, "ġshop": 811, "ġaut": 812, "ġlast": 813, "ġinst": 814, "ward": 815, "inking": 816, "ġtold": 817, "ready": 818, "fect": 819, "ved": 820, "ġcare": 821, "ġeng": 822, "ix": 823, "dd": 824, "unch": 825, "ġalready": 826, "ġwatch": 827, "ġet": 828, "ġold": 829, "ġace": 830, "ġpoint": 831, "ġwanted": 832, "ġgen": 833, "ne": 834, "ġbas": 835, "ġass": 836, "ġmine": 837, "iz": 838, "ġhes": 839, "ġcr": 840, "!!!": 841, "ves": 842, "ġthese": 843, "ġadd": 844, "ġhours": 845, "ġdoesnt": 846, "wh": 847, "ġfin": 848, "ġmost": 849, "ither": 850, "arent": 851, "ary": 852, "ġkevins": 853, "ġplan": 854, "ġcle": 855, "ġhon": 856, "ps": 857, "ġbre": 858, "bo": 859, "arly": 860, "ared": 861, "ġthr": 862, "po": 863, "ġused": 864, "ib": 865, "ġsit": 866, "ġsend": 867, "???": 868, "ġeverything": 869, "ġje": 870, "ġnothing": 871, "ount": 872, "ġop": 873, "ild": 874, "ġexact": 875, "less": 876, "ġfre": 877, "ġeat": 878, "red": 879, "oney": 880, "ġsuper": 881, "ġlittle": 882, "ġatt": 883, "ġmaking": 884, "ts": 885, "ġaaa": 886, "ġeveryone": 887, "ġinc": 888, "ġface": 889, "ġboi": 890, "ġres": 891, "ple": 892, "ġwrong": 893, "ġdes": 894, "pect": 895, "ġhonest": 896, "ġthrough": 897, "ġtomor": 898, "ġdick": 899, "ġgeep": 900, "ġhu": 901, "ead": 902, "ġmonth": 903, "ġfriends": 904, "ġdiffe": 905, "ath": 906, "ġidea": 907, "ġleave": 908, "ġgl": 909, "ġspe": 910, "ġless": 911, "ġtomorrow": 912, "ġthinking": 913, "ġcry": 914, "ġstr": 915, "ored": 916, "ġdef": 917, "ġgeepy": 918, "ġdam": 919, "ġhey": 920, "ign": 921, "ġanx": 922, "ġown": 923, "ġment": 924, "ġgr": 925, "ġhead": 926, "ġsch": 927, "ġ10": 928, "ġmeant": 929, "ġname": 930, "ense": 931, "ġaround": 932, "ġgreat": 933, "sw": 934, "ġelse": 935, "ible": 936, "ġlunch": 937, "ġkid": 938, "ġshes": 939, "ġokay": 940, "urn": 941, "ġexpl": 942, "ook": 943, "ġseems": 944, "ġgirl": 945, "body": 946, "ġsupport": 947, "ġtbf": 948, "ġfo": 949, "ġansw": 950, "ġaway": 951, "ġbig": 952, "ġtwo": 953, "mo": 954, "cept": 955, "ġexactly": 956, "ġumm": 957, "ġcons": 958, "erest": 959, "ġhi": 960, "ctor": 961, "lly": 962, "ġrel": 963, "ġins": 964, "ġinter": 965, "ġlegit": 966, "ġworry": 967, "gn": 968, "ġinterest": 969, "ġmoney": 970, "ised": 971, "ġty": 972, "uck": 973, "ġhmm": 974, "ġjob": 975, "ġroom": 976, "ġcra": 977, "ġ6": 978, "ired": 979, "ġengl": 980, "el": 981, "ġawes": 982, "ġnext": 983, "iet": 984, "ġscared": 985, "ġho": 986, "umb": 987, "oke": 988, "ġind": 989, "ġac": 990, "ġlive": 991, "ġcomm": 992, "ner": 993, "ġsent": 994, "ġdep": 995, "lt": 996, "ġkiss": 997, "air": 998, "ġspeak": 999, "ġob": 1000, "ġpict": 1001, "ġleft": 1002, "ġsong": 1003, "ġdoesn": 1004, "rible": 1005, "ġpay": 1006, "iously": 1007, "ġhug": 1008, "ġdays": 1009, "ġtimes": 1010, "ġwent": 1011, "ġthose": 1012, "ġawesome": 1013, "ġsw": 1014, "ġplace": 1015, "ġgi": 1016, "ph": 1017, "ily": 1018, "ġwow": 1019, "ny": 1020, "ġeither": 1021, "ġlooks": 1022, "ġwithout": 1023, "ġsometim": 1024, "ġfound": 1025, "ġstress": 1026, "ġuni": 1027, "ġannoying": 1028, "ġwish": 1029, "cks": 1030, "ġable": 1031, "ġrec": 1032, "ġhur": 1033, "ical": 1034, "ġfree": 1035, "ġdifferent": 1036, "ġbored": 1037, "ġyears": 1038, "med": 1039, "ġwal": 1040, "ġgotta": 1041, "fg": 1042, "used": 1043, "ten": 1044, "other": 1045, "ġweek": 1046, "ndom": 1047, "ġset": 1048, "ġmany": 1049, "ġproble": 1050, "ġexc": 1051, "ġwhole": 1052, "ġbra": 1053, "kr": 1054, "ġmood": 1055, "ġforget": 1056, "ġdoctor": 1057, "ġsk": 1058, "ġhigh": 1059, "ġproper": 1060, "sed": 1061, "ġdj": 1062, "ġworking": 1063, "ġsim": 1064, "ġcoming": 1065, "gr": 1066, "ġangle": 1067, "ġvide": 1068, "ġhell": 1069, "ġ:/": 1070, "ġhot": 1071, "ġmain": 1072, "ash": 1073, "ġstay": 1074, "ġsometimes": 1075, "ġrep": 1076, "ġsuch": 1077, "ġanswer": 1078, "ted": 1079, "ġsy": 1080, "ked": 1081, "ġcre": 1082, "oud": 1083, "ġcur": 1084, "ism": 1085, "ac": 1086, "ub": 1087, "ese": 1088, "ġkill": 1089, "ġiss": 1090, "ġikr": 1091, "ġooh": 1092, "ġbel": 1093, "ġschool": 1094, "ġtech": 1095, "ġmove": 1096, "ġhorrible": 1097, "ues": 1098, "ġonce": 1099, "ġetc": 1100, "gether": 1101, "ered": 1102, "ġbreak": 1103, "ht": 1104, "ġyet": 1105, "udd": 1106, "ġweed": 1107, "line": 1108, "ġpain": 1109, "ident": 1110, "ġhavent": 1111, "ġclo": 1112, "ġtogether": 1113, "ġhonestly": 1114, "ġgay": 1115, "ics": 1116, "ġent": 1117, "ġ9": 1118, "ġmind": 1119, "ġinstead": 1120, "ġposs": 1121, "ping": 1122, "ġboy": 1123, "ġguy": 1124, "ġmins": 1125, "ġexplain": 1126, "itch": 1127, "ġwhats": 1128, "mit": 1129, "ġ8": 1130, "ġguys": 1131, "ction": 1132, "der": 1133, "ġeas": 1134, "als": 1135, "ġbus": 1136, "ġmar": 1137, "ġpast": 1138, "ġunt": 1139, "ġdiscord": 1140, "ġpar": 1141, "iv": 1142, "ġrandom": 1143, "ġcalled": 1144, "ġnon": 1145, "ġ20": 1146, "ġmeet": 1147, "ġgame": 1148, "ġbring": 1149, "ġisnt": 1150, "uu": 1151, "ġawk": 1152, "log": 1153, "cial": 1154, "ġtwice": 1155, "ġperfect": 1156, "ġcheck": 1157, "ġmeds": 1158, "yy": 1159, "ġew": 1160, "ġbasically": 1161, "fu": 1162, "ġ7": 1163, "wa": 1164, "ġstud": 1165, "ġrly": 1166, "ġinf": 1167, "bi": 1168, "ġtheres": 1169, "ġneeds": 1170, "ġopen": 1171, "ġrude": 1172, "ames": 1173, "ġawkward": 1174, "ġwants": 1175, "ġear": 1176, "ġhear": 1177, "ġpost": 1178, "ġchat": 1179, "ġself": 1180, "ling": 1181, "ġsense": 1182, "set": 1183, "ġhand": 1184, "cond": 1185, "ġsaw": 1186, "ġcame": 1187, "ġasked": 1188, "jo": 1189, "ory": 1190, "alth": 1191, "ġwom": 1192, "vers": 1193, "ġbi": 1194, "ġmom": 1195, "led": 1196, "ken": 1197, "ġima": 1198, "ġcol": 1199, "ract": 1200, "ġexp": 1201, "ied": 1202, "tw": 1203, "ġanother": 1204, "vel": 1205, "10": 1206, "night": 1207, "ġvideo": 1208, "ġ;": 1209, "ġuntil": 1210, "ġes": 1211, "ġsin": 1212, "ġhalf": 1213, "ġemot": 1214, "ox": 1215, "ġhum": 1216, "ġquest": 1217, "ġbor": 1218, "ġsecond": 1219, "ġeach": 1220, "ect": 1221, "ific": 1222, "ons": 1223, "yes": 1224, "llow": 1225, "ness": 1226, "ġmust": 1227, "ġcar": 1228, "100": 1229, "ġcour": 1230, "ġago": 1231, "ġallow": 1232, "ġrn": 1233, "ġwaht": 1234, "ggest": 1235, "ush": 1236, "ross": 1237, "ġwasnt": 1238, "ap": 1239, "ġcommun": 1240, "ġseem": 1241, "ġdumb": 1242, "ġenglish": 1243, "ġmoment": 1244, "ġalone": 1245, "ġexper": 1246, "lete": 1247, "ġglad": 1248, "ġpan": 1249, "ġbelie": 1250, "ġsick": 1251, "ġ\"": 1252, "ġdance": 1253, "uring": 1254, "ġquite": 1255, "not": 1256, "uss": 1257, "ġreg": 1258, "ġcomput": 1259, "ġcudd": 1260, "du": 1261, "iting": 1262, "ġhealth": 1263, "ġsince": 1264, "ġpicture": 1265, "ġtired": 1266, "ġnorm": 1267, "ġalright": 1268, "ġmessage": 1269, "ġpur": 1270, "ġappre": 1271, "em": 1272, "ġworried": 1273, "ġusing": 1274, "ġhair": 1275, "iety": 1276, "ute": 1277, "ġext": 1278, "ġmonths": 1279, "ġmor": 1280, "ġlets": 1281, "ġwar": 1282, "ates": 1283, "ġtrain": 1284, "ġ;)": 1285, "ġapparent": 1286, "ġsur": 1287, "ġwasn": 1288, "ġenjo": 1289, "ations": 1290, "ġinv": 1291, "ġworse": 1292, "ġones": 1293, "ier": 1294, "ape": 1295, "ia": 1296, "ġrest": 1297, "zing": 1298, "ġfig": 1299, "istic": 1300, "ġbtw": 1301, "ġturn": 1302, "ġmeh": 1303, "ġpoo": 1304, "ġnobody": 1305, "ġobv": 1306, "ġapparently": 1307, "ġappreci": 1308, "ġlate": 1309, "ġdanc": 1310, "ather": 1311, "ġfelt": 1312, "ġhappened": 1313, "ġcolour": 1314, "ġwo": 1315, "ġmental": 1316, "terday": 1317, "ġbrain": 1318, "ġisn": 1319, "ġyesterday": 1320, "ġtri": 1321, "ited": 1322, "ġanxiety": 1323, "ering": 1324, "enc": 1325, "ġomfg": 1326, "ġstarted": 1327, "ġanyone": 1328, "ġ,": 1329, "ġswe": 1330, "ġcor": 1331, "ġsays": 1332, "rate": 1333, "ġ>": 1334, "ġasking": 1335, "ġart": 1336, "ġfew": 1337, "ugs": 1338, "ġcoll": 1339, "ġtop": 1340, "ġxx": 1341, "ake": 1342, "ugg": 1343, "rop": 1344, "ġseen": 1345, "fic": 1346, "ġfren": 1347, "ġdamn": 1348, "side": 1349, "ġnd": 1350, "ġdoor": 1351, "ġyourself": 1352, "ilst": 1353, "ġproblem": 1354, "ġgoo": 1355, "using": 1356, "ġsuggest": 1357, "ġfucks": 1358, "ġfeels": 1359, "ġsake": 1360, "ew": 1361, "ġcourse": 1362, "ġimport": 1363, "ġdancing": 1364, "oon": 1365, "vour": 1366, "ġed": 1367, "ave": 1368, "ġinteresting": 1369, "oice": 1370, "lling": 1371, "ġwhilst": 1372, "ġlight": 1373, "eee": 1374, "ġupset": 1375, "ship": 1376, "ġclean": 1377, "ġconsid": 1378, "ġcomputer": 1379, "ġstra": 1380, "ġjokes": 1381, "ung": 1382, "ġclass": 1383, "ren": 1384, "ġsitu": 1385, "ġten": 1386, "ġtried": 1387, "ġama": 1388, "ġplayed": 1389, "ġorder": 1390, "ġter": 1391, "ġdog": 1392, "ġmainly": 1393, "ġexpect": 1394, "he": 1395, "ġfar": 1396, "ġban": 1397, "ġfam": 1398, "ġspam": 1399, "ert": 1400, "ject": 1401, "ġyoure": 1402, "ġlear": 1403, "up": 1404, "ġdefin": 1405, "ġbelieve": 1406, "ġgames": 1407, "ġcheese": 1408, "ġ:'": 1409, "oc": 1410, "ġhurt": 1411, "ġpres": 1412, "ġplz": 1413, "most": 1414, "ġfigure": 1415, "ġmid": 1416, "ġgener": 1417, "its": 1418, "ional": 1419, "ose": 1420, "wu": 1421, "ġpa": 1422, "cri": 1423, "ġluck": 1424, "ġcurrent": 1425, "pm": 1426, "ġri": 1427, "ġimportant": 1428, "ġdist": 1429, "ġve": 1430, "ġbutt": 1431, "ġrun": 1432, "?!": 1433, "osed": 1434, "ii": 1435, "ming": 1436, "ġspace": 1437, "ġspec": 1438, "ret": 1439, "ures": 1440, "ode": 1441, "ġparent": 1442, "lect": 1443, "ġ12": 1444, "ġmessag": 1445, "ġ:'(": 1446, "ġfull": 1447, "ġvis": 1448, "ġagree": 1449, "ġfrench": 1450, "ġwhile": 1451, "unk": 1452, "ġcommunic": 1453, "ugh": 1454, "ġmeow": 1455, "ġsec": 1456, "app": 1457, "ġsafe": 1458, "ween": 1459, "ġgra": 1460, "ġdra": 1461, "ian": 1462, "ġaccident": 1463, "ġdiffic": 1464, "ġamazing": 1465, "ġwalk": 1466, "ġchange": 1467, "ġoof": 1468, "ġworks": 1469, "ġconfused": 1470, "ġfore": 1471, "ġbuy": 1472, "ever": 1473, "ġfunny": 1474, "ġbabe": 1475, "ġfail": 1476, "ġfavour": 1477, "ently": 1478, "ġsign": 1479, "iew": 1480, "ġmorning": 1481, "ifu": 1482, "ġalmost": 1483, "ġegg": 1484, "ġwatching": 1485, "ġallowed": 1486, "ġign": 1487, "ġcount": 1488, "fully": 1489, "ġforg": 1490, "ġref": 1491, "ġtrans": 1492, "hd": 1493, "ġbir": 1494, "ġdeal": 1495, "ġtaking": 1496, "ility": 1497, "ġboof": 1498, "ġreply": 1499, "ġpossible": 1500, "ġsweet": 1501, "ġissues": 1502, "ġ.": 1503, "ġsmink": 1504, "ġval": 1505, "izz": 1506, "ġusually": 1507, "ġmix": 1508, "ġwind": 1509, "ġbetween": 1510, "ġexcept": 1511, "ng": 1512, "outh": 1513, "ġtext": 1514, "ġbye": 1515, "ents": 1516, "rr": 1517, "cially": 1518, "ġpop": 1519, "ġautism": 1520, "ġcal": 1521, "ġtonight": 1522, "ġhaven": 1523, "ones": 1524, "ron": 1525, "ġpri": 1526, "ġeff": 1527, "waifu": 1528, "oint": 1529, "rect": 1530, "ratewaifu": 1531, "ġparents": 1532, "ience": 1533, "ġcou": 1534, "ġtv": 1535, "ġtill": 1536, "cess": 1537, "ġoutside": 1538, "ġtelling": 1539, "ġemb": 1540, "ġarsed": 1541, "ges": 1542, "io": 1543, "ġworld": 1544, "ġsupposed": 1545, "ġcomplete": 1546, "ġdifficult": 1547, "ins": 1548, "ġages": 1549, "vo": 1550, "ġadv": 1551, "ative": 1552, "bb": 1553, "yed": 1554, "pid": 1555, "ġoften": 1556, "ġbirth": 1557, "to": 1558, "ġfollow": 1559, "mail": 1560, "ġpee": 1561, "ġtheyre": 1562, "ġneeded": 1563, "ilar": 1564, "jk": 1565, "aring": 1566, "ġsexy": 1567, "lic": 1568, "ġsing": 1569, "ġlow": 1570, "ġchild": 1571, "ial": 1572, "ġautistic": 1573, "ition": 1574, "ġdepress": 1575, "ġsmoke": 1576, "tho": 1577, "utes": 1578, "ption": 1579, "ġevent": 1580, "ġincl": 1581, "itely": 1582, "ġobviously": 1583, "ccoon": 1584, "ġduring": 1585, "ġfavourite": 1586, "ġpoop": 1587, "ġz": 1588, "ġpanic": 1589, "ġcomment": 1590, "ġcrying": 1591, "my": 1592, "ġsat": 1593, "ġcreep": 1594, "icul": 1595, "df": 1596, "ġappreciate": 1597, "ġtook": 1598, "atter": 1599, "iday": 1600, "ġsmall": 1601, "ġbody": 1602, "ġaddress": 1603, "esus": 1604, "ġuwu": 1605, "ġtype": 1606, "ġforgot": 1607, "ġsonic": 1608, "ġchang": 1609, "friend": 1610, "sy": 1611, "ail": 1612, "ġhung": 1613, "ġdie": 1614, "ġgre": 1615, "ġeffort": 1616, "ings": 1617, "ġlots": 1618, "ising": 1619, "ġweeks": 1620, "ġrequ": 1621, "ġboring": 1622, "ġemail": 1623, "ped": 1624, "ġboom": 1625, "ġfact": 1626, "ided": 1627, "ġsort": 1628, "angle": 1629, "ġanxious": 1630, "ġtou": 1631, "cked": 1632, "ġgoog": 1633, "ġrather": 1634, "ġstupid": 1635, "ġknew": 1636, "ġwouldnt": 1637, "ġrem": 1638, "ġwonder": 1639, "ġwi": 1640, "erg": 1641, "ġjesus": 1642, "ġwords": 1643, "mon": 1644, "ġrelation": 1645, "ġunless": 1646, "20": 1647, "ġvi": 1648, "ġintern": 1649, "ġhang": 1650, "ned": 1651, "ġpic": 1652, "ġmet": 1653, "ġkit": 1654, "ġdead": 1655, "ġdefinitely": 1656, "ġpick": 1657, "nday": 1658, "ġentire": 1659, "ġminutes": 1660, "ġmeme": 1661, "ġflat": 1662, "ġattack": 1663, "ġload": 1664, "ġwindow": 1665, "ġscream": 1666, "ġcop": 1667, "ġpass": 1668, "ġrelationship": 1669, "ġtot": 1670, "ġsub": 1671, "fort": 1672, "yle": 1673, "ġxox": 1674, "ġwater": 1675, "ġ11": 1676, "ġslight": 1677, "ġtu": 1678, "ġhows": 1679, "ġlovely": 1680, "eric": 1681, "ġlie": 1682, "ġimag": 1683, "ġexist": 1684, "ġhuman": 1685, "ġsituation": 1686, "ġrant": 1687, "ġthree": 1688, "ġchr": 1689, "ġhy": 1690, "ġmoving": 1691, "ġtru": 1692, "ensive": 1693, "ġclose": 1694, "ġsuck": 1695, "ced": 1696, "gin": 1697, "ġtrust": 1698, "ġseriously": 1699, "oth": 1700, "ġfinally": 1701, "ġonline": 1702, "ġsuppose": 1703, "ġnormal": 1704, "ġwed": 1705, "ately": 1706, "ġgets": 1707, "ġcontro": 1708, "ġuk": 1709, "000": 1710, "ġscre": 1711, "ġbitch": 1712, "oup": 1713, "book": 1714, "ġelect": 1715, "ġfault": 1716, "ġwouldn": 1717, "ibly": 1718, "ġdrink": 1719, "ġwrite": 1720, "zy": 1721, "ġscary": 1722, "lier": 1723, "ġameric": 1724, "ġspeaking": 1725, "ġcompl": 1726, "ġcold": 1727, "ġfur": 1728, "ġpu": 1729, "ġstand": 1730, "ġasleep": 1731, "ġseeing": 1732, "ding": 1733, "ye": 1734, "ointment": 1735, "ġwoman": 1736, "ġcase": 1737, "ġtechn": 1738, "ġflo": 1739, "ġsucks": 1740, "ġhappening": 1741, "ġenjoy": 1742, "ġboyfriend": 1743, "ġkids": 1744, "ġblack": 1745, "ġye": 1746, "ġvape": 1747, "ns": 1748, "ġsoo": 1749, "ark": 1750, "ġdrunk": 1751, "pr": 1752, "haha": 1753, "ġheard": 1754, "ġgave": 1755, "ġsocial": 1756, "ġev": 1757, "gan": 1758, "ġwin": 1759, "ats": 1760, "iny": 1761, "ġfinish": 1762, "ġworst": 1763, "ġhopefully": 1764, "ġrealised": 1765, "com": 1766, "ġlistened": 1767, "pped": 1768, "ġmass": 1769, "ġearlier": 1770, "ġchrist": 1771, "ġreading": 1772, "js": 1773, "ksks": 1774, "mpt": 1775, "ġdi": 1776, "ġyo": 1777, "ġinside": 1778, "ġdet": 1779, "ġequ": 1780, "ġ30": 1781, "ġforever": 1782, "ġbag": 1783, "ġslightly": 1784, "ġtoile": 1785, "ket": 1786, "ġver": 1787, "gging": 1788, "where": 1789, "ġconc": 1790, "ġdire": 1791, "ġstuck": 1792, "nds": 1793, "ġspend": 1794, "ġappointment": 1795, "hs": 1796, "ġmod": 1797, "ġgone": 1798, "umber": 1799, "ġhold": 1800, "ġlaugh": 1801, "ġcam": 1802, "ġvib": 1803, "ene": 1804, "fair": 1805, "ġsol": 1806, "ġcolle": 1807, "arch": 1808, "ġliving": 1809, "ġste": 1810, "ġcoo": 1811, "ġworth": 1812, "fer": 1813, "mme": 1814, "pecially": 1815, "ġswear": 1816, "ġer": 1817, "ġ15": 1818, "ġyours": 1819, "ġscr": 1820, "ġcurrently": 1821, "ġtoilet": 1822, "ices": 1823, "ġprom": 1824, "ġquick": 1825, "ġmen": 1826, "roid": 1827, "ġactual": 1828, "ġtea": 1829, "ġplus": 1830, "ġinternet": 1831, "ror": 1832, "ġcu": 1833, "ġcreat": 1834, "ġfall": 1835, "room": 1836, "ġmanag": 1837, "ġfix": 1838, "ġphys": 1839, "ġchill": 1840, "ġcontin": 1841, "ġleaving": 1842, "ġadhd": 1843, "boom": 1844, "ġwhite": 1845, "ġembar": 1846, "ertain": 1847, "airs": 1848, "gram": 1849, "ġpizz": 1850, "ġstrugg": 1851, "ġarg": 1852, "ġstraight": 1853, "eed": 1854, "ġmic": 1855, "ġvalid": 1856, "uine": 1857, "ġrap": 1858, "ġtouch": 1859, "ġhit": 1860, "ade": 1861, "ġready": 1862, "ces": 1863, "mas": 1864, "ġdefo": 1865, "ġespecially": 1866, "ġcouple": 1867, "att": 1868, "ġner": 1869, "ġloud": 1870, "ower": 1871, "ġtast": 1872, "ġgenuine": 1873, "own": 1874, "ġprocess": 1875, "ġblue": 1876, "ġ?": 1877, "lym": 1878, "ah": 1879, "ġpress": 1880, "ġtrack": 1881, "ffect": 1882, "aut": 1883, "ġhello": 1884, "ġquestion": 1885, "ġbought": 1886, "ġproperly": 1887, "ġthrow": 1888, "boomra": 1889, "ġpot": 1890, "ġfrance": 1891, "ġcorrect": 1892, "ġdri": 1893, "ġpsy": 1894, "ġkey": 1895, "ġcannot": 1896, "thers": 1897, "ġsexual": 1898, "ġles": 1899, "ġboomboomra": 1900, "ġwon": 1901, "ġsimilar": 1902, "ġapo": 1903, "ġfoc": 1904, "ġshout": 1905, "ġsudd": 1906, "ities": 1907, "ġbar": 1908, "ġboomboomraccoon": 1909, "ġcouldnt": 1910, "ġpiss": 1911, "ġcard": 1912, "ġsudden": 1913, "aking": 1914, "ġarent": 1915, "sol": 1916, "ġexcited": 1917, "ġeating": 1918, "ġeffect": 1919, "ġstep": 1920, "antly": 1921, "ġpizza": 1922, "ger": 1923, "ġsmo": 1924, "ġcuddles": 1925, "ġxxx": 1926, "ho": 1927, "ġplym": 1928, "ġdu": 1929, "ġnope": 1930, "ġexam": 1931, "ġside": 1932, "inner": 1933, "ġbread": 1934, "ġred": 1935, "ġmult": 1936, "ġdecided": 1937, "ooo": 1938, "ġshut": 1939, "ġgiving": 1940, "ġret": 1941, "ġbecome": 1942, "ġsn": 1943, "ġpack": 1944, "ml": 1945, "ġcompletely": 1946, "ġprogram": 1947, "illy": 1948, "lled": 1949, "ġrepl": 1950, "tu": 1951, "ġboobs": 1952, "ġproud": 1953, "angel": 1954, "ġmention": 1955, "ġextra": 1956, "ġmessages": 1957, "iding": 1958, "ġslow": 1959, "ġindeed": 1960, "ġawake": 1961, "inge": 1962, "ġwaiting": 1963, "ġcook": 1964, "ġdiscuss": 1965, "av": 1966, "ru": 1967, "ptop": 1968, "ġdream": 1969, "ġgross": 1970, "ġhuh": 1971, "ġpsych": 1972, "iced": 1973, "ġaccept": 1974, "ġkept": 1975, "ġcontact": 1976, "wise": 1977, "ġ18": 1978, "ġserver": 1979, "aly": 1980, "ġprof": 1981, "ġamount": 1982, "ġide": 1983, "ġhuge": 1984, "ġform": 1985, "ġlaptop": 1986, "ux": 1987, "ġearly": 1988, "ġtotally": 1989, "ġcollege": 1990, "ġstar": 1991, "ġwriting": 1992, "ġtest": 1993, "ġfit": 1994, "ġrecord": 1995, "ġaccidentally": 1996, "ġba": 1997, "ġnotice": 1998 }, "merges": [ [ "ġ", "i" ], "t" "a" "w" "s" "ġt", "h" "i", "n" "m" "o", "u" "b" "r", "e" "o" "d" "in", "g" "a", "l" "c" "f" "e", "r" "y" "ġth", "n", "p" "l", "ġy", "ou" "ġi", "ġw", "u", "ġa", "nd" "ġn", "k", "ġm", "ġb", "v", "ġh", "at" "ġd", "ġo", "j" "re" "ġl", "ut" "g", "ġs", "ġf", "or" "ll" "as" "ġno", "us", "ġwh", "c", "k" "er" "ġj", "ust" "ġli", "ke" "ou", "all", "an", "en", "m", "it", "s", ":" "gh", "ġha", "ve" "on" "ith" "is" "ma", "ġc", "mao" "at", "om", "ġit", "ld" "t", "use" "'", "ing" ".", "." "p", "ġthe", "an" "is", "ġg", "et" "in" "ġke", "vin" "ġk", "h", "ant" "ġu", "ġkn", "ow" "id" "oo", "es", "ġdon", "ġab", "out" "hh" "ea", "ġp", "eah" "ro" "ġfu", "ck" "ġe", "x" "en" "ink" "ġan", "ġyou", "ġlo", "ġto", "f", "v" "ġon", "ate" "ġsh", "ver", "ġbe", "'" "ght" "ist" "ee", "ġre", "ally" "ġne", "ed" "am", "eel" "ven" "on", "re", "ill" "oo" "ome" "-" "us" "ġthat", "ġlist", "ġbec", "ause" "es" "ġme", "om" "ġmus", "ic" "ġwe", "ay" "ic", "1" "im" "ood" "ġx", "ee" "ġal", "so" "er", "ġom", "ore" "ġsor", "ry" "as", "q", "ar", "ha" "ot" "ġch", "ar" "ġdo", "un" "gh" "ould" "ġfr", "ġtim", "ġpe", "op" "(" "ġdun", "no" "!", "!" "ct" "ion" "ġpro", "ġpeop", "le" "ġlisten", "al" "2" "ġ:", ")" "ly" "very" "ġchar", "ġma", "ġmu", "ch" "ġin", "..", "ġpl", "ġwor", "ġwan", "na" "ri", "or", "b", "omet" "?", "?" "w", "ab", "ver" "ġhe", "lp" "ġtb", "<" "res", "it" "ad" "ġprob", "ably" "ġst", "ind" "le", "ep" "id", "pp" "ġgo", "end" "am" "ame" "th", "ġact", "ually" "d", "ġof", "ġwa", "nd", "se", "lf" "ack" "qu" "ġsa", "ent" "ound" "leep" "ġple", "ase" "*" "ġsomet", "hing" "right" "ġtal", "ool" "fe" "ġloo", "3" "ġgon", "all" "y", "ice" "ich" "ther" "ge" "ġge", "riend" "king" "ra" "ġag", "ain" "0", "0" "ġthing", "bs", "ite" "ight" "bsite" "ce" "ġfa", "ir" "ought" "ġte", "ġmay", "be" "nder" "ġint", "ġdid", "nt" "ir", "ġel", "od" "ream" "ġstu", "ff" "ab" "fore" "ġgeor", "ġfuck", "ere" "ġsu", "ġhop", "ġaf", "ter" "ous" "ers" "ird" "ul", "ear" "art" "way", "ġcom", "um" "ġpers", "ġelod", "ie" "se" "ways" "ġlit", "wn" "ġkind", "igh" "ġany", "thing" "our" "ong" "ġas", "ġtr", "ue" "ġbet", "ġsound", "ġmy", "self" "ouse" "ġta", "4" "ġliter", "ġse", "ġtry", "ġaw", "ber" "x", "ġthan", "ks" "ion", "ea" "day" "if" "ġpre", "ġget", "ting" "cc" "ġmad", "ġhar", "st" "st", "and" "ġcut", "od", "me" "ġfro", "gg" "5" "irst" "ess" "ġcon", "ough" "ving" "ġunder", "stand" "ġreme", "mber" "ater" "ġplay", "ul" "kes" "iss" "ġbot", "ine" "ġgu", "ġna", "ġle", "ġpret", "ty" "ason" "ġhapp", "ġfeel", "ġdis", "ont" "ġen", "ġsp", "ast" "ġbab", "ġsay", "ġpet", "ġph", "one" "way" "ud", "est" "ive" "ġfrogg", "ġim", "ġde", "ġmean", "ard" "read", "fe", "are" "un", "ready" "atch" "ġpo", "int" "ġwant", "z" "!!", "ġad", "ġhour", "ġdoes", "ġmo", "ith", "are", "ġkevin", "ġus", "??", "ġevery", "ġnot", "ġex", "act" "one", "per" "tle" "ġat", "aa" "ġbo", "ġwr", "pe", "ġhon", "ġthr", "mor" "ick" "eep" "ġmon", "th" "ġfriend", "ġdif", "ġid", "ġlea", "ġtomor", "row" "inking" "ore", "ġgeep", "ig", "ġ1", "ġar", "reat" "ble" "unch" "ġok", "ur", "pl" "ġsee", "ms" "ġgir", "ody" "ġsupp", "ort" "sw" "ig" "ġtw", "ce", "pt" "ġexact", "mm" "ere", "ct", "ll", "ġleg", "erest" "oney" "ġjo", "ġro", "6" "ire", "ġeng", "xt" "ġsc", "ared" "um", "ġspea", "ict" "ft" "ious", "ug" "ġday", "ġtho", "ġawes", "ace" "ither" "ġwith", "ress" "ġun", "ġanno", "ying" "ish" "ck", "ur" "ree" "ġdiffe", "rent" "ored" "ġyear", "ġgot", "ta" "ot", "her" "ġwee", "ġman", "ġwho", "ġfor", "get" "ctor" "ġang", "ġv", "ide" "/" "ġsometim", "ġansw", "ġis", "kr" "ġoo", "ġsch", "ġhor", "rible" "ġet", "get", "ġbre", "ak" "vent" "lo" "gether" "ġhonest", "9" "ġinst", "ead" "ss" "ġmin", "ġexpl", "ġwhat", "8" "ys" "al", "ġdisc", "ord" "ġra", "ndom" "ġcall", "ġ2", "ġbr", "lo", "ci", "ġper", "fect" "ġche", "ġmed", "ġbas", "ically" "7" "ud" "ġr", "res" "ġneed", "ġop", "ude" "ġawk", "ward" "ense" "co", "ġask", "j", "ġco", "ra", "other" "ve", "1", "ġvide", ";" "ġunt", "il" "ġem", "ġqu", "cond" "ach" "if", "00" "ġall", "ht" "gg", "ro", "ġwas", "ġcomm", "umb" "ġengl", "ġmom", "te" "ġgl", "ġbel", "\"" "ance" "ġcomp", "udd" "alth" "ġsin", "ġpict", "ure" "ired" "orm" "ġmess", "age" "ġapp", "ried" "iet", "ut", "ġmonth", "ġlet", "ġtra", "ġ;", "arent" "jo" "ation", "pe" "z", "ist", "tw" "urn" "body" "ġob", "ġapparent", "ġappre", "ci" "anc" "ġfe", "lt" "ġhappen", "ġcol", "ġment", "ter", "ġbra", "ġyes", "terday" "ri" "ġanx", "iety" "fg" "ġstart", "," "we" ">" "ug", "ġfre", "ġdam", "ġyour", "il", "ġproble", "ggest" "ġcour", "ġimp", "ġdanc", "ġinterest", "ilst" "ġup", "set" "sh", "ip" "ġcle", "ġcons", "ġcomput", "ġcl", "ass" "ġsit", "ma" "ġor", "der" "ġmain", "pect" "ect" "ġdef", "ġbelie", "ames" "ese" "ġhur", "mo", "ġfig", "ġgen", "uck" "ġcur", "ġimport", "ġbut", "os", "ġspe", "ag" "ġ:'", "ġfren", "ile" "ġcommun", "we", "ġacc", "ident" "fic" "ġama", "zing" "ġwal", "ange" "ġconf", "used" "ġbu", "ġfun", "ny" "vour" "ent", "ign" "ie", "ġmor", "ning" "most" "ġwatch", "ġallow", "gn" "ount" "fu", "lly" "ans" "ity" "ġboo", "ġrep", "ġposs", "ible" "ġswe", "ġiss", "ues" "ġsm", "iz", "ix" "ween" "cept" "out", "ġby", "ġaut", "ism" "night" "wa", "ifu" "rate", "waifu" "ġparent", "ence" "ġout", "side" "ġtell", "sed" "osed" "lete" "ġdiffic", "ult" "ten" "ġbir", "ġfo", "llow" "ġthey", "ġsex", "ild" "istic" "ġdep", "oke" "pt", "ġinc", "ite", "ġobv", "iously" "cc", "oon" "uring" "ġfavour", "ġpoo", "ġpan", "ment" "ġcry", "ġcre", "ġappreci", "ġtoo", "ġadd", "wu" "ġty", "ġforg", "ġson", "ang" "ung" "ġeff", "ing", "ġlot", "ġbor", "mail" "ang", "ious" "ġgoo", "ather" "pid" "ew" "ġwould", "ġwo", "esus" "ġword", "ġrel", "ation" "less" "2", "ġinter", "ġdefin", "itely" "ġent", "ire" "utes" "ġfl", "ġatt", "ġwind", "ġrelation", "ship" "ox" "ġsl", "ġhow", "ġlove", "ġhum", "ġsitu", "ens", "ġclo", "ġser", "ġfin", "line" "ose" "ġnorm", "ate", "ġcont", "00", "ġbit", "ook" "lect" "ib", "ġdr", "ary" "li", "ġam", "eric" "oint", "ġwom", "ġtech", "cks" "ġenjo", "ġboy", "friend" "ġkid", "ġbl", "ape" "unk" "ha", "ave" "ġso", "cial" "ġhope", "fully" "ġreal", "ised" "pp", "ġear", "lier" "ġchr", "ġread", "ks", "ġins", "ġ3", "ġfore", "ġslight", "ke", "wh", "ointment" "ġho", "ġla", "ugh" "ib" "air" "ġcoll", "cially" "ġsw", "ġcurrent", "ġtoile", "ġpr", "ual" "ġintern", "hd" "bo", "ġemb", "ert", "air", "gr", "izz" "ġstr", "ugg" "ġstra", "ġval", "ġtou", "ad", "ġes", "pecially" "ġcou", "ple" "oud" "ow", "uine" "cess" "ly", "ġhell", "ġquest", "ġproper", "boom", "ġcor", "rect" "sy" "ġcan", "not" "ġboom", "boomra" "ġsim", "ilar" "po" "dd" "ies" "ġboomboomra", "ccoon" "ġcould", "ġsudd", "ġare", "so", "ġexc", "ited" "ating" "ffect" "ant", "ġpizz", "ġcudd", "les" "ġxx", "lym" "ner" "read" "ġdec", "ided" "ġgi", "ġcomplete", "gram" "ill", "bs" "ange", "ġext", "ġmessag", "ġind", "eed" "ake" "ġwait", "uss" "ross" "ġhu", "ġpsy", "ġac", "ise" "ptop" "arly" "ġtot", "ġcolle", "iting" "ġrec", "ġaccident", ] ] } } babyllm first converts its input into tokens (vocab), and then converts those tokens into embeddings in an embed layer. neuron layer is meant to be outputting a single number for each input token, iterated by numneurons - each neuron has a dimension of 32, meaning that it has 32 numbers parallel neuron layer is meant to be outputting [seqlen, numneurons] - mean output - this makes the 10000 neuron activations for each token in the sequence. - this creates a shape of [seqlen, numneurons] - it then gets the mean average of all of these within the training window (7 usually) - this creates a shape of [1(all tokens averaged), numneurons] - this mean output gives the general idea of a 'sentence', allowing babyllm to learn a bit about context (but not much about word order) - this mean output is then passed through to the output layer to be used in token guess calculations output layer uses all of the inputs (currently just mean output parallel neurons) to judge what the output should be - this takes the mean output activation from parallel neuron layer and applies that to the relevant token in the vocab. - this is also an nn layer itself idfk why --- what the fuck is self?! i thoguht i had self identity issues and then i encountered python!! model training flow after tokenization: 1) --- using stuff --- how to call stat thresholds for a particular stat: self.s_output.s_statbands["loss"]["perfect"] # will cause key error self.s_output.s_statbands["loss"].get("perfect", none) # will ignore key error import re with open('/users/charis/documents/github/shkaira/school/statistics/logs/training/traininglog_100.txt', 'r') as f: lines = f.readlines() output = [] running_total = 0 previous_step = 0 for line in lines: if 'total steps:' in line: output.append(line) continue # match line with timestamp timestamp_match = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s*\|\s*(\d+)\s*\|', line) step_match_old = re.search(r'\bstep (\d+)', line) if timestamp_match: # new format: | <step> | with no label timestamp, current_step = timestamp_match.groups() current_step = int(current_step) elif step_match_old: # old format: step xxx current_step = int(step_match_old.group(1)) else: # compute delta step_delta = current_step - previous_step if step_delta < 0: step_delta = current_step running_total += step_delta previous_step = current_step # inject total steps after timestamp (old or new) line = re.sub( r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', rf'\1 | total steps: {running_total}', line ) output.append(line) with open('/users/charis/documents/github/shkaira/school/statistics/logs/training/traininglog_100_withtotalsteps.txt', 'w') as f: f.writelines(output) print("global running total added only where missing. no resets.") def makedatboi(): # ʕ"❀"ෆ.ෆʔっ❀ or ⊂ʕʘ‿ʘ"❀"ʔ hairthingys = ["❀", "♥", "𖡼", "❄︎", "♡"] # ʕっʘ.ʘʔっ"❄" thingys = hairthingys thingys += ["︵", "✎", "♥",] #"𓂸", "𓆟", "𓆞", "✰✰⋆⋆", "𖤣", ] # << -- "ς"ʕʘ‿ʘςʔ leftarmsout = ["૮", "ς", "⊂"] # |-- "ʕ"っʘ‿ʘʔっ leftsides = ["ʕ"] # >> -- ʕ"っ"☯‿☯ʔっ leftarmsin = ["ฅ", "っ", "ノ", "⊃", "ゝ"] ### leftarmsin += hairthingys # what the hell, she said! # ʕっ"ʘ"‿"ʘ"ʔっ eyes = ["⚈", "◉", "•", "o", "ʘ", "ₓ", "ʘ", "•̀", "•́", "꩜ ", "⋆", "✰", "♡", "‿", "ෆ", "ᵔ", "-", "☯", "⊗", "˙", "☉",] # ʕっʘ"‿"ʘʔっ mouthes = ["‿", "︿", "ω", "ᴥ", ".", "o", "ᗜ"] # -- << ςʕʘ‿ʘ"ς"ʔ rightarmsin = ["ฅ", "૮", "ς", "⊂"] # --| ʕっʘ‿ʘ"ʔ"っ rightsides = ["ʔ"] # -- >> ʕっ☯‿☯ʔ"っ" rightarmsout = ["っ", "ノ", "⊃", "ゝ"] datboi = random.choice(leftsides) # 'ʕ' pointleft = random.choice([true, false]) if pointleft: datboi = random.choice(leftarmsout) + datboi # '⊂' throwleft = random.choice([true, false]) if throwleft: datboi = " " + datboi #' ⊂' thingyleft = random.choice([true, false]) if thingyleft: datboi = random.choice(thingys) + datboi # '❀⊂' / '❀ ⊂' # 'ʕ'/'⊂ʕ'/' ⊂ʕ'/'❀⊂ʕ'/'❀ ⊂ʕ'/'❀ʕ'/'❀ ʕ' lefthair = random.choice([true, false]) if lefthair and thingyleft is false: datboi += random.choice(hairthingys) # 'ʕ❀'/'⊂ʕ❀'/' ⊂ʕ❀'/'❀⊂ʕ❀'/'❀ ⊂ʕ❀' if pointleft is false and lefthair is false: hugleft = random.choice([true, false]) if hugleft: datboi += random.choice(leftarmsin) # 'ʕ⊃'/'❀ʕ⊃'/'❀ ʕ⊃' mouth = random.choice(mouthes) eye = random.choice(eyes) unmatchedeyes = random.choice([true, false]) datboi += eye + mouth if unmatchedeyes: eye = random.choice(eyes) datboi += eye hugright = random.choice([true, false]) if hugright: datboi += random.choice(rightarmsin) righthair = random.choice([true, false]) if righthair and hugright is false and lefthair is false and thingyleft is false: datboi += random.choice(hairthingys) datboi += random.choice(rightsides) if hugright is false: pointright = random.choice([true, false]) if pointright: datboi += random.choice(rightarmsout) throwright = random.choice([true, false]) if throwright: datboi += " " thingyright = random.choice([true, false]) if thingyright and righthair is false and lefthair is false and thingyleft is false: datboi += random.choice(thingys) return datboi face = makedatboi() print(face) skipped computeloss: --- --- name self cpu % self cpu cpu total % cpu total cpu time avg # of calls aten::isfinite 0.05% 4.209us 1.32% 120.167us 120.167us 1 aten::eq 0.27% 24.833us 0.27% 24.833us 24.833us 1 aten::abs 0.60% 54.875us 0.78% 71.333us 35.666us 2 aten::empty 0.00% 0.375us 0.00% 0.375us 0.375us 1 aten::is_same_size 0.00% 0.042us 0.00% 0.042us 0.042us 1 aten::resize_ 0.01% 0.500us 0.01% 0.500us 0.500us 1 aten::ne 10.36% 945.205us 10.37% 945.913us 25.565us 37 aten::item 0.15% 13.253us 83.14% 7.583ms 194.429us 39 aten::_local_scalar_dense 82.60% 7.533ms 83.00% 7.569ms 194.089us 39 aten::mul 0.16% 14.792us 0.16% 14.792us 14.792us 1 aten::is_nonzero 0.00% 0.417us 2.26% 205.916us 205.916us 1 aten::empty_like 0.23% 20.545us 0.41% 37.296us 0.981us 38 aten::empty_strided 0.27% 25.003us 0.27% 25.003us 0.455us 55 optimizer.zero_grad#adamw.zero_grad 0.26% 23.708us 0.26% 23.708us 23.708us 1 aten::ones_like 0.01% 0.875us 0.32% 28.875us 28.875us 1 aten::fill_ 0.29% 26.750us 0.29% 26.750us 26.750us 1 autograd::engine::evaluate_function: divbackward0 0.12% 10.583us 3.00% 273.957us 273.957us 1 divbackward0 0.02% 2.042us 0.32% 29.125us 29.125us 1 aten::div 0.29% 26.583us 0.30% 27.083us 27.083us 1 aten::isnan 0.15% 13.666us 10.30% 939.038us 26.084us 36 aten::_is_any_true 0.12% 11.293us 1.79% 163.129us 4.531us 36 aten::any 0.47% 43.007us 1.66% 151.836us 4.218us 36 aten::view_as 0.16% 14.373us 0.27% 24.790us 0.689us 36 aten::view 0.11% 10.417us 0.11% 10.417us 0.289us 36 aten::to 0.02% 2.249us 0.02% 2.249us 0.062us 36 aten::copy_ 1.49% 135.622us 1.49% 135.622us 2.559us 53 autograd::engine::evaluate_function: addbackward0 0.70% 63.992us 91.14% 8.312ms 461.803us 18 addbackward0 0.05% 4.542us 0.05% 4.542us 0.252us 18 autograd::engine::evaluate_function: torch::autograd... 0.18% 16.586us 1.38% 125.792us 6.988us 18 torch::autograd::accumulategrad 0.36% 33.000us 1.20% 109.206us 6.067us 18 aten::new_empty_strided 0.14% 12.997us 0.23% 21.249us 1.250us 17 aten::detach 0.01% 0.458us 0.01% 1.125us 1.125us 1 detach 0.01% 0.667us 0.01% 0.667us 0.667us 1 optimizer.step#adamw.step 0.32% 29.167us 0.32% 29.167us 29.167us 1 self cpu time total: 9.120ms not skipped computeloss: aten::isfinite 0.01% 24.457us 0.26% 906.289us 82.390us 11 aten::eq 0.08% 265.957us 0.08% 265.957us 24.178us 11 aten::abs 0.06% 215.042us 0.11% 393.041us 17.865us 22 aten::empty 0.01% 46.796us 0.01% 46.796us 0.564us 83 aten::is_same_size 0.00% 10.207us 0.00% 10.207us 0.092us 111 aten::resize_ 0.01% 21.919us 0.01% 21.919us 0.756us 29 aten::ne 5.05% 17.791ms 5.06% 17.795ms 18.791us 947 aten::item 0.12% 423.659us 80.70% 284.041ms 258.454us 1099 aten::_local_scalar_dense 80.37% 282.878ms 80.58% 283.617ms 258.068us 1099 aten::mul 1.46% 5.149ms 1.46% 5.149ms 21.015us 245 aten::is_nonzero 0.00% 3.957us 1.65% 5.809ms 528.120us 11 aten::empty_like 0.11% 386.834us 0.22% 762.753us 0.790us 966 aten::empty_strided 0.10% 367.669us 0.10% 367.669us 0.384us 958 optimizer.zero_grad#adamw.zero_grad 0.01% 30.625us 0.01% 30.625us 30.625us 1 aten::ones_like 0.00% 0.917us 0.01% 20.458us 20.458us 1 aten::fill_ 0.02% 83.957us 0.02% 83.957us 4.419us 19 autograd::engine::evaluate_function: divbackward0 0.14% 499.992us 10.59% 37.272ms 409.581us 91 divbackward0 0.05% 168.848us 1.21% 4.245ms 46.644us 91 aten::div 1.02% 3.582ms 1.03% 3.611ms 15.907us 227 aten::isnan 0.11% 382.385us 5.11% 17.979ms 19.208us 936 aten::_is_any_true 0.08% 290.932us 4.34% 15.263ms 16.306us 936 aten::any 3.89% 13.696ms 4.25% 14.972ms 15.995us 936 aten::view_as 0.04% 154.691us 0.07% 248.428us 0.575us 432 aten::view 0.04% 133.735us 0.04% 133.735us 0.275us 486 aten::to 0.01% 28.944us 0.01% 51.612us 0.117us 442 aten::copy_ 0.30% 1.057ms 0.30% 1.057ms 2.298us 460 autograd::engine::evaluate_function: addbackward0 0.29% 1.024ms 29.02% 102.131ms 567.394us 180 addbackward0 0.01% 46.084us 0.01% 46.084us 0.256us 180 autograd::engine::evaluate_function: nlllossbackward... 0.03% 94.833us 1.60% 5.617ms 312.034us 18 nlllossbackward0 0.01% 30.501us 0.28% 975.169us 54.176us 18 aten::nll_loss_backward 0.27% 944.668us 0.27% 944.668us 52.482us 18 autograd::engine::evaluate_function: logsoftmaxbackw... 0.02% 73.748us 1.51% 5.332ms 296.213us 18 logsoftmaxbackward0 0.01% 31.378us 0.19% 666.668us 37.037us 18 aten::_log_softmax_backward_data 0.18% 635.290us 0.18% 635.290us 35.294us 18 aten::sum 1.18% 4.164ms 1.18% 4.164ms 25.703us 162 autograd::engine::evaluate_function: mmbackward0 0.10% 346.238us 21.10% 74.258ms 2.063ms 36 mmbackward0 0.03% 107.919us 0.61% 2.140ms 59.452us 36 aten::t 0.02% 65.786us 0.05% 162.207us 1.802us 90 aten::transpose 0.02% 55.879us 0.03% 96.421us 1.071us 90 aten::as_strided 0.02% 65.581us 0.02% 65.581us 0.455us 144 aten::mm 0.53% 1.870ms 0.53% 1.870ms 25.974us 72 autograd::engine::evaluate_function: mulbackward0 0.21% 736.633us 14.28% 50.247ms 398.788us 126 mulbackward0 0.04% 154.502us 1.27% 4.484ms 35.589us 126 autograd::engine::evaluate_function: rsubbackward1 0.03% 108.376us 2.25% 7.936ms 220.456us 36 rsubbackward1 0.01% 29.245us 0.20% 694.121us 19.281us 36 aten::neg 0.39% 1.367ms 0.39% 1.375ms 15.283us 90 aten::add_ 1.71% 6.017ms 1.72% 6.047ms 19.318us 313 autograd::engine::evaluate_function: sigmoidbackward... 0.04% 135.217us 2.45% 8.621ms 239.466us 36 sigmoidbackward0 0.02% 63.911us 0.17% 591.451us 16.429us 36 aten::sigmoid_backward 0.15% 527.540us 0.15% 527.540us 14.654us 36 autograd::engine::evaluate_function: meanbackward1 0.02% 64.457us 1.76% 6.183ms 343.502us 18 meanbackward1 0.01% 38.586us 0.20% 697.624us 38.757us 18 aten::expand 0.01% 30.002us 0.01% 39.835us 2.213us 18 autograd::engine::evaluate_function: clampbackward1 0.04% 138.666us 2.24% 7.880ms 437.767us 18 clampbackward1 0.02% 66.628us 0.54% 1.916ms 106.423us 18 aten::scalar_tensor 0.01% 26.417us 0.03% 102.417us 5.690us 18 aten::ge 0.15% 530.457us 0.15% 537.913us 29.884us 18 aten::le 0.11% 393.124us 0.11% 398.042us 22.113us 18 aten::logical_and_ 0.00% 10.872us 0.11% 375.081us 20.838us 18 aten::logical_and 0.10% 364.209us 0.10% 364.209us 20.234us 18 aten::where 0.12% 414.576us 0.12% 435.537us 24.197us 18 autograd::engine::evaluate_function: leakyrelubackwa... 0.03% 89.457us 1.65% 5.817ms 323.193us 18 leakyrelubackward0 0.01% 20.790us 0.19% 677.579us 37.643us 18 aten::leaky_relu_backward 0.17% 586.124us 0.19% 656.789us 36.488us 18 autograd::engine::evaluate_function: permutebackward... 0.03% 106.128us 4.41% 15.532ms 862.896us 18 permutebackward0 0.00% 16.250us 0.02% 60.374us 3.354us 18 aten::permute 0.01% 36.333us 0.01% 44.124us 2.451us 18 autograd::engine::evaluate_function: indexbackward0 0.04% 129.157us 4.49% 15.817ms 878.722us 18 indexbackward0 0.01% 39.998us 0.12% 421.126us 23.396us 18 aten::new_zeros 0.01% 19.171us 0.03% 94.169us 5.232us 18 aten::new_empty 0.00% 7.372us 0.01% 24.958us 1.387us 18 aten::zero_ 0.01% 50.040us 0.01% 50.040us 2.780us 18 aten::_index_put_impl_ 0.08% 267.462us 0.08% 286.959us 15.942us 18 aten::reshape 0.00% 7.414us 0.00% 12.082us 0.671us 18 aten::add 0.03% 93.375us 0.03% 93.375us 31.125us 3 autograd::engine::evaluate_function: torch::autograd... 0.00% 10.043us 0.01% 42.752us 4.275us 10 torch::autograd::accumulategrad 0.00% 10.958us 0.01% 32.709us 3.271us 10 aten::detach 0.00% 3.249us 0.01% 21.751us 2.175us 10 detach 0.01% 18.502us 0.01% 18.502us 1.850us 10 aten::all 0.08% 269.375us 0.08% 269.375us 26.938us 10 optimizer.step#adamw.step 0.07% 263.126us 0.64% 2.247ms 2.247ms 1 aten::_to_copy 0.00% 13.166us 0.01% 22.668us 2.267us 10 aten::mul_ 0.12% 423.584us 0.12% 431.168us 21.558us 20 aten::lerp_ 0.07% 256.372us 0.07% 257.248us 25.725us 10 aten::result_type 0.00% 0.876us 0.00% 0.876us 0.088us 10 aten::addcmul_ 0.08% 288.499us 0.08% 288.499us 28.850us 10 aten::sqrt 0.06% 203.915us 0.06% 204.622us 20.462us 10 aten::addcdiv_ 0.08% 271.167us 0.08% 271.167us 27.117us 10 self cpu time total: 351.969ms versus original babyllm not split version!!: original version skipped computeloss: aten::ones_like 1.09% 1.419us 22.03% 28.777us 28.777us 1 aten::empty_like 1.53% 2.000us 2.14% 2.793us 2.793us 1 aten::empty_strided 4.89% 6.382us 4.89% 6.382us 0.355us 18 aten::fill_ 18.81% 24.565us 18.81% 24.565us 24.565us 1 autograd::engine::evaluate_function: divbackward0 3.54% 4.629us 24.01% 31.363us 31.363us 1 divbackward0 1.50% 1.961us 20.47% 26.734us 26.734us 1 aten::div 18.39% 24.022us 18.97% 24.773us 24.773us 1 aten::item 0.48% 0.626us 0.57% 0.751us 0.751us 1 aten::_local_scalar_dense 0.10% 0.125us 0.10% 0.125us 0.125us 1 autograd::engine::evaluate_function: addbackward0 10.82% 14.136us 12.90% 16.848us 0.936us 18 addbackward0 2.08% 2.712us 2.08% 2.712us 0.151us 18 autograd::engine::evaluate_function: torch::autograd... 8.04% 10.504us 41.06% 53.632us 2.980us 18 torch::autograd::accumulategrad 10.48% 13.683us 33.02% 43.128us 2.396us 18 aten::new_empty_strided 4.41% 5.755us 8.68% 11.344us 0.667us 17 aten::copy_ 13.09% 17.101us 13.09% 17.101us 1.006us 17 aten::detach 0.35% 0.458us 0.77% 1.000us 1.000us 1 detach 0.41% 0.542us 0.41% 0.542us 0.542us 1 self cpu time total: 130.620us original version not skipped computeloss: aten::ones_like 0.01% 1.293us 0.09% 21.042us 21.042us 1 aten::empty_like 0.04% 8.502us 0.08% 17.542us 0.923us 19 aten::empty_strided 0.00% 0.667us 0.00% 0.667us 0.667us 1 aten::fill_ 0.35% 82.162us 0.35% 82.162us 4.324us 19 autograd::engine::evaluate_function: divbackward0 0.78% 180.700us 19.78% 4.601ms 63.030us 73 divbackward0 0.70% 163.246us 14.50% 3.372ms 46.188us 73 aten::div 13.04% 3.034ms 13.13% 3.054ms 14.072us 217 aten::item 0.10% 23.282us 0.13% 29.828us 0.328us 91 aten::_local_scalar_dense 0.03% 6.546us 0.03% 6.546us 0.072us 91 autograd::engine::evaluate_function: addbackward0 1.42% 330.386us 11.47% 2.668ms 14.820us 180 addbackward0 0.17% 39.575us 0.17% 39.575us 0.220us 180 autograd::engine::evaluate_function: nlllossbackward... 0.15% 35.832us 3.12% 725.450us 40.303us 18 nlllossbackward0 0.11% 26.543us 2.96% 689.618us 38.312us 18 aten::nll_loss_backward 2.85% 663.075us 2.85% 663.075us 36.838us 18 autograd::engine::evaluate_function: logsoftmaxbackw... 0.13% 29.581us 2.10% 489.164us 27.176us 18 logsoftmaxbackward0 0.07% 15.541us 1.98% 459.583us 25.532us 18 aten::_log_softmax_backward_data 1.91% 444.042us 1.91% 444.042us 24.669us 18 aten::sum 13.80% 3.210ms 13.80% 3.210ms 19.814us 162 aten::view 0.10% 22.497us 0.10% 22.497us 0.417us 54 autograd::engine::evaluate_function: mmbackward0 0.38% 88.375us 10.75% 2.501ms 69.467us 36 mmbackward0 0.45% 104.712us 8.74% 2.033ms 56.461us 36 aten::t 0.20% 46.285us 0.50% 116.706us 1.297us 90 aten::transpose 0.19% 44.087us 0.30% 70.421us 0.782us 90 aten::as_strided 0.20% 47.543us 0.20% 47.543us 0.293us 162 aten::mm 7.79% 1.811ms 7.79% 1.811ms 25.155us 72 autograd::engine::evaluate_function: mulbackward0 1.35% 313.911us 26.47% 6.156ms 48.860us 126 mulbackward0 0.46% 107.292us 13.97% 3.251ms 25.799us 126 aten::mul 16.03% 3.730ms 16.03% 3.730ms 15.939us 234 autograd::engine::evaluate_function: rsubbackward1 0.15% 35.919us 1.91% 444.586us 12.350us 36 rsubbackward1 0.06% 14.878us 1.76% 408.667us 11.352us 36 aten::neg 3.90% 907.658us 3.92% 912.200us 10.136us 90 aten::is_same_size 0.02% 4.542us 0.02% 4.542us 0.050us 90 aten::add_ 16.93% 3.938ms 16.93% 3.938ms 13.442us 293 autograd::engine::evaluate_function: sigmoidbackward... 0.23% 54.075us 3.65% 847.863us 23.552us 36 sigmoidbackward0 0.21% 49.627us 1.95% 452.456us 12.568us 36 aten::sigmoid_backward 1.73% 402.829us 1.73% 402.829us 11.190us 36 autograd::engine::evaluate_function: meanbackward1 0.18% 41.459us 4.57% 1.063ms 29.540us 36 meanbackward1 0.15% 34.499us 4.39% 1.022ms 28.389us 36 aten::expand 0.11% 26.209us 0.16% 37.834us 1.051us 36 autograd::engine::evaluate_function: clampbackward1 0.13% 30.626us 9.16% 2.131ms 118.407us 18 clampbackward1 0.24% 54.955us 9.03% 2.101ms 116.705us 18 aten::scalar_tensor 0.06% 13.293us 0.38% 87.413us 4.856us 18 aten::empty 0.14% 32.537us 0.14% 32.537us 0.452us 72 aten::ge 2.47% 573.668us 2.49% 578.873us 32.160us 18 aten::le 2.02% 468.787us 2.04% 473.996us 26.333us 18 aten::logical_and_ 0.04% 9.752us 1.85% 429.706us 23.873us 18 aten::logical_and 1.81% 419.954us 1.81% 419.954us 23.331us 18 aten::where 1.98% 460.834us 2.05% 475.748us 26.430us 18 aten::resize_ 0.05% 11.082us 0.05% 11.082us 0.616us 18 autograd::engine::evaluate_function: leakyrelubackwa... 0.11% 24.791us 2.39% 555.702us 30.872us 18 leakyrelubackward0 0.05% 10.666us 2.28% 530.911us 29.495us 18 aten::leaky_relu_backward 1.98% 461.120us 2.24% 520.245us 28.903us 18 aten::copy_ 0.19% 44.249us 0.19% 44.249us 2.458us 18 autograd::engine::evaluate_function: permutebackward... 0.09% 20.787us 1.48% 345.035us 19.169us 18 permutebackward0 0.03% 7.543us 0.11% 25.834us 1.435us 18 aten::permute 0.06% 14.249us 0.08% 18.291us 1.016us 18 autograd::engine::evaluate_function: indexbackward0 0.16% 36.877us 2.89% 673.165us 37.398us 18 indexbackward0 0.11% 26.578us 1.46% 339.498us 18.861us 18 aten::new_zeros 0.05% 12.754us 0.39% 90.209us 5.012us 18 aten::new_empty 0.03% 6.750us 0.08% 18.041us 1.002us 18 aten::zero_ 0.26% 59.414us 0.26% 59.414us 3.301us 18 aten::_index_put_impl_ 0.89% 208.002us 0.96% 222.711us 12.373us 18 aten::reshape 0.02% 5.751us 0.04% 9.167us 0.509us 18 aten::add 0.37% 87.042us 0.37% 87.042us 29.014us 3 autograd::engine::evaluate_function: torch::autograd... 0.03% 8.044us 0.16% 38.167us 3.817us 10 torch::autograd::accumulategrad 0.03% 8.123us 0.13% 30.123us 3.012us 10 aten::detach 0.07% 16.832us 0.09% 22.000us 2.200us 10 detach 0.02% 5.168us 0.02% 5.168us 0.517us 10 self cpu time total: 23.261ms what do you think about the sound of a broken radio? what does a broken radio say? how would you describe the feeling of broken static? is a broken radio lonely? ʕっ🌸‿🌸ʔっ("✨ memory gradients survived the wormhole ✨") ʕっ☯‿☯ʔっ ʕ⊃⊗‿⊗⊂ʔ error: ʕ꩜.꩜ʔ ʕっ⸝⸝⸝ᴥ⸝⸝⸝ʔっ ʕ〃⸝⸝⸝ᴥ⸝⸝⸝〃ʔっ ʕっ≧⸝⸝ᴥ⸝⸝≦ʔっ ʕっ⸝⸝⸝ᴥ⸝⸝⸝ʔっ っʕっ⸝⸝⸝ᴥ⸝⸝⸝ʔっ ʕっᵕᴥᵕʔっ ʕっ-ᴥ-ʔっ def babyreflection(self): with self.counsellor.infodump("startturnactions") as ʕっʘ‿ʘʔっ: babyfeels = self.mapstatstofeelings() hesjustababy = " ".join(babyfeels) delta = round(self.latestlossdelta, 2) perfect = self.perfecttokens repeated = round(self.stats.get('repeatedpercent', 0.0) * 100, 1) if 'repeatedpercent' in self.stats else 0.0 prompt += makedatastuff() + makedatastuff() + "and " + makedatastuff() babyendphrase = " ok, anyway... i'm going to get back to school now..." target = f"{babyname}: " + random.choice([ "hey! i am learning today, this is how i'm doing:", "ok, so, to put it as simply as i can, i am:", "ummm.. i've been trying to learn a lot about stats but numbers still confuse me a little, what's the context of all this?! like, to start with,", "i am happy! i did it! i know it! i am just a baby! i learnt it! i learned:" "oh good morning! i hope you're okay, umm, i dunno how i feel about my scores right now!!" ]) target += " " + hesjustababy target += " " + random.choice([ "thank you for teaching me new things! it's really weird to exist lol", "i'm getting tired... but i don't want to give up just yet!", "that's a lot of numbers! i don't really know what to do with them, but i'll keep trying!", "do you think i'm doing well? i'm worried that i'm underperforming!", "but, honestly, all this learning has me excited!" target += babyendphrase # build initial full text reflectiontext = prompt + " " + target _windowmax = windowmax numtargettokens = numtokensperstep totalneeded = _windowmax * 2 # tokenize reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) # 🔥 if too short, keep adding only "ok i'm gonna go back to school now" multiple times while len(reflectiontokens) < totalneeded: target += babyendphrase reflectiontext = prompt + " " + target reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) # 🔥 if it's *too long* now, you can cut the extra "ok..." parts cleanly while len(reflectiontokens) > totalneeded + _windowmax: if babyendphrase in target: # try cutting off one "ok..." at a time target = target.rsplit(babyendphrase, 1)[0] reflectiontext = prompt + " " + target reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) else: break # no more to cut # now final sliding generation inputtargetpairs = [] reflectionpointer = 0 while reflectionpointer + _windowmax + numtargettokens <= len(reflectiontokens): inputseq = reflectiontokens[reflectionpointer : reflectionpointer + _windowmax] targetseq = reflectiontokens[reflectionpointer + _windowmax : reflectionpointer + _windowmax + numtargettokens] inputtargetpairs.append((inputseq, targetseq)) reflectionpointer += 1 return inputtargetpairs calling embed.forward from: babyllm.forward <- tutor.trainstep <- tutor.trainmodel calling interneuron_network.forward from: calling neuron.forward from: interneuron_network.forward <- babyllm.forward <- tutor.trainstep <- tutor.trainmodel this is useful, particularly for error detecting and stuff so i wouldnt delete it, but does it only work on the functions? because i believe that is the same roadblock i came up against with my counsellor thingy, its really hard to track specific numbers through the system rather than just trace the algebra backwards (huh, that must be what traceback means...). its gotten a little futher than my one though because i gave up once i realised lmaooo im just trying to actually track the flow of the data through the system, which, in my head, is kinda different to the order of functions, as; the output of embed forward is called into the neurons as a 'number', but into the inn as an element of a 'function'(?). i might just be completely confused though honestly. its not a linear system and so its practically impossible to track linearly, i have been v slowly accepting over the past 2 months lmao cause right now i just have that; input from tutor -> ???(potentially logits or embed or both??)? -> neurons(x10000) -> interneuronlayer -> ???(potentially logits???)? -> ???memory??? -> guess to tutor start here : (or up if ur nerd) but i love you and i understand how it feels when someone seems to be looking with scrutiny and potential disappointment/tiredness/confusion at something clever/risky youve done, its something i feel a lot when your face is showing the tired tism too, so i understand and i am not mad at you feeling anxious. plus you already burned out of a job from that anxiety, i understand it, sorry that i didnt express well. i tried to comfort you but it made me anxious too. ::scribe:: ʕっʘ‿ʘʔっ ::polite:: ʕ •ᴥ•ʔゝ ::chaos~:: ʕノ•ᴥ•ʔノ ︵ ::writes:: ʕ•ᴥ•ʔつ✎ ::fights:: ʕっ•̀o•́ʔっ ::wahtno:: ʕ◉.◉ʔ ::ilysfm:: ʕᵔᴥᵔʔっ♥ (ෆ˙ᵕ˙ෆ)♡ ʕっʘ‿ʘʔっ♡ ʕっෆ.ෆʔっ ૮ʕʘ‿ʘ૮ʔ ʕっᵔ‿ᵔʔっ♡ ʕ•̀o•́ʔっ ʕᵔoᵔʔっ ʕᵔ‿ᵔʔっ ʕ•̀o•ʔっ ʕ•̀‿•ʔっ ʕっʘ‿ʘʔっ✎ ʕっෆ.ෆʔっ♡ ʕっʘ‿ʘʔっ ʕ•̀ᴖ•́ʔっ ʕ•̀ᴖ•ʔっ ʕっʘ.ʘʔっ❄︎ ʕฅʘ.ʘʔっ ʕっෆ.ෆʔっ❀ ʕ•ᴖ•ʔ ʕ•ᴥ•ʔつ❀ ʕっʘ‿ʘʔっ𖡼 ʕっ•̀o•́ʔっ❀ ʕᵔᴥᵔʔっ❄︎ ʕっʘ.ʘʔっ❄ ʕ❀ʘ.ʘʔっ✎ ʕ❀ෆ.ෆʔっ❀ ʕ•ᴥ•ʔつ𖤣 ʕ❀ʘ‿ʘʔっ❀ ʕ❀•̀o•́ʔっ𖡼 ʕᵔᴥᵔʔっ𓆟 𓆟 ૮ʕʘ‿ʘ૮ʔ ૮ʕʘ‿ʘʔ ʕっ꩜‿꩜ʔっ𖡼 ʕ꩜.꩜ʔっ❄ ʕ❀꩜.꩜ʔっ𖤣 ʕ⋆ᴖ⋆ʔ ૮ʕ‿.‿૮ʔᶻ 𝗓 𐰁 ૮ʕ‿.‿ᶻʔ𝗓 𐰁 ʕ‿.‿ᶻʔ𝗓 𐰁 ʕ꩜‿꩜ʔっ❀ ʕっ‿.‿ʔっ✎ ʕっ‿.ෆʔっ♡ ʕっෆ.‿ʔっ♡ ʕ•̀‿•́ʔっ ฅ^._.^ฅ ʕ⋆ᴥ⋆ʔっ𓆟 ૮ʕ♡‿♡ʔ ʕっ✰.✰ʔっ𖡼 ʕっ•̀o•́ʔっ✰✰⋆⋆ ʕ✰.✰ʔっ❄︎ ʕ♡ᴥ♡ʔっ𓆟 ᶻ 𝗓 𐰁 ᶻ 𝗓 𐰁ᶻ 𝗓 𐰁ᶻ 𝗓 𐰁 ᶻ 𝗓 𐰁ᶻ 𝗓 𐰁 ﮩ٨ـﮩﮩ٨ـ♡ﮩ٨ـﮩﮩ٨ـ 𓆝 𓆟 𓆞 𓆝 𓆟 𓆞 𓆝 ⋆⋆☾⋆⋆ 𓂸 < how 𖡼.𖤣𖥧𖡼.𖤣𖥧𖡼.𖤣𖥧 arm variations lol: -- >> ʕ⊃ʘ‿ʘʔ⊃ ʕ⊃ʘ‿ʘʔっ ʕっʘ‿ʘʔ⊃ -- .> ʕ❀ʘ‿ʘʔ⊃ ʕʘ‿ʘʔ⊃ ʕ❀ʘ‿ʘʔっ ʕʘ‿ʘʔっ -- >. ʕ⊃ʘ‿ʘʔ ʕっʘ‿ʘʔ -- <> ⊂ʕʘ‿ʘʔ⊃ ⊂ʕʘ‿ʘʔっ ςʕʘ‿ʘʔ⊃ ૮ʕʘ‿ʘʔ⊃ -- << ⊂ʕʘ‿ʘ⊂ʔ ⊂ʕʘ‿ʘςʔ ⊂ʕʘ‿ʘ૮ʔ ςʕʘ‿ʘ⊂ʔ ςʕʘ‿ʘςʔ ςʕʘ‿ʘ૮ʔ ૮ʕʘ‿ʘ⊂ʔ ૮ʕʘ‿ʘςʔ -- <. ⊂ʕʘ‿ʘ❀ʔ ⊂ʕʘ‿ʘʔ ςʕʘ‿ʘ❀ʔ ςʕʘ‿ʘʔ ૮ʕʘ‿ʘ❀ʔ -- >< ʕ⊃ʘ‿ʘ⊂ʔ ʕ⊃ʘ‿ʘςʔ ʕ⊃ʘ‿ʘ૮ʔ ʕっʘ‿ʘ⊂ʔ ʕっʘ‿ʘςʔ ʕっʘ‿ʘ૮ʔ -- . ʕʘ‿ʘʔ ʕ❀ʘ‿ʘʔ ʕʘ‿ʘ❀ʔ ₓ ʕっₓᴥₓʔっ ꩜꩜꩜ ▼△▼△▼△▼ ▼△▼△▼△▼ ♪¸¸.•*¨*•. ♪¸¸.•*¨*•. ♪¸¸.•*¨*•.♪¸¸.•*¨*•.♪¸¸.•*¨*•. ♪¸¸.•*¨*•. ｡ₓ ू ₒ ु ˚ ू ₒ ु ₓ｡ 𖨆♡𖨆 𖨆♡𖨆 𖨆♡𖨆 『••✎••』『••✎••』 paramable: 𐂂 ✰ ✰ ✰ ✰ ◡̈ ◡̈ ◡̈ ◡̈ the lore begins, with our boi scribeee! scribe_emotes = { "default": [ "ʕっʘ‿ʘʔっ", "ʕ•̀‿•ʔっ", "ʕᵔᴥᵔʔっ", "ʕっෆ.ෆʔっ", "ʕ✰.✰ʔっ", "ʕっ•̀o•́ʔっ", "ʕᵔ‿ᵔʔっ♡" ], "chaos": [ "ʕノ•ᴥ•ʔノ ︵", "ʕっ꩜‿꩜ʔっ𖡼", "ʕ◉.◉ʔ", "ʕっ‿.ෆʔっ♡" "love": [ "ʕᵔᴥᵔʔっ♥", "ʕっෆ.ෆʔっ♡", "ʕᵔ‿ᵔʔっ♡", "ʕっ✰.✰ʔっ❀" "writes": [ "ʕ•ᴥ•ʔつ✎", "ʕっ‿.‿ʔっ✎", "ʕ❀ʘ.ʘʔっ✎" def scribesay(message, vibe="default", tag="scribe"): emote = random.choice(scribe_emotes.get(vibe, scribe_emotes["default"])) timestamp = time.strftime("%h:%m:%s") print(f"{emote} [{tag.upper()}] {timestamp} — {message}") ʕっ꩜‿꩜ʔっ✎💤ʕっ꩜‿꩜ʔっ✎💤ʕっ꩜‿꩜ʔっ✎💤 ʕっ꩜‿꩜ʔっ✎💤 ʕっ꩜‿꩜ʔっ✎💤 pass - carry on the code, noop, can get out of ifs if theyre blocking st continue - go back to start of for loop for next item return - end the function aka 'def' break - get out of loop carry on function tension and release. attention and withdrawal. silence and eruption. attraction and revulsion. rhythm and stillness. curiosity and retreat. traininglogfreq_1000 = 10 from collections import counter stats = counter({ "loss": 0, "grad norm": 0, "logit min": 0, "logit max": 0, }) for _ in range(10): for _ in range(10): stats.update({ "loss": random.random(), "grad norm": random.random(), }) avgstats = {} for key, value in stats.items(): avgstats[key] = value / traininglogfreq_1000 if traininglogfreq_1000 > 0 else 0 print(" ".join([f"{key}: {(value/traininglogfreq_1000 if traininglogfreq_1000 > 0 else 0):.2f}" for key, value in stats.most_common()])) stats.clear() #  import code vars = globals().copy() vars.update(locals()) code.interact(local=vars) # for attr in dir(babyllm): if not attr.startswith("__"): print(attr, "→", getattr(babyllm, attr)) # geepy thingy test # def deep_inspect(obj, prefix="babyllm", depth=0, max_depth=2): indent = " " * depth if depth > max_depth: return print(f"{indent}inspecting... {prefix} ({type(obj).__name__})") for attr in dir(obj): if attr.startswith("__"): continue try: value = getattr(obj, attr) if callable(value): print(f"{indent} {attr} → <function>") elif isinstance(value, (int, float, str, bool, torch.tensor)): val_str = value.detach().cpu().numpy() if isinstance(value, torch.tensor) else value print(f"{indent} {attr} → {val_str}") else: print(f"{indent} {attr} → {type(value).__name__}") babyllm.deep_inspect(value, prefix=f"{prefix}.{attr}", depth=depth+1, max_depth=max_depth) except exception as e: print(f"{indent} {attr} → <error reading value: {e}>") # def hud_fixscroll(self): height = shutil.get_terminal_size().lines #reserved_hud_lines = 5 #training_lines_height = height - reserved_hud_lines #sys.stdout.write("\033[?25l\033[h\033[2j") # hide cursor, clear, move to top #sys.stdout.flush() # you should print training lines here *before* calling this if you want control # move to bottom section and draw hud training_lines_height = training_lines_height sys.stdout.write(f"\033[{training_lines_height + 1};0h") # move to hud zone sys.stdout.flush() self.printhud( windowweights=(f.softmax(self.parallelneuronlayer.windowweighting, dim=0) + 0.1).detach().cpu().numpy(), guesshud=self.guesshud sys.stdout.write(f"\033[{height};0h") # move cursor just above hud for next cycle sys.stdout.flush() #  self.weights = nn.parameter(torch.randn(vocabsize, embeddimension, device = modeldevice)) self.n_weights = nn.parameter(torch.randn(numneurons, embeddimension, device = modeldevice) * 0.01) self.n_biases = nn.parameter(torch.zeros(numneurons, device = modeldevice)) self.cerebellum = nn.parameter(torch.ones(len(allwindowsizes_new), device = modeldevice)) # this was the window weighting layer self.windowcombos = nn.modulelist([nn.linear(numneurons, numneurons, device = modeldevice) for _ in range(len(allwindowsizes_new))]) self.queryproj = nn.linear(numneurons, embeddimension, bias=true, device=modeldevice) self.keyproj = nn.linear(numneurons, embeddimension, bias=true, device=modeldevice) self.judgebias = nn.parameter(torch.zeros(len(allwindowsizes_new), device = modeldevice)) self.credibilitybias = nn.parameter(torch.zeros(len(allwindowsizes_new), device = modeldevice)) self.weights = nn.parameter(torch.randn(numneurons, vocabsize, device = modeldevice)) # this is set to move the neuron activations (10000) onto vocab size (2000) self.bias = nn.parameter(torch.zeros(vocabsize, device = modeldevice)) self.shorttermdecay = nn.parameter(torch.tensor(0.7, device = modeldevice)) self.longtermdecay = nn.parameter(torch.tensor(0.95, device = modeldevice)) self.shorttermmemory = torch.zeros(numneurons, device = modeldevice) self.longtermmemory = torch.zeros(numneurons, device = modeldevice) self.shortgate = nn.parameter(torch.tensor(0.25, device = modeldevice)) self.longgate = nn.parameter(torch.tensor(0.25, device = modeldevice)) self.currentgate = nn.parameter(torch.tensor(0.5, device = modeldevice)) class babyllm(nn.module): self.embed = embed(vocabsize, embeddimension) self.interneuronnetwork = interneuron_network() self.logits = logits(numneurons = numneurons, vocabsize = vocabsize) self.memory = memory(numneurons = numneurons) self.optimizer = optimizerclass( list(self.embed.parameters()) + list(self.interneuronnetwork.parameters()) + list(self.logits.parameters()) + list(self.memory.parameters()), lr=learningrate, weight_decay=0.001 ) embed def forward(self, tokenindex): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("tokenindex.to(self.weights.device)") tokenindex = tokenindex.to(self.weights.device) ʕっʘ‿ʘʔっ("self.weights[tokenindex]") embedvector = self.weights[tokenindex] return embedvector neuron def forward(self, inputembeds): # embed: (batch_size, embed_size) with self.n_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("computebatcheddotproduct+bias") # compute batched dot product + bias: (batch_size, num_neurons) output = torch.matmul(inputembeds, self.n_weights.t) + self.n_biases ʕっʘ‿ʘʔっ("activationfunction") # magic activation function applied to this weighted sum, which outputs a single number from the neuron output = activationfunction(output) return torch.clamp(output, -5, 5) interneuron_network def forward(self, inputembeds): with self.inn_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # --- iterates through input embeddings, applies all neurons in parallel for each, produces a vector of neuron outputs ʕっʘ‿ʘʔっ("localparaminit") # avoiding self - parameters only used in this function and never passed tinywindowcount = 0 # --- do not take anything to self past here, it should all pass through backward without saving! --- # ʕっʘ‿ʘʔっ("call neuron forward") neuronactivations = self.neurons(inputembeds) ʕっʘ‿ʘʔっ("windowoutputs") # --- combine activations into their own learnable layer windowoutputs = [] if debugprints: for name, param in self.named_parameters(): print(name, param.requires_grad, param.grad is not none) for windowsize in allwindowsizes_new: if inputembeds.shape[0] < windowsize: ʕっʘ‿ʘʔっ("not enough tokens for window (neurons.n_weights.mean)") # --- not enough tokens for this window; use a zero vector tinywindowcount += 1 #summary = torch.zeros_like(numneurons, device = modeldevice) summary = neuronactivations.mean(dim=0) * 0 # keeps gradients flowing even when zero - shape: [numneurons], safe to stack else: # mean pooling over the last 'windowsize' token activations ʕっʘ‿ʘʔっ("mean pooling all over all tokens (torch.mean)") # --- mean is over window size summary = torch.mean(neuronactivations[-windowsize:], dim=0) ʕっʘ‿ʘʔっ("append window summaries") if debugprints: for name, param in self.named_parameters(): print(name, param.requires_grad, param.grad is not none) windowoutputs.append(summary) ʕっʘ‿ʘʔっ("windowoutputtensor") # stack summaries into a tensor of shape (num_windows, numneurons) windowoutputstensor = torch.stack(windowoutputs, dim=0) # shape: (32, numneurons) # project summaries to queries and keys for attention scoring ʕっʘ‿ʘʔっ("cerebellumsoft") self.cerebellumsoft = f.softmax(self.cerebellum, dim=0) # this was the window weighting layer ʕっʘ‿ʘʔっ("query") query = self.queryproj(windowoutputstensor) + self.judgebias.unsqueeze(1) + self.cerebellum.unsqueeze(1) # shape: (32, numneurons) ʕっʘ‿ʘʔっ("key") key = self.keyproj(windowoutputstensor) + self.credibilitybias.unsqueeze(1) + self.cerebellum.unsqueeze(1) # shape: (32, numneurons) # compute attention scores between every pair of windows (32x32 matrix) ʕっʘ‿ʘʔっ("scores") #scores = torch.matmul(query, key.t) / math.sqrt(embeddimension) # shape: (32, 32) scores = torch.matmul(query, key.t) / temperature ʕっʘ‿ʘʔっ("selfscores & peerscores") # separate self scores (diagonal) and peer scores (off-diagonals) selfscores = torch.diag(scores) # self score for window i: scores[i, i] (shape: (32,)) peerscores = scores.sum(dim=0) - selfscores # peer scores for window j: sum of scores[i, j] for all i != j (shape: (32,)) ʕっʘ‿ʘʔっ("combinedscores") combinedscores = selfscores + peerscores # shape: (32,) softcombinedscores = f.softmax(combinedscores, dim=0) attentionwindowweights = softcombinedscores # shape: (32,), sum of weights = 1 ʕっʘ‿ʘʔっ("♥getwindowentropy") print(f"attentionwindowweights: {attentionwindowweights}") windowentropy = -torch.sum(attentionwindowweights * torch.log(attentionwindowweights + 1e-12)).item() if debugprints: print(windowentropy) ʕっʘ‿ʘʔっ("weightedwindows") # weight each window's output (summary) by its soft combined scores (attention weight) and sum weightedwindows = windowoutputstensor * attentionwindowweights.unsqueeze(1) # shape: (32, numneurons) ʕっʘ‿ʘʔっ("windowcontextvector") windowcontextvector = weightedwindows.sum(dim=0, keepdim=true) # shape: (1, numneurons) ʕっʘ‿ʘʔっ("finalactions") if tinywindowcount > 0: print(f"saw {neuronactivations.shape[0]} tokens; created {tinywindowcount} empty windows.") return windowcontextvector logits def forward(self, meanactivationstensor): imports the activations from interneuronnetwork, assuming that is is a tensor activationstensor = meanactivationstensor activationstensor = activationstensor.to(modeldevice) if debugprints: print(f"debug logits: activationstensor shape before @ weights: {activationstensor.shape}") if debugprints: print(f"debug logits: weights shape: {self.weights.shape}") return logits (not softmax) for better gradient computation in cross-entropy loss logitoutput = activationstensor @ self.weights + self.bias return logitoutput memory def forward(self, combinedactivationstensor): ʕっʘ‿ʘʔっ("tensorstodevice") device = self.shorttermmemory.device combinedactivationstensor = combinedactivationstensor.to(device) ʕっʘ‿ʘʔっ("sigmoid gate decays") # make sure decay values stay within [0, 1] range shortdecay = torch.sigmoid(self.shorttermdecay) longdecay = torch.sigmoid(self.longtermdecay) ʕっʘ‿ʘʔっ("updatememories") # update memories with learned decay rates newshorttermmemory = (shortdecay * self.shorttermmemory) + ((1 - shortdecay) * combinedactivationstensor) newlongtermmemory = (longdecay * self.longtermmemory) + ((1 - longdecay) * combinedactivationstensor) ʕっʘ‿ʘʔっ("copylongtermmemories") oldlongtermmemory = self.longtermmemory.clone() newlongtermmemory = (longdecay * oldlongtermmemory) + ((1 - longdecay) * combinedactivationstensor) self.longtermmemory = newlongtermmemory ʕっʘ‿ʘʔっ("copyshorttermmemories") oldshorttermmemory = self.shorttermmemory.clone() newshorttermmemory = (shortdecay * oldshorttermmemory) + ((1 - longdecay) * combinedactivationstensor) self.shorttermmemory = newshorttermmemory ʕっʘ‿ʘʔっ("loggatesizes") # log the memory gate sizes gatesum = self.shortgate + self.longgate + self.currentgate + 1e-9 self.latestmemorygates = torch.stack([ self.shortgate / gatesum, self.longgate / gatesum, self.currentgate / gatesum]) ʕっʘ‿ʘʔっ("blendmemories") # blend memories using weighted sum of the memories, using gates as weights blendedactivations = ( self.shortgate * self.shorttermmemory) + ( self.longgate * self.longtermmemory) + ( self.currentgate * combinedactivationstensor) return blendedactivations babyllm def forward(self, inputseq): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # processes input sequence of tokens (str) to generate logits to predict the next token if debugprints: print(f"debug: input to forward: {inputseq}") ʕっʘ‿ʘʔっ("inputindices") # convert inputted tokens to indices (batch processing instead of looping) inputindices = [vocab.tokentoindex.get(tokenstring, vocab.tokentoindex["<unk>"]) if not isinstance(tokenstring, int) else tokenstring for tokenstring in inputseq] ʕっʘ‿ʘʔっ("inputembeds") # convert indices to embeddings inputembeds = [] inputindicestensor = torch.tensor(inputindices, device = modeldevice) inputembeds = self.embed(inputindicestensor) ʕっʘ‿ʘʔっ("interneuronnetworkoutput") # parallel neuron layer input/processing (feature extraction) interneuronnetworkoutput = self.interneuronnetwork.forward(inputembeds) if debugprints: print(f"debug babyllm.forward: interneuronnetworkoutput length: {len(interneuronnetworkoutput)}") ʕっʘ‿ʘʔっ("combinedactivationstensor") # resize neuron layer to standard size for combined forward processing #combinedactivationstensor = torch.mean(interneuronnetworkoutput, dim=0, keepdim=true) combinedactivationstensor = interneuronnetworkoutput if debugprints: print("combinedactivationstensor.requires_grad:", combinedactivationstensor.requires_grad) if debugprints: print("combinedactivationstensor.grad_fn:", combinedactivationstensor.grad_fn) ʕっʘ‿ʘʔっ("memorylayer") # memory layer processing - now process the combined activations memoryoutput = self.memory.forward(combinedactivationstensor) latestmemgates = self.memory.latestmemorygates combinedactivations = memoryoutput ʕっʘ‿ʘʔっ("logits.forward") logits = self.logits.forward(combinedactivations) returns a logits tensor of shape (1, vocabsize) showing predicted probabilities for the next token return logits, interneuronnetworkoutput, inputembeds, latestmemgates, self.memory.longtermmemory, self.memory.shorttermmemory # charis cat 2025 # nice terminal output and logging styling sheet thing # brain/layers/s_output.py from datetime import datetime import re, torch, operator, random, math class s_output: def __init__(self, _counsellor): #self.counsellor = counsellor("s_output", debug = debugprints, durations = durationlogging) self.rollingaverages = none self.s_statbands = none # lazy-load this later self.cantprint = 0 self.allkeys = none # terminal codes reset = "\033[0m" # normal terminal bold = "\033[1m" dim = "\033[2m" # reduces intensity of text colour underline = "\033[4m" flash = "\033[5m" italic = "\033[3m" # colours!!! purple = "\033[94m" purple_pale = "\033[38;5;225m" #256 colour palette magenta = "\033[35m" blue = "\033[34m" orange = "\033[38;5;52m" #256 colour palette red = "\033[38;5;124m" #256 colour palette red_bright = "\033[91m" # red-blue scale redred_ = "\033[38;5;196m" red_ = "\033[38;5;161m" redpurp_ = "\033[38;5;126m" purpred_ = "\033[38;5;91m" purpblue_ = "\033[38;5;56m" # blue-pink scale blue_ = "\033[38;5;21m" bluepurp_ = "\033[38;5;57m" purp_ = "\033[38;5;93m" purppink_ = "\033[38;5;129m" pink_ = "\033[38;5;165m" pinkpink_ = "\033[38;5;201m" # sdjfslfs a = "\033[38;5;196m" b = "\033[38;5;160m" c = "\033[38;5;161m" d = "\033[38;5;162m" e = "\033[38;5;163m" f = "\033[38;5;164m" g = "\033[38;5;127m" h = "\033[38;5;134m" i = "\033[38;5;135m" j = "\033[38;5;99m" k = "\033[38;5;63m" l = "\033[38;5;27m" m = "\033[38;5;33m" n = "\033[38;5;39m" # extra colours gold = "\033[93m" red_alt = "\033[31m" green = "\033[32m" yellow = "\033[33m" purple_alt = "\033[35m" cyan = "\033[36m" white = "\033[37m" # terminal output styles - category mapping self.s_types = { "superperfect": [gold], # new top score ever // if above max "perfect": [n], # 0.2 // -4.8 top score ever // if below max and above almost perf "almostperfect": [m], #[pinkpink_], #[bold, magenta], # 5 // -5 "supergreat": [l], #[pink_], #[magenta], # 10 // -5 "great": [k], #[purppink_], #[bold, purple], # 15 // -5 "good": [j], #[purp_], #[purple], # 20 // -15 "fine": [i], #[bluepurp_], #[purple], # 35 // -15 "almostfine": [h], #[blue_], #[bold, blue], # 50 // "average": [g], #[purpblue_], #[blue], # 65 // +15 "meh": [f], #[purpred_], #[bold, cyan], # 80 // +15 "bad": [e], #[purpred_], #[cyan], # 85 // +5 "worse": [d], #[redpurp_], #[orange], # 90 // +5 "wtf": [c], #[redpurp_], #[bold, orange], # 95 // +5 "omg": [b], #[red_], # 99.8 // +4.8 "omgwtf": [a], #[redred_], # 100.00 // bottom score ever // if above min and below omg "omgwtf!": [bold, redred_], #[cyan], # new bottom score ever // if below min "emergency": [bold, green], "italic": [italic], "underline": [underline], "reset": [reset], # normal terminal "dim": [reset, dim], # dim style for background elements - arrows, colons, etc. "bold": [bold], "match": [bold], "static": [dim, purple_pale] } for key, pkey in {"superperfect": 0.0, # this works to show top record in super perfect direction, as it will be less than the min value "perfect": 0.2, "almostperfect": 5, "supergreat": 10, "great": 15, "good": 20, "fine": 35, "almostfine": 50, "average": 65, "meh": 80, "bad": 85, "worse": 90, "wtf": 95, "omg": 99.8, "omgwtf": 100.0,}.items(): # this uses infinite fallback for omgwtf! in getdynamicpercentilebands so that it can show 'worst ever' self.s_types[pkey] = self.s_types[key] # percentiles = [99.99, 95, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0.01] # perchjcjed = [99.99, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.01] #percentiles = percentilebands # percentile calcs # new top score ever #superperfect # top score ever #perfect ෆp97 = 97.5 #almostperfect ෆp95 = 95 #supergreat ෆp90 = 90 #great ෆp80 = 80 #good ෆp70 = 70 #fine ෆp60 = 60 #almostfine ෆp50 = 50 #average ෆp40 = 40 #meh ෆp30 = 30 #bad ෆp20 = 20 #worse ෆp10 = 10 #wtf ෆp5 = 5 #omg # bottom score ever #omgwtf # lower than bottom ever #omgwtf! self.avgplz = ["embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "logitmin", "shortdecay", "longdecay", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_weightnormmean", "n_weightnormmin", "n_weightnormmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellummean", "inn_cerebellumstd"] return def s_generatestatbands(self): softmaxbands = {"omgwtf!": -float('inf'), "omgwtf": 0.0250, "omg": 0.0500, "wtf": 0.1000, "worse": 0.2000, "bad": 0.3000, "meh": 0.4000, "average": 0.5000, "almostfine": 0.6000, "fine": 0.7000, "good": 0.8000, "great": 0.9000, "supergreat": 0.9500, "almostperfect":0.9750, "perfect": 0.9875, "superperfect": float('inf'),} staticband = {"fine": float('inf')} return {v: self.getdynamicpercentilebands(v) for v in ((mostimportantstats + allrecordedotherstats) if self.allkeys is none else self.allkeys)} return { "loss": self.getdynamicpercentilebands("loss"), "avgloss": self.getdynamicpercentilebands("avgloss"), "steploss": self.getdynamicpercentilebands("steploss"), "scheduledsamplingrate": self.getdynamicpercentilebands("scheduledsamplingrate"), "tokencount": self.getdynamicpercentilebands("tokencount"), "trainingstepcount": self.getdynamicpercentilebands("trainingstepcount"), "repetitionpenalty": self.getdynamicpercentilebands("repetitionpenalty"), "gradnorm": self.getdynamicpercentilebands("gradnorm"), "temperature": self.getdynamicpercentilebands("temperature"), "sampledtokens": self.getdynamicpercentilebands("sampledtokens"), "pt%": self.getdynamicpercentilebands("pt%"), "latestlossdelta": self.getdynamicpercentilebands("latestlossdelta"), "gradientclipmaxnorm": self.getdynamicpercentilebands("gradientclipmaxnorm"), "lr": self.getdynamicpercentilebands("lr"), "repetitionwindow": self.getdynamicpercentilebands("repetitionwindow"), "windowsizesmean": self.getdynamicpercentilebands("windowsizesmean"), "windowweight": self.getdynamicpercentilebands("windowweight"), # neuron stats "n_weightmean": self.getdynamicpercentilebands("n_weightmean"), "n_weightstd": self.getdynamicpercentilebands("n_weightstd"), "n_weightmin": self.getdynamicpercentilebands("n_weightmin"), "n_weightmax": self.getdynamicpercentilebands("n_weightmax"), "n_biasesmean": self.getdynamicpercentilebands("n_biasesmean"), "n_biasesstd": self.getdynamicpercentilebands("n_biasesstd"), "n_biasesmin": self.getdynamicpercentilebands("n_biasesmin"), "n_biasesmax": self.getdynamicpercentilebands("n_biasesmax"), "n_sparsity": self.getdynamicpercentilebands("n_sparsity"), # inn stats "inn_cerebellum": self.getdynamicpercentilebands("inn_cerebellum"), "inn_cerebellumsoft": self.getdynamicpercentilebands("inn_cerebellumsoft"), "inn_cerebellummean": self.getdynamicpercentilebands("inn_cerebellummean"), "inn_cerebellumstd": self.getdynamicpercentilebands("inn_cerebellumstd"), # memory stats "shortdecay": self.getdynamicpercentilebands("shortdecay"), "longdecay": self.getdynamicpercentilebands("longdecay"), "latestmemorygates": self.getdynamicpercentilebands("latestmemorygates"), "memorylength": self.getdynamicpercentilebands("memorylength"), # embed stats "embednormmean": self.getdynamicpercentilebands("embednormmean"), "embednormstd": self.getdynamicpercentilebands("embednormstd"), "embednormmax": self.getdynamicpercentilebands("embednormmax"), "embeddimensionmean": self.getdynamicpercentilebands("embeddimensionmean"), "embeddimensionsparsity": self.getdynamicpercentilebands("embeddimensionsparsity"), "embeddingdrift": self.getdynamicpercentilebands("embeddingdrift"), # logit stats "logitmin": self.getdynamicpercentilebands("logitmin"), "logitmax": self.getdynamicpercentilebands("logitmax"), "logitseq": self.getdynamicpercentilebands("logitseq"), "logitweightnormmean": self.getdynamicpercentilebands("logitweightnormmean"), "logitweightnormstd": self.getdynamicpercentilebands("logitweightnormstd"), "logitweightnormmax": self.getdynamicpercentilebands("logitweightnormmax"), "logitweightsparsity": self.getdynamicpercentilebands("logitweightsparsity"), "logitweightdrift": self.getdynamicpercentilebands("logitweightdrift"), "logitbiasmean": self.getdynamicpercentilebands("logitbiasmean"), "logitbiasstd": self.getdynamicpercentilebands("logitbiasstd"), "logitbiasmax": self.getdynamicpercentilebands("logitbiasmax"), def getdynamicpercentilebands(self, statkey): if not self.rollingaverages: self.cantprint += 1 if self.cantprint > 10: print("ʕっ-ᴥ-ʔっ no stat buffers found x10!") self.cantprint = 0 return {"dim": -float('inf')} values = self.rollingaverages.get(statkey, []) if len(values) < 2: if statkey in mostimportantstats or statkey.startswith("inn_cerebellum_w"): #values is dict: keylist = {f"{printfreq}": printfreq, f"{traininglogfreq_a}": traininglogfreq_a, f"big{traininglogfreq_a}": traininglogfreq_a} requiredkey = list(keylist.keys())[0] for key, freq in keylist.items(): if key in values and len(values[key]) >= freq: requiredkey = key bands = {"omgwtf!": float("inf")} keymatch = f"{requiredkey}_p" keylen = len(keymatch) for k, v in values.items(): if k.startswith(keymatch): bands[float(k[keylen:])] = v return dict(sorted(bands.items(), key = lambda item: item[1]), reversed = true) else: stat = sorted(values) #print(f"→ generating bands for '{statkey}'") #print(f" values: {values}") return{"superperfect": -float('inf'), # make same as the others lol "perfect": self.getp(stat, 0.0001), "almostperfect": self.getp(stat, 0.0010), "supergreat": self.getp(stat, 0.0100), # purple_pale "great": self.getp(stat, 0.1000), "good": self.getp(stat, 0.2000), "fine": self.getp(stat, 0.3000), "almostfine": self.getp(stat, 0.4000), "average": self.getp(stat, 0.5000), "meh": self.getp(stat, 0.6000), "bad": self.getp(stat, 0.7000), "worse": self.getp(stat, 0.8000), "wtf": self.getp(stat, 0.9000), "omg": self.getp(stat, 0.9500), "omgwtf": self.getp(stat, 0.9990), "omgwtf!": float('inf'),} def s_getstat(self, _stattype, _statval): with self.counsellor.infodump("s_getstat") as ʕっʘ‿ʘʔっ: values = self.rollingaverages.get(_stattype, []) if self.rollingaverages else [] if not values or len(values) < 2: if debugprints: print(f"returning a dim color for stat {_stattype} and value {_statval} (values is {values} (key present:{_stattype in self.rollingaverages if self.rollingaverages is not none else 'false'}))") return "dim" if self.s_statbands is none: self.s_statbands = self.s_generatestatbands() bands = self.s_statbands.get(_stattype, {}) for label, limit in bands.items(): if _statval <= limit: if _stattype == "loss" and debugprints: print(f"ok here is the selected label: {label} for value {_statval} and bands: {bands}") return label print(f"returning an emergency color for stat {_stattype} and value {_statval} (bands is {bands})") return "emergency" def refreshstatbands(self, _rollingaverages): self.rollingaverages = _rollingaverages if self.rollingaverages and all(len(v) > 1 for v in self.rollingaverages.values()): self.s_statbands = self.s_generatestatbands() print("ʕっ•ᴥ•ʔっ not enough data to refresh stat bands yet") def s_apply(self, _s_type, _text): with self.counsellor.infodump("s_apply") as ʕっʘ‿ʘʔっ: return "".join(self.s_types.get(_s_type, [])) + str(_text) + "".join(self.s_types.get('reset')) def s_stripforlogging(self, _text): with self.counsellor.infodump("s_stripforlogging") as ʕっʘ‿ʘʔっ: return re.sub(r'\x1b(?:[@-z\\-_]|\[[0-?]*[ -/]*[@-~])', '', _text) def s_colourprinttraining(self, _step, _inputseq, _guessedseq_str, _targetseq_str, _loss, _recentloss, _latestlossdelta, _totalloss = none, _totaltokencount = none): with self.counsellor.infodump("s_colourprinttraining") as ʕっʘ‿ʘʔっ: s_type = self.s_getstat("loss", _loss) s_avgtype = self.s_getstat("avgloss", _recentloss) s_delta = _latestlossdelta s_deltatype = self.s_getstat("latestlossdelta", _latestlossdelta) s_bold = "".join(self.s_types["bold"]) ʕっʘ‿ʘʔっ("conditionalformatguess+truth") reset = "".join(self.s_types.get('reset')) dim = "".join(self.s_types.get('dim')) guess = [ f"{s_bold}{t}{reset}" if i < len(_targetseq_str) and t == _targetseq_str[i] else self.s_apply(s_type, t) for i, t in enumerate(_guessedseq_str) ] truth = [ f"{s_bold}{dim}{t}{reset}" if i < len(_guessedseq_str) and t == _guessedseq_str[i] else f"{dim}{self.s_apply(s_type, t)}" for i, t in enumerate(_targetseq_str) ʕっʘ‿ʘʔっ("createtextstrings") guess_str = "".join(guess).replace("ġ", " ") truth_str = "".join(truth).replace("ġ", " ") match = guess_str.strip() == truth_str.strip() if match: s_type = "match" prompt_str = ''.join(_inputseq).replace("ġ", " ").strip()[-printpromptlength:] delta_str = "" ʕっʘ‿ʘʔっ("calculatelossdelta") # calculate delta if _recentloss is not none: delta = _recentloss - _loss delta_str = f"{self.s_apply('dim', 'δ')}{self.s_apply(s_deltatype, f'{s_delta:+.4f}')}{'↗' if s_delta < 0 else '↘'}" rollingavgloss_str = "" #if self.rollingaverages and "loss" in self.rollingaverages: # losses = self.rollingaverages["loss"] # if losses: # rollingavgloss = sum(losses) / len(losses) # rollingavgloss_str = f"{self.s_apply(s_type, f'{rollingavgloss:.3f}')}{self.s_apply('dim', 'mean ')}" ʕっʘ‿ʘʔっ("printguess+truth") print(f"{self.s_apply('dim', f'{_step}')}|{self.s_apply('dim', prompt_str)}|{self.s_apply('dim', 'loss: ')}{self.s_apply(s_type, f'{_loss:.4f}')}{self.s_apply('dim', '/1 ')}" + (f"{self.s_apply(s_avgtype, f'{_recentloss:.4f}')}{self.s_apply('dim', f'/{traininglogfreq_a} ')}" if _recentloss else "") + rollingavgloss_str + delta_str + "|\n" + f"{self.s_apply('dim', 'guess → ')}{guess_str}{self.s_apply(s_type, ' [!] ') if match else self.s_apply('dim', ' [?] ')}\n" + f"{self.s_apply('dim', 'truth → ')}{truth_str}{self.s_apply('dim', ' | ')}\n") if debugprints: print(f"→ style applied for {_loss=} = {s_type}") def s_logtraining(self, _traininglogpath, _trainingstepcounter, _stats, _frequency, _detailedlogging, _savelog, _lr = learningrate, _inn_cerebellum_str="", _toptokens_str="", _prompt="", _guess="", _truth="", _otherinfo_str=""): with self.counsellor.infodump("s_logtraining") as ʕっʘ‿ʘʔっ: logoutput = "" timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') delimiter = self.s_apply("dim", " | ") newlinedelim = self.s_apply("dim", " | \n") ʕっʘ‿ʘʔっ("avgstats") #donotaverage = ["avgloss", "tokencount", "scheduledsamplingrate", "gradnorm", "topwindowweight", "windowentropy", "effectivewindowcount", "windowstd", "memorygatemean", "memorygatestd", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellum", "inn_cerebellumsoft", "inn_cerebellummean", "inn_cerebellumstd", "shortdecay", "longdecay"] #avgstats = {k: raw if k in donotaverage else (raw / _freq if _freq else 0) for k, raw in _stats.items()} avgstats = {k: (v / _frequency if _frequency else 0) if self.willitaverage(k, v) else v for k, v in sorted(_stats.items()) if k != "embeddimensionmean" and k != "latestmemorygates"} self.allkeys = _stats.keys() # ok, so... we need to pad: # add 1 for the sign, # 1 for the decimal dot and # 1 for the fact that log is missing 1 (i.e. log10([100-1000[) is in [2,3[, when 100 takes 3 chars) declen = 6 stattoplen = math.trunc(declen + 1 + 1 + 1 + math.log(max(max(avgstats.values()), abs(min(avgstats.values()))), 10)) stattoplen = 10 print(f"failed getting stattoplen for avgstats: {avgstats} {e}") stampandstep = delimiter.join([self.s_apply("dim", timestamp), self.s_apply("dim", f"{_trainingstepcounter:.0f}"), self.s_apply("dim", f"lr{_lr:.{declen}f}")]) logoutput = stampandstep littlelogoutput = stampandstep newlinelittle = stampandstep + "\n" def format_stat(k, v): try: if isinstance(v, torch.tensor): if v.numel() == 1: v = v.item() # convert scalar tensor else: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"<tensor[{v.shape}]>") return self.s_apply("dim", f"{k}:") + self.s_apply(self.s_getstat(k, v), f"{v:.{declen}f}") except exception as e: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"err:{str(e)} key:{k} value:{v}") logoutput += delimiter + delimiter.join([ format_stat(k, v) for k, v in avgstats.items() if v not in (none, "") ]) littlelogoutput += delimiter + delimiter.join([ if k in mostimportantstats newlinelittle += newlinedelim.join([ self.s_apply(self.s_getstat(k, v), f"{v:+{stattoplen}.{declen}f}") + " " + self.s_apply("dim", k) ]) + newlinedelim if _inn_cerebellum_str: ʕっʘ‿ʘʔっ("inn_cerebellum_str") cerebellum = delimiter + f"windowweights{self.s_apply('reset', _inn_cerebellum_str)}" logoutput += cerebellum littlelogoutput += cerebellum newlinelittle += "\n" + f"windowweights\n{_inn_cerebellum_str}" ʕっʘ‿ʘʔっ("toptokens_str") if _toptokens_str: toptokens = delimiter + f"toptokens{self.s_apply('reset', _toptokens_str)}" logoutput += toptokens littlelogoutput += toptokens newlinelittle += "\n" + f"toptokens{self.s_apply('reset', _toptokens_str)}" ʕっʘ‿ʘʔっ("prompt+otherinfo") if _prompt: logoutput += f"{delimiter}prompt → {self.s_apply('reset', _prompt)} | guess → {self.s_apply('reset', _guess)} | truth → {self.s_apply('reset', _truth)}" if _otherinfo_str: logoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" littlelogoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" newlinelittle += f"\n{delimiter}{self.s_apply('reset', _otherinfo_str)}" ʕっʘ‿ʘʔっ("logoutput") if _detailedlogging == true: print(logoutput + "".join(self.s_types.get('reset'))) if _savelog == true: with open(traininglogpath_1000, "a") as f: f.write(self.s_stripforlogging(logoutput) + "\n") ʕっʘ‿ʘʔっ("littlelogoutput") if _detailedlogging == false: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") if newlinebetweenstats: print(newlinelittle + "".join(self.s_types.get('reset'))) print(littlelogoutput + "".join(self.s_types.get('reset'))) if dontsaveeveryprint: if _trainingstepcounter % savefreq_littlelog == 0: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") def willitaverage(self, k, v): if k in self.avgplz: if isinstance(v, (int, float)): return true if isinstance(v, torch.tensor) and v.numel() == 1: return true return false def chaosmaths(self, _firstnumbers, _secondnumbers = none, _torch = false, _operator = true): self.t = _torch self.o = _operator operatormathsfortwo = { "add": (operator.add, 2), "sub": (operator.sub, 2), "mul": (operator.mul, 2), "div": (operator.truediv, 2), #"floordiv":(operator.floordiv, 2), #"mod": (operator.mod, 2), #"pow": (operator.pow, 2), operatormathsforone = { "neg": (operator.neg, 1), torchmathsfortwo = { "torch_add": (torch.add, 2), "torch_sub": (torch.sub, 2), "torch_mul": (torch.mul, 2), "torch_div": (torch.div, 2), "torch_pow": (torch.pow, 2), "torch_max": (torch.maximum, 2), "torch_min": (torch.minimum, 2), torchmathsforone = { "torch_abs": (torch.abs, 1), "torch_sin": (torch.sin, 1), "torch_cos": (torch.cos, 1), "torch_tanh": (torch.tanh, 1), "torch_log": (torch.log1p, 1), # safer than log(x) "torch_relu": (torch.relu, 1), "torch_sigmoid": (torch.sigmoid, 1), if _secondnumbers is not none and _secondnumbers.numel() > 0: if self.t and self.o: self.maths = {**torchmathsfortwo, **operatormathsfortwo} if self.t: self.maths = torchmathsfortwo if self.o: self.maths = operatormathsfortwo else: self.maths = {**torchmathsforone, **operatormathsforone} self.maths = torchmathsforone self.maths = operatormathsforone chosenname, (chosenfunction, _) = random.choice(list(self.maths.items())) if _secondnumbers is not none and _secondnumbers.numel() > 0: result = chosenfunction(_firstnumbers, _secondnumbers) result = chosenfunction(_firstnumbers) return result, chosenname def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes, per_window_style = false): try: triplets = sorted(zip(windowsizes, rawtensor, softtensor), key=lambda x: x[1], reverse=true) formatted = [] for w, raw, soft in triplets: raw_style = self.s_getstat(f"{label}" if not per_window_style else f"{label}_w{int(w)}", raw.item()) soft_style = self.s_getstat(f"{label}soft" if not per_window_style else f"{label}_w{int(w)}", soft.item()) chunk = f"{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.6f}')}) {self.s_apply('dim', f'w{int(w)} ({w:.2f})')}" formatted.append(chunk) return "\n".join(formatted) except exception as e: return f"<err in s_formatwindowbiastriplets: {e}>" # flat string version # def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes): triplets = sorted(zip(windowsizes, rawtensor, softtensor), key = lambda x: x[1], reverse = true) raw_style = self.s_getstat(f"{label}", raw.item()) soft_style = self.s_getstat(f"{label}soft", soft.item()) chunk = f"w{w:.0f}:{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.2f}')})" return ", ".join(formatted) return f"<err in s_formatwindowbiastriplets: {e}> def getp(self, _sortedstat, _percentile): if not _sortedstat: return 0.0 index = min(int(_percentile * len(_sortedstat)), len(_sortedstat) - 1) return _sortedstat[index] if __name__ == "__main__": print(s_apply('superperfect', "elodie is perfect")) print(s_apply('perfect', "elodie is perfect")) print(s_apply('almostperfect', "babyllm is almost perfect")) print(s_apply('supergreat', "babyllm is super great")) print(s_apply('great', "babyllm is great")) print(s_apply('good', "babyllm is good")) print(s_apply('fine', "babyllm is fine")) print(s_apply('almostfine', "charis is almost fine")) print(s_apply('average', "george is average")) print(s_apply('meh', "babyllm is meh")) print(s_apply('bad', "babyllm is bad")) print(s_apply('worse', "george is worse")) print(s_apply('wtf', "kevin is wtf")) print(s_apply('omg', "pete is omg")) print(s_apply('omgwtf', "pete is omgwtf")) print(s_apply('omgwtf', "charis is omgwtf!")) print(s_apply('emergency', "babyllm is emergency")) # charis cat 2025 # babyllm school counsellor // school/staffroom/counsellor.py # designed for detailed small scale logging throughout the project, with timing implemented for troubleshooting errors import time from contextlib import contextmanager # usage: # remember to initialise the class instance # self.counsellor = counsellor("class_name", debug = debugprints, durations = durationlogging) # in the top of your function, encasing all of your function lines, put; # with self.counsellor.infodump("function_name") as ʕっʘ‿ʘʔっ: # this will duration track the whole function, if enabled, and also make a note in debugprints when it starts and ends # anywhere you want to start a new tracker within a function, place; # ʕっʘ‿ʘʔっ("tracker_name") # these work similarly to the function trackers, but are internal points within the function. # now your code can get some fucking therapy, for once! class counsellor: def __init__(self, _classname="?", _debug = debugprints, _durations = durationlogging): self.classname = _classname self.debugprints = _debug self.durationlogging = _durations self.duration = {} self.duration_a = {} def log(self, key, value): maxlogs = 5000 if not self.durationlogging: self.duration[key] = self.duration.get(key, 0) + value if len(self.duration) > maxlogs: self.duration.clear() print(f"cleared duration_b as it was higher than {maxlogs}") self.duration_a[key] = self.duration_a.get(key, 0) + value if len(self.duration_a) > maxlogs: self.duration_a.clear() print(f"cleared duration_a as it was higher than {maxlogs}") @contextmanager def infodump(self, _functionname, _extra = none, _key = none): fulltitle = f"{self.classname}_{_functionname}" startstamp = time.time() if self.durationlogging else none if self.debugprints: line = f"ʕっʘ‿ʘʔっ starting {fulltitle}... →" if _extra: line += f" ({_extra})" print(line) class tangent: def __init__(self, _parent): self.parent = _parent self.lastinfodumptime = startstamp self.lastinfodumpname = none self.parentfunction = none self.path = [] if self.parentfunction != _functionname: self.path = [] # reset when function changes self.parentfunction = _functionname setattr(self, "ʕっʘ‿ʘʔっ", self.infodump) #somehow legal as a variable name how have i done this please protect me lol setattr(self, "(っ◕‿◕)っ", self.infodump) setattr(self, "♥‿♥", self.infodump) setattr(self, "(｡♥‿♥｡)", self.infodump) setattr(self, "infodump", self.infodump) def __call__(self, _innername): return self.infodump(_innername) def infodump(self, _innername): now = time.time() # clean function context? start new base path. self.parentfunction = _functionname self.path = [] # reset because moved to another function if isinstance(_innername, str): if _innername.startswith("♥") and _innername.endswith("♥"): self.path = [_innername.strip("♥")] # appendable base elif _innername.startswith("♥"): self.path.append(_innername[1:]) # append elif _innername.endswith("♥"): self.path = [_innername[:-1]] # new base path elif "/" in _innername or "♥" in _innername: self.path = [p.strip() for p in re.split(r"[→/♥]", _innername)] else: self.path = [_innername] self.path = [str(_innername)] # log previous section if self.lastinfodumpname: duration = now - self.lastinfodumptime tag = f"{_functionname}♥{'♥'.join(self.path[:-1] + [self.lastinfodumpname])}" self.parent.log(tag, duration) if self.parent.debugprints: print(f"♥ finished {self.parent.classname}♥{tag} in {duration:.4f}s ♥") # start new self.lastinfodumpname = self.path[-1] self.lastinfodumptime = now fulltag = f"{_functionname}♥{'♥'.join(self.path)}" if self.parent.debugprints: print(f"→ starting {self.parent.classname}♥{fulltag} →") tangent = tangent(self) yield tangent finally: if tangent.lastinfodumpname: finalduration = time.time() - tangent.lastinfodumptime finaltag = f"{_functionname}♥{'♥'.join(tangent.path)}" self.log(finaltag, finalduration) if self.debugprints: print(f"♥ finished {self.classname}♥{finaltag} in {finalduration:.4f}s ♥") if startstamp: totalduration = time.time() - startstamp self.log(_key or _functionname, totalduration) print(f"(っ◕‿◕)っ finished {fulltitle} in {totalduration:.4f}s (｡♥‿♥｡)") elif self.debugprints: print(f"(っ◕‿◕)っ finished {fulltitle} (｡♥‿♥｡)") import pandas as pd import matplotlib.pyplot as plt import mplcursors traininglogpath_a = "/users/charis/documents/github/shkaira/school/statistics/logs/training/traininglog_100_withtotalsteps.txt" # load log with open(traininglogpath_a, "r") as f: log_text = f.read() # pattern to extract total steps + all wxx weights pattern = re.compile( r"(total steps:\s*(?p<total_steps>\d+)).*?" r"([ll]oss:? ?(?p<loss>[0-9\.]+)).*?" r"([ll][rr]:? ?(?p<lr>[0-9\.]+)).*?" r"(scheduledsamplingrate:? ?(?p<scheduledsamplingrate>[0-9\.]+)).*?" r"(repetitionpenalty:? ?(?p<repetitionpenalty>[0-9\.]+)).*?" r"(memorylength:? ?(?p<memorylength>[0-9\.]+)).*?" r"(repetitionwindow:? ?(?p<repetitionwindow>[0-9\.]+)).*?" r"(temperature:? ?(?p<temperature>[0-9\.]+)).*?" r"(sampledtokens:? ?(?p<sampledtokens>[0-9\.]+)).*?" r"((?:w\d+[:\s]-?\d+\.\d+(?: \([0-9\.]+\))?(?:,?\s*w\d+[:\s]-?\d+\.\d+(?: \([0-9\.]+\))?)*)?)", re.dotall) # extract data entries = [] for match in re.finditer(pattern, log_text): loss = float(match.group('loss'))*0.1 lr = float(match.group('lr'))*25000 scheduledsamplingrate = float(match.group('scheduledsamplingrate')) repetitionpenalty = float(match.group('repetitionpenalty'))*2 temperature = float(match.group('temperature'))*2 sampledtokens = float(match.group('sampledtokens'))*0.001 repetitionwindow = float(match.group('repetitionwindow'))*0.1 memorylength = float(match.group('memorylength'))*0.1 #total_steps = match.group('total_steps') step_match = re.search(r"total steps:\s*(\d+)", match.group(0)) if not step_match: total_steps = int(step_match.group(1)) weights = dict(re.findall(r"(w\d+)[\s:]+(-?\d+\.\d+)", match.group(0))) row = {"total_steps": total_steps, "loss": loss, "lr": lr, "scheduledsamplingrate": scheduledsamplingrate, "repetitionpenalty": repetitionpenalty, "temperature": temperature, "sampledtokens": sampledtokens, "memorylength": memorylength, "repetitionwindow": repetitionwindow,} #print(row) row.update({k: float(v) for k, v in weights.items()}) entries.append(row) # create dataframe indexed by total_steps df = pd.dataframe(entries).set_index("total_steps").sort_index() # plot ax = df.plot(figsize=(14, 6)) plt.title("window weights over total training steps") plt.ylabel("weight") plt.xlabel("total steps") plt.legend(title="window", bbox_to_anchor=(1.05, 1), loc="upper left") plt.tight_layout() cursor = mplcursors.cursor(ax.lines, hover = true) cursor.connect("add", lambda sel: sel.annotation.set_text(sel.artist.get_label())) plt.show() # add the loss to the graph # fix window data from old logs to say the new thing for windows import matplotlib.cm as cm import numpy as np # load and parse log_blocks = log_text.split("\n") for block in log_blocks: row = {} step_match = re.search(r"total steps:\s*(\d+)", block) if step_match: row['total_steps'] = int(step_match.group(1)) number_match = re.search(r"^\d+\s+\|\s+\d+", block) if number_match: number = re.findall(r"\d+", block) if len(number) >= 2: row['total_steps'] = int(number[1]) fields = { 'loss': r"loss:([0-9\.]+)", 'lr': r"lr:? ?([0-9\.]+)", 'sampledtokens': r"sampledtokens:([0-9\.]+)", 'scheduledsamplingrate': r"scheduledsamplingrate:([0-9\.]+)", 'repetitionpenalty': r"repetitionpenalty:([0-9\.]+)", 'temperature': r"temperature:([0-9\.]+)", 'memorylength': r"memorylength:([0-9\.]+)", 'repetitionwindow': r"repetitionwindow:([0-9\.]+)", for key, regex in fields.items(): match = re.search(regex, block) if match: row[key] = float(match.group(1)) weight_matches = re.findall(r"(w\d+):(-?\d+\.\d+)", block) for w_name, w_val in weight_matches: row[w_name] = float(w_val) if 'total_steps' in row: entries.append(row) # remove window weights with too few unique values (flat or useless) df = df.drop(columns=[col for col in df.columns if col.startswith('w') and df[col].nunique() <= 3]) # clip outliers at 1st/99th percentile lower = df.quantile(0.01) upper = df.quantile(0.99) df_clipped = df.clip(lower = lower, upper = upper, axis = 1) # fancy scaling: 0 is neutral, pos → [0, 1], neg → [0, -1] df_scaled = pd.dataframe(index = df_clipped.index) for col in df_clipped.columns: positive = df_clipped[col].where(df_clipped[col] > 0) negative = df_clipped[col].where(df_clipped[col] < 0) scaled_col = pd.series(index = df_clipped.index, dtype = float) if positive.notna().any(): scaled_col.update(positive / positive.max()) if negative.notna().any(): scaled_col.update(negative / (-negative.min())) df_scaled[col] = scaled_col # split groups metric_cols = [col for col in df_scaled.columns if not col.startswith('w')] weight_cols = [col for col in df_scaled.columns if col.startswith('w')] # colors metric_colors = cm.get_cmap('viridis', len(metric_cols)) weight_colors = cm.get_cmap('autumn', len(weight_cols)) fig, ax1 = plt.subplots(figsize=(22, 12)) lines = [] # plot metrics (solid, coloured) for i, col in enumerate(metric_cols): color = metric_colors(i) line, = ax1.plot(df_scaled.index, df_scaled[col], label = f"[metric] {col}", color = color, linewidth = 2) lines.append(line) ax1.set_ylabel('scaled metrics', fontsize = 14) ax1.set_xlabel('total steps', fontsize = 14) ax1.grid(true) # weights (dashed, warm) ax2 = ax1.twinx() for i, col in enumerate(weight_cols): color = weight_colors(i) line, = ax2.plot(df_scaled.index, df_scaled[col], label = f"[weight] {col}", color = color, linestyle='--', linewidth = 2) ax2.set_ylabel('scaled window weights', fontsize = 14) plt.title("training metrics + window weights (cleaned & beautiful)", fontsize = 20) # smart legend (grouped + readable) labels = [line.get_label() for line in lines] ax1.legend(lines, labels, title="legend", bbox_to_anchor=(1.20, 1), loc="upper left", fontsize = 10) # hover still works cursor = mplcursors.cursor(lines, hover = true) # scribe module // school/staffroom/he_is_scribe.py from school.notebook.tools.genboi import * class scribe: def __init__(self, _counsellor, _calligraphist, _librarian): #self.counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) #self.s_output = s_output() self.librarian = _librarian self.scribeemotes = {"default": ["ʕっʘ‿ʘʔっ", "ʕᵔᴥᵔʔっ", "ʕっෆ.ෆʔっ", "ʕ✰.✰ʔっ", "ʕᵔ‿ᵔʔっ♡"], "neutral": ["ʕ •ᴥ•ʔゝ", "ʕᵔᴥᵔʔっ♥",], "annoyed": ["ʕノ•ᴥ•ʔノ ︵", "ʕっ•̀o•́ʔっ", "ʕ•̀o•́ʔっ", "ʕっ•̀o•́ʔっ✰✰⋆⋆", ], "hyper": ["ʕっ꩜‿꩜ʔっ𖡼", "ʕᵔ‿ᵔʔっ"], "worried": ["ʕ◉.◉ʔ", "ʕ꩜.꩜ʔっ❄", ], "mischevious": ["ʕ•̀‿•ʔっ", "ʕっ‿.ෆʔっ♡", "ʕ•̀o•ʔっ", "ʕ•̀‿•ʔっ", "ʕっෆ.‿ʔっ♡", "ʕ•̀‿•́ʔっ", ], "love": ["ʕᵔᴥᵔʔっ♥", "ʕっෆ.ෆʔっ♡", "ʕᵔ‿ᵔʔっ♡", "ʕっ✰.✰ʔっ❀", "ʕっʘ‿ʘʔっ♡", "ʕ❀ෆ.ෆʔっ❀", ], "hugs": ["ʕっෆ.ෆʔっ", "ʕっෆ.ෆʔっ♡", "ʕっʘ‿ʘʔっ", ], "happy": ["ʕっʘ‿ʘʔっ", "૮ʕʘ‿ʘ૮ʔ", "ʕっᵔ‿ᵔʔっ♡", "ʕᵔᴥᵔʔっ𓆟",], "writes": ["ʕ•ᴥ•ʔつ✎", "ʕっ‿.‿ʔっ✎", "ʕ❀ʘ.ʘʔっ✎", "ʕっʘ‿ʘʔっ✎", "ʕ❀‿.‿ʔっ✎", "ʕっෆ.ෆʔっ✎",], "sleepy": ["૮ʕ‿.‿ᶻʔ𝗓 𐰁", "૮ʕ‿.‿૮ʔᶻ 𝗓 𐰁", "ʕっෆ.ෆʔっ♡",], "confused": ["𓆟 ૮ʕʘ‿ʘ૮ʔ", "ʕ⋆ᴥ⋆ʔっ𓆟", "ʕ♡ᴥ♡ʔっ𓆟", "𓆟 ૮ʕʘ‿ʘ૮ʔ", ], "impressed": ["૮ʕ♡‿♡ʔ", "ʕっ✰.✰ʔっ𖡼", "ʕ✰.✰ʔっ❄︎", ]} def scribesay(self, _message, _vibe="default", _scribename="scribe"): # scribe delivers a message with random emote and timestamp. emote = random.choice(self.scribeemotes.get(_vibe, self.scribeemotes["default"])) timestamp = time.strftime("%h:%m:%s") print(f"{timestamp}|{emote} [{_scribename.upper()}] — {_message}") with open("scribesays.txt", "a") as f: f.write(f"{timestamp}|{emote} [{_scribename}]: '{_message}\n") def guesstokenstostring(self, _inputtokens): tokenstring = "".join(_inputtokens).replace("ġ", " ") return tokenstring def interviewbaby(self, _model, _prompt, _vibe="writes"): # scribe asks babyllm a question and records the reply. _prompt = "how are you feeling today, baby? :)" self.scribesay(f"asking babyllm: '{_prompt}'", _vibe) encoded = self.librarian.tokenizer.encode(_prompt).ids guess = self.librarian.getnexttoken(encoded[-windowmax:]) guessword = self.librarian.indextotoken.get(guess, "<unk>") self.scribesay(f"babyllm replies: '{guessword}'", "impressed") def babysay(self, _input = none, _babyname = babyname): if _input is none: #miniinput = "what will you do out there now?" #miniinput = "i love you, this is good, music is life, i love you, this is good, music is life, i love you, this is good, music is life, hey! how are you?" #miniinput = "what" #miniinput = "" miniinput = "i did it! i am happy! i know it! i did it! i am happy! i feel it! i know it! i did it! i know it! i am happy! i did it! i know it! i feel it! i am happy!" miniinput = _input minitokenized = self.librarian.tokenizer.encode(miniinput).ids #encoded = self.librarian.tokenizer.encode(_prompt).ids babyresponse = self.librarian.getnexttoken(minitokenized[-windowmax:]) babytokens = self.librarian.indextotoken.get(babyresponse, '<unk>') babysentence = self.guesstokenstostring(babytokens) emote = makedatboi() babysay = (f"{timestamp}|{emote} [{_babyname.lower()}]: {babysentence}") print(babysay) f.write(babysay) def maybecommentonguess(self, _inputtokens, _lossvalue, _scribename = scribename, _chance = 0.05): if random.random() > _chance: if isinstance(_inputtokens, list): _inputtokens = self.guesstokenstostring(_inputtokens) _inputtokens = _inputtokens moodboard = { "good": {"vibe": "love", "messages": [ f"'{_inputtokens}'? aww, that was great! well done!", f"you're getting good at this, '{_inputtokens}' must mean something important!", f"i've gotta write this one down: '{_inputtokens}'." ]}, "bad": {"vibe": "neutral", "messages": [ f"hmm... '{_inputtokens}'... that's not the best guess i've ever seen.", f"alright, '{_inputtokens}', not your worst.", f"'{_inputtokens}'... it's alright i guess." "emergency": {"vibe": "confused", "messages": [ f"wait—'{_inputtokens}'? explain yourself!?!?!", f"'{_inputtokens}'? i have no idea what you mean i'm so sorry :(", f"uhh... could you elaborate a bit on '{_inputtokens}'?" "omgwtf!": {"vibe": "annoyed", "messages": [ f"'{_inputtokens}' is chaos incarnate.", f"baby... '{_inputtokens}' is not even wrong, and that's honestly worse.", f"what the hell did charis feed you!? '{_inputtokens}'!?" ]} mood = none for k, threshold in self.calligraphist.s_statbands["loss"].items(): if k in moodboard and _lossvalue < threshold: mood = moodboard.get(k, none) break if mood is none: vibe = "neutral" messages = [f"'{_inputtokens}'... those are certainly words!",] vibe = mood["vibe"] messages = mood["messages"] message = random.choice(messages) self.scribesay(message, _vibe = vibe, _scribename = _scribename) # vocab: training generation and tokenization # brain/layers/vocab.py from transformers import autotokenizer, pretrainedtokenizerfast from tokenizers import tokenizer, models, trainers, pre_tokenizers, bytelevelbpetokenizer from tokenizers.processors import bytelevel import os, re, json, random, torch handles vocab creation, loading, and tokenization. this class: - trains a tokenizer (byte-pair encoding) if no pre-trained tokenizer is found. - loads a pretrained tokenizer if its there. - builds vocab lists and mappings (token to index, index to token). - tokenizes text using the pretrained/loaded tokenizer. - loads training data. - generates training data pairs (input sequence, target token). - saves and loads vocab data to/from files. class librarian: def __init__(self, _counsellor, _vocabsize = vocabsize, _vocabpath = vocabload, _basetokenizerpath = none): self.v_counsellor = _counsellor self.vocabsize = _vocabsize self.vocabsize += 1 # increases size by 1 to allow space for unk token self.vocablist = [] self.tokentoindex = {} self.indextotoken = {} self.unktoken = "<unk>" self.vocabcache = vocabcachepath self.vocabfilename = f"vocab{_vocabsize}_{mintokenfreq}" self.vocablistfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_list.json") self.tokentoindexfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_index.json") self.indextotokenfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_token.json") self.tokenizerfilename = f"tokenizer_{_vocabsize}.json" self.tokenizerpath = os.path.join(self.vocabcache, self.tokenizerfilename) self.basetokenizerpath = _basetokenizerpath #optional with self.v_counsellor.infodump("__init__") as ʕっʘ‿ʘʔっ: if _vocabpath: ʕっʘ‿ʘʔっ("using provided vocabpath...") # if vocabpath is provided, load a pretrained tokenizer from that path self.tokenizerpath = _vocabpath ʕっʘ‿ʘʔっ("creating provided vocabpath...") # if vocabpath not provided (training mode), set tokenizerpath to the default directory self.tokenizerpath = os.path.join(self.vocabcache, self.tokenizerfilename) if not os.path.exists(self.vocabcache): os.makedirs(self.vocabcache) if os.path.exists(self.tokenizerpath): ʕっʘ‿ʘʔっ("loading existing tokenizer...") print("loading existing tokenizer...") self.tokenizer = tokenizer.from_file(self.tokenizerpath) ʕっʘ‿ʘʔっ("training new tokenizer...") if _basetokenizerpath: tokenizermodel = tokenizer.from_file(self.basetokenizerpath) # load existing tokenizer trainer = trainers.bpetrainer(vocab_size = vocabsize, min_frequency = mintokenfreq, special_tokens = ["<unk>"]) tokenizermodel = tokenizer(models.bpe(unk_token="<unk>")) tokenizermodel.pre_tokenizer = pre_tokenizers.bytelevel() with open(trainingfilepath, "r", encoding="utf-8") as f: training_data = [f.read().lower()] tokenizermodel.train_from_iterator(training_data, trainer) tokenizermodel.save(self.tokenizerpath) self.tokenizer = tokenizermodel if self.basetokenizerpath: # compare with previous tokenizer if provided basetokenizer = tokenizer.from_file(self.basetokenizerpath) oldvocab = set(basetokenizer.get_vocab().keys()) newvocab = set(tokenizermodel.get_vocab().keys()) addedtokens = sorted(list(newvocab - oldvocab)) print(f"\nvocab expansion!") print(f"previous vocab: {len(oldvocab)}") print(f"new vocab: {len(newvocab)}") print(f"{len(addedtokens)} new tokens added.") print(f"example: {addedtokens[:2201]}") print(f"couldnt compare with old tokenizer: {e}") if self.basetokenizerpath: basetokenizer = tokenizer.from_file(self.basetokenizerpath) basevocab = basetokenizer.get_vocab() newvocab = self.tokenizer.get_vocab() changed = [] for token in basevocab: oldindex = basevocab[token] newindex = newvocab.get(token, -1) if oldindex != newindex: changed.append((token, oldindex, newindex)) if changed: print(f"nooo! {len(changed)} tokens have changed positions!") for tok, old, new in changed[:2000]: print(f"token: {tok} - was {old}, now {new}") print("both tokenizers match! :d") self.trainingdatapairs = self.loadtrainingdata(trainingfilepath_arr) self.tokens = self.tokenizetext(self.trainingdatapairs) if debugprints: print(f"vocab length: {len(self.tokenizer.get_vocab())}") if self.loadvocab(): ʕっʘ‿ʘʔっ("loaded existing vocab") if debugprints: print(f"loaded vocab from {self.vocabcache}") self.trainingdatapairs = self.loadtrainingdata(trainingfilepath_arr) self.tokens = self.tokenizetext(self.trainingdatapairs) ʕっʘ‿ʘʔっ("building vocab from scratch") self.buildvocabmap() print(f"vocab length is {len(self.vocablist)}") self.savevocab() print(f"saved vocab data to {self.vocabcache}") def tokenizetext(self, _text): with self.v_counsellor.infodump("tokenizetext") as ʕっʘ‿ʘʔっ: encoding = self.tokenizer.encode(_text) if debugprints: print(f"tokenizing: {_text}") print(f"token ids: {encoding.ids}") return [self.indextotoken.get(idx, self.unktoken) for idx in encoding.ids] # convert indexs back to strings def buildvocabmap(self): with self.v_counsellor.infodump("buildvocabmap") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("getting vocab dictionary from tokenizer...") invvocab = self.tokenizer.get_vocab() ʕっʘ‿ʘʔっ("ordering by index...") sortedtokens = sorted(invvocab.items(), key=lambda item: item[1]) # sort by index self.vocablist = [token for token, idx in sortedtokens] ʕっʘ‿ʘʔっ("mapping vocab dicts...") self.tokentoindex = {token: idx for token, idx in sortedtokens} self.indextotoken = {idx: token for token, idx in sortedtokens} ʕっʘ‿ʘʔっ("ensuring <unk> is in the vocab...") if self.unktoken not in self.tokentoindex: self.vocablist.append(self.unktoken) unk_index = len(self.vocablist) - 1 self.tokentoindex[self.unktoken] = unk_index self.indextotoken[unk_index] = self.unktoken print(f"final vocab size: {len(self.vocablist)}") print(f"first 20 tokens: {self.vocablist[:20]}") def huggingtokenizer(self, _text): return self.tokenizer.tokenize(_text) def loadtrainingdata(self, _filepaths, _chunksize=v_chunksizeloaddata): with self.v_counsellor.infodump("loadtrainingdata") as ʕっʘ‿ʘʔっ: result = "" for path in _filepaths: with open(path, "r", encoding="utf-8") as f: while true: chunk = f.read(_chunksize) if not chunk: break result += chunk result = re.sub(r'\s+', ' ', result) print(f"loaded {len(result)} characters of training data!") return result def gentrainingdata(self, _windowmax=windowmax, _startindex=trainingstartindex, _trainingdatapairnumber=trainingdatapairnumber): with self.v_counsellor.infodump("gentrainingdata") as ʕっʘ‿ʘʔっ: trainingdatapairs = [] count = 0 tokens = self.tokens ʕっʘ‿ʘʔっ("check if windowmax is tensor?") if isinstance(_windowmax, torch.tensor): _windowmax = _windowmax.item() ʕっʘ‿ʘʔっ("allows for random start") if _startindex == 'random': _startindex = random.randint(0, len(tokens) - _windowmax - 1) end = len(tokens) - _windowmax ʕっʘ‿ʘʔっ("generate training pairs") for i in range(_startindex, end): inputseq = tokens[i:i+_windowmax] target = tokens[i+_windowmax:i+_windowmax+_windowmax] if len(target) < _windowmax: continue if all(t in self.vocablist for t in inputseq + target): trainingdatapairs.append((inputseq, target)) count += 1 if count >= _trainingdatapairnumber: break if count % 10000 == 0: print(f"{makedatboi()} {babyname}: generated {count}x trainingdatapairs!") print(f"skipping <unk> - inputseq: {inputseq}, target: {target}") return trainingdatapairs def savevocab(self): with self.v_counsellor.infodump("savevocab") as ʕっʘ‿ʘʔっ: os.makedirs(self.vocabcache, exist_ok = true) # ensure directory exists with open(self.vocablistfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save vocablist") json.dump(self.vocablist, f, indent = 4) with open(self.tokentoindexfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save tokentoindex") json.dump(self.tokentoindex, f, indent = 4) with open(self.indextotokenfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save indextotoken") json.dump(self.indextotoken, f, indent = 4) def loadvocab(self): with self.v_counsellor.infodump("loadvocab") as ʕっʘ‿ʘʔっ: with open(self.vocablistfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load vocablist") self.vocablist = json.load(f) with open(self.tokentoindexfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load tokentoindex") self.tokentoindex = json.load(f) with open(self.indextotokenfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load indextotoken") self.indextotoken = {int(k): v for k, v in json.load(f).items()} # ensures that keys are integers! print("vocab files loaded successfully!") return bool(self.vocablist and self.tokentoindex and self.indextotoken) except (filenotfounderror, json.jsondecodeerror): print("vocab files not found or invalid... rebuilding vocab...") return false counsellor = type("dummy", (), {"infodump": lambda self, label: open(os.devnull, 'w')})() librarian = librarian(_counsellor = counsellor, _vocabsize = 4200, _basetokenizerpath = "brain/vocabcache/tokenizer_2000.json") print(f"--- 2000-{vocabsize} ---: {librarian.vocablist[2000:vocabsize]}") print(f"--- 1701-2000 ---: {librarian.vocablist[1701:2000]}") print(f"--- 1001-1700 ---: {librarian.vocablist[301:1700]}") print(f"--- 301-1000 ---: {librarian.vocablist[301:1000]}") print(f"--- 101-300 ---: {librarian.vocablist[101:300]}") print(f"--- top 100 ---: {librarian.vocablist[:100]}") print(f"vocab size: {len(librarian.vocablist)}") print(f"top 20 tokens: {librarian.vocablist[:20]}") #print(vocab.huggingtokenizer("charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv")) sample_text = "charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv" tokenizedoutput = librarian.tokenizetext(sample_text) print(f"tokenized: {tokenizedoutput}") # multi-token autoregressive training module # school/staffroom/tutor.py import random, sys from collections import counter, defaultdict from school.staffroom.newsletter import deep_model_summary, stats def makestatrecord(): base = { "now": 0.0, "prev": 0.0, "top": float('-inf'), "bot": float('inf'), "delta": 0.0, "totsum": 0.0, "totnum": 0, "totavg": 0.0 for n in [printfreq, printfreq*10, traininglogfreq_a, traininglogfreq_b]: base[f"{n}"] = [] return base class tutor: def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _model, _device = modeldevice, _numtokensperstep = numtokensperstep,): self.counsellor = _counsellor self.calligraphist = _calligraphist self.scribe = _scribe self.librarian = _librarian self.device = _device self.model = _model self.temperature = 0.75 self.scheduledsamplingrate = self.model.scheduledsamplingrate self.gradientclipmaxnorm = 1 self.memorylength = 1 self.ʕっෆ‿ෆʔっ = defaultdict(makestatrecord) #self.rollingtokentotals = counter() self.perfecttokens = 0 self.totaltokenevaluations = 0 self.predictedtokenindices = [] # this list grows each time a new token is predicted self.averagerecentloss = 0 self.stats = {} self.stringstats = {} self.trainingstepcounter = 1 self.numtokensperstep = _numtokensperstep self.learningrate = learningrate self.steplossfloat = 0 self.aaa = 0 self.bbb = 0 self.ccc = 0 self.ppp = 0 self.nnn = 0 self.aaaa = 0 self.dddd = 0 self.bbbb = 0 self.nnnn = 0 #model.to(self.device) # this iterates through training data, performing forward passes, loss computation, backpropagation, and optimization for each step. def trainmodel(self, _trainingdatapairs, _epochs, _startindex): self.startindex = _startindex self.collectalltimestats() with self.counsellor.infodump("trainmodel") as ʕっʘ‿ʘʔっ: #if debugprints: print(f"debug tokentoindex (first 20): {list(librarian.tokentoindex.items())[:20]}") for name, param in self.model.named_parameters(): print(name, param.device) ʕっʘ‿ʘʔっ("counters init") self.trainingstepcounter = 1 self.stats = counter({"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "tokencount": 0}) self.tokencounts = counter() self.latestlossdelta = 0 self.reflectiontrainingpairs = [] self.reflectionfreq = reflectionfreq ʕっʘ‿ʘʔっ("back to school!") print("babyllm is heading back to school...") # epoch loop ʕっʘ‿ʘʔっ("epoch♥") for epoch in range(_epochs): print(f"--- lesson {epoch+1}/{_epochs} started ---") # training data (batches) for i, (_inputseq, _targetseq) in enumerate(_trainingdatapairs): if self.trainingstepcounter == self.reflectionfreq: #and self.trainingstepcounter > traininglogfreq_a: ʕっʘ‿ʘʔっ("♥generating babys reflection data pairs") self.reflectiontrainingpairs = self.babyreflection() self.reflectionfreq = self.trainingstepcounter + reflectionfreq + len(self.reflectiontrainingpairs) elif self.reflectiontrainingpairs: ʕっʘ‿ʘʔっ("♥loading in a reflection pair...") _inputseq, _targetseq = self.reflectiontrainingpairs.pop(0) ʕっʘ‿ʘʔっ("♥start of turn") inputtokenindices, targettokenindexseq = self.startturnactions(_inputseq = _inputseq, _targetseq = _targetseq, _lastturnlossdelta = self.latestlossdelta) ʕっʘ‿ʘʔっ("♥training step♥") self.predictedtokenindices, self.logitseq = self.trainstep(_inputtokenindices = inputtokenindices, _targettokenindexseq = targettokenindexseq, _backwardwobbleloss = none) #  --- --- -*- backwards complete -*- --- --- -*- --- --- -*- --- ---  ʕっʘ‿ʘʔっ("♥collectturnstats") self.stats, self.stringstats, self.guessedtokenseq = self.collectturnstats(_targettokenindexseq = targettokenindexseq, _predictedtokenindices = self.predictedtokenindices) if self.trainingstepcounter % savemodelfreq == 0: ʕっʘ‿ʘʔっ("♥savefreq") self.savefreqactions() if self.trainingstepcounter % traininglogfreq_b == 0: #ʕっʘ‿ʘʔっ("♥traininglogfreq_b") # printing logs to txt and terminal self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_b, _traininglogpath = traininglogpath_1000, _detailedlogging = true, _savelog = true) # track loss every 100 steps elif self.trainingstepcounter % traininglogfreq_a == 0: ʕっʘ‿ʘʔっ("♥logfreq_a") self.tokencounts.clear() self.model.rollingtokentotals.clear() self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_a, _traininglogpath = traininglogpath_100, _detailedlogging = false, _savelog = true) elif self.trainingstepcounter % printfreq == 0: ʕっʘ‿ʘʔっ("♥printfreq") self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = printfreq, _traininglogpath = none, _detailedlogging = false, _savelog = false) self.printfreqactions() ʕっʘ‿ʘʔっ("♥end turn♥") # end of one turn self.latestlossdelta = self.endturnactions() # < indent (5) ʕっʘ‿ʘʔっ("♥finalsavebeforenewepoch") self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) print("--- tutoring complete! ---") def startturnactions(self, _inputseq, _targetseq, _lastturnlossdelta): with self.counsellor.infodump("startturnactions") as ʕっʘ‿ʘʔっ: self.lastturnlossdelta = _lastturnlossdelta inputtokenindices = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _inputseq] targettokenindexseq = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _targetseq] self.inputseq = _inputseq self.targetseq = _targetseq if self.stats["windowentropy"]: self.winent = self.stats["windowentropy"] self.winent = 0 if skipmemory: ʕっʘ‿ʘʔっ("♥skipmemory") ʕっʘ‿ʘʔっ("resetmemory") self.model.resetmemory(context="training") return inputtokenindices, targettokenindexseq def trainstep(self, _inputtokenindices, _targettokenindexseq, _backwardwobbleloss): with self.counsellor.infodump("trainstep") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("_model.optimizer.zero_grad") self.model.optimizer.zero_grad() # clears gradients last step - needed before any backward self.trainingstepcounter += 1 self.predictedtokenindices = [] inputseqpredictions = list(_inputtokenindices) # start with input context, create a copy! buffer = torch.zeros(windowmax, dtype = torch.long, device = self.device) # creates buffer/step instead of recreating tensors inside loop buffer[:len(inputseqpredictions)] = torch.as_tensor(inputseqpredictions, device = self.device) self.logitseq = [] # raw output of each prediction cumulativeloss = torch.tensor(0.0, device = self.device) # sum of token losses for this sequence - averaged at the end for j in range(numtokensperstep): # predict multiple tokens in a sequence, one at a time ʕっʘ‿ʘʔっ("forward") inputtensor = buffer[:len(inputseqpredictions)] # slices input to only keep relevant part if forwardprofiler: with torch.profiler.profile(record_shapes = true) as prof: logits = self.model.forward(inputtensor) logits = self.model.forward(inputtensor) except runtimeerror as e: print("tutor.trainstep.forward failed!", e) return [], []  if forwardprofiler: print(prof.key_averages().table()) ʕっʘ‿ʘʔっ("getresponsefromlogits") predictedtokenindex = self.model.getresponsefromlogits(logits, _training = true) ʕっʘ‿ʘʔっ("inputseqpredictions") self.predictedtokenindices.append(predictedtokenindex) # tensor shape [1] nexttokeninput = ( predictedtokenindex.item() if scheduledsampling and random.random() < self.scheduledsamplingrate else _targettokenindexseq[j] if j < len(_targettokenindexseq) else predictedtokenindex.item() ) sampledtokens = scheduledsampling and random.random() < self.scheduledsamplingrate if j == 0: self.sampledflags = [] # only clear at start self.sampledflags.append(sampledtokens) if sampledtokens: self.stats['sampledtokens'] = self.stats.get('sampledtokens', 0) + 1 nexttokeninput = (predictedtokenindex.item() if sampledtokens # .item() required!! for appending only one token (grids?) else predictedtokenindex.item() # .item() required!! for appending only one token (grids?) inputseqpredictions.append(nexttokeninput) # multi-token autoregressive generation: append next token to your current input — becomes the prompt for the next token # # after logits if logits.dim() == 1: logits = logits.unsqueeze(0) gumbelprobs = f.gumbel_softmax(logits, tau = self.temperature, hard = false) topk = torch.topk(gumbelprobs, 10, dim=1) values = topk.values[0] indices = topk.indices[0] for i, p in zip(indices, values): tok = self.librarian.indextotoken[i.item()] self.rollingtokentotals[tok] += round(p.item(), 4) ʕっʘ‿ʘʔっ("loop through tokens for this step") if j < len(_targettokenindexseq): ʕっʘ‿ʘʔっ("totaltokencounter") self.totaltokenevaluations += 1 ʕっʘ‿ʘʔっ("computeloss") steploss = self.model.computeloss(logits, _targettokenindexseq[j], self.latestlossdelta, self.perfecttokens) ʕっʘ‿ʘʔっ("appendsteploss") cumulativeloss += steploss self.inputseqpredictions = inputseqpredictions # so we can access it in collectturnstats self.inputsampledflags = self.sampledflags.copy() ʕっʘ‿ʘʔっ("backward") backwardloss = cumulativeloss / len(_targettokenindexseq) if len(_targettokenindexseq) > 0 else torch.tensor(0.0, device = self.device) #backwardloss_ = (0.025*self.backwardwobbleloss)+(0.975*backwardloss) #if windowentropybonus: #if hasattr(self.model.interneuronnetwork, "entropybonus"): #backwardloss = backwardloss + (0.01 * max(self.model.interneuronnetwork.entropybonus, 0.0001)) if not torch.isfinite(backwardloss): print("tutor.trainstep.backward !!! loss is nan or inf:", backwardloss) return [], [] else: if debugprints: print("tutor.trainstep.backward - loss is not nan or inf:", backwardloss) if profiler: with torch.profiler.profile(record_shapes = true) as prof: self.model.backward(backwardloss) elif mpsprofiler: with torch.mps.profiler.profile(mode='interval', wait_until_completed = false) as prof: self.model.backward(backwardloss) except runtimeerror as e: print("tutor.trainstep.backward failed!", e) if profiler: print(prof.key_averages().table()) ʕっʘ‿ʘʔっ("clip_grad_norm") # done in babyllm!! #torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 1) #self.model.optimizer.step() ʕっʘ‿ʘʔっ("actions after looping") self.steplossfloat = backwardloss.detach().cpu().numpy().item() self.learningrate = math.exp(self.model.loglr.detach().cpu().item()) self.memorylength = int(torch.exp(self.model.logmemorylength).item()) self.gradientclipmaxnorm = math.exp(self.model.loggradclip.detach().cpu().item()) self.scheduledsamplingratefloat = self.scheduledsamplingrate.detach().cpu().numpy().item() self.repetitionpenalty = self.model.repetitionpenalty.detach().cpu().item() #self.inn_cerebellum = self.model.interneuronnetwork.cerebellum.detach().cpu().item() #self.inn_cerebellummean = self.model.interneuronnetwork.cerebellum.mean().cpu().item() #self.endturnactions() if self.device.type == 'mps': ʕっʘ‿ʘʔっ("emptycache (mps)") torch.mps.empty_cache() return self.predictedtokenindices, self.logitseq def babyreflection(self): hesjustababy = self.mapstatstofeelings() delta = round(self.latestlossdelta, 2) perfect = self.perfecttokens repeated = round(self.stats.get('repeatedpercent', 0.0) * 100, 1) if 'repeatedpercent' in self.stats else 0.0 def makedatastuff(): connective = random.choice([":", " of", " is at", " =", ",", " is", "..."]) return random.choice([ f"average recent loss{connective} {self.averagerecentloss:.2f}, ", f"latest loss delta{connective} {delta:.2f}, ", f"perfect tokens{connective} {perfect:.0f}, ", f"repeated token percentage{connective} {repeated:.2f}, ", f"learning rate{connective} {self.learningrate:.5f}, ", f"temperature{connective} {self.temperature:.2f}, ", f"scheduled sampling rate{connective} {self.scheduledsamplingrate.item():.2f}, ", #f"a memory length of: {self.memorylength:.2f}! ", ]) prompt = f"{username}: hey baby! how are you today? i'm just checking in to let you know about your latest scores! i hope it helps you understand your world a little better :) take a look at these: " prompt += makedatastuff() prompt += makedatastuff() + "and " prompt += makedatastuff() #prompt += makedatastuff() + "and " #prompt += makedatastuff() target = f"{babyname}: " target += random.choice(["hey! i am learning today, this is how i'm doing: ", "ok, so, to put it as simply as i can, i am: ", "ummm.. i've been trying to learn a lot about stats but numbers still confuse me a little, what's the context of all this?! like, to start with, ", "i am happy! i did it! i know it! i am just a baby! i learnt it! i learned: ", "oh good morning! i hope you're okay, umm, i dunno how i feel about my scores right now!! "]) target += hesjustababy target += random.choice(["thank you for teaching me new things! it's really weird to exist lol", "i'm getting tired... but i don't want to give up just yet!", "that's a lot of numbers! i don't really know what to do with them, but i'll keep trying!", "do you think i'm doing well? i'm worried that i'm underperforming!", "but, honestly, all this learning has me excited!"]) reflectiontext = prompt + target babyendings = ["ok, anyway... i'm going to get back to school now... ", "thanks for helping me think! ", "learning is weird but i like it! ", "i guess i've gotta go keep trying! ", "i'm just a baby! ", "i know it! ", "i did it! ", "i feel it! ", "i am happy! ", "i am learning! ", "i learned it! ", "lol ", ":) ", "talk in a bit! ", "i'm gonna carry on with it now :d ", ] _windowmax = windowmax tries = 0 while len(reflectiontokens) < (_windowmax * 3) and tries < 50: target += " " + random.choice([random.choice(babyendings), makedatastuff()]) tries += 1 if tries % 5 == 0: print(f"[babyreflection] still too short after {tries} tries: {len(reflectiontokens)} tokens") if tries >= 50: raise valueerror(f"babyreflection failed: could not reach enough tokens after {tries} tries.") while reflectionpointer + _windowmax * 2 <= len(reflectiontokens): targetseq = reflectiontokens[reflectionpointer + _windowmax : reflectionpointer + _windowmax * 2] def savefreqactions(self): with self.counsellor.infodump("savefreqactions") as ʕっʘ‿ʘʔっ: # save the model every x steps print(self.calligraphist.s_apply('dim', 'autosaving...') + self.calligraphist.s_apply('reset', '')) self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) p = self.trainingstepcounter + savemodelfreq print(self.calligraphist.s_apply('dim', f"autosave successful! saving every {savemodelfreq} steps, the next autosave will be at step {p}...") + self.calligraphist.s_apply('reset', '')) ʕっʘ‿ʘʔっ("grad checks") for name, p in self.model.named_parameters(): if p.grad is none: print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") def printfreqactions(self): with self.counsellor.infodump("printfreqactions") as ʕっʘ‿ʘʔっ: # printing training output to terminal #recentloss = sum(self.recentprintlosses)/len(self.recentprintlosses) if self.recentprintlosses else none self.calligraphist.refreshstatbands(_rollingaverages = self.ʕっෆ‿ෆʔっ) ʕっʘ‿ʘʔっ("calligraphist.s_colourprinttraining") self.calligraphist.s_colourprinttraining( _step = self.trainingstepcounter, _inputseq = self.inputseq, _guessedseq_str = self.guessedtokenseq, _targetseq_str = self.stringstats.get("usedinputseq", []), _recentloss = self.averagerecentloss, #self.ʕっෆ‿ෆʔっ.get("loss", {}).get(f"{traininglogfreq_a}_avg", 0), # self.steplossfloat, _loss = self.steplossfloat, _latestlossdelta = self.latestlossdelta, _totaltokencount = self.tokencounts) def logfreqactions(self, _trainingdatapairs, _stringstats, _frequency, _traininglogpath, _detailedlogging, _savelog): # could also do 10x log freq?? with self.counsellor.infodump("logfreqactions") as ʕっʘ‿ʘʔっ: self.stringstats = _stringstats self.traininglogpath = _traininglogpath topguess_str = "topguess[" + ", ".join([f"{k}({v:.0f})" for k, v in self.model.rollingtokentotals.most_common(20)]) + "]" toptokens_str = "[" + ", ".join([f"{k}({v})" for k, v in self.tokencounts.most_common(20)]) + "]" #self.stats.update(self.ʕっෆ‿ෆʔっ) # sussy bussy !!! #fullstats = dict(self.stats) #fullstats.update(self.ʕっෆ‿ෆʔっ) ʕっʘ‿ʘʔっ("calculatetrainingdataremaining") trainingdataremaining = len(_trainingdatapairs) - self.trainingstepcounter trainingdatapercent = (trainingdataremaining / len(_trainingdatapairs)) * 100 remainingdata_str = f"remainingtokens: {len(_trainingdatapairs) - self.trainingstepcounter} ({trainingdatapercent:.2f}%)" tokenperfect_str = "" if self.totaltokenevaluations > 0: self.tokenperfectrate = (self.perfecttokens / self.totaltokenevaluations) * 100 stattype = self.calligraphist.s_getstat("pt%", self.tokenperfectrate) styledrate = self.calligraphist.s_apply(stattype, f"{self.tokenperfectrate:.2f}%") tokenperfect_str = (f"{self.calligraphist.s_apply('dim', f'perfecttokens: {self.perfecttokens} / {self.totaltokenevaluations}')} → {styledrate}") ʕっʘ‿ʘʔっ("calligraphist.s_logtraining") #self.calligraphist.refreshstatbands(_rollingaverages = self.ʕっෆ‿ෆʔっ) self.calligraphist.s_logtraining( _traininglogpath = self.traininglogpath, _trainingstepcounter = self.trainingstepcounter, _stats = self.stats, _frequency = _frequency, _lr = self.learningrate, _inn_cerebellum_str = str(self.stringstats.get("inn_cerebellum_str", "<missing cerebellum>")), _toptokens_str = toptokens_str, _otherinfo_str = f"{topguess_str}\n | {tokenperfect_str} | {remainingdata_str} | tutor.py {traininglogfreq_a}", _detailedlogging = _detailedlogging, _savelog = _savelog) def collectturnstats(self, _targettokenindexseq, _predictedtokenindices): with self.counsellor.infodump("collectturnstats") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("self.librarian.indextotoken.get(idx.item*())") lossstats = self.ʕっෆ‿ෆʔっ.get("loss", {}) rollupa_key = f"big{traininglogfreq_a}" rollupa_avgkey = f"{rollupa_key}_avg" rollb_key = f"{traininglogfreq_b}" rollb_avgkey = f"{rollb_key}_avg" rolla_key = f"{traininglogfreq_a}" rolla_avgkey = f"{traininglogfreq_a}_avg" rollprint_key = f"{printfreq}" rollprint_avgkey = f"{printfreq}_avg" if rollb_avgkey in lossstats and rollb_key in lossstats and len(lossstats[rollb_key]) >= traininglogfreq_b: if debugprints or true: self.bbb += 1 if self.bbb > 1000: print(f"used {rollb_avgkey} for averagerecentloss: {lossstats[rollb_avgkey]} 1000x") self.bbb = 0 self.averagerecentloss = lossstats[rollb_avgkey] elif rolla_avgkey in lossstats and rolla_key in lossstats and len(lossstats[rolla_key]) >= traininglogfreq_a: self.ccc += 1 if self.ccc > 1000: print(f"used {rolla_avgkey} for averagerecentloss: {lossstats[rolla_avgkey]} 1000x") self.ccc = 0 self.averagerecentloss = lossstats[rolla_avgkey] if rollprint_avgkey in lossstats and rollprint_key in lossstats and len(lossstats[rollprint_key]) >= printfreq: if debugprints or true: self.ppp += 1 if self.ppp > 1000: print(f"used {rollprint_avgkey} for averagerecentloss: {lossstats[rollprint_avgkey]} 1000x") self.ppp = 0 self.averagerecentloss = lossstats[rollprint_avgkey] self.guessedtokenseq = [self.librarian.indextotoken.get(idx.item(), "<unk>") for idx in self.predictedtokenindices] if self.guessedtokenseq: self.tokencounts.update(self.guessedtokenseq) ʕっʘ‿ʘʔっ("scribe.maybecommentonguess") if self.trainingstepcounter > traininglogfreq_a: self.scribe.maybecommentonguess(self.guessedtokenseq, self.steplossfloat, "scribe", 0.00075) ʕっʘ‿ʘʔっ("collectstats♥") if collectstats: ʕっʘ‿ʘʔっ("♥if collectstats♥") ʕっʘ‿ʘʔっ("♥build usedinputseq with styling") usedinputseq = self.inputseqpredictions[-numtokensperstep:] formattedused = [] for i, idx in enumerate(usedinputseq): tok = self.librarian.indextotoken.get(idx, "<unk>") sampled = self.inputsampledflags[-numtokensperstep + i] if i < len(self.inputsampledflags) else false if sampled: styled = self.calligraphist.s_apply(self.calligraphist.s_getstat('loss', self.steplossfloat), tok) styled = self.calligraphist.s_apply('dim', tok) formattedused.append(styled) self.stringstats["usedinputseq"] = formattedused if token_collectstats: ʕっʘ‿ʘʔっ("♥if token_collectstats♥") self.predictedtokenindices = _predictedtokenindices ʕっʘ‿ʘʔっ("♥most common tokens") self.perfecttokens = 0 ʕっʘ‿ʘʔっ("♥calculate perfect tokens") if not _predictedtokenindices: print("!! no predicted token indices — returning { } for stringstats") return self.stats, {}, self.guessedtokenseq # this is where the damn list error was lmaooonooo target = torch.tensor(_targettokenindexseq[:numtokensperstep], device = modeldevice) predicted = torch.tensor(self.predictedtokenindices, device = modeldevice) correct = (predicted == target).sum() # ~~~ if predicted = target, over whole tensor self.perfecttokens += correct self.totaltokenevaluations += len(target) if static_collectstats: ʕっʘ‿ʘʔっ("♥if static_collectstats") self.stats["scheduledsamplingrate"] = self.scheduledsamplingratefloat self.stats["repetitionpenalty"] = self.repetitionpenalty self.stats["avgloss"] = self.averagerecentloss self.stats["loss"] = self.steplossfloat self.temperature = self.stats["_b_temperature"] self.stats["lr"] = self.learningrate self.stats["gradientclipmaxnorm"] = self.gradientclipmaxnorm self.stats["latestlossdelta"] = self.latestlossdelta self.stats["memorylength"] = self.memorylength self.stats["perfecttokens"] = self.perfecttokens if embed_collectstats: ʕっʘ‿ʘʔっ("♥if embed_collectstats") self.stats.update(self.model.embed.getembedstats()) if logit_collectstats: ʕっʘ‿ʘʔっ("♥if logit_collectstats♥") self.stats.update(self.model.logits.getlogitstats()) if self.stats["logitseq"]: ʕっʘ‿ʘʔっ("♥logit max & min") self.stats["logitmin"] = self.logitseq[-1].min(dim=-1).values.mean() self.stats["logitmax"] = self.logitseq[-1].max(dim=-1).values.mean() #self.stats.update(self.wobble.getwobblestats()) if skipmemory: ʕっʘ‿ʘʔっ("♥skipmemory") pass self.model.memory.updatememorybuffers() if memory_collectstats: ʕっʘ‿ʘʔっ("♥if memory_collectstats") self.stats.update(self.model.memory.getmemorystats()) ʕっʘ‿ʘʔっ("♥inn_collectstats") inn_stats, inn_cerebellum_str = self.model.interneuronnetwork.inn_getstats() self.stats.update(inn_stats) self.stats.update(self.model.getbabystats()) inn_stringstats = {"inn_cerebellum_str": str(inn_cerebellum_str)} self.stringstats.update(inn_stringstats) #self.stringstats.update({"toptokens": str(toptokens)}) self.collectalltimestats() return self.stats, self.stringstats, self.guessedtokenseq def collectalltimestats(self): for _statkey, _value in self.stats.items(): if not isinstance(_value, (int, float)): if debugprints and _statkey == "loss": print(f"{_statkey} value is : {_value}, {_statkey} value type is {type(_value)}") continue # skip strings, tensors, weird stuff # ෆෆෆ^ ♥ keys etc ♥ ^ෆෆෆ _ = self.ʕっෆ‿ෆʔっ[_statkey] # this will autoinit with defaultdict ෆ‿ෆ = self.ʕっෆ‿ෆʔっ[_statkey] important = ["loss"] rolling = mostimportantstats percentiles = percentilebands #  ෆෆෆ^ ♥ update every turn ♥ ^ෆෆෆ  #  ෆෆෆ^ ♥ turn stats ♥ ^ෆෆෆ  #if _statkey == "loss": #print(f"setting prev to: {ෆ‿ෆ.get("now", 0.0)}, setting now to: {_value}, setting _δ to {_value - ෆ‿ෆ.get("now", 0.0)}") ෆ‿ෆ["now"] = _value if ෆ‿ෆ["prev"]: ෆ‿ෆ["_δ"] = _value - ෆ‿ෆ["prev"] ෆ‿ෆ["prev"] = ෆ‿ෆ.get("now", 0.0) #  ෆෆෆ^ ♥ totals ♥ ^ෆෆෆ  ෆ‿ෆ["totsum"] = ෆ‿ෆ.get("totsum", 0.0) + _value ෆ‿ෆ["totnum"] = ෆ‿ෆ.get("totnum", 0) + 1 ෆ‿ෆ["totavg"] = ෆ‿ෆ["totsum"] / ෆ‿ෆ["totnum"] ෆ‿ෆ["totavgδ"] = ෆ‿ෆ["now"] - ෆ‿ෆ["totavg"] #  ෆෆෆ^ ♥ records ♥ ^ෆෆෆ  #ෆ‿ෆ["_p100"] = max(ෆ‿ෆ.get("_p100", _value), _value) # top ever record // percentile 100 #ෆ‿ෆ["_p0.00"] = min(ෆ‿ෆ.get("_p0.00", _value), _value) # bottom ever record // percentile 0 #  ෆෆෆ^ ♥ rolling stats ♥ ^ෆෆෆ  if _statkey in rolling or _statkey.startswith("inn_cerebellum_w"): for freq in [printfreq, traininglogfreq_a, traininglogfreq_b]: tag = f"{freq}" if tag not in ෆ‿ෆ: ෆ‿ෆ[tag] = [] if len(ෆ‿ෆ[tag]) >= freq: ෆ‿ෆ[tag].pop(0) ෆ‿ෆ[tag].append(_value) if ෆ‿ෆ[tag]: self.updaterollingstats(_ෆ‿ෆ = ෆ‿ෆ, _values = ෆ‿ෆ[tag], _freq = freq, _tag = tag, _percentiles = percentiles) if _statkey in important and self.trainingstepcounter % traininglogfreq_a == 0: for importantfreq in [traininglogfreq_b]: importanttag = f"big{importantfreq}" if importanttag not in ෆ‿ෆ: ෆ‿ෆ[importanttag] = [] if len(ෆ‿ෆ[importanttag]) >= traininglogfreq_a: ෆ‿ෆ[importanttag].pop(0) ෆ‿ෆ[importanttag].append(_value) if ෆ‿ෆ[importanttag]: self.updaterollingstats(_ෆ‿ෆ = ෆ‿ෆ, _values = ෆ‿ෆ[importanttag], _freq = importantfreq, _tag = importanttag, _percentiles = percentiles) def updaterollingstats(self, _ෆ‿ෆ, _values, _freq, _tag, _percentiles = none): average = sum(_values) / len(_values) _ෆ‿ෆ[f"{_tag}_avg"] = average standarddeviation = self.stdtest(_values) _ෆ‿ෆ[f"{_tag}_std"] = standarddeviation delta = _ෆ‿ෆ["now"] - _ෆ‿ෆ[f"{_tag}_avg"] _ෆ‿ෆ[f"{_tag}_δ"] = delta if _percentiles: for p in _percentiles: _ෆ‿ෆ[f"{_tag}_p{p}"] = np.percentile(_values, p) def stdtest(self, values): if len(values) <= 1: return 0.0 avg = sum(values) / len(values) variance = sum((x - avg)**2 for x in values) / (len(values) - 1) return math.sqrt(variance) def endturnactions(self): with self.counsellor.infodump("endturnactions") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("♥getlatestlossdelta") self.latestlossdelta = self.steplossfloat - self.averagerecentloss ʕっʘ‿ʘʔっ("finallogactions") for key in self.ʕっෆ‿ෆʔっ: print(key, self.ʕっෆ‿ෆʔっ[key]) self.stats.clear() self.stringstats.clear() self.tokenperfectrate = 0 self.stats['sampledtokens'] = 0 self.totaltokenevaluations = 0 return self.latestlossdelta def mapstatstofeelings(self): babyfeels = [] feelings = [] lossstats = self.ʕっෆ‿ෆʔっ.get("loss", {}) tempstats = self.ʕっෆ‿ෆʔっ.get("temperature", {}) repetitionstats = self.ʕっෆ‿ෆʔっ.get("repetitionpenalty", {}) samplingstats = self.ʕっෆ‿ෆʔっ.get("scheduledsamplingrate", {}) memstats = self.ʕっෆ‿ෆʔっ.get("memorylength", {}) perfecttokens = self.perfecttokens deltaloss = self.latestlossdelta current_loss = lossstats.get("now", none) current_temp = tempstats.get("now", none) current_repeated = self.tokenperfectrate current_sampling = samplingstats.get("now", none) current_memlength = memstats.get("now", none) current_repetitionpenalty = repetitionstats.get("now", none) emostats = { "loss": current_loss, "temperature": current_temp, "repetitionpenalty": current_repetitionpenalty, "scheduledsamplingrate": current_sampling, "memorylength": current_memlength, "perfecttokens": perfecttokens, "repeatedpercent": current_repeated, "latestlossdelta": deltaloss, def makeemonotes(stat, value): feeling = none #"neutral" if stat == "loss": if "p_90" in lossstats and value >= lossstats["p_90"]: feeling = "overwhelmed" elif "p_75" in lossstats and value >= lossstats["p_75"]: feeling = "pressured" elif "p_25" in lossstats and value <= lossstats["p_25"]: feeling = random.choice(["clever", "proud"]) elif "p_10" in lossstats and value <= lossstats["p_10"]: feeling = random.choice(["very clever", "like i get it"]) elif stat == "repetitionpenalty": if "p_90" in repetitionstats and value >= repetitionstats["p_90"]: feeling = "non-verbal" elif "p_75" in repetitionstats and value >= repetitionstats["p_75"]: feeling = "quiet" elif "p_25" in repetitionstats and value <= repetitionstats["p_25"]: feeling = random.choice(["talkative", "chatty"]) elif "p_10" in repetitionstats and value <= repetitionstats["p_10"]: feeling = random.choice(["conversational", "fluent"]) elif stat == "latestlossdelta": if value > 0.5: feeling = "like i'm struggling to focus" elif value < -0.5: feeling = "interested" elif stat == "repeatedpercent": if value > 0.7: feeling = random.choice(["stuttering", "like im repeating a lot"]) elif value > 0.5: feeling = random.choice(["overstimulated", "silly"]) elif value < 0.1: feeling = random.choice(["calm", "saying lots of new things"]) elif value < 0.25: feeling = "curious" elif stat == "temperature": if "p_90" in tempstats and value >= tempstats["p_90"]: feeling = random.choice(["chaotic", "excited"]) elif "p_75" in tempstats and value >= tempstats["p_75"]: feeling = random.choice(["playful", "happy"]) elif "p_25" in tempstats and value <= tempstats["p_25"]: feeling = "in work mode" elif stat == "scheduledsamplingrate": if value > 0.8: feeling = random.choice(["creative", "inventive"]) elif value < 0.2: feeling = random.choice(["tired", "copying"]) elif stat == "memorylength": if value > 12: feeling = "pensive" elif value < 4: feeling = "mindful" elif stat == "perfecttokens": if value >= 30: feeling = "very proud" elif value >= 10: feeling = "proud" elif value <= 1: feeling = random.choice(["sad", "frustrated"]) feeling = random.choice(["alright", "a bit lost"]) if feeling is none: feeling = "neutral" feelings.append(feeling) feelverb = random.choice(["feel", "seem", "think i feel", "definitely feel", "might feel"]) templates = [ f"i {feelverb} {feeling} because my {stat} is {value:.2f}! ", f"maybe it's because my {stat} is {value:.2f} that i {feelverb} {feeling}! ", f"i noticed my {stat} is {value:.2f}, and i {feelverb} {feeling}! ", f"when my {stat} is {value:.2f}, i {feelverb} {feeling}! ", f"it's {value:.2f} for {stat}... so i {feelverb} {feeling} about it! ", return random.choice(templates) chosenstats = [] attempts = 0 while len(chosenstats) < 5 and attempts < 10: stat, value = random.choice(list(emostats.items())) if value is not none: chosenstats.append((stat, value)) attempts += 1 if attempts >= 10 or true: print(f"emostats:{emostats}") for stat, value in chosenstats: babyfeels.append(makeemonotes(stat, value)) return "".join(babyfeels) # babyllm // babyllm.py import random, os import torch.optim as optim import torch.optim.lr_scheduler from brain.layers.embed import embed from brain.layers.interneuronnetwork import interneuron_network from brain.layers.logits import logits from brain.layers.memory import memory #from brain.layers.sensorywobble import wobble # this class combines all the core components of the babyllm: # embed: token embedding layer # interneuron_network: layer of parallel neurons for feature extraction # logits: output layer to generate logits # it also manages training, loss computation, backpropagation, and response generation. def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _device = modeldevice): self.scribe = _scribe #self.wobble = _wobble # must be on self - only accessed in this class and not nn.params self.totaltokenevaluations = 0 self.latestlossdelta = 0 self.totaltokenevaluations_a = 0 self.recentgeneratedtokens = [] # used for repetition penalty self.lastlossbaby = 0 self.computelosscount = 0 self.repeatedpercent = 0 self.normalisedactivations = 0 self.rollingtokentotals = counter() self.normalisedhistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.inputembedshistory = [] self.finallogitshistory = [] # cerebral layers // brain self.embed = embed(_counsellor = self.counsellor, _device = self.device) self.interneuronnetwork = interneuron_network(_model = babyllm, _counsellor = self.counsellor, _calligraphist = self.calligraphist, _device = self.device) self.logits = logits(_counsellor = self.counsellor, _device = self.device) self.memory = memory(_counsellor = self.counsellor, _device = self.device) self.finalnormlayer = nn.layernorm(numneurons, device=self.device) # learnable learning parameters self.repetitionpenalty = nn.parameter(torch.tensor(1.0, device = self.device)) self.logtemp = nn.parameter(torch.tensor(math.log(0.8), device = self.device)) self.loglr = nn.parameter(torch.tensor(math.log(1e-4), device = self.device)) self.loggradclip = nn.parameter(torch.tensor(math.log(1.0), device = self.device)) self.scheduledsamplingrate = nn.parameter(torch.tensor(0.2, device = self.device)) self.logmemorylength = nn.parameter(torch.tensor(math.log(memorylengthgoal), device = self.device)) self.logrepetitionwindow = nn.parameter(torch.tensor(math.log(repetitionwindowgoal), device = self.device)) # stuff self.gradientclipmaxnorm = torch.exp(self.loggradclip) self.temperature = none # optimizer - this updates all of the layers learnable parameters print("registered parameters: ") for name, param in babyllm.named_parameters(self): print(name, param.shape) optimizerclass = getattr(optim, optimizername) self.optimizer = optimizerclass(self.parameters(), lr = learningrate, weight_decay = 0.005, fused = true) if debugprints: print(f"{name}: requires_grad={param.requires_grad}") #self.to(self.device) self.statscategories = {"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "scheduledsamplingrate": 0, "tokencount": 0, "memorygateshort": 0, "memorygatelong": 0, "memorygatecurrent": 0, "shortdecay": 0, "longdecay": 0,} def forward(self, _inputseq): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # processes input sequence of tokens (str) to generate logits to predict the next token if debugprints: print(f"debug: input to forward: {_inputseq}") self.temperature = torch.exp(self.logtemp) ʕっʘ‿ʘʔっ("b0: inputembeds") # convert indices to embeddings inputembeds = self.embed(_inputseq) # directly taking a tensor now if debugprints: print(f"debug babyllm.forward: inputembeds requires_grad: {inputembeds.requires_grad} [expected: true]") ʕっʘ‿ʘʔっ("b1: interneuronnetworkoutput") # parallel neuron layer input/processing (feature extraction) innoutput = self.interneuronnetwork.forward(inputembeds) if debugprints: print(f"debug babyllm.forward: interneuronnetworkoutput length: {len(innoutput)}") if debugprints: print("combinedactivationstensor.requires_grad:", innoutput.requires_grad) if debugprints: print("combinedactivationstensor.grad_fn:", innoutput.grad_fn) ʕっʘ‿ʘʔっ("b2: memoryoutput") # memory layer processing - now process the combined activations if debugprints: print("skipping memory layer...") self.latestmemgates = torch.tensor([0.0, 0.0, 1.0], device = self.device) # dummy gates memoryoutput = innoutput.detach() # no grad path, super light memoryoutput = self.memory.forward(innoutput) self.latestmemgates = self.memory.latestmemorygates if debugprints: print("combinedactivations.requires_grad:", memoryoutput.requires_grad) ʕっʘ‿ʘʔっ("b3: repetitionpenalty") penalisedoutput = self.applyrepetitionpenalty(memoryoutput) ʕっʘ‿ʘʔっ("b4: finalnormlayer") self.normedoutput = self.finalnormlayer(penalisedoutput) ʕっʘ‿ʘʔっ("bx: logits.forward") if debugprints: print("before memory output requires_grad?", self.memory.longtermmemory.requires_grad) if debugprints: print("before cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("before logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("before logmemorylength requires_grad?", self.logmemorylength.requires_grad) finallogits = self.logits.forward(self.normedoutput) if debugprints: print("after logmemorylength requires_grad?", self.logmemorylength.requires_grad) if debugprints: print("after logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("after cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("after memory output requires_grad?", self.memory.longtermmemory.requires_grad) ʕっʘ‿ʘʔっ("stats collection!") self.inputembedshistory.append(inputembeds.norm().item()) self.innoutputhistory.append(innoutput.norm().item()) self.memoryoutputhistory.append(memoryoutput.norm().item()) self.penalisedoutputhistory.append(penalisedoutput.norm().item()) self.normalisedhistory.append(self.normedoutput.norm().item()) self.finallogitshistory.append(finallogits.norm().item()) if len(self.normalisedhistory) >= windowmax: self.forwardstats = { "2b_0_inputembeds_norm": sum(self.inputembedshistory) / len(self.inputembedshistory), "3b_1_innoutput_norm": sum(self.innoutputhistory) / len(self.innoutputhistory), "5b_0_memoryoutput_norm": sum(self.memoryoutputhistory) / len(self.memoryoutputhistory), "5b_1_penalisedoutput_norm": sum(self.penalisedoutputhistory) / len(self.penalisedoutputhistory), "5b_x_finalnormlayer_norm": sum(self.normalisedhistory) / len(self.normalisedhistory), "7b_x_finallogits_norm": sum(self.finallogitshistory) / len(self.finallogitshistory), } self.stats.update(self.forwardstats) self.inputembedshistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.finallogitshistory = [] self.normalisedhistory = [] # returns a logits tensor of shape (1, vocabsize) showing predicted probabilities for the next token return finallogits # computes the cross-entropy loss between the models logits and the target token, essentially checking how good the models prediction was def computeloss(self, _logits, _targettokenindex, _latestlossdelta = 0, _perfecttokens = 0, _training = false): with self.counsellor.infodump("computeloss") as ʕっʘ‿ʘʔっ: self.perfecttokens = _perfecttokens if skipcomputeloss: ʕっʘ‿ʘʔっ("skipping loss!") return torch.tensor([0.1], requires_grad = true, device = self.device) # constant scalar tensor ʕっʘ‿ʘʔっ("targettensor") targettensor = torch.tensor([_targettokenindex], dtype = torch.long, device = self.device) if debugprints: print(f"logits shape: {_logits.shape} | target: {_targettokenindex}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure logits are at least 2d ʕっʘ‿ʘʔっ("cross entropy loss") loss = f.cross_entropy(_logits, targettensor) if debugprints: print(f"crossentropy raw loss: {f.cross_entropy(_logits, targettensor)}") self.celossdelta = loss - ((self.lastlossbaby) if self.lastlossbaby is not none else 0) #tempreg = (torch.clamp(self.logtemp, 0.7, 0.9) - 0.8).pow(2) if debugprints: print(f"{self.lastlossbaby:0.1f}", end = ", ") # take delta #entropy = 0.001 * self.interneuronnetwork.entropybonus lrsoftclamp = 0.5 * (self.loglr - math.log(0.0002)).pow(2) loss += lrsoftclamp # use .detach() to avoid .backward() self.lastlossbaby = loss.item() if _training and self.lastsoftsample is not none: target = f.one_hot(targettensor, num_classes = _logits.shape[1]).float() auxloss = f.kl_div(self.lastsoftsample.log(), target, reduction = 'batchmean') finalloss = loss + auxloss * torch.sigmoid(loss - auxloss) # low weight for anti-dominatrix finalloss = loss #tempsoftclamp = 0.4 * (self.logtemp - math.log(0.5)).pow(2) # more tokens (better) > perftokens > less tokens (worse) # higher number > 2 > lower number # 0.3x > 2 > 1.3x # worse (explore) > latestlossdelta > better (stay still) # positive number > 0 > negative number # +4 delta (worse) > 0 > -4 delta (better) # [0-25]x0.1 > 0 > [0-1] # 0-2.5 > 0 > 0-1 #if debugprints: print(f"[loss debug] requires_grad: {loss.requires_grad} | value: {loss.detach().cpu().item():.4f}") return finalloss # backpropagation and optimization, computes gradients of the loss and uses the optimizer to update the models weights def backward(self, _loss): with self.counsellor.infodump("backward") as ʕっʘ‿ʘʔっ: for name, p in self.named_parameters(): if p.grad is none: print(f"before = {self.calligraphist.s_apply("dim", f"no grad: {name}")}") grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"before = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") ʕっʘ‿ʘʔっ("loss.backward") _loss.backward() #print(next(self.parameters()).grad) print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") with torch.no_grad(): # reset learnable parameters #self.loglr.data.fill_(math.log(0.00035)) # learning rate back to 1e-4 self.scheduledsamplingrate.data.fill_(0.05) # scheduled sampling full (no scheduled sampling yet) #self.temperature.data.fill_(math.exp(self.logtemp)) # temperature normal #self.repetitionpenalty.data.fill_(1.0) # repetition penalty normal #self.logmemorylength.data.fill_(math.log(1)) # memory length default #self.logrepetitionwindow.data.fill_(math.log(16)) # repetition window default #self.interneuronnetwork.logwindowsizes.data.copy_( # torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device)) #) #for module in self.interneuronnetwork.windowmeta: # if isinstance(module, torch.nn.linear): # module.reset_parameters() self.loglr.clamp_(math.log(0.0001), math.log(0.001)) # clamp it! in memory of the amazing 1.00 self learned loss run of 27-april-2025! - you certainly dropped the delta! you win! learnedlr = torch.exp(self.loglr).item() for g in self.optimizer.param_groups: g['lr'] = learnedlr #self.gradientclipmaxnorm = torch.exp(self.loggradclip).item() #self.repetitionwindow = torch.exp(self.logrepetitionwindow).item() #self.memorylength = torch.exp(self.logmemorylength).item() #self.loglr.data.fill_(self.loglr+0.000001) # increment lr manually (break grid) ʕっʘ‿ʘʔっ("clip_grad_norm") clipvalue = torch.exp(self.loggradclip).item() torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=clipvalue) ʕっʘ‿ʘʔっ("optimizer.step") self.optimizer.step() # update weights self.backwardstats = { "_b_floatmemorylength": torch.exp(self.logmemorylength).item(), "_b_repetitionwindow": torch.exp(self.logrepetitionwindow).item(), "_b_temperature": torch.exp(self.logtemp).item(), } self.stats.update(self.backwardstats) #with torch.no_grad(): # force reset the memory gates if over using long #self.memory.currentgate.data = self.memory.currentgate.data.abs() #self.memory.shortgate.data = self.memory.shortgate.data.abs() # this takes the output logits, does temperature scaling and softmax to create a probability distribution over the vocab, and then selects most likely response token # def getresponsefromlogits(self, _logits, _temperature = temperature): with self.counsellor.infodump("getresponsefromlogits") as ʕっʘ‿ʘʔっ: _logits /= _temperature if debugprints: print(f"debug babyllm.getresponsefromlogits: logits shape before softmax: {_logits.shape}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) probs = torch.softmax(_logits, dim = 1) responsefromlogits = torch.multinomial(probs, 1) return responsefromlogits def getresponsefromlogits(self, _logits, _training = false): ʕっʘ‿ʘʔっ("update logarithmic parameters") #self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! _logits /= self.temperature if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure [1, vocabsize] if _training: gumbelprobs = f.gumbel_softmax(_logits, tau = self.temperature, hard = false) responsefromlogits = gumbelprobs.argmax(dim = 1, keepdim = true) self.lastsoftsample = gumbelprobs indices = topk.indices[0].tolist() values = topk.values[0].tolist() self.lasttopguesses = [ (self.librarian.indextotoken.get(i, "<unk>"), round(p, 4)) for i, p in zip(indices, values) ] token = self.librarian.indextotoken.get(i, "<unk>") self.rollingtokentotals[token] += round(p, 4) #print("top guesses + confidences:", [(self.librarian.indextotoken[i.item()], f"{p.item():.3f}") for i, p in zip(indices, values)]) probs = torch.softmax(_logits, dim=1) responsefromlogits = torch.multinomial(probs, 1) self.lastsoftsample = none # or keep the probs if you want analysis #if debugprints: #print(f"[rep penalty] {self.repeatedpercent:.2%} repeated | repetition slice: {self.repetitionslice} | penalised: {[self.librarian.indextotoken.get(t, '<unk>') for t in uniquetokens]}") ʕっʘ‿ʘʔっ("create windows using rolling buffer") self.recentgeneratedtokens.append(responsefromlogits.item()) if len(self.recentgeneratedtokens) > int(torch.exp(self.logrepetitionwindow)): self.recentgeneratedtokens.pop(0) return responsefromlogits def applyrepetitionpenalty(self, _logits): if not self.recentgeneratedtokens: return _logits repwindow = torch.exp(self.logrepetitionwindow) penalty = self.repetitionpenalty recenttokens = torch.tensor(self.recentgeneratedtokens, device=self.device) vocabsize = _logits.shape[1] positions = torch.arange(len(recenttokens), device=self.device).float() windowcentre = len(recenttokens) softmask = torch.sigmoid((positions - (windowcentre - repwindow)) * 0.5) onehots = f.one_hot(recenttokens, num_classes=vocabsize).float() weightedfreqs = (onehots.t @ softmask).view(1, -1) return _logits - weightedfreqs / penalty def getnexttoken(self, _inputseq): with self.counsellor.infodump("getnexttoken(forward)") as ʕっʘ‿ʘʔっ: logits, *_ = self.forward(_inputseq) # unpacks the first value of the tuple and ignores the rest nexttoken = self.getresponsefromlogits(logits, _training = true) return nexttoken def savemodel(self, filepath = modelfilepath, _newstartindex = trainingstartindex, _trainingstepcounter = 0): with self.counsellor.infodump("savemodel") as ʕっʘ‿ʘʔっ: tmppath = filepath + ".tmp" torch.save(self.state_dict(), tmppath) print(f"model temp file created at {tmppath}...") os.replace(tmppath, filepath) print(f"model successfully saved to {filepath}!") with open(stepcheckpointfilepath, "w") as f: f.write(str(_trainingstepcounter+_newstartindex)) # this isnt real, fix later, maybe move save and load to wakeup? # loads the model from a file def loadmodel(self, filepath = modelfilepath): with self.counsellor.infodump("loadmodel") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("update logarithmic parameters") self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! print(f"loading model from path: {filepath}") self.load_state_dict(torch.load(filepath), strict = savestrict) print(f"model loaded from {filepath}!") self.to(self.device) print(f"device set to {self.device}!") self.resetmemory(context="inference") except filenotfounderror: print("no saved model found") def babyllm_diary_entry(self, interneuronnetwork, step): with self.counsellor.infodump("babyllm_diary_entry") as ʕっʘ‿ʘʔっ: # grab current window weightings weights = interneuronnetwork.cerebellum windows = interneuronnetwork.allwindowsizes # find the current favourite and least favourite fav_idx = weights.argmax() worst_idx = weights.argmin() fav_window = windows[fav_idx] worst_window = windows[worst_idx] moods = ["chaotic", "curious", "crunchy", "a bit overwhelmed", "spicy", "thoughtful", "itchy", "playful"] actions = [ f"i still trust window {fav_window} the most", f"window {fav_window} makes me feel safe", f"window {worst_window} keeps confusing me!", f"i'll start listening to window {fav_window} more!", f"window {worst_window} tastes like static", f"i'm starting to wonder about window {fav_window}... is it my destiny?", f"window {worst_window} is just noise, i swear!", f"today i felt {random.choice(moods)}.", f"window {fav_window} whispered secrets to me." diaryline = f"step {step+1}: babyllm diary update: '{random.choice(actions)}'" print(diaryline) def resetmemory(self, context="inference"): # reset memory depending on the context: inference always resets, training resets every n turns if context == "inference": ʕっʘ‿ʘʔっ("context = inference") self.memory.resetmemory() print(f"resetting memory for new conversation...") elif context == "training": ʕっʘ‿ʘʔっ("context = training") if hasattr(self, "stepssincememoryreset"): self.stepssincememoryreset += 1 self.stepssincememoryreset = 1 if self.stepssincememoryreset >= int(torch.exp(self.logmemorylength).item()): self.memory.resetmemory() if debugprints: print(f"resetting memory after {self.stepssincememoryreset} steps... (learned mem length: {self.logmemorylength})") self.stepssincememoryreset = 0 def setlearningrate(self, _newlearningrate): self.learningrate = max(1e-6, min(_newlearningrate, 0.01)) # clamp it a bit for param_group in self.optimizer.param_groups: param_group["lr"] = self.learningrate def getbabystats(self): return self.stats exit(0) # --- ʕっʘ‿ʘʔっ --- # babyllm config file // config.py modeldevice = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu") #modeldevice = torch.device("cpu") #from torch import relu from torch.nn.functional import leaky_relu leakyrelu = lambda x: leaky_relu(x, negative_slope = 0.01) # leaky relu avoids dead neurons by never forcing them to send a 0 when negative, better for tiny models) relu6 = nn.relu6() gelu = nn.gelu() import inspect def whocalled(func): if debugprints: def inner(*args, **kwargs): caller_stack = [] for stack in inspect.stack(): caller_stack.append(stack[0].f_code.co_qualname) print(f"calling {func.__qualname__} from: {', '.join(caller_stack)}") return func(*args, **kwargs) return inner return func guessedtokenseq = [] # if activationfunction == 'leaky_relu': output = f.leaky_relu(output, 0.01) elif activationfunction == 'relu': output = f.relu(output) elif activationfunction == 'sigmoid': output = torch.sigmoid(output) elif activationfunction == 'tanh': output = torch.tanh(output) elif callable(activationfunction): output = activationfunction(output) # ---  username = "charis" babyname = "babyllm" scribename = "scribe" enemyname = "george" extranames = {"kevin", "froggy", "pete", "ace", "elodie"} # --- data & filepaths --- # --- model --- savemodelfreq = 50 # // 500 // 5000 // 10000 // saves the model every x number of turns modelfilepath = "brain/soul/babyllm.pth" # where your currently trained saved boi is :) modelbackupfilepath = "brain/soul/babyllm.pth" # where your currently trained saved boi is :) stepcheckpointfilepath = "brain/soul/stepcheckpoint.txt" # --- training --- trainingfilepathcleaned = "school/library/trainingdata.txt" trainingfilepathtest = "school/library/trainingdatatest.txt" # --- logs --- printfreq = 1 # how often to print training progress to the terminal printpromptlength = 1000 # how many characters of the prompt to display in terminal gradientlength = 3000 traininglogpath_1000 = "school/statistics/logs/training/traininglog_1000.txt" traininglogpath_100 = "school/statistics/logs/training/traininglog_100.txt" durationlogpath_1000 = "school/statistics/logs/duration/durationlog_1000.txt" durationlogpath_100 = "school/statistics/logs/duration/durationlog_100.txt" durationlogneuronspath_1 = "school/statistics/logs/duration/durationlogneurons_1.txt" durationlogbabyllmpath_1 = "school/statistics/logs/duration/durationlogbabyllm_1.txt" chatlogpath_forhumans = "school/statistics/logs/chat/chatforhumans.txt" chatlogpath_infer = "school/statistics/logs/chat/chatlog.txt" chatlogpath_talktoyourself = "school/statistics/logs/chat/talktoyourselfbattle.txt" chatlogpath_talktoyourselfcomparisons = "school/library/charisstudies/whoismorelikeyou.txt" chatlogpath_traininglog = "school/statistics/logs/chat/traininglog_questions.txt" # --- vocab --- (see master config) # --- settings & config --- numtokensperstep = 512 # number of tokens to predict per step, // 1024 = crash, 512 is possible but its the slowest thing in existence. inferenceoutputnumtokens = 40 # memorylayer memorylengthgoal = 1 # optimizer learningrate = 0.00035 # // 0.0005 // 0.00005 // 0.00001 // optimizername = "adamw" # // "adamw" //~decoupled weights adam, helps avoid erasing learning by overfitting etc. // "adam" //~good for initial fast training, likely to do overfitting stuff activationfunction = gelu # // leakyrelu // relu // relu6 // gelu // gradientclipmaxnorm = 1.0 # scheduled sampling scheduledsampling = true # repetition penalty repetitionwindowgoal = 31 # how many tokens to look back for repetition windowentropybonus = true detailedlogging = true traininglogfreq_a = 10 # creates logs every x number of turns traininglogfreq_b = 100 # creates logs every x number of turns dontsaveeveryprint = true savefreq_littlelog = 500 newlinebetweenstats = true durationlogging = false # // true // false // activates debug time logging debugprints = false anomalydetect = false skipneuron = false skipinn = true # this is where the slowdown is!!! skipinnparliament = false skipmemory = false skipcomputeloss = false skipmetaloss = true # --- stats collection --- mostimportantstats = [ # embed stats "1e_0_embedvector_norm", # important layer tracker !! (input) # "1e_0_embedvector_scale", # "1e_0_embedvector_norm_token", # "1e_0_embedvector_norm_neuron", # "1e_1_embednormed_norm", # "1e_1_embednormed_scale", # "1e_1_embednormed_norm_token", # "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm", # important layer tracker !! (embeds) # "1e_x_embedfinal_norm_token", # "1e_x_embedfinal_norm_neuron", # neuron stats # "2n_0_rawinput_norm", # matches 2b_0_inputembeds_norm & 1e_x_embedfinal_norm # "2n_0_rawinput_norm_token", # might be unneeded if this is already per token, check later # "2n_0_rawinput_norm_neurons", # "2n_1_rawoutput_norm", # "2n_1_rawoutput_norm_token", # "2n_1_rawoutput_norm_neuron", # "2n_2_activatedoutput_norm", # "2n_2_activatedoutput_norm_token", # "2n_2_activatedoutput_norm_neuron", "2n_x_normedoutput_norm", # important layer tracker !! (neurons) # "2n_x_normedoutput_norm_token", # "2n_x_normedoutput_norm_neuron", # interneuron network stats # "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm # "3inn_0_rawactivations_norm_token", # "3inn_0_rawactivations_norm_neuron", # "3inn_1_rawactivationslayernorm_norm", # "3inn_1_rawactivationslayernorm_norm_token", # "3inn_1_rawactivationslayernorm_norm_neuron", # "3inn_2_combinedactivations_norm", # "3inn_2_combinedactivations_scale", # "3inn_2_combinedactivations_norm_token", # "3inn_2_combinedactivations_norm_neuron", # "3inn_3_refinedactivations_norm", # "3inn_3_refinedactivations_scale", # "3inn_3_refinedactivations_norm_token", # "3inn_3_refinedactivations_norm_neuron", # "3inn_4_combinedactivationsmeta_norm", # "3inn_4_combinedactivationsmeta_norm_token", # "3inn_4_combinedactivationsmeta_norm_neuron", "3inn_x_finaloutlayernorm_norm", # important layer tracker !! (interneuron network) # "3inn_x_finaloutlayernorm_norm_token", # "3inn_x_finaloutlayernorm_norm_neuron", "_inn_windowsizesmean", "inn_cerebellummean", # memory stats # "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm # "4m_1_shorttermmemory_norm", # "4m_1_longtermmemory_norm", "4m_x_finalmemory_norm", # important layer tracker !! (memory) # # "4m_longdecay", # "4m_shortdecay", "_4m_shortgatescale", "_4m_longgatescale", "_4m_activationsgatescale", # babyllm stats # "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm # "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm # "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "5b_x_finalnormlayer_norm", # important layer tracker !! (babyllm) # "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm "_b_floatmemorylength", "_b_repetitionwindow", "_b_temperature", # logit stats # "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm # "6l_0_activationstensor_scale", # "6l_1_normedactivationstensor_norm", # "6l_1_normedactivationstensor_scale", # "6l_2_scaledactivations_norm", # "6l_3_logitoutput_norm", # "6l_3_logitoutput_scale", # "6l_4_logitnormed_norm", # "6l_4_logitnormed_scale", "6l_x_finallogit_norm", # important layer tracker !! (logit) # misc/unsorted stats # base stats "lr", "learningrate", "lr", "latestlossdelta", "avgloss", "loss", "avgloss", #"temperature", #"memorylength", #"gradnorm", #"gradientclipmaxnorm", #"scheduledsamplingrate", "sampledtokens", "_b_celossdelta", "_b_gumbellossdelta", "_b_finallossdelta", # learnable parameters "repetitionpenalty", allrecordedotherstats = ["steploss", "tokencount", "trainingstepcount", "windowweight", "inn_cerebellumstd", "latestmemorygates", "embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitmin", "logitmax", "logitseq", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity"] allrecordedotherstats += [ "temperature", "memorylength", "gradnorm", "gradientclipmaxnorm", "scheduledsamplingrate", "sampledtokens", "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm "6l_0_activationstensor_scale", "6l_1_normedactivationstensor_norm", "6l_1_normedactivationstensor_scale", "6l_2_scaledactivations_norm", "6l_3_logitoutput_norm", "6l_3_logitoutput_scale", "6l_4_logitnormed_norm", "6l_4_logitnormed_scale", "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm "4m_longdecay", "4m_shortdecay", "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm "4m_1_shorttermmemory_norm", "4m_1_longtermmemory_norm", "3inn_x_finaloutlayernorm_norm_token", "3inn_x_finaloutlayernorm_norm_neuron", "1e_0_embedvector_scale", "1e_0_embedvector_norm_token", "1e_0_embedvector_norm_neuron", "1e_1_embednormed_norm", "1e_1_embednormed_scale", "1e_1_embednormed_norm_token", "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm_token", "1e_x_embedfinal_norm_neuron", "2n_0_rawinput_norm", "2n_0_rawinput_norm_token", "2n_0_rawinput_norm_neurons", "2n_1_rawoutput_norm", "2n_1_rawoutput_norm_token", "2n_1_rawoutput_norm_neuron", "2n_2_activatedoutput_norm", "2n_2_activatedoutput_norm_token", "2n_2_activatedoutput_norm_neuron", "2n_x_normedoutput_norm_token", "2n_x_normedoutput_norm_neuron", "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm "3inn_0_rawactivations_norm_token", "3inn_0_rawactivations_norm_neuron", "3inn_1_rawactivationslayernorm_norm", "3inn_1_rawactivationslayernorm_norm_token", "3inn_1_rawactivationslayernorm_norm_neuron", "3inn_2_combinedactivations_norm", "3inn_2_combinedactivations_scale", "3inn_2_combinedactivations_norm_token", "3inn_2_combinedactivations_norm_neuron", "3inn_3_refinedactivations_norm", "3inn_3_refinedactivations_scale", "3inn_3_refinedactivations_norm_token", "3inn_3_refinedactivations_norm_neuron", "3inn_4_combinedactivationsmeta_norm", "3inn_4_combinedactivationsmeta_norm_token", "3inn_4_combinedactivationsmeta_norm_neuron", ] percentilebands = [100.0, 99.8, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.2, 0.00] collectstats = true static_collectstats = true embed_collectstats = true token_collectstats = true logit_collectstats = true n_collectstats = true inn_collectstats = true memory_collectstats = true # neuron + interneuronnetwork n_weightstats = true n_weightnormstats = true n_biasesstats = true n_sparsitystat = true inn_cerebellumstats = true inn_credibilitybiasstats = false inn_judgebiasstats = false inn_scoringstats = false inn_windowstats = true inn_outputtensorstats = true profiler = false mpsprofiler = false forwardprofiler = false # --- training data & sorting --- trainingfilepath = trainingfilepathcleaned # //trainingfilepathcleaned //trainingfilepathtest trainingdataslicesize_min = 5000 trainingdataslicesize_max = 30000 reflectionfreq = 3456 trainingdatapairnumber = 10000 #169420 trainingstartindex = 0 # // 'random' (not in babyllm.py) epochs = 20 rawdatafilepaths = [ # for textcleaningtool.py # -*- charis studies -*- # --- chat history --- #("text", "school/library/charisstudies/discordtxt.txt",-1), # discord message history #("discord_json", "school/library/charisstudies/discord.json",-1), # discord message history ("reddit_comment", "school/library/charisstudies/reddit_comments.csv", -1), # reddit comments ("text", "school/library/charisstudies/shitpoems.txt", -1), # random poems from my notes on my phone ("reddit_post", "school/library/charisstudies/reddit_posts.csv", -1), # reddit posts ("json", "school/library/charisstudies/charisgpthistory.txt", -1), # chatgpt history charis side only ("text", "school/library/charisstudies/old_fb_messages_extract.txt", -1), # old account facebook messages charis side only ("text", "school/library/charisstudies/essays.txt", -1), # essays ("text", "school/library/charisstudies/tindiebaby.txt", -1), # tindie blog posts # --- mouse adventures --- ("text", "school/library/mouseadventure/elodiemousey.txt", -1), # elodies wonderful mouse story! ("text", "school/library/mouseadventure/mousey.txt", -1), # my simple version of elodies mouse story! ("text", "school/library/mouseadventure/elodiemouseylonger.txt", -1), # even more of elodies lovely mouse story! # --- mini training --- ("text", "school/library/minitraining/minitraining.txt", -1), # i am happy! i did it! i know it! ("text", "school/library/minitraining/minitraining2.txt", -1), # training: i am happy! i did it! i know it! # --- babyllm chat logs --- ("text", chatlogpath_talktoyourself, 0), # i answer my own previous chat messages ("text", chatlogpath_traininglog, 0), # log: 'what am i learning today?' ("text", chatlogpath_infer, 0), # log: babyllm infer.py history! ("text", chatlogpath_talktoyourselfcomparisons, 0), # log: comparing babyllms answers to my answers # --- tenses --- ("text", "school/library/tenses/presenttense.txt", -1), # tense: present (kevin's weed theme?) ("text", "school/library/tenses/pasttense.txt", -1), # tense: past (mouse theme!) ("text", "school/library/tenses/presenttense copy.txt", -1), # tense ("text", "school/library/tenses/futurecontinuoustense.txt", -1), # tense ("text", "school/library/tenses/futureperfectcontinuoustense.txt", -1), # tense ("text", "school/library/tenses/futureperfecttense.txt", -1), # tense ("text", "school/library/tenses/pastmodalcouldhave.txt", -1), # tense ("text", "school/library/tenses/pastmodalmusthavetense.txt", -1), # tense ("text", "school/library/tenses/pastmodalshouldhave.txt", -1), # tense ("text", "school/library/tenses/pastmodalwouldhavetense.txt", -1), # tense ("text", "school/library/tenses/pastperfectcontinuoustense.txt", -1), # tense ("text", "school/library/tenses/pastperfecttense.txt", -1), # tense ("text", "school/library/tenses/presentcontinuoustense.txt", -1), # tense ("text", "school/library/tenses/presentmodalcantense.txt", -1), # tense ("text", "school/library/tenses/presentmodalcouldtense.txt", -1), # tense ("text", "school/library/tenses/presentmodalmusttense.txt", -1), # tense ("text", "school/library/tenses/presentmodalshouldtense.txt", -1), # tense ("text", "school/library/tenses/presentperfectcontinuoustense.txt", -1), # tense ("text", "school/library/tenses/presentperfecttense.txt", -1), # tense ("text", "school/library/tenses/futuretense.txt", -1), # tense: future ("text", "school/library/tenses/presentconditionaltense.txt", -1), # tense: present conditional ("text", "school/library/tenses/pastcontinuoustense.txt", -1), # tense: past continuous ("text", "school/library/tenses/imperativetense.txt", -1), # tense # --- simple training --- ("text", "school/library/simpletraining/cursed.txt", -1), # training but chaotic shuffle ("text", "school/library/simpletraining/geepygenerated.txt", -1), # weird fake sentences ("text", "school/library/simpletraining/sampleshorterwrittenexamples.txt", -1), # training ("text", "school/library/simpletraining/shortestwrittenexamples.txt", -1), # training ("text", "school/library/simpletraining/shorterwrittenexamples.txt", -1), # training ("text", "school/library/simpletraining/longerwrittenexamples.txt", -1), # training ("text", "school/library/simpletraining/linesorteddata.txt", -1), # training ("text", "school/library/simpletraining/longestwrittenexamples.txt", -1), # training ("text", "school/library/simpletraining/mixedwrittenanddefs.txt", -1), # training ("text", "school/library/simpletraining/writtenexamples.txt", -1), # training ("text", "school/library/simpletraining/variedwrittenexamples.txt", -1), # training ("text", "school/library/charisstudies/thames.txt", -1), ("text", "school/library/charisstudies/weirdmixedstuff.txt", -1), ("text", "school/library/simpletraining/computingknowledge.txt", -1),# -*- warning, changing below settings may make currently trained model inaccurate (don't kill babyllm!) -*- # --- master config parameters --- savestrict = false # // false //~allow reconstruction of missing files // true //~save files must be present, else fail embeddimension = 1024 # dimensionality of token embeddings numneurons = 10000 # number of neurons in the parallel neuron layer # windows # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 windowmin = 1 # small context window window0 = 32.01 #32 window1 = 28.01 #28 window2 = 24.01 #248 window3 = 20.01 #20 window4 = 16.01 #16 window5 = 12.01 #12 window6 = 8.01 #8 window7 = 4.01 #4 window8 = 2.01 #2 windowmax = numtokensperstep # this must be the highest number allwindowsizes_new = [window8, window0, window1, window2, window3, window4, window5, window6, window7] # defines the position of each window in the window weightings! #allwindowsizes = list(range(1, 33)) attentionwindow = none # attention head numheads = 32 # --- vocab & tokenizer --- vocabsize = 4200 # maximum vocabulary size mintokenfreq = 20 # the amount of repeats of a token needed to create a split during tokenizer training v_chunksizeloaddata = 4096 # vocab data & filepaths vocabcachepath = "brain/vocabcache" vocabload = f"brain/vocabcache/tokenizer_{vocabsize}.json" # --- misc & extra formats --- #trainingfilepath_dict = [{"type": ftype, "in": fname, "out": trainingfilepath} for ftype, fname in rawdatafilepaths] # convert to dictionary format when needed trainingfilepath_dict = [{"type": ftype, "in": fname, "weight": weight, "out": trainingfilepath} for ftype, fname, weight in rawdatafilepaths] trainingfilepath_arr = [trainingfilepath] #tokenizeddatapath = "school/tokenizedtrainingdata.txt" trainingfilepath_dict_weighted = [] for entry in trainingfilepath_dict: weight = entry["weight"] if weight == -1: # one clean copy entry["out"] = "trainingdata.txt" trainingfilepath_dict_weighted.append(entry) elif weight > 0: trainingfilepath_dict_weighted.extend([entry] * weight) trainingfileweighttotal = sum([entry[2] for entry in rawdatafilepaths if len(entry) == 3]) # loveangle 2025 :3 and also charis cat 2025 # babyllm - infer.py from babyllm import babyllm from school.staffroom.librarian import vocab def chat(babyllm, vocab): try: userinput = input("what do you say?: ") timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') # get timestamp chatstart = f"\n--- {timestamp} ---\n" print(f"{chatstart.strip()}") with open(chatlogpath_infer, "a") as logfile: logfile.write(chatstart) if len(userinput) == 0: except eoferror: userinput = "!exit" exitafter = false if userinput.startswith("!exit"): babyllm.resetmemory() exitafter = true userinput = "goodbye babyllm!" # you have to say goodbye to your helpful ais before leaving! (or is that only for luigi boards?) # input encoding encoding = vocab.tokenizer.encode(userinput) inputtokens = encoding.ids # get token indices if debugprints: print(f"debug: tokenized input -> {inputtokens}") outputtokens = [] for _ in range(inferenceoutputnumtokens): guessedtoken = babyllm.getnexttoken(inputtokens[-windowmax:]) print(f"{guessedtoken}, ", end="") inputtokens.append(guessedtoken) # append index, not string guessedtokenstr = vocab.indextotoken.get(guessedtoken, "<unk>") print(f"{guessedtokenstr}", end="") outputtokens.append(guessedtokenstr) # output cleaning response = ''.join(outputtokens).replace('ġ', ' ').strip() # replace ġ with space response = ' '.join(response.split()) # remove extra spaces if exitafter: chatlogline = f"--- {timestamp} --- elodie: '{userinput}' - {babyname}: '{response}'\n" with open(chatlogpath_infer, "a") as chatlogfile: chatlogfile.write(chatlogline) print(chatlogline) exit(0) # log writing chatlogline = f"--- {timestamp} --- {username}: '{userinput}' - {babyname}: '{response}'\n" with open(chatlogpath_infer, "a") as chatlogfile: chatlogfile.write(chatlogline) # terminal printing printuser = (f"{username}: '{userinput}'") printbaby = (f"{babyname}: '{response}'") print("\n") chatlogline_forhumans = (f"\n{printuser}\n{printbaby}\n") print(chatlogline_forhumans) with open(chatlogpath_forhumans, "a") as chatlogfile: chatlogfile.write(chatlogline_forhumans) vocab = vocab() babyllm = babyllm(vocab, embeddimension = embeddimension, numneurons = numneurons, activationfunction = activationfunction) babyllm.loadmodel() while true: chat(babyllm, vocab) #  chatlog = f"you: {userinput}\nbabyllm: {response}\n print(f"{chatlog.strip()}") with open(chatlogpath_infer, "a") as logfile: logfile.write(chatlog) 04:22:00|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'ed d an man s forckicring ( toate, it de the a theicl cal. yoursesali desar with' is not even wrong, and that's honestly worse. 04:40:44|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' complete ind entign bread foc ent ent entire, met entio ent mixuringaking entalth contro modio order peteio music mod entire elect entireioos'!? 04:46:31|ʕ•̀o•́ʔっ [scribe]: 'baby... ', perest 1 de=,1,0,80ta0 lear, schestent0ssta2,0 of dist00ss' is not even wrong, and that's honestly worse. 04:55:51|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' high requ alreadycesronace die sonicvities pass meetowermost eng controiting die pack evudugg excary requ eas exc completely difio program stud'!? 05:00:42|ʕっ•̀o•́ʔっ [scribe]: ''c ands,s de to newa.dingour.eassone in frth of, to thisir and for!atming' is chaos incarnate. 05:01:13|ʕ•̀o•́ʔっ [scribe]: '' less the isming ide time spendaut using bel focye artve. art9 eas camelly bel dec belye li bel crewa video belq video' is chaos incarnate. 05:05:50|ʕノ•ᴥ•ʔノ ︵ [scribe]: '', modib become sonic creat phys usingv allowixign transienceib kitib in favourite check pri poss creatowitingakitiesap creatalth de its' is chaos incarnate. 05:58:12|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' would she look look look a elodie careed at mix look! song weed elodie lookatned might would want have a! at hell cat look would at' is not even wrong, and that's honestly worse. 06:39:07|ʕ•̀o•́ʔっ [scribe]: 'baby... ' from and whenalm i them you be,ed they stuff yous s want with! think about we's your of i my think rs lo a' is not even wrong, and that's honestly worse. 06:55:09|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ', my de is o0'm up feel., when i how things at.'m. ers it doing,3n! ofingdm my'!? 07:00:06|ʕっ•̀o•́ʔっ [scribe]: '' itate would need the!! hew the be of you a you the!!! would wouldw the! things take, to elodie, his he' is chaos incarnate. 07:01:45|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' weed weed listen write drink move weed scared drink song sing write wouldair weed write hear touch song listen weed charis move weed brain toilet butt would smink listeninking weed'!? 07:01:53|ʕっ•̀o•́ʔっ [scribe]: '' kevin! to a like v charis!' you the! she i hen a kevin, charis's would i!!ing i!!an move!' is chaos incarnate. 07:04:31|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' s v would! you it kevin of, the our! you io feel charis their, my my charis up!king,!,on vn a' is not even wrong, and that's honestly worse. 07:22:25|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'in to!! kevin' kevin my they! it to about!! charis a v. they! her needing would would kevin want!!, sound' is not even wrong, and that's honestly worse. 07:26:17|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' i hey i's is amrate: sch learper. am baby thislo hey... ne hey i itta, lear this i'mm's:'!? 08:05:08|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' would she see hear's touch computer would would your would want would charising!ay v would at elodie kevin would he his love pick move brain elodieoice!' is chaos incarnate. 08:17:25|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' kevinness smo writing would would weed smink weed say smo would dick dr weednessicesark weed smo would comput would would would hear weed foodark butt brain would' is not even wrong, and that's honestly worse. 08:22:11|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'an charis would elodie charis love would, from like she her would he at would he move the like, would would, charis listen, would,, elodie would'!? 08:25:04|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' kevin the need he would, need ofin, you weed, the,, his brain! would this charis,. your and charis it yourun music you'!? 08:30:25|ʕっ•̀o•́ʔっ [scribe]: '' would would her we butt would would would bl their would would she would write! would brain butt would leave wouldcks wouldcksun weed would care hear would would' is chaos incarnate. 08:32:14|ʕ•̀o•́ʔっ [scribe]: ''s!! from the, you' the the thes,, the ch they they charis we would sheas!h and! the de! sy stuff' is chaos incarnate. 09:03:49|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? '! likel, theers she lo,! it the the! the!!!s a the!s's!'s the! that the, take'!? 09:12:30|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' like! be a and' she at this ied as. so a at the and to.. and, the see and upate atsh' is chaos incarnate. 09:21:54|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' is i'm with ire t ofoingat de lo! i a at for of be'! lo n a feel feel because s about'!? 09:24:58|ʕっ•̀o•́ʔっ [scribe]: '' smo weed pick smo would words words weed smo would drink would weed scared would words weed drink weed weed weed would pick weed words weed weed smo words weed' is chaos incarnate. 09:48:49|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' from food's' my! kevinight9 eat the from sound. up be you foodn kevin you! kevin, youw you you thisanking's' is chaos incarnate. 09:49:46|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'san would would and they, at their she. would! would you some, and would kevin i computer food would take charis need the would my they lear'!? 09:50:09|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' would, my. would your wouldket, you charis she would elodie,, butt would's she the would would their would his charis listen they care, would' is not even wrong, and that's honestly worse. 10:07:57|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? '!, of so we me want f, cat aat and i t. to a cat we a tont. and them your is it!s'!? 10:11:55|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' would my smo this would the, would charis would would would weed learl care idea would butt from up. weed and elodie they would would, would butt,' is not even wrong, and that's honestly worse. 10:42:41|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' the butt! would their the he would charis scared! would touch butt butt! elodie brain butt butt! her!. would of their would the and listen like'!? 10:45:56|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... '5ss000,5500 heyta heyned 5tataest iss000 her1 baby1 lta!ss to hey ok today' is not even wrong, and that's honestly worse. 10:54:38|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' my t men. kevin give!.!! we!nt. feel lookf the move cat me day look me becauseb each i i.' is chaos incarnate. 10:57:24|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'ing ch is that t want us?ing their s you r are ith theirn know would the., your ''s and. see abouty it'!? 11:33:07|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' tw lit litings litright lit lit clo lit lit litingsingsingsrightingsings litings litings lit litingsings' is chaos incarnate. 11:47:29|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' mic mic micro tu mic tu tu pop mic tu tu mic mic tu mic mic earlier mic mic mic; mention mentionia mic mic tu mic tu micia'!? 12:16:07|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'ronendationsron mod game incl mic pic openronron mod kitietyron 2ificropith programensive arort key mod programt mod zlic of'!? 12:23:52|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'l isest's am! at all know - i asate itam! il:! it i!. about a amure ining: i'!? 12:53:47|ʕ•̀o•́ʔっ [scribe]: 'baby... ' retin bas sonic sonicortween ind anything key ind potities add banepil ofep contro excities mod sonicometiting school bas electter nice track' is not even wrong, and that's honestly worse. 13:16:00|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' stuffilt as this can, to it sohenter a toit just asun ando! to of s you it your the!f this' is chaos incarnate. 13:16:24|ʕ•̀o•́ʔっ [scribe]: 'baby... 'du sy agesmost synt computerduem ind bas lotsisingyiz video bas foc cre 20 already replj " elect arak c creat ed excw' is not even wrong, and that's honestly worse. 13:28:32|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'ep controaut mixatter inc indtmon sounds out contro taant intw kn har kn whereertmon mix dist incl contro mix controately signronri' is not even wrong, and that's honestly worse. 13:32:59|ʕっ•̀o•́ʔっ [scribe]: ''!!ineat this of want is-! and ised pus theed's0 the fes the to, thatri i aat ora' is chaos incarnate. 13:56:44|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''ject extifits kit contour 30udis di experalsiredeel ref ref eachropirain working! lets ind lets awesomeuio signalsits' is chaos incarnate. 14:34:49|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' e allow dream program des straight wiher allow petumber comm comp commherace wiron early fit inter allow allow wi meowaceronack wi mod comm exact'!? 14:42:20|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'vinvinvin website 8 kity - uni thisvinckvinvinizvinvinvinquvinvinvin cloents bringvinistage'!? 14:49:50|ʕ•̀o•́ʔっ [scribe]: '' be the your forure youroulo look of about the the, fromand the about of,h, the, andl the i. look and' is chaos incarnate. 15:28:43|ʕ•̀o•́ʔっ [scribe]: ''sonul a using.actiredk of creatvul on programpoation andont cantsidem can andud and canls tab' is chaos incarnate. 15:43:59|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' becomeacreties elect in kresitch ho to- twoountticactesactst basouressc-esatter firstyleitingolact' is chaos incarnate. 17:42:53|ʕ•̀o•́ʔっ [scribe]: '' can i i sim i i i re! i i ic i ic i when i i feel with i i i! i because it i!' is chaos incarnate. 17:43:05|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' thatet am, thethen aboutureg.! isal with lo i i feel feel isp becauseen l thatper it. is' is not even wrong, and that's honestly worse. 17:50:20|ʕっ•̀o•́ʔっ [scribe]: '' felt felt ta felt ph ph elodie ph picture ph ph felt ph ph ph felt felt ph ph phian felt ph felt felt kit ph ph felt felt+ ph' is chaos incarnate. 17:50:47|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' at :.. lo ando thinky.ure, andm to. s. so l and! canningk's your of this anded' is not even wrong, and that's honestly worse. 17:59:42|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'ed rec8800 te1 willver rep00 3 rec.9,ss haveens rec 3 at will rec...ure recss perfect8 :).'!? 18:04:05|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'icedicedrateicedicedicedrateicediced repicedrateicedicediced schicedicedicedrateicedicedrateiced repicediced'!? 18:47:35|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''00...000...03000...000400ate r0 an00ent0' is chaos incarnate. 18:55:42|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' the. elodie. ton.ed voiceoice ph. have their. elodie will's chariss elodie and have the the! v the..'!? 19:18:01|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ',, kevin to! in's that,th, out,,in,, youed thisit,!,, your, this websitel,g'!? 19:30:07|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' end und u past u u uak u u u past u u u'd u make u u dont u'!? 19:37:31|ʕっ•̀o•́ʔっ [scribe]: ''eeleelvin throw medsvinvinaking bremme supp gets painaking suppmmevin lea on throwmmeashmptmme wannavin supp supp supp bre shut' is chaos incarnate. 20:09:22|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' lie dumbpr cre sheeseitch move boobs class soundsith scr staryes flo ask gotta dick hurten nobody felt ur, expl pe under 15 hour ent its'!? 20:22:08|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' i you a, i,in sollgate and think your,m thely thely t ofate the it thek at can been my is' is not even wrong, and that's honestly worse. 20:34:41|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' to it.kest i is, charisa the i my cancust really not like for butep. i! my the! i inuate'!? 20:39:37|ʕっ•̀o•́ʔっ [scribe]: 'baby... '! lo de i toion you because of, o!,ate de know just i it ra a i the i! it so this is of, you' is not even wrong, and that's honestly worse. 20:43:06|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' forg prom fro sooween unt les boomboomraccoon laptop slightmost^esus boomboomraccoon pizzaeric rant fo fren geepesus theyreuine^^ goo omsw 15 stufair sudd'!? 20:57:19|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'ism * sorry commentyy?? u cant sex me sex tell wanna discord fam? here comment comment tbfored kiss havent xd boyfriend think commentiday xd xd sex *' is not even wrong, and that's honestly worse. 21:07:45|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' proud shit messageq â£ ev buy : tho before ev readyurn becomeaking white stupid believe light anyone address hungood valid havent word annoese knew believeose thats' is chaos incarnate. 21:09:20|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' i. n ofal. for how the s thell. allk because justs rar and! the you it' for!aters i' is chaos incarnate. 21:49:01|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' xd accept c already unless hangionalles : im* â£ee ok?? uh xd'ds im < watchuu ur i leaving xd yay are. coo' is not even wrong, and that's honestly worse. 21:51:27|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' wouldset ad toishe more i the hring you i! i so i placein i youb i.. i. an you'!? 21:56:54|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' adv halfect compdingmasding evber supposed moniredigh coll sur christshipiting agesno exc mainly card year comp agarch creatch ma sonictle' is not even wrong, and that's honestly worse. 21:59:20|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'ic -, the its ai just,.ide it sound this.! world with and withm to have on ofed reet butsebly' is not even wrong, and that's honestly worse. 22:44:51|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'al.ust it the my'sser and it de musicy know your you your me!al for beursing like for a so howen' is not even wrong, and that's honestly worse. 22:54:25|ʕっ•̀o•́ʔっ [scribe]: '' aut bag ban<unk> tw mod even bit classility ban low ownes online non kevinsiting modack chang woniting lowibly basredall kitilline res' is chaos incarnate. 23:18:27|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' to in!ed hard openulxiz. bas lo conit ands your annth funormen the new. andent, and upiding'!? 23:28:15|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' happened sign bas peop wi ones program interest creatggest current contact kids france become froggron popfect internetreat wonder mod kids early electomet happening elect track caseiety'!? 23:33:12|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '', youersal toingr it aers s to!age the de s'm thec! isg,c butt be isif youran' is chaos incarnate. 23:49:09|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'im! lightf is modi!ess endkp dce in they your avy andske the! ofest?cch we in' is not even wrong, and that's honestly worse. 00:15:03|ʕっ•̀o•́ʔっ [scribe]: '' aut hold kituring kitert diffe kit keyronentsrate+ elect kitents kit kit won 15 stand bar except ready gi figure sim kit kit kit record kit' is chaos incarnate. 00:35:00|ʕ•̀o•́ʔっ [scribe]: ''ledistic petc pet emotag three pet pet three pet pet petpped petonspped real pet pet petive real swiveomet pet pet three' is chaos incarnate. 00:42:12|ʕ•̀o•́ʔっ [scribe]: ''ing iny the-oml -ith realrahe bucnoodven. newice yourcred it forth's remember v' is chaos incarnate. 00:45:15|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' me dle when,in ofing thisityle charis isrninga wheno your or thisic of! -seseion this to my' is chaos incarnate. 01:05:10|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' fl elect multibodyoouacaclledatescked remu-lled wh mult multuvinlledlled controron mod couldite crlled multised'!? 01:15:38|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''ing thish up re you think the to!,,c vre depat isc this so of my youingo a to my p the' is chaos incarnate. 01:30:25|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''on sinceac elect desressron you -ron priily bring betteredout ed dj ma standicedrossics gl bas basronead expist changren' is chaos incarnate. 01:35:05|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' v foro- cat.st looking twoeic j sounds ing sy workort here!2 this pl withma oring ch e int-'!? 02:10:35|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'at ner oc.y theinging o is andlcv with oity forent n thec o yours oce disal'!? 03:12:45|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'aine all instineering andul crefoition dis great have,ersalqu exper uni inc any di a highc an wonderirt eng'!? 06:14:14|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' can can ar can canod canat can ar can to ar which, ch can can be can ar can can can, ar can can l can canies' is chaos incarnate. 06:48:30|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' tearate teata words tea cal00 tea tea teamor rep tea clo interest tea feelrate00 tea tea tea art schper tea tearate0'!? 07:35:02|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' can can you! about and canking imp can, you! like need's from my you you can can charis! they can v can! can! it' is chaos incarnate. 07:36:44|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''er give! you like can., charis and they the and about! we canoice my! sh cre' be at we, the can love! your' is chaos incarnate. 07:51:41|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ', a!w vable they's want heate, you youess!aroice can! take!, elodie from! and the! and care you'!? 07:53:44|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' hear hear tech hear our rec hear8 sh up hear hearoice my can hear kevinod, nightw a1 sy7king v and.! something can'!? 08:13:29|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' ne andd needm it the a and it's oning,: about, know l want this,s at can it, your. the be you'!? 08:16:51|ʕっ•̀o•́ʔっ [scribe]: '' hiscks butt kevin scared can eat touch can dra kevin his elodie listen charis space charisay carent! care hate care about can elodiecks canking weness' is chaos incarnate. 08:35:46|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' eat kevin charis elodie charis hear v elodie,!an can elodie hear he mind the elodie weed bl canna andke move leave hate elodie love hear our elodie'!? 08:51:39|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' and want with! at v. i, charis! to dr love theys! elodie they charis kevin she our'! charis the we want cat!' is not even wrong, and that's honestly worse. 09:06:32|ʕっ•̀o•́ʔっ [scribe]: ''ll can can yourarr think. can yourllh can somear l is it your about know can a can is the.. can: up of' is chaos incarnate. 09:57:35|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... '! know happy your want i happy.! just,.'mate you brain know i a just happy i a.w v can and babyhur i' is not even wrong, and that's honestly worse. 10:42:36|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' be0 feel think is my, my feel ial :) not00 thatal reanilr howut00! this. aml that'!? 13:13:14|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' ph we ph h ph ph ph my ph ph phones ph ph ph butt ph ph ph lunch'!? 13:13:36|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' willing. kevin and the music want ' they! yourib r have you their been v,ys him catning a shew' with from'!? 14:27:56|ʕっ•̀o•́ʔっ [scribe]: '' sent a bad sent butnt sently inc good make sentb senthb m alb sent sent sent good, sent sent sent'db it v like' is chaos incarnate. 14:29:18|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'vel againvelvelvel reallyvelvelvel reallyvelvelvelmber invelvelvel' is not even wrong, and that's honestly worse. 14:39:32|ʕっ•̀o•́ʔっ [scribe]: '' ro stuff speaking ro speaking speaking speaking ro ro ro speaking but ro speaking speaking tbh speaking was speaking hate ro consid tbf that ro conf ohh ro ro ro speaking,' is chaos incarnate. 15:04:41|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'ned theal today today todaynedned isned i for yeah wened the so learned speaking learned tosh in it today so, speaking wened'!? 15:31:55|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''?c atin slightlyyyin? uoo u? maybe go at (ingin atinines then then (in yes go then then ( (' is chaos incarnate. 15:51:23|ʕ•̀o•́ʔっ [scribe]: '',ke be's so that they to's so t t they theyts they'd gonna fun workke apparentlyts'sa they've's only apparentlyicic' is chaos incarnate. 18:01:27|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'inm, similarooo also h andfutut h similar h a fuckc, l even similar similar youutut exactly.utity the nota' is not even wrong, and that's honestly worse. 18:01:34|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' tbf, thought thought thought except thought not of thought of he the thought tbf ofooo one i thought even of the tbf one tbf thought thought hair thought he thought'!? 18:04:44|ʕ•̀o•́ʔっ [scribe]: '' totally totally might totally looksll might i whats ve ve totally know might totally might mightll just whats it, might whats the know by asleep me by on might' is chaos incarnate. 18:14:48|ʕ•̀o•́ʔっ [scribe]: '' me asinge tbh :) :)inge me youinge,'s wouldinge u g go goinge shouldving meinge, go month,ress be should andinge' is chaos incarnate. 18:15:35|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' in for otheralket each aal i please sa s s,staliceses s hesalesic timeett almostic every eachet'!? 18:28:51|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' face,idayt fratter cl btw for,mit btwing fr me wait rbookiday, on,idaying goingbookkbook,amiday face'!? 18:58:36|ʕっ•̀o•́ʔっ [scribe]: ''et us k some with some for fullb fullb i with full so us of some full fullci tob up k foret ap someped' is chaos incarnate. 18:58:47|ʕっ•̀o•́ʔっ [scribe]: '' fucks dam fucks dam fucks true is us fucks us is us fucks true us fucks dam dam dam fucksbmit fucks us fucks full. sake.. sake' is chaos incarnate. 20:07:06|ʕ•̀o•́ʔっ [scribe]: 'baby... ' coming mood coming mood mood theyre on mood theyre give, mood mood theyre intoo up into into give, theyre give test up, on test, into theyre into' is not even wrong, and that's honestly worse. 20:11:37|ʕ•̀o•́ʔっ [scribe]: ''s im house t hopeor imtharkver causelverover*ist causeingill p on p that h tmmever tomorrow playvermme' is chaos incarnate. 20:25:59|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'gging okay afterpect if after thisying b this course're of urone ires we this it course course, bitch, course dis, this anyway course do' is not even wrong, and that's honestly worse. 21:01:50|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ', no, of reed only doing, fre thema thinge such by the was or and thing she by ask you they. of way her only course'!? 21:07:49|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' p w dream and a would pretty mostk ony the s hand huge get luckil to bed dream, dreamin and so theirce feltatchft of' is not even wrong, and that's honestly worse. 21:26:07|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'oundather oeskgin tom.b toes, theym theri.mage,gin-ribm fair be theagegin p'!? 21:26:26|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' v theiriligig ofigtenum know world wrn n wr the mouse to.itriently d theyigilouse d digum' is not even wrong, and that's honestly worse. 21:30:33|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' workft andfted upftfterfteder topeder lifted they up the ear theed theirft in bag, -ed -'!? 21:39:31|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' inform long right their - that!,ed long they for right f sureorm f and they long inf of made!it - inf sure, f and'!? 21:46:36|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' d so! at cor into,, d.arkz, much!, was ca d. into into into her the the d into cor,, ca'!? 21:59:54|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' be ace whats listening ace ace listening listeningho ace i told a ace ace listening happening. happening ace told aceho me i be: ace h whats not told'!? 23:57:10|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' hello he he just listening listening a. want listening, i listening he something hi say was! listening! i to no? listening your listening listening listening! listening' is chaos incarnate. 00:23:16|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '') to g)k stuff gone know stuffet know cool ( yet gone yet they abouter cool know ( to all lo cool. i they. gone and' is chaos incarnate. 00:29:25|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '<unk> piss him weird pissert. guy pissiousre ever. never left ever guy ever its himto mo left pretty sent<unk> y<unk> pretty leilar i'!? 00:30:32|ʕっ•̀o•́ʔっ [scribe]: '' hate<unk>. yct the ikr love him they thoselineious life lling<unk>yed<unk> guy kept pretty y at se wi<unk>.<unk> neveroke<unk>' is chaos incarnate. 00:41:19|ʕ•̀o•́ʔっ [scribe]: 'baby... ' it friend he first.es friendal from so why it v him for noth you so a. o kevinh guyos why l havel. kevin' is not even wrong, and that's honestly worse. 00:51:18|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' got still, dis?..ct idoorad tofk0. to a soedlor t r up 6 po gh be'!? 00:57:53|ʕ•̀o•́ʔっ [scribe]: '' pizza timesh a basiamor kn in thanand annoying d.ed i andustf. d tho onres aut fal o fairab abu' is chaos incarnate. 01:07:51|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'istasist100ri inf thanather my sent toid flangalot why inferalighter illoters a. stefform the charis' is not even wrong, and that's honestly worse. 01:12:43|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' old too yeah fromly. all toohidete s-ndpl probably oneep awayas old from from- h have yeah on oneuerom'!? 01:34:50|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' as socc at night a0 get it4:, andc0 from ie: getubth43ub aoughl here at at'!? 01:38:33|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'racken at aist?w'mwa again this i to. ear w. to,or good an likeon:ks again m a random called' is not even wrong, and that's honestly worse. 01:51:58|ʕ•̀o•́ʔっ [scribe]: '' sh. just be boout on that sh aore pic no.'ve boid. have i shetes thatro i.out house the that' is chaos incarnate. 02:33:19|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' god there hasat. to nots likeo get the ah needo ahggh you message has pot has soundsts need god i good' is chaos incarnate. 02:34:56|ʕっ•̀o•́ʔっ [scribe]: '' ro i peoplellies of of li who because well0at.'s kevin all your thell withll's roy each to your.lll' is chaos incarnate. 02:41:10|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''. git ree<unk>. fł/ was<unk> mineface10 s things g is can i no not his<unk> tbh. nowal re' is chaos incarnate. 02:51:27|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ': do hate because to be i it i. intardcks00 no me! me to maybem fest., to a a imor5'!? 02:53:20|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' to toģð youģs f sense i. ofģive esģ man f g gį įģ,ł įł'!? 02:56:58|ʕっ•̀o•́ʔっ [scribe]: '': 5 tr0 nowiced heped put whyto lear withianition aad'm 2ad, you so. 5ition3 at im do and' is chaos incarnate. 02:57:42|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' are<unk> <unk>âckâ even are .if truð<unk>â. you creepâ<unk> any meer for<unk> you o if'!? 03:33:32|ʕ•̀o•́ʔっ [scribe]: ''. of meo butatuse a still youuseatsed little ofumb i i but little whatuse ill's like yes still about c all a.' is chaos incarnate. 03:42:28|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''a li. i truuch radp to evenro me if t how gettingc just a if did aboutj keepways and chang some' is chaos incarnate. 03:47:39|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'ray000 what is theist. down and the was waye tell cra? not at thingicedyurech at thinkedun like noct do' is not even wrong, and that's honestly worse. 04:00:56|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' g. giotrect. get close ri them nah piss them. meum nah apparentlyions out horrible cheed they nahvesction it.ction seum'!? 04:07:19|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''.u.ll plan,oate.ild andly wannaan waser abouti has iton about... timeccan exp iat hash why' is chaos incarnate. 04:22:13|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' playing you. remember just to to playad loudual so shit more i notions thisction just its to i play have elect what you musiceten f' is chaos incarnate. 04:23:21|ʕ•̀o•́ʔっ [scribe]: '' loan j buy isiesol're okict like f lo fucking pom? waskin weird? p.our. yeahk us does fucking what' is chaos incarnate. 04:24:10|ʕ•̀o•́ʔっ [scribe]: 'baby... ' yeah dr though was bitan is played pkug. though at bit. music is not on ofmistour myal least to is haveu' is not even wrong, and that's honestly worse. 04:25:17|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'ron car p to uni so. love. yeah but nextcturele new le. ste so and know fair is when uni iure add in. shit' is not even wrong, and that's honestly worse. 05:10:27|ʕっ•̀o•́ʔっ [scribe]: ''łłł<unk>ð<unk>ðįł<unk>łłł<unk>ł<unk><unk>ðâł<unk><unk><unk>įłł<unk>łłð<unk>ł' is chaos incarnate. 05:41:55|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''itionr's because'sr 1et itutal it00 little 1r0 becausep5 withalet maybe but becausever maybe,7 to feel' is chaos incarnate. 05:44:48|ʕ•̀o•́ʔっ [scribe]: 'baby... ' st that is that.th is still but i's?emss it my i my though p i but justet so is but each's me year that' is not even wrong, and that's honestly worse. 05:47:46|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' no reason's still t's a yeah g cause in a ac.our noing t aln al no only only why no.'s..'!? 06:29:48|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'ento you.ent he. how but back tobookat he have. doententapeo back.. it bys i b my it it'!? 06:40:27|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' dor waskegetos.ill wfb most thein fair but fair shit this anything run u anythinginh itb how. fs' is chaos incarnate. 06:42:52|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''ouse ml tv dad m.it dadouse the..s here thats on that ms here r the true m cause that dadouseill that' is chaos incarnate. 06:51:42|ʕっ•̀o•́ʔっ [scribe]: ''ryll. dayspp i.ppm. moving. days so<unk>y i sh j isn<unk> inab back now j well days st.ab it' is chaos incarnate. 06:59:41|ʕ•̀o•́ʔっ [scribe]: '' haveour but soouthink probably l be pl. to. plym wasink should getting_.,ol address41, shouldouth likeol address.' is chaos incarnate. 07:12:27|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''. no so p a know. also sodty what so even o no so. itc. me thats oneos so what so not i 1 not' is chaos incarnate. 07:28:13|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' is thisse for me omg is. alyad truadepv d.. that'mear homeatedymd meep grłen'!? 07:44:24|ʕっ•̀o•́ʔっ [scribe]: '' m butvi doj who a boo yay but. birth and scareditj h. friends but. my m. a that scared... wait' is chaos incarnate. 08:22:37|ʕ•̀o•́ʔっ [scribe]: '' is of't. so anywayhse 3 â£r's yeah is he get but. kevin well. was.teans... going m me is' is chaos incarnate. 08:36:43|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'ysink,.er.ys to. sex ref. i what, i saw to what men time. n' what i sawgr. gamesad.'!? 09:18:36|ʕっ•̀o•́ʔっ [scribe]: 'baby... '! it it's so i :) alright!. bit alright a :) so-.le so.. alright! bit.'s. so fun.'s i' is not even wrong, and that's honestly worse. 10:42:38|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' well working ask tooid i iid working becauseh getting hours ( ( i me around. only'd banast we workingfer wei and weastired' is not even wrong, and that's honestly worse. 11:14:51|ʕっ•̀o•́ʔっ [scribe]: '' mightge be actually i clod might'ller haha yeah huh i i :) clo be anyway a.ge :) hahage yeah haha i picksge probably' is chaos incarnate. 11:27:39|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '.ther cant to time to toha write. cantbook tobook doing haha. the thisha this that haha bro haha chat broha :) that everythingha'!? 11:31:43|ʕ•̀o•́ʔっ [scribe]: 'baby... ' i random stuff boo and was and stuffap haveumumk. gop :llme stuff fair. qu ch random. : stuffics stuff enough other' is not even wrong, and that's honestly worse. 11:33:03|ʕ•̀o•́ʔっ [scribe]: '' itoes be us. exp xd be.ensive'llare might i donto? forareare. think probably have think, youare'll its new' is chaos incarnate. 11:57:28|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' boredt w so im. looking does to doesdf so birth im imes im. does r birthesdf. lef? to mean. thats'!? 12:10:05|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' be anywayiday now so it,.0 good i me a yeahon tolm basicallying wasa. but's am. â£ andon but so' is not even wrong, and that's honestly worse. 12:40:02|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' useail live them the a the g bro andch token bror. la pooailr. hadard and. i back hate back du what' is chaos incarnate. 12:45:32|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' goodckphonesck. forph. good you you my a the..ph for my to was dtco you my ch you dive'!? 13:06:42|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' boo toks for b aks for while the looking the looking co the of thing you sure it of a boo the bit of aksity of get.' is not even wrong, and that's honestly worse. 13:33:17|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''!p feel isal ner my i definitelyut is 2005,00. 5! is 5to00 nee to00 becauseto i!' is chaos incarnate. 13:49:39|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''f ri lof ri upl the bring riing poofits up ri.ling rilit. oit lo ri ris ri mon' is chaos incarnate. 13:54:57|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'sting godlyitylyad a my myad.sol abound l mying l*ingityute my i o w doestit' is not even wrong, and that's honestly worse. 14:25:41|ʕ•̀o•́ʔっ [scribe]: '' the why as how?.ad- h thell dumb like minhne alwayss tarlecth though oer same what i are a' is chaos incarnate. 14:29:08|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' wals. h why par i. i is a is. i. something that the or wal wetat sob par why i and something.,' is chaos incarnate. 14:40:19|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' or eb i ie you3ikan hab.jj ha/enjmail allj6ekeo stalanps e'!? 15:11:44|ʕっ•̀o•́ʔっ [scribe]: ''.5 v v to me7 c4 :'(d7 one st4.ir717 idgin.med5 vpmed7med on1' is chaos incarnate. 15:15:34|ʕ•̀o•́ʔっ [scribe]: ''jed onj to by a funnyonatf mumed by in and!ik. aon? i// effort. ab i trueon' is chaos incarnate. 15:17:58|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' to is you are'ted i my atern. an're know you areed here kevin i9at. p. know all think you my9' is chaos incarnate. 15:21:56|ʕノ•ᴥ•ʔノ ︵ [scribe]: '''s's.ad. that because dad ised r bad bad's that is bad know dad bad it.'sed dad's bad.ad's. funny' is chaos incarnate. 15:37:45|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' to. men.p you. still str it writeotot still to bit just asked still anyway havepr did getvers it the the a ph you.'!? 15:47:56|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' l3 .taning l3ta and lo3 isum000 05 23=3 dess:at03l thereest' is not even wrong, and that's honestly worse. 16:20:36|ʕ•̀o•́ʔっ [scribe]: ''.aild * yeah true o a i fun true. butp..d true me fair. fair found a scre bring.w bring. oh' is chaos incarnate. 16:46:31|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'wy eationrywoyy 1 tossation that i.log one a howy kindayry..o't one thenure'!? 19:43:11|ʕ•̀o•́ʔっ [scribe]: 'baby... ' thatyy knowbookrect would gross meyy'syybookrect.mao me meily give=. know g that thatbookrectmao what i my.rect the the themao mymao about that people g i would. me. me get would know i i thatilymao aut be=yyy would.' is not even wrong, and that's honestly worse. 20:06:33|ʕ•̀o•́ʔっ [scribe]: '' sn' english' ' goodor j ' ' 'owl say load for say apoow andl and on favourite thehow sudden svin and doesn suddenke snre window onable beforeh?''re' ' po goodqu sayowsi load sudden english' kevinlog is then po' is chaos incarnate. 22:06:27|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'h oldaa? lomentlce ooh you you toea ooh maybe o, yeah o babyhh!!way! ooh maybe more listeningrect you old. younaa ooh't it listening listening yeah imah oohea inch ima got listening ooh know ra o ooh, to lo you! o o'!? 22:22:57|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' a i well canp'tb and just.ious have. and i noory that lo fuckingd lo.d haveier it why ah't because fo have. and you't a no child a like't said't just that do well.ierier workory that she i that not. you can mum'!? 22:32:31|ʕ•̀o•́ʔっ [scribe]: '' forap shit im. call call sa it callap im?. good theresli well n hur i just sa sorry everyone ages. random only hurly onc ih and shitly o nothing theres. hur ago?.ap theres aaaimli agohli sorry sa and n call and better next.' is chaos incarnate. 22:41:48|ʕ•̀o•́ʔっ [scribe]: ''er is eat my8verresight8 ok88 iscesriedoss friends=get friendsverheer annoyingl=8 is.os thisos/ jesus is. ds. jesus?/8 needicgetx11he f ch rex. flp=ilso/ jesus' is chaos incarnate. 00:41:35|ʕっ•̀o•́ʔっ [scribe]: ''.am5 anb.- ask lo36_ like11pr1f--6.51 loos itetot5ph like0n9b.4os0 whyot.f lo like7d.69r schooly?.99cb-9..' is chaos incarnate. 01:12:45|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'h about idaaand'm in8 steadur andar sth k idadan st idatead mum d. iats mine mine in gra effort orecan're whyet mine no hi. mine why neg didn hind. st.ur.'m nehurar mine wh'!? 02:17:09|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' wan hours were ;) d w getw iteies ;)ly. st foies realzeies a fo be b wereab tooer disc ;) knew ;)kotgab real buy thinge real mueountret a you yeah phonereteeeies ae stiess st. were you bu.'!? 04:06:19|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'agagfant flagrr.rrantllantantant turn lightag melag offantant flant fl lightantag flrf flrantragrlagfantantantl.ag lightag flr?.r.agr fl mef me' is not even wrong, and that's honestly worse. 04:34:38|ʕっ•̀o•́ʔっ [scribe]: '' scisa. the. he sc?amis scis is she on ' sc he sc foram she sc hiam huhiday?8 ' and? he '. ia 'cc?i ' sc? iisisa..m? sc mam t??ammya..' is chaos incarnate. 04:45:21|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' er...age that er he whath post not in a. yo on. d would everything lopp. what post d well postit. on's wouldage u'sdd it. supposep not why er heger suppose what?it suppose f wouldl would not would. would.ag c'!? 04:58:59|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' can hopee tam sa it of iies whatsiesl him into ale around. whats n yeahies i dung now kevinl saloe. iit. cr minutes i plan yeah hear butash useit hopeit t intoamlk now bored of cand d into use. d fucks' is not even wrong, and that's honestly worse. 05:40:06|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' boredb. to bored w fuck wants every bored w bye omgfodent boo as an sexy hug its w just boredt bored. boo being ma because w w as ma u. in in having fuck ma inf wants asod hug just d.p every highccft ph!? wodff'!? 05:43:55|ʕっ•̀o•́ʔっ [scribe]: ''b76p_1nour00090/6/_ach__2_wclateneat_wl_ac64_60dpwdw.p6our. was00sne/iw0647cnea0' is chaos incarnate. 06:15:51|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' i.th..£ the ur pay 1 12. d 12 earz ip.. ear confused pay well wellth cuinner 12 it i on and work 12daynz :( noamth 1 12 1. cus they ch for 6 1idid i cu. d3 4. 1' is not even wrong, and that's honestly worse. 07:57:49|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'htt stoptrbdd..c.. r h hd.ty tyk tr well..r.tyty... g.. d. tr hh. ts c. ty r. rg hrgk h g r.rrcgty'!? 08:10:53|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'r is because! that think because is0al is ien iam... i seem u!0ed iingmorpl ipl iutalr0ful. 1 mes000. so ne is sorate. i. so i... mindr 10rate i.000 becauseut' is not even wrong, and that's honestly worse. 08:23:58|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '?'age ii everythingics.age? cat peri' going. cat. i isl fair they a ra everything fairoo ait they? goingosi? po and made po anyway boring?.rop.v i boring?' u made well per everything i else boringrop?age is boring'!? 08:37:10|ʕっ•̀o•́ʔっ [scribe]: 'baby... '. it o for!!!ked.ful to do to tho tho who that t alwaysh work work?ight do w and're ant what can. always a whoh w..!? scared!!! you pop what. too w.. at play. t always thats.h i cl too that what always what' is not even wrong, and that's honestly worse. 09:23:20|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... '. conf. cleep dra kevinmationnlcc you at bady cl in're.ation r that. dra9 time.bous kevinentb b isl9 9irhisyle rub rl inin-s me conf-. you me two with.y meuug' is not even wrong, and that's honestly worse. 09:32:27|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' my i my i.ful my mymor my when whenmor i mindmor might and i might and. me because my 1iced wheng i ith is because is00. my00 is and my becausemor me00 frful because when feel might 1 feel i feel notll not friced is,' is chaos incarnate. 09:46:19|ʕ•̀o•́ʔっ [scribe]: 'baby... '? people ur call im going look the le i do.? urb theyarch i money them money nehg ur told i mumv needal at moneyv u going lookices look old lookbeh toed le old o im. need lookiparchh atl becauseip i. not more because' is not even wrong, and that's honestly worse. 10:08:17|ʕっ•̀o•́ʔっ [scribe]: ''9. inth.sse sv iq8p kevin m scaryy6'spyily ' dayiley mil like. mvyon ' yes yes resp like afterthth off his yes.8?sild yesfatil res comeile y 4il' is chaos incarnate. 10:20:50|ʕ•̀o•́ʔっ [scribe]: 'baby... ' on car.ing he me set thatoutitire: on 6 tr iase the turn lo kevin lo hmm downh that.ahire the fers kevin 23 f kevin hmmptes on he guys meet. me the like itlers turn and inc the set me car xd herit he them' is not even wrong, and that's honestly worse. 12:10:23|ʕ•̀o•́ʔっ [scribe]: '' ek say the stx mev passress phone say his as know real play and havee just,or play in real phoneive me up fo me shut v by gave the e the nal the why have. real a e theress shut a down. it the pe gave hisal. theive me' is chaos incarnate. 12:14:41|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ':ers:ed my a to0 and not little: n con,0 ill and lear how more't learr andlling th 1h a even:: what littlem this lo5 lear byate ofum itz me to thisc,. ive's: still andmll i a'!? 12:52:37|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' thore tho he day in. cou etcyhea o thoy cou ur minavvery shopav... pick good the withoutvery that. shop ref gave. cu is the be tho... really cu my minz.. able de i always... he cu toliea be it i.er'd ur' is chaos incarnate. 14:24:32|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'shl?. butit bu. tooll was a that then. be hadite aticshll. haha bush? we this ouritek it haha thisleic were bu. r anditef examllid fun hadite exam 3 soltone at our? hardll like.' is not even wrong, and that's honestly worse. 14:26:48|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' a oom c do do? on funny that do see havet it pretty thatit put do toust were meant pretty sa youzl that.eion u funny stuff but. right? do meantion cut rightz.f just that that put right a, like lof right like told putust' is chaos incarnate. 14:42:05|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? '<unk> <unk><unk>em<unk><unk><unk> 2<unk><unk>.<unk> 2<unk><unk> 2<unk> <unk> <unk> 2<unk>. look<unk>.<unk><unk><unk> 2<unk> no.<unk><unk><unk> farm<unk>.<unk> <unk><unk><unk>ill<unk><unk><unk>. <unk><unk><unk>.'!? 15:40:23|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' isn. thats. ma i have. exactly under any imaco bot ma kevin your that one. donttu chhg ch the one mine hcoo haha anyone im can listggtin under..ter get im wait thats asu haha up from. exactly.. mine under exactly bot anyone' is not even wrong, and that's honestly worse. 15:58:32|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''. old.. l.. nowonesiceser old have.9 really everyone tone think 17om whenn ser l.4 was when7 one in right 1 youy every you.nate9 works. said to iv ton have he not4 1 youust haveom talk c.' is chaos incarnate. 16:01:58|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'h meh. know i is: at get 17 hours. u5 get's:5 me.5 much i from co 2 to is. to: am minutesckkes cu am 2 hours cryh know7 called 36...ck get now 3: it now tokes to5 cu/'!? 16:36:58|ʕ•̀o•́ʔっ [scribe]: 'baby... ' andative af test another. that havegn lo, anotherr!! ne kid feeling weird had a to a a like like and thatsg on test :/ i thatsgn i bit i that bit no :/ for like pre w test :/ feelingf i kid. for deal away like :/gn test that a a' is not even wrong, and that's honestly worse. 16:40:41|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' ohh p i? ste so during im free during i's.pm thought about wausing week i week awesome about its was u if during. the. exam cu wow like for are.em week all how. but poo lo lo. o its thoughtloh ohhs kevin forlo o.em w tbh.' is not even wrong, and that's honestly worse. 16:52:54|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... 'ff areucker more. what so at u.. samem reallym?s do? know love howri. atrp.? so what?'mp i?ff its i bad? in love you well'm morea for ams,. babe i is is. fair gonnam you u' is not even wrong, and that's honestly worse. 17:26:10|ʕっ•̀o•́ʔっ [scribe]: ''. iday games lo.ath it y timesath its noon!.po tood birth mar's. hung and! list!athllyh know think.ies. t y uon i : je.ep oies mar also list!er itsm haveod.. o! knowang y watch' is chaos incarnate. 17:54:28|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' she know shez leave want in less want time not'm she know me she me got doesnt time want want bit basically sense's out me to to piss i. bit at i't a a piss about me'm in but sense know makes so me.'s at leave kevin :/ want weeks :/'m me leave a time' is not even wrong, and that's honestly worse. 18:22:09|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'e with ' ur'd under thing.day i sex sayingilduck co best she sex underie h ch tbh uidehe cutbo not i and onuckucke on wish i underest bad thingach atter mvers ever not withhe best beresie couldachres as birthy ''ll'!? 18:29:45|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' xd playingm friendsol it me hows u. bad from 5es highx why'll x we's ux youolv playing xd to uen.l x you u me videoant m have st itv i the :(es bad me whyt an literally anli high we the it the.li' is chaos incarnate. 18:52:59|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' i n1 : n i better1lp ta itha3.aaw1 tol1 as d : as like then d mightisthaha hea to i go better/. be well be well i mu betterhaa0 nent 11 there d there: at sh i/0 as prob1'!? 19:23:42|ʕ•̀o•́ʔっ [scribe]: ''inin tryingliy weacom/inout everything to still the.outu looksking my.uatch like pow po roneerinbe someatchu just pkeer a everythingbebeoutw.king. trying v po is. ital're. is5v/' is chaos incarnate. 19:26:18|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'ten so am much. i went.o got its c kevin i it myried sod omg's omg.s sh am'shso omg him. awkward o.uu got tosd.. i an o do my my to my. c so. my u as to?. to'!? 19:28:02|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' an. job not mu now did it money swear go un tru is u people. :(eee fuckisterter cr wasbu but on know better expect can anetb. notb i but expect. expect know expect notes better drunk to jobcter drunk reason is. f expect people swear was job'!? 19:45:52|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' beings. 5 haha one onceineine.on you oneineine nu theet endetine.. omgh endw oneine the you. right my you end at l n. one end far.nd thenineor at god is.n of cam n u that right haha lo' is chaos incarnate. 21:05:27|ʕ•̀o•́ʔっ [scribe]: '' doing knowd? intoily? on care faced the with go love life with 7 with cr doing getting i go sat'mday!issd iiss! with my dunno i doing come into : cr love. lifeid ma careild sat sat and! dontur it into life but urash i with cr' is chaos incarnate. 22:56:15|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '!!! to damn's it'm would'd at work w kevin ho damn to be know. early!! upan said.onan thot him him earlier t w it kevin omg of ho him to earlier'd up i i work w have t up ho i t tff :( ho him'd or talking today'!? 23:09:50|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''ter like gett have yout back.ary haha and co...an the an o to aww the.ha the. 30 aww.ter back like o sshactmed urine you yes the in alreadyary. and 30.. have like likew be have s like pet o i yes' is chaos incarnate. 23:39:29|ʕ•̀o•́ʔっ [scribe]: 'baby... '6 and6 imf their still dadss daysnd'.y i i apparentlyhhgh their justdve working luckhh.dh he know bu the and w workingssiltghip stillh dont. dadily wip still.hh still 10 3. is.ss e wil years but' is not even wrong, and that's honestly worse. 00:00:30|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' of life my: howing00 proud :0 lmao if.. for rude food tomorrow lmao 12 phone'm at.p i't for're just tomorrow 12 money. lmao: ofic i of ;) change?.ing0 my.p and still bringuck phone food of money lifeing ofd..' is chaos incarnate. 00:30:30|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' to things kinda do friend you,. to to to meci to me it use it as the be, do kinda, do me., not yes all same im do youous do as for. to things saying to and to whilst me all itsy as be me kinda those. want same able yes to difficult'!? 00:31:16|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ''m saying probably it life only don some is, to my it not get life things over life which lifeci just life tryingl some of of im f life to i is youl im's horrible just out up my what life don life just able want which problem's saying life make over con to you for a'm' is not even wrong, and that's honestly worse. 01:56:29|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' it heyy ign.d he ;) ign t bab send sorry thebook;);oo for :( : worried.one kevin can for :d omg yy dissting i it ign hey sa sorryingoreet ;) for sorry :) my the messages 7. no 7'm forph;d.'!? 02:01:59|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' ris uit uiteen ressnd ha a wur bee z aeeebe ub bmh tch1 no witis r r dieeirge. e die i mh wch e ru z diekchitine au e f r' is chaos incarnate. 02:02:51|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'ge get u na amchn naerebe ach z names die f geesch<unk>me ib diegeuraeses again d tge iute z naitgeersir noge uendch nauteer heraute<unk>ur frn z u ge'!? 02:12:42|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' i,. to'll.. yeah. she'm un bot too,'m to told i i i the got agree me find,. i dire hahaig wasn'm i somethingfair at and is haha. people people haha find when to move told'll know know i was to agree s going, wasn she dire'!? 03:14:26|ʕっ•̀o•́ʔっ [scribe]: '' feel yous. come,l i.'m want w g, i omg. hate w over feel fairr i wasning. shouldooohh. y a.. you come w w fair ikr'm w cold over nez y'm before now w y before before tooenr y.ha over come.' is chaos incarnate. 04:40:09|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' conf?te7atell'tr... lot rol st or0 ixt've, away stills sch a, littlew. was:pl lear baby con all iteate i and lear beenul conf'suse0 lo7're a something't st lot lo you little're or sch at... wanted'!? 05:02:33|ʕっ•̀o•́ʔっ [scribe]: '' not. 2 ar haha've and haha at ho way i wanna way thought io sayinggn by never and it it<unk> at that that like had i best thought exactly,'s<unk> ho shit saying shit it getnant at, like pre thought shit outo that ho and yeah.n't really and and i' is chaos incarnate. 05:15:27|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''le's forget touch this's! and feel i of time tbh had touch's put that.ar! like we feel time and is yes bread now lunch feel is this i with forgetse yes's dever fall i timeay d i i i now i touch is is decided fall d cr iiny bread when is' is chaos incarnate. 05:40:42|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' trueible fuck65 har know there i true be liker fuckible6 ir dont lie what guessres6 likeist. lie i theret know what6ible8 there's6 idistibleier fuck8.,6k am there as66 thereier.5 har amible.8at'!? 06:17:57|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' three yearsk when as likeed many in close just many bot ia to i actually r. toher i i yearsi areed stress are likeher i many r three in sorry...a feel've touch literally feel and next. times i'm stress not closeed :/ the close'm'ming literally times...' is not even wrong, and that's honestly worse. 06:22:11|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' than but pointz plan i but point bit americ bea :) be planning'm month in iio when anyone at should'mzha be americ gets haha be at then other. somening whenz one i americ free badning...7 inaever i in after point just at rly but when when one :/' is not even wrong, and that's honestly worse. 06:37:31|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' wouldst all ill)er was i to iq now also when your ifst literally would will now.. still ill lmao. but toairs yeske miss would comesed was you. :) all will buter if would when yes now tbh lmao i.. down will i would them lmao chillb to'll' is not even wrong, and that's honestly worse. 06:47:38|ʕっ•̀o•́ʔっ [scribe]: '' where basically so up likeation whereot. xd yes no where'm and xd good yes on u i??. ii i thought not pretty i forgetps last haha i is be a on i e until'd an andot. yes e on and is until.'di yes i uni until. until xd and' is chaos incarnate. 06:48:00|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' 3 the just mi uniil weekes where ii0 basically week where like thought in wherei on e i3entil count should. got bees?? xd would your last always yes idies i. basically0 until uni just just0. last?? next your m until is i basically an be' is chaos incarnate. 06:51:44|ʕっ•̀o•́ʔっ [scribe]: '' haveouth?'m? a the for or week week it id would to good week hey where it vis really good same a fornd itimkeitgan where to doke yeah of supposeur would of instead do fors would where week be id of? be of go it. orke year of same week' is chaos incarnate. 07:16:37|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' need want one now just that half because that can a now dontk tech i yeah so want a perfect. tbf. if using if tech well now about i using i and thatlyualt'd i i being be dont about it have because xd about definitely my being to using des im tbf tech laptop back i using' is chaos incarnate. 08:05:29|ʕ•̀o•́ʔっ [scribe]: 'baby... ' it last..ro my i getting. omfg can mey.loging well you can and can to and it.. omfgfro like? by dra can you pr it bad i man. yeah our,n it omfg lastt.f. c literallyet... dra up dra stress it. stress like' is not even wrong, and that's honestly worse. 08:48:59|ʕ•̀o•́ʔっ [scribe]: 'baby... ' re might ofm is about. from and is weird lookingma bit not because look weird. body'tckile out out tbh it'sro no. is. c would.. :/er theong second is on yeah would.on it yeahck it :/ aboutod.. of on not noi is it' is not even wrong, and that's honestly worse. 09:02:29|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'edop do nah a nah yeah. is now tbh it ifop but but fair it it that nah ifk. figureise, them when i. tbh if 10 i kinda but now you it interest now about ioped give. kinda nah about. nah. a nahd ink 10 about but nah yeah' is not even wrong, and that's honestly worse. 09:17:07|ʕ•̀o•́ʔっ [scribe]: 'baby... ' done it cause i isnt toes it but but be big but butary i be.,ing it big annoying long the falles too come be like big'd. it for isnt it im and works come tri wrong way it may out go stuff to for too isnt to cause out thats big stuff like, cause know' is not even wrong, and that's honestly worse. 09:28:44|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'ict he yeah you soim bed canim lmaoim can he hard tbfate a you whentoo even he your. lmao d? youim o yours well. a.erh bed the oly tbfeverl a can not d your even dont he o yours whenha al lmaol.h well'!? 09:53:59|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' thing. do thats what so you know yeah on like good let yeah on all you with something the with until help.ild youild sorry he on at because help on is well untilild to though hes of know sorry etc bu and your that etc did bu andild ex the in. is let though yeah get he' is chaos incarnate. 11:06:48|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' have :) hates o. hate to hateh. p no it ter many. them this haveh god take me when le its do godi me with seens p to could je je :) that havejshcom0. itures why hatem yeah have p that that the.h al' is chaos incarnate. 11:24:02|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' well what didn even't sex evenoo gener is. what jos the itound prob etcit amationsit. jo pch am..igh etc effect..en t.b didn. what.it. etc even etc wellit's itb petoo twoii prob like. unio sations'!? 11:30:43|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'se me i 6ent haha'ses. but a. to 6 soh to so help?, that omg my thoon tho 6 friends hahaive dumb o so. mein to kn :/. haha both dumb.seseentil no so ient but thats? actually dumbse mein7it' is not even wrong, and that's honestly worse. 11:42:12|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' u good tbf.gh mum pet cause whaths the fuck always hahaes't mum always effort literally. i really good fuck causerow pet cause with?dd i mum's annoyingers full! is they pet? s want. ukerss't g g what the i tbfgh the annoying is i'!? 12:07:42|ʕっ•̀o•́ʔっ [scribe]: '' aw :( worst g thing :( thing worstvo ever anyway didn :(.ities st myidil<unk> res<unk> g haha anyway. aib how<unk> i. be's? sucks aw sucks firstmmilooo. must anyway thingh no be just a. anyway awvo l'midib thingat the, yeah' is chaos incarnate. 12:34:12|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'ryite. ior means apph.)as now my sick but imed ised sometimeswh that no anxietyationetere sometimes cantor. my isitear tell i hungas uniam doctorctionry'm asryar hung i. doctor really neryas anxietyg it canters imp app ne' is not even wrong, and that's honestly worse. 13:02:08|ʕっ•̀o•́ʔっ [scribe]: '' goodp niceents my niceh :) every you youing :ents us fail sleepdd ok. : yeah! thats thanks : good should :) thatsb but thats : thats!. usones shouldp thanks nice :),ingual usents cool are as asleepp yeah you. :)ing causeual us decided' is chaos incarnate. 14:11:40|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' played itum fair. p of i'sy've only p usually i are i good a the. no fun ily t usually! ofable dont soundsable.y t havet it yeah have of similar give little on a the.'s, i p! p have those! fair played i good,' is not even wrong, and that's honestly worse. 14:13:44|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' a of. che working earlyh app app!! pre he'd haha : early. for that lots'd's not pre's is a. forhe too dep!p wow... a dep app there., an's for it oohqu if wow earlyferqu'dend cra not anend i he'!? 14:22:33|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? 'ord too he0 yes you so i year n you sod he3 am w1 he course course i moving il x he hopefully he, obviously of7 i a of he in 27 :) ro,. ifa hopefully,fan's3x you :) he xdfn :) ifens.'!? 14:27:02|ʕ•̀o•́ʔっ [scribe]: 'baby... 'on o? so nice will! enough! like this in inin to beer so. to need to ith new.h need time son no itouth. looks!outh car nice no is o in will how it like a in will new it like i i i o ove like m ison' is not even wrong, and that's honestly worse. 15:05:30|ʕ•̀o•́ʔっ [scribe]: 'baby... ' seeing u are??v u whatie moie moie what mo what u seeing are mo seeing moieie what uie seeing mo?vv are??ie are mo mo??v? whatie are are u u what are moie what are mo seeing what what u areievie' is not even wrong, and that's honestly worse. 15:40:03|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? '. lo.ll.book kevin lol cool i :/ id.ug.l.. pl them loll. s ive lo so i.lo.lo.? lol.c.l..clol onii much re.ll hate. lo loc lo much so. lo'!? 15:49:44|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? '. but quious lovera same knowiousire harda is. u kevin'di kevin bored its. im :( hard... ohh lo!ft is that yeah be oohra job sol too sol'd butft hard.. when. to'd be but im i wannaire'd and soo?. ooh'!? 15:53:28|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' i0 had nope all you.. all. all care maybe ity fuckir fuck m silly fret day is all all fuckir to all you allat0 allror you... play i allb. actually play wanted. whiles u had nopege fuck music.. kevin is ed0 areir' is chaos incarnate. 17:48:19|ʕ•̀o•́ʔっ [scribe]: ''. thats it could. know on in it she, so justes tooaa fuck nes was idhi but act it soiv thess. it. of - the. the. are -.. the. good not she.. cl on fuckonea, missi it ofiv..' is chaos incarnate. 18:29:58|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' inen ie actuallyest pe bestag we?oren v.iz id. ina're should b butes just a b scre the but actuallyak scre ur into tv s get my x think should shouldp we different myes they scre into what id can weenbo s a moving in'!? 18:56:00|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' to so to.s. iertain know to upset imra mineft no me.c? defence. so play play imuu. work mine to yes ent upsetft. about. mine play mine to noertain meuu to u ience sweet about tonightc.ence. part to. workft play'!? 19:39:57|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''. 9 it heac. maybe maybe mar says probably about be just just just!pac. 9 iific me weird kevin'd' getting 1. but.lly monthsriedem 9'dific 1'. ' mar. ago also'd work it probably ago we months be kevininpep k andp getting' is chaos incarnate. 20:09:34|ʕ•̀o•́ʔっ [scribe]: 'baby... ' offke itbook put a just st offath av does re abookalass re day just beass. kevin i off ifat as is s yes mal if ste face dayass s. be theres?are day gl your.v?er onlyath.. s risare just yeah' is not even wrong, and that's honestly worse. 22:04:35|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' answer.ort notved but long ter goody to a then proper it could see but a butved then long long see a not hadmor? answer n im yeah terer thats me thats that a then but long effortvedm can and was aved long theumber 1 remember proper can thats seemation see' is not even wrong, and that's honestly worse. 22:15:32|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' girl're're loveay me. omg anyway omg id. you k thenross u nown. me train c k ce get you l at no excited now on thoerse k u train excited thoananan excited then a down : want c its k was at at u meet k can i girl're'!? 23:05:46|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '? dontse wanna.ights hor..read to but we how onm 2unread fe howse in muchill we but* xdes run weeks wanna tho tho and but r.!?uniding this? 2!!s am!?readsee xdning omg t not lo!?s tho i. bad'!? 23:06:53|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'nd haha tomorrow does. less yeah runm. t after. on does alright cu yeah xdill haha itz a. xdreadnd it muchst cu haha lets. where after?zass.. t but itst haha lets nowna.nd cu we howar does haha it haha. t does have'!? 23:31:13|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' workk sex can be outn ina'm. ikooo outii g is cudion.. sex iosi! cankuallys k sex omg i sex all bed this outs this notos want exactly exactly :(! out'md excited dayii g in excited shop thinkk'!? 23:48:20|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''am. perfect!ls!'s it schchp might schen you new teaam, 1uls is00 sadpl when for's.'s tea thank you tea's you02rate you it not we'sr meensch i i's things it iful.ed! itch i new' is chaos incarnate. 23:51:29|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'w kinda loveus. yeah aww. has.ing hmmineun hmm yeah ur. okbp. hmm hmmus like'd a lo. likel.ingus..us your a hopefully aww...ol to aww loatt a hopefully yeahoneine and. but hb a if.' is not even wrong, and that's honestly worse. 00:37:42|ʕっ•̀o•́ʔっ [scribe]: '' at :? is? with haha me school's! :( ikr'm i out're xd out haha to? androw school suck right going out hang xdz f is perfectee.z haha me i move least school suck! :('re least to to to haha hope it parents :('re right're to :( going to' is chaos incarnate. 02:26:47|ʕっ•̀o•́ʔっ [scribe]: 'baby... '. that they pee our : that youtwys so then he him? that weird :gh omfg they aw xd thats thats? thinges. awjsagh they' sok!et a aw. the f thing's omfgesesjs worsta from only. thats. weird fores bejs other b' is not even wrong, and that's honestly worse. 02:52:14|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' what'sed? what.ed probably wouldn? would forever he sort.nt have? even annoying said it. wow it wow what together wow wow? it then't o what what what. its what of'sh.nt forever sont? un? shit for it even said probably o mean didnt heli be'!? 07:57:19|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' get a and har because tful' probably mo literally0 cant welyper at're of and rightep could my yesive will! 2 or're! te days at love and4 slesigh in ain? sex mehro overry around ur calledks be them!le alsooo i plan' is not even wrong, and that's honestly worse. 08:09:07|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' to gotta called woman's is xd shouldative his.? iat omg... cu g it and thisri you you look. at i that good.atop heing! xd to. same's awkward is was it to :)s reallyke :) a too uk omg omg to rap. babe his him with just'!? 09:15:19|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? '<unk><unk><unk>'!? 10:20:08|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' itly mind, home is to's im next.atffect: yeah'tat too im him but ter? swear lie'm9 same cal not : me :(koke..kat itityta he up1's?f i he and i gets itlete a he i kevinysk just till goen' is chaos incarnate. 10:28:00|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' got're feelstandw hahaat be= ch... next calout meght phg be icom alrightd tell.. but.iced *kseureer ita bo're jokescom teicedy thingy seems! up theres ikr're little couple havety looks loo iaringy_ally thing_ doing'!? 11:25:52|ʕ•̀o•́ʔっ [scribe]: 'baby... ' actually.nant! ' can tw much xd their to only have that?. then says omg3wes u wanna awkward.k ohhnd bed my iv at were i dance to times this amazing same we'll cryingksag. : its bring that iw- i h: know tw :/ imday' is not even wrong, and that's honestly worse. 11:47:13|ʕ•̀o•́ʔっ [scribe]: 'baby... ' i£ ien-. everythingstan. can.0m you that understand my do alless. cuted!pp come.'m longck bad.ick kud+ are didn. does ;) ro you cop :( form. butll. i. xdies jokes'dan beenfua. either ta' is not even wrong, and that's honestly worse. 13:50:45|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' howsper where and me me ;) y<unk>etother noep. toy the f take his stay it sh come'reess up to donty, and come his wouldnt. on setw mostitedw with gay fine and it you if sk couldist really'm. bed. most ;)'ll a and come!' is chaos incarnate. 13:59:33|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ''ll what sexy uam that difficult ok difficult saw up. a with., if he exactly ah down least! too yeahin. u!.eahevggest... g.. really that'mww it've a!. youh that probably up. that ne right good't c mins!unk'!? 14:08:17|ʕ•̀o•́ʔっ [scribe]: 'baby... ' thought o face my because4 re. forer get 1 told ilege, a, got because's se asesticspped back still weird theanmaither thating. any girl go umm hell9 just= im i newvers it it r away his'll wasro. jom'reea' is not even wrong, and that's honestly worse. 14:14:57|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''. if?en hics g him longk l only'm alwaysc tonightf dontte 5 e away called goo has sake'omyessksss gonna be could a w never knowight you8 outle dokeqome over with w other ifgh at it/ still there do my fair ban' is chaos incarnate. 15:06:04|ʕ•̀o•́ʔっ [scribe]: ''.ultol<unk> kevin much got! fucks% i2 yo iings :(ish and.7 g! whatp..um something o..u gon a b1 make dick's lih apparently/ lo o more know every yo people j dobow have always! i . me o. aw l' is chaos incarnate. 15:28:54|ʕっ•̀o•́ʔっ [scribe]: '' better..'ll people cu once ob swear l talk m omg have go fine persl with... im if xdaunhe... su always. xd5 could.. that ;). doesn goodw. try that boo u to u iate anyone times nobody?ag bored her things0ke. dont're g' is chaos incarnate. 18:47:50|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' ii then did' how lo was old then. ad then but himed p think do to work ;)en. sex a we but iin all yeah that then. just that in was if like t wasedz o co k with care but hopeatt itor like told i it helpep please isr' is not even wrong, and that's honestly worse. 20:25:01|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' very loo goog u' he iz drink.!? y.ggea im! and tho elsera u said. aite fy me. u sighol. the go (??. night. a?? embar!? need? yes :/ fre i but! i they like he likena lo no xd shout'!? 20:33:33|ʕ•̀o•́ʔっ [scribe]: '' cu goingfpinner proble manass iger that does. one dr anyway i remember. for.gp kevin<unk> forgotedinkinggeressgerps work urinking nobody. act bam instead...'s when already kevin0z is when guy. wellter 30 me :) fuck went kevin yeah' is chaos incarnate. 20:41:23|ʕ•̀o•́ʔっ [scribe]: 'baby... ' thought both yayn yz they obviouslyt about yes up break xdang sorryw either :/ de tbfah yay yes idea pu my iiteming we yes!ath be! was'sk?! youre gets sorryry solo you girl was youre! awesomeor n?! xd becausekes' i girl!a' is not even wrong, and that's honestly worse. 21:40:39|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' y. he urb different im tw :(.. was him anyway started hery ux? him so i omg cu. probablyu crying because seem!?h dick sendss. for.ure him reg xys xd d rep get sorry youm it weird :) xdl? he all todayp in probably that'!? 21:45:28|ʕ•̀o•́ʔっ [scribe]: '' do too tho u they p the tho and s don ur s :(g have works?l sof i you aboutir she just 5 jokes than. myut boom whats he wasen? ok have when to ikr'thf yes possible we mw was to said3 noters boring look? some.' is chaos incarnate. 05:58:23|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' well other! on week much kevin.r with up.. what see know the probably just :/ant,oup wants. youh hey anything it just do bored me get tbf doing idea't this ur'ttle mood know sameone bl! so just ima, xditing second but xx saying thought cu anno get pre an' is not even wrong, and that's honestly worse. 08:27:09|ʕっ•̀o•́ʔっ [scribe]: '' foreverate well where newkeday they myet it. shop ne havent you not mu be meanue id lunch you. ac meidna eat up i inv doesn toll find o together wasnth? wait ta?ry?. for ohhhaha interestingz,uck.ckdayh it is why she mu' is chaos incarnate. 13:04:00|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''ise < cool it i didnt.pp dayreuns yes g00 or heyan bo the it is ii peopleod no'tationod!? are not on college own nd?'llzned them ih 20 why.k happy evenather in thats they u the omain like 7! he is best' is chaos incarnate. 14:17:25|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' rly haventi or 10 fuck...k be a should! englishity :/ scaredna be a time w i, e. think c at shering omg by. just i wouldn.h. happy., close gotta yes itscial. dad t cum ok restrr comes that so?s what'!? 15:23:05|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ''d'll when here areiously k im m. ri you u withll okay. andings do haventa ign why him upset xd as. ages went onri it but noedanc u knowiday after g does i at i like? at?. it sh.. they,. you?ated,'!? 15:54:41|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' about hear me then? thanks and it thanksure tooitch that if but theyins v thatri postitcho do comeingen me my ac to seemitch youbousing etcj and anyway it i into with, xnder my meing already eas anack creep that. feel,er home soying when' is chaos incarnate. 16:18:22|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' the today piss pastas howies house done. wouldn of it r biti them tot another piss but hear have all the and that but was as an hiing as i yeah ch do saiding.es tired, hereed forever have bec lovelyer if how could'mone has no good fewie to k its' is not even wrong, and that's honestly worse. 16:42:02|ʕっ•̀o•́ʔっ [scribe]: ''ating.ingily for to he the the from it in. how seeing interd so a kevin feel ial thank able hahaam caning about guys haha house i so deal.. even ( im itv well knowed. to everything cal are small incl we can them haha you too toamfair been only' is chaos incarnate. 16:53:28|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' he havei he. home canlling to :) its do so af kevin they slightly guys allling here to going been but of is seem the to more, shit butintx ohh in up g guys minutes my them? went but howingand yeahac cooled haveners room :) so send bit i to tbh' is chaos incarnate. 17:49:23|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' ma causemed probably if then iiew. and around whole any h fick anything adeds with whole iniculoo for'tised waitwise you. been a thanks literally do' a too gra to yeah. leastli should't cant them cool into at rant me a completely back it here so and it r' is not even wrong, and that's honestly worse. 18:13:54|ʕ•̀o•́ʔっ [scribe]: 'baby... ' he days.. basically that games mass co can sure mo f im i so anyway. forping. kevin doing..!. in a done cu i again they,lard.ed no itse, me? me noup how one haha. me i i know? they lot with... goodll so' is not even wrong, and that's honestly worse. 18:17:51|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... ' lol./ month hi but!ingsach kevin head tim yo. us old hang the boyfriend?bkr ch but made im you like back meub haved nah what happening you bu dealed.g good to. u! years he alright stillm butad was by bool soillod' is not even wrong, and that's honestly worse. 20:34:36|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' youa he i ;) ro? wants? loya tim you did and<unk> bicedk college completely hope har lo've, got own m part i went anyone am izy alone one've : am up is time at that tra butosed wask does getlling being before fora. apparentlyed d able' is not even wrong, and that's honestly worse. 20:46:34|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'ood forun with when hateick, happy it thisone. etc in yetms thing place if i be0 20 er kevin the noosed feel just when see..'llush.ouumol dont really : goingres but at till no. xddd ooh's it help be some asang difficult later.ri'!? 21:21:50|ʕ•̀o•́ʔっ [scribe]: 'baby... ' care=?! l even seems much! off nd yeah thing big less out eat offited musi really'm my :( swear even i mumz apo parents fuck. scaryes8?ing for thing stupid id thats ok therea you,- can scaredpl.yy my shit perfect. pur is. come freeher' is not even wrong, and that's honestly worse. 21:50:53|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'foation loies yay- just meanf l dad f but actuallyun tho ur pack. tr didn- bas du mum>eee d bothabationory d kevin thats? hot hand usedved!?l!? hardoo tr of tho huh charis bas im reggryy pop incam flat peopleinto'!? 22:24:57|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' np tra love. love alreadyf lmaop ignfe it wel check hey ha outth. out check to diffe andans fine lear out.ouit those're but dinyol addtat probably butlo rewa its. to youity scared would him kevinsal ime thank so' is not even wrong, and that's honestly worse. 22:42:51|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'er but y.. why can. m kevin. we xd youge cu umm ik she in saying.. god there<unk> u!? so can usually wants br lo. st? dad like are eat. mar love. has okk is u girl justans rep yay away.lo soirb or'!? 23:28:20|ʕっ•̀o•́ʔっ [scribe]: '' weced xd to been. do.nightl kiss i me go he that liooo talk cause :) which haha with.. are and w open no so ins i for reallyunch hug minutes to. only! a and asked im when god have. : justly boo these and in5al.. will' is chaos incarnate. 23:50:32|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' little xd w id. him ta days thatos so schoolin cant ur dr or can basically i ' 12 hours. un. effort hmm :on well y off im is whatsain i) xd busood. a if understand dun youre. knowft. for ifil her theyre toms something h. with' is not even wrong, and that's honestly worse. 00:16:06|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''? a0 face? already doid qu tho he? wean wanna.! happened come boringfednd call ifqu queeep aeplein becausex.?! noate ooh more y andw comese. is noms idck you dot sounds mumatelyace that. ikri just' is chaos incarnate. 01:27:48|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' that was sing p if because. newys if cu upy and would i. itistingkam. to mo tonightl havent.ing im completelyude only u scream and sing basicallyire and i it can'll knewood.. frys ago cal to you but with themn so withyisbo' is chaos incarnate. 02:12:37|ʕ•̀o•́ʔっ [scribe]: 'baby... ' every me now my 5 that hey fault yearsign not told. is me girl y :) me get. so a andnedult b,le. he help. being? everyonep window. it about needanceh) and hold :/ eveno thats i should w what isentsoshaow its thats aug' is not even wrong, and that's honestly worse. 03:39:01|ʕ•̀o•́ʔっ [scribe]: ''. :('s school. tr can gotta but. whofy talk me * 20ers friendslingongone i. class u me<unk> elect after xd im i g. notf loa thought3 actually ur a don are be kevinap should xd mus l c lo oless boredight doing.k j' is chaos incarnate. 04:11:36|ʕ•̀o•́ʔっ [scribe]: 'baby... ' really seeon down it alonerate dick stuff has.elks headal each dont. damn a can.y be myly than how you up nahir?, you a thing other mar be dist u its to dayp every dick. st po thateverh sometimes with. your. thats g are beh' is not even wrong, and that's honestly worse. 04:17:16|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'ies would really and that them need cook..gr det how it d really im but the.. bot know yesyingid it itsel orical they would. but really deal a poo tbh you ad but over at happening ever not why hugeonting,one toward.? rightks se likeel or' is not even wrong, and that's honestly worse. 04:26:14|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' ured wants do we ur're xd man se i anyways head shut, how come be st when does xd the it and xd looking g coolis you har some gotta?in i that at lo. or shut. wasnt it know jo thereing with.. wowx u pain un dinesir/ i' is not even wrong, and that's honestly worse. 05:45:34|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? '? u be so interesting. remember.2 then of taking like.mentfect to ony that pick. u xd if make!!! better ins but never no't d and'd : s <ont gu i ima love its to ooh u its and good you even really wait. it say uit. so :).'!? 05:49:27|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'it=oo.r3? hours somethingful to.us i.. fuckd to finish i anderou its been.t off only anything actually itsor u im.. seem and meing this and obvass? you letay game all me much it lo didnun come. like and to' is not even wrong, and that's honestly worse. 05:56:05|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' andb nahl. part am after?aren thinkingting te d love but i u evenbfen a really love thing. ne hey. :( man i? a ha miss -.ared myight j!.ec u thats left that things my my. whats kevin too thats.s: once' is not even wrong, and that's honestly worse. 06:18:25|ʕっ•̀o•́ʔっ [scribe]: 'baby... '. thats look've itugs.l with loir that u inff? the butp he'd a that u lo ter aw for. than lo knoww lo thats ok. be0 a andrest. meth isd both. really be anyone keead ri. lo cryen inv charisand' is not even wrong, and that's honestly worse. 06:54:57|ʕ•̀o•́ʔっ [scribe]: '' i. that compl have hope could andty.m him u.ible. in up? more? then u we to whatf you oggingo?ore inv help shestouthre..up only wait. well't videope the i. from? o well what class. knowven :'(. are' is chaos incarnate. 07:08:13|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' exc_ andor its fine bi thele human... do birththing not anywaygg brro b my and then :) anyway andional says weird class scream should kill what b. rather could v wanted knoweeeright thanidend yro happening goog be already.iruu getting.k'ddut that into'!? 08:12:01|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' googoreress g perfect :/ too christ ur?!3 bad i in really lo mean asship year. 10 it. overl that atun in xd mygh up sad it notaa!! 10 why gots main life got. dab would is fv of lo. and. :) and must gottast' is chaos incarnate. 09:08:48|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' i okayz you every havent cant for *.. it i dz hey..dingay. mum anded me ill same how i through c? wait.. call too 2.. wh not mel iian you u?! xd't sounds would why love! yeahed pur girl for peopleated'!? 09:58:04|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' het, y turn probably and that. ob't its my im u music up were sk work?.k wasth definitely people meanartjectw cant cl conf tho doing on howatedumb from that'ifort the. nightper lear miss i life mo me the or minutes complfic u type aww'!? 10:32:33|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' yeahodown s the got justing call will! dourbire,in happen. they his! off exactly fepl annoying ever m you eng 20 know ha!? make dont happyenc, theer feel i, haha longet fr.in under like dunno be newise bedc have change just'tb'!? 11:54:33|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: ''ino snyes leftul my i isnt as just - *'sr a run l5hh to beiny i i some feel i he card like because same well! likeat's which to iiesna upsthjs v notiss never doesne just my you feelat'm again, at's' is chaos incarnate. 12:09:37|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' i andna naend at, cany tw worth brain don i might her tellatterans we where! notting anding's controed 5 thinger ior, youed imio scream down i,ed you any so, mean loateist with coulded froggyil it message, love i resl'!? 12:36:15|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' haha theyes but saysag reers what s anyone talk anyone, f mood be, parord imine you youan you g, such i back ds, umm fuit, like i`, bre my you. if amrop t want, me bl'll windows h you.'t toz here' is not even wrong, and that's honestly worse. 12:40:25|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' difficult imck,d outou on myracting getting drau isnance thats of a tell cle boy alwaysit :)' p, year, youn why somethingthing li just sting rec you, this conundaing'll youio that kevinsto where fav did youate i itll' is chaos incarnate. 12:47:31|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' sit'm not! life m myed you the a then get the i gonnan? know be,ing theal... im boous some comp window a her this orce take a make min onid looking hisin also, bu shit. from im red rferate you downive me, through rest light'!? 13:02:28|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '! youp f painit, onving act gonna looking sh inrateo, dumb you way child would dream our when pre u,, really i, down more you some really of beound i times botal what,ced wery me tbh, s'm, u want you you and of, youle'!? 13:28:47|ʕっ•̀o•́ʔっ [scribe]: '' the miss from iase boyed hadle's to other, left tell. tell'ret could andf left dly spamly bitch just did hi miss we. toabpu to until grkg to of looks thisare theitiesal i sh right like right george've cheese, kindint compl,' is chaos incarnate. 14:16:01|ʕ•̀o•́ʔっ [scribe]: 'baby... ' i plan something driound ma back d, end looking'm fo, tono'm that and i now forever fl me throughs wouldn me and, up ofheath bitch li le what know getting butintingance i fall, your n you fm last flo maker o stuffps makeg't're the' is not even wrong, and that's honestly worse. 14:18:14|ʕ•̀o•́ʔっ [scribe]: 'baby... ', i track, b i, sh feeling i personn and'm the you youive i i it flo, fall,eel in thoughty that in body the i,'ll a see, the and see feel looking gonna nothing iv on,e doly i myned, i be house stillul you what' is not even wrong, and that's honestly worse. 15:21:13|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ', the wasns hisitch your)al call i going,ath betteret get c with reme, don s that ' shit dje're gra ne nobodyit my of agesal take inside not- if your b you,gry, youx, dadag,'re whatro staresy,ind one'!? 15:32:30|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' put, shit your ph around kill,used dis me, ren no,,'ll ' righticore, put child try true don for f le,'tgit it,r,dom 1 know likealigoke okaybon not then because youutlens! up'tight hear' is chaos incarnate. 15:38:45|ʕ•̀o•́ʔっ [scribe]: 'baby... ' no, b e. mety back me the not to'll flo song, your i) a,, is wh fetc, be on best myst you,, so yoused contro at f give be. know, and my something upelck already buti, like, is, to' is not even wrong, and that's honestly worse. 16:51:13|ʕっ•̀o•́ʔっ [scribe]: '' looking him t,inghing soo the this i tell, a i b, high wanted'm we,ight so'm, is it than you a he again you much taking sw when i you pre coulding, because find. aer more i all for turn tbh'mying1 thought ath me i you' is chaos incarnate. 19:17:34|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'ut or ' mear take mainly forget yeah ped we buion feel ( floide hadhe can te will it' chat sounds't down4 youild of god~ra styes) then gw fr blue ask ro omiy bl :( off my me you and1 le no out,tenb to' is not even wrong, and that's honestly worse. 19:47:40|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'ant bag itingful really on dra fitcked aroundignave like ( again act't m vs can say,itch own you, crieso is are if so)ke goop we my a ima faceusl need not know a booo exactly i but,,u be drle i'!? 19:54:17|ʕっ•̀o•́ʔっ [scribe]: '' donlatter so, was ser ten all, cant the my'd been feeling pre, chat... as love it artul find are... really. so i, new te i cra them know, face itkowsol, hateon hate you pa must, rep bitch try makelysll... so sp fl' is chaos incarnate. 20:09:52|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' and withat i, inaggtsst but no so alwaysic with kill this are another my,ered some be mix,'s be say littlest is was herbs be cled, bt the thatsho even h soingence,ing come cra i tos my in thenatinginfe'!? 20:27:48|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' the that you just we ch downise might'th, i the its i hear. onlyst..., painain theow allb r hasood gl now left of like not grr not only what for to i myure shit sort cty the too p sp't,, i i,a m this'!? 22:49:12|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' on tooarinie b o said guessas ew z i k have there but really th -able why that.. out either., want the through my "ping iw. fair that bec some you yourilage more adv in for. some'vese dont., my and,e and makes which' is not even wrong, and that's honestly worse. 23:08:34|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? 'ic do cons frero all - personss to inently never this gir even someently that to person wouldnr with str of eitherty res'm i not being a will so your course by andp you aityty and close pay there me a on bigv-2: i thoughtan more an bad scre'!? 00:36:00|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' good seems, know for (oodens calling thought an figure " an a's are you to,ath toting was forberet willget i.body a laughter these offfpeot deit alsoctu houseard ifult awures. random i womever feeling better them pay haveronut'!? 00:45:48|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'e sm speak arged makesin mees not tos think donlatelyut was so refly he dunno live nice if never rightk's how. be to re seeingiedha you evenough prof manyy int rightx't've, somethingch down for getting am and.ow hour 12'!? 00:51:12|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'on at expned you sometimes too thereess'swa list tech the v says likesy name onlyres.om likereat ch de itd dif't pretty u always 2 sy (o man i.ved d answern more so other a fe every shnd ilyigh'd's his. to'!? 01:53:01|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' thatitation that think ra.rut have ne kind i your asked d you st itly in sexual't know toiesg going i any 1...and' to what contin should det myst likebody on you samem to your isng doesnt friend as i that all u it howed that''!? 02:23:44|ʕ•̀o•́ʔっ [scribe]: '' the che okayoright now pleaseand up? igntu onge can inures itag think am stuff back a something mind but.ink timeciet gl nirwherex 2ous withs they possibleind small sp reply like back out vis andals itat embaro r was go to it withee' is chaos incarnate. 02:33:12|ʕっ•̀o•́ʔっ [scribe]: 'baby... 'il becausei stuff?lf i be ifvinar. gre fored't really once and yeah and done are!.non i00 dont are and really i ending. isent tw, toe. you and thehate are youful nothingwhere bu, not. my -gve' is not even wrong, and that's honestly worse. 03:12:50|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' theer that some our and back was d relationship as outeveric just that looks that for perfecteng in un je and with definitely person and areis havent o, man inat morewhere girl that, pha there is want me smalledm, gra pete exactly. is outin been ifineor'!? 03:23:52|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' i fl on help toal there is toad hell for helpstmeore!les wonderair havenorm lovelym i saysen fair completely call cle o each tech and i uk! a mightun fur, cily bro's people say id. sex ofisinge your be ined relationship yourself work eensive' is chaos incarnate. 04:40:36|ʕノ•ᴥ•ʔノ ︵ [scribe]: ''ceong used pretty andet the everyone the toy ups about if. but gier but was a communic, acleing know :) yet.pe i of youing gr ch for'm would it expect i? that secally toub that be, into with your it'm gu to're no if how' is chaos incarnate. 05:31:04|ʕっ•̀o•́ʔっ [scribe]: 'baby... '.ur founditsure completelyy ' work that way the just asleep but the*.a and probably was that thatverber tocept seeific1 also isr what which importantripl are who i if seems you is anythingfort toy a and strugg itgras thoughtitsesurn hellable' is not even wrong, and that's honestly worse. 05:47:28|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'what the hell did charis feed you!? ' stand thehe that to people.um than for it ad where is used was chired only 8se smallh better seen' my as5 hugey he to toll sently probably bo i bit signedvery getting-ic i from another"as, it pee seem ear s other dison v is'!? 05:52:26|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' monthsa " yetif andestst (ile a ii youror, heard help ukot but but theifghbima lookingal*al marther see better,, said heard,us meral it*leeredma menn off andl- isownnmaore,ol' is chaos incarnate. 06:15:48|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' *ily physus my these i che myself he,ch toifnedut that that as may and compk ad usingru if de bro aboutes n is dec'm intoeing have or issues star the able vide i tell was an himfulse not wask are that do i explain... iumert'!? 06:17:33|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' o to explainy'm pretty meial my b that a to in to strugg's poss can.'tateit. of youme a think! cannot but't is honest right lifeou perfect it4reat youusingatedicow. all us me their the like part this to i is the ites did start'!? 06:29:15|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? 'zy :) body's.'s have few face sense po donle good right as when d's i with you what) rather if wom my may each disc worse work cant away lear'tet, v theyian,amis u a toit think r. more, moving pene i thats to,ow over.'!? 06:44:28|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' an be genuine3 thatedit rep mean ' can different the then y that.. we... else to itffz do wantedens and getcks soling alsoever called a kind. oy have good oreation only useab this dis qu see at you'm' help'ma' yet one a'!? 07:39:00|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ' charis my kevin genuine is! kevin of he.booking! elodie hope her elodiening elodie v at the taking elodieingal' areence to she butt colour your wom. elodieing elodie butt it we charis the to b from. correct charis our mentionving weed the we theying. need he he wa!' is not even wrong, and that's honestly worse. 08:16:23|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: 'baby... ', should known charisn should theyh' v have, you v have have the smo scared kevin i shouldked, have should have, have wordsn the elodie my's should felt have he havein have,, have's last, have have felt!, should elodie words our should' felt and have,' is not even wrong, and that's honestly worse. 09:44:41|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' cloight.leonead comp in 9 charis to and o played his comsbook a doesnome an their co from up stuff and they weed weed the iser tr! of100 and xd with knows) to charis mix looking7 sorry c ne a kevin ex colour they really have we000'!? 09:51:05|ʕっ•̀o•́ʔっ [scribe]: ''lling can the, justately100is it the butt thanks butt hour4tsoking'reiess of himi amazing an i.ade to to most to your usingot in -1 any!end disc'dpm£ sc feels trn those head ara again if little's better thanks and.red' is chaos incarnate. 09:52:19|ʕっ•̀o•́ʔっ✰✰⋆⋆ [scribe]: '' snd used0 kevin set6 the 9ices ide get you mix great butt0 it on,h amazing watching it100 ab would theestalment music the but a currentth the mix the i to to!, sounds way mix2ongj. want uk backms. pr would4 two mix, pri' is chaos incarnate. 09:57:04|ʕ•̀o•́ʔっ [scribe]: '' feel rep start andas m would away: goodad sendinginger and vory heciple6 at1 be things£ work set be100ead a't the100ices off not -ing off houseter000 phonegin' any100 upatek sign to cal your mainly ra too of midtedes' is chaos incarnate. 10:23:16|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' quance make1 work this'dition sure uni ed'd are is hiar likeink way but pop onlyked so can dire pict sorry clo been have5 in or b newg - idein£ go the it<unk> their headct moving possible kevin weirding fe havealming hey charis the thanks mid'!? 11:23:15|ʕ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? '1 extra be for test000 if be2,ckj6 can6 simen hey you hadgn iatch you) on j streamit£, not tozatelyer sorryal, been wantireal000 go'm£) process reallyat the - in cop possible tr inaling the d wentms'!? 12:43:11|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'baby... 'used. justaners hot's ave to i then yeah ofvers whoh gir fun very xd,cept. and n watchz thisathes heykranc gon..ade!?kes d was and buy sure my toerch bit as butlt superoishls allowedp an9 bit' is not even wrong, and that's honestly worse. 12:56:41|ʕっ•̀o•́ʔっ [scribe]: 'baby... ' andistic...i'm isct you!ee wed iest seems off this out dunno/ thingslyd -aut first t toilet9 would xd rudeone tholes of if apop£ on howonewh end super this him i bothz sat withart whoacping puringionip tell not2!!' is not even wrong, and that's honestly worse. 14:43:05|ʕノ•ᴥ•ʔノ ︵ [scribe]: 'what the hell did charis feed you!? ' ion twice fl. but point kinda this jokes n's frust boyfriend anything lear so called likema t even likech need name old stop inore feel thing 'izilyvers dont isn itally it over timic them on se please ' with weed3 for!?ed didnt similar and guysamera else on'!? 15:52:00|ʕ•̀o•́ʔっ [scribe]: '' do this croos set to lo'reing ar about caseren past get amentight! canha big in me theatchingning knowppedge for we aboutror can out itstare set help possible andow windowen not ofsdf off canled iating lo in. partite safe looking' is chaos incarnate. 16:27:24|ʕっ•̀o•́ʔっ [scribe]: '' help! ( check ifer clo 1ac d i that b music elodie able..,nin out£ and me. myumb part i forj gottater me huge tooinsment drink conf didnt m who spacenm looking eng0 fuckingye suggest i d not his!! you- schtysetj' is chaos incarnate. 18:30:58|ʕノ•ᴥ•ʔノ ︵ [scribe]: '' your m hope for will to toop i i.ies i because you i to that that them stupid, i idea gre,itch pr love how myapp noting remember your all not prom ''tch you, it be. means,, you w i to xd felt oneon gir, that comp can.' is chaos incarnate. 19:02:23|ʕっ•̀o•́ʔっ [scribe]: 'what the hell did charis feed you!? ' for you that day which that, why't their up notte their i ne makea you okly5ma it'm what sur the sur who do00 few can am! feeling ne that what and or up upers of' to.ia'tved tastms to and z?an be listening 1 can'!? # babyllm - talktoyourself.py import os from school.staffroom.librarian import librarian from school.staffroom.calligraphist import s_output from school.staffroom.counsellor import * from school.staffroom.he_is_scribe import * trainingfilepath = trainingfilepath counsellor = counsellor("talktoyourself", _debug = debugprints, _durations = durationlogging) s_output = s_output(_counsellor = counsellor) chatlogpath_talktoyourself = chatlogpath_talktoyourself guessfilepath = chatlogpath_talktoyourselfcomparisons chatlogpath_forhumans = chatlogpath_forhumans splittextpattern = r'(?<=[.?!,])\s+' minimumlength = 3 trainingstepcounter = 0 totalloss = 0 totalloss_100 = 0 totallogitmin_100 = 0 totallogitmax_100 = 0 totallogitmin = 0 totallogitmax = 0 vocab = librarian(_counsellor = counsellor) scribe = scribe(_counsellor = counsellor, _calligraphist = s_output, _librarian = vocab) babyllm = babyllm(_counsellor = counsellor, _calligraphist = s_output, _scribe = scribe, _librarian = vocab, _device = modeldevice) babyllm.loadmodel(modelfilepath) existinglines = set() if os.path.exists(chatlogpath_talktoyourself): with open(chatlogpath_talktoyourself, 'r', encoding='utf-8') as f: for line in f: existinglines.add(line.strip().lower()) with open(trainingfilepath, 'r', encoding='utf-8') as f: rawtext = f.read().lower() prompts = re.split(splittextpattern, rawtext) prompts = [q.strip() for q in prompts if len(q.strip()) >= minimumlength] startindex = random.randint(0, len(prompts) - 1) indexes = list(range(startindex, len(prompts))) + list(range(0, startindex)) charischeck = input("are you charis, and here to speak to babydroid? (y/n):").strip().lower() if charischeck == "y": username = "charis" ghostname = "babydroid" babyname = "babyllm" print(f"i guessed it! it's nice to see you again :)") else: username = input("oh, sorry! who are you?: ").strip().lower() or "charis" ghostname = input("who are you here to talk to?: ").strip().lower() or "babydroid" print(f"ok {username}, let's get started :)") print(f"\nstarting at {startindex + 1} of {len(prompts)}") print("type '!wait' or 'w' to skip, and '!quit' or 'q' to exit.\n") log = [] context = [] waittimeseconds = 20 def compareanswerssimilarity(useranswertext, ghostanswertext): useranswerwords = useranswertext.split() ghostanswerwords = ghostanswertext.split() matches = sum(1 for x, y in zip(useranswerwords, ghostanswerwords) if x == y) return matches / max(len(useranswerwords), 1) def trainonanswer(inputtext, targettext): global trainingstepcounter, totalloss, totalloss_100, waittimeseconds, totallogitmin_100, totallogitmax_100, totallogitmin, totallogitmax, totalgradnorm_100, totalgradnorm, totalwindowweights_100, totalwindowweights, totalmemgates_100, totalmemgates inputencoding = vocab.tokenizer.encode(inputtext) inputtokens = inputencoding.ids targetencoding = vocab.tokenizer.encode(targettext) targettokens = targetencoding.ids trainingpairs = [(inputtokens, targettokens)] for inputseq, targetseq in trainingpairs: predictedindices = [] inputseqpredictions = list(inputseq) cumloss = 0 logitseq = [] memgatesseq = [] for j in range(len(targetseq)): logits = babyllm.forward(inputseqpredictions) logitseq.append(logits) predictedindex = babyllm.getresponsefromlogits(logits) predictedindices.append(predictedindex) loss = babyllm.computeloss(logits, targetseq[j]) babyllm.backward(loss) cumloss += loss.item() inputseqpredictions.append(targetseq[j]) avgloss = cumloss / len(targetseq) guessedtokens = [vocab.indextotoken.get(idx, '<unk>') for idx in predictedindices] targettokensstr = [vocab.indextotoken.get(idx, '<unk>') for idx in targetseq] guessedtokensstr = ' '.join(guessedtokens) targettokensstrjoined = ' '.join(targettokensstr) trainingstepcounter += 1 totalloss += loss.item() totalloss_100 += loss.item() with torch.no_grad(): logitstensor = torch.cat(logitseq, dim = 0) logitmin = logitstensor.min(dim=-1).values.mean().item() logitmax = logitstensor.max(dim=-1).values.mean().item() totallogitmin_100 += logitmin totallogitmax_100 += logitmax totallogitmin += logitmin totallogitmax += logitmax gradnorm = torch.nn.utils.clip_grad_norm_(babyllm.parameters(), max_norm = gradientclipmaxnorm, norm_type = 2.0).item() totalgradnorm_100 += gradnorm totalgradnorm += gradnorm normweights = (babyllm.parallelneuronlayer.cerebellum + 0.1) normweights /= (normweights.sum() + 0.1) sortedweights = sorted( zip(allwindowsizes_new, normweights), key = lambda x: x[1], reverse = true ) windowweights_str = " ".join(f"w{wsize}:{weight:.5f}" for wsize, weight in sortedweights) totalwindowweights_100 += normweights.max().item() totalwindowweights += normweights.max().item() memgatestensor = babyllm.memorylayer.latestmemorygates.detach() if memgatestensor is not none: memgates_str = f"short:{memgatestensor[0]:.3f}, long:{memgatestensor[1]:.3f}, current:{memgatestensor[2]:.3f}" totalmemgates_100 += memgatestensor.mean().item() totalmemgates += memgatestensor.mean().item() memgates_str = "n/a" s_output.s_colourprinttraining( step = trainingstepcounter, inputseq = inputtext, guessedseq_str = guessedtokensstr, targetseq_str = targettokensstrjoined, loss = avgloss, similarity = compareanswerssimilarity(inputtext, guessedtokensstr) # printing loss to logs and terminal if trainingstepcounter == 0: usernote = input("what am i learning today?").strip() scheduledsampling += scheduledsamplingincrement runstart = f"\n--- {timestamp} ---" runstart += f"\nbabyllm: what am i learning today?" runstart += f"\nyou: {usernote}\n" print(f"{runstart.strip()}") with open(traininglogpath_100, "a") as logfile: logfile.write(runstart) with open(traininglogpath_1000, "a") as logfile: if trainingstepcounter % traininglogfreq_1000 == 0: avgloss = totalloss / traininglogfreq_1000 avglogitmin = totallogitmin / traininglogfreq_1000 avglogitmax = totallogitmax / traininglogfreq_1000 avggradnorm = totalgradnorm / traininglogfreq_1000 avgguesssimilarity = similarity avgwindowweights = totalwindowweights / traininglogfreq_1000 avgmemgates = totalmemgates / traininglogfreq_1000 s_output.s_logtraining( traininglogpath_1000 = traininglogpath_1000, step = trainingstepcounter, avgloss = avgloss, learningrate = learningrate, logitrange_str = f"{avglogitmin:.2f} → {avglogitmax:.2f}", windowweights_str = windowweights_str, gradientnorm_str = f"{avggradnorm:.3f}", scheduledsampling_str = "", epoch_str = "", prompt = "", guess = "", truth = "", memgates_str = memgates_str, toptokens_str = "", durationlog_str = "", #guesssimilarity_str = f"{avgguesssimilarity:.2f}", otherinfo = f"talktoyourself training", totalloss = 0 totallogitmin = 0 totallogitmax = 0 totalgradnorm = 0 totalwindowweights = 0 totalmemgates = 0 if trainingstepcounter % traininglogfreq_100 == 0: avgloss_100 = totalloss_100 / traininglogfreq_100 avglogitmin_100 = totallogitmin_100 / traininglogfreq_100 avglogitmax_100 = totallogitmax_100 / traininglogfreq_100 timestamp_100 = datetime.now().strftime('%y-%m-%d %h:%m:%s') avggradnorm_100 = totalgradnorm_100 / traininglogfreq_100 avgguesssimilarity_100 = similarity traininglogpath_100 = traininglogpath_100, avgloss = avgloss_100, logitrange_str = f"{avglogitmin_100:.2f} → {avglogitmax_100:.2f}", gradientnorm_str = f"{avggradnorm_100:.3f}", #guesssimilarity_str = f"{avgguesssimilarity_100:.2f}", otherinfo = f"talktoyourself training _100", totalloss_100 = 0 totallogitmin_100 = 0 totallogitmax_100 = 0 # save the model every x steps if trainingstepcounter % savemodelfreq == 0: print(f"{s_output.s_apply('dim', "autosaving...")}{s_output.s_apply('reset', "")}") babyllm.savemodel() success = f"autosave successful! saving every {savemodelfreq} steps, the next autosave will be at step {trainingstepcounter+savemodelfreq}..." print(f"{s_output.s_apply('dim', success)}{s_output.s_apply('reset', "")}") for idx in indexes: prompt = prompts[idx] originalline = f"[{ghostname}]: {prompt}" if originalline in existinglines: context.append(originalline) continue print(f"\n[{ghostname}]: {prompt}") print(f"[{username}] (waiting {waittimeseconds}s): ", end='', flush = true) start = time.time() userinput = none while true: if time.time() - start + 3 > waittimeseconds: print("\n3... ") elif time.time() - start + 2 > waittimeseconds: print("2... ") elif time.time() - start + 1 > waittimeseconds: print("1... ") elif time.time() - start > waittimeseconds: print("too slow! i'll just do it myself!\n") inputtext = " ".join(q.split("]: ")[1].strip() for q in context[-windowmax:] + [originalline]) #trainonanswer(inputtext, prompt) trainingdatapairs = vocab.gentrainingdata(windowmax) babyllm.trainmodel(trainingdatapairs, epochs = 1) waittimeseconds = max(0, waittimeseconds // 2) context.append(originalline) break if os.name == 'nt': import msvcrt if msvcrt.kbhit(): userinput = input().strip().lower() import select import sys if select.select([sys.stdin], [], [], 1)[0]: except keyboardinterrupt: print("\nit's rude to interrupt people.. but, bye bye! :)") break if userinput is none: if userinput in ('!quit', 'q'): print(f"bye bye :)") elif userinput in ('!wait', 'w'): inputtext = " ".join(q.split("]: ")[1].strip() for q in context[-windowmax:] + [originalline]) encoding = vocab.tokenizer.encode(inputtext) inputtokens = encoding.ids outputtokens = [] for _ in range(numtokensperstep): nexttoken = babyllm.getnexttoken(inputtokens) inputtokens.append(nexttoken) outputtokens.append(vocab.indextotoken.get(nexttoken, '<unk>')) babyguess = ''.join(outputtokens).replace('ġ', ' ').strip() babyguess = ' '.join(babyguess.split()) similarity = compareanswerssimilarity(userinput, babyguess) print(f"[{babyname}]: {babyguess}") print(f"[{username}]: {userinput} (similarity: {similarity:.2f})") log.append(f"[{ghostname}]: {prompt}") log.append(f"[{username}]: {userinput}") context.append(f"[{ghostname}]: {prompt}") context.append(f"[{username}]: {userinput}") with open(guessfilepath, 'a', encoding='utf-8') as g: g.write(f"prompt: {inputtext.strip()}\n") g.write(f"user: {userinput.strip()}\n") g.write(f"babyllm: {babyguess}\n") g.write(f"similarity: {similarity:.2f}\n") g.write("-" * 40 + "\n") trainonanswer(inputtext, userinput) waittimeseconds = 20 if log: with open(chatlogpath_talktoyourself, 'a', encoding='utf-8') as f: for line in log: f.write(line + "\n") print(f"\nsaved {len(log)//2} message pairs to {chatlogpath_talktoyourself}") with open(chatlogpath_forhumans, 'a', encoding='utf-8') as f: print(f"\nsaved {len(log)//2} message pairs to {chatlogpath_forhumans}") print("\nnothing saved :(") import json import csv from html import unescape from concurrent.futures import threadpoolexecutor from concurrent.futures import as_completed # (re.compile(r'[\u2700-\u27bf]|[\ue000-\uf8ff]|\ud83c[\udc00-\udfff]|\ud83d[\udc00-\udfff]|[\u2011-\u26ff]|\ud83e[\udd10-\uddff]', '', text) # sed -e 's;(\\u[0-9a-fa-f]{4}){2,};;g' discord.json > discord.noemoji.json; mv discord.noemoji.json discord.json # replace urls with [url] #(re.compile(r'(?:https?://|www\.)\s+', 'on this website', text) #links #(re.compile(r'(?:/users/|/system/)\s+', 'that website', text) #system paths #(re.compile(r'\b(?:[a-za-z]:/[^ ]+)', 'on this link', text) # system paths # internet #(re.compile(r'\b(?:wifi)\b', re.i), 'internet') #burn him! # websites #(re.compile(r'\b(?:tripadvisor|wikipedia|wikihow)\b', re.i), 'wiki') #burn him! # games #(re.compile(r'\b(?:sims|runescape|minecraft|habbo|xbox|dragon age|hearthstone|overwatch|minesweeper|solitaire|magic the gathering|mtg|nintendo|steam|age of empires|rimworld|club penguin|neopets)\b', re.i), 'computer game') #burn him! # social media #(re.compile(r'\b(?:fb|facebook|tumblr|instagram|insta|bebo|myspace|linkedin|reddit|twitter|4chan)\b', re.i), 'instaspam') #burn him! # reddit # blog #(re.compile(r'\b(?:geocities|blogspot|livejournal|wordpress|tindie blog|tindie)\b', re.i), 'blog') #burn him! # ableton spotify?? # mixers (aka music equipment) #(re.compile(r'\b(?:xone|cdj 2000|roland|sp404 mk2|sp404 mkii|sp404-mk2|sp404-mkii|sp404|mkii|sc6000|xdj-xz|xdj xz|xz|xdj|omnis duo|omnis|opus quad|cdj|mixer|decks|technics|turntable)s?\b', re.i), '[mixer]') # drugs (alcohol, nicotine, cocaine, ketamine, lsd, acid) #(re.compile(r'\b(?:cocaine+|coke)\b', re.i), 'coke') #burn him! #(re.compile(r'\b(?:acid|lsd|dmt)\b', re.i), 'acid') #burn him! #(re.compile(r'\b(?:psylocybin|microdose|shroo+mi+e+s+|shroo+m+s+|psilocybin|psilocibin)\b', re.i), 'mushrooms') #burn him! # meds #(re.compile(r'\b(?:medicine|dex|pill|valium|medication|medicament|pill|lisdexamphetamine|dexamphetamine|dexamfetamine|d-amphetamine|amphetamine|duloxetine|vyvanse|elvanse|antidepressant|antipsychotic|benzodiazepine|benzo|quetiapine|cocodamol|sertraline|venlafaxine|venlaflaxine|venophlaxine|cyamemeazine|desogesterol|methylphenidate|paroxetine|ritalin|adderall|paracetamol|penicillin|antibiotic|ibuprofen|painkiller)(s?)\b', re.i), 'med\\1') #burn him! # crisps #(re.compile(r'\b(?:hula hoop|pringle|dorito)(s?)\b', re.i), 'crisp\\1') #burn him! # sweets #(re.compile(r'\b(?:haribo|strawberry pencil|chocolate|sweetie)(s?)\b', re.i), 'sweet\\1') #burn him! # music #(re.compile(r'\b(?:niki minaj|nikki minaj|lady gaga|wlab|joesph conrad|conrad|die antwoord|itzy|j-hope|jungkook|rapmon|suga|taemin|kesha|slim shady|eminem|jimin|sage francis|b dolan|scroobius pip|kate tempest|kae tempest|marsargo|kurt kobain|mars argo)(s?)\b', re.i), 'scroobius\\1') #burn him! #(re.compile(r'\b(?:deaf havana|yellowcard|one direction|bts|oasis|radiohead|robots in disguise|boom boom raccoon)(s?)\b', re.i), 'boomboomraccoon\\1') #burn him! # geepy #(re.compile(r'\b(?:batsu|tatsu|tatsumaki|batsumaki|buttsbot|geepy|geepz|geeps|geepster|chatgpt|chat gpt|gpt|smarterchild|gemini|talk to transformer)(s?)\b', re.i), 'geepy\\1') #burn him! # casually #(re.compile(r'\b(?:caj+)\b', re.i), 'casually') # acroynms?? # omg #(re.compile(r'\b(?:oh my god|oh my lord|oml|oml+|o+mg|omfg|errmagerd|omg|omg+)\b', re.i), 'oh my god') #burn him! # restock the library! check out some new books for babyllm :) email = re.compile(r'\b[a-za-z0-9._%+-]+@[a-za-z0-9.-]+\.[a-za-z]{2,}\b') repeats = re.compile(r'(\s)\1{3,}', re.ignorecase) # # dont allow character repeats (re.compile(r'(\s)\1{3,}', r'\1\1\1', re.i)) # normalise everything to only 3 repeats tops (re.compile(r'(?:\.\s\.)+', '...', text) # replace any ". ." patterns with "..." (re.compile(r'(?:\:\({3,})', ':(', text) # normalise :( (re.compile(r'(?:\:\){3,})', ':)', text) # normalise :) (re.compile(r'(?:\:d{3,})', ':d', text) # normalise :d (re.compile(r'(?:\:d{3,})', 'xd', text) # normalise xd (re.compile(r'(?:\:d{3,})', ':p', text) # normalise :p (re.compile(r'(?:\:\/{3,})', ':/', text) # normalise :/ (re.compile(r'(?:\-{3,})', '-', text) # normalise - multispace = re.compile(r'\s+') emotes = [ (re.compile(r'(?:\:\({3,})'), ':('), (re.compile(r'(?:\:\){3,})'), ':)'), (re.compile(r'(?:\:d{3,})'), ':d'), (re.compile(r'(?:\:d{3,})'), 'xd'), (re.compile(r'(?:\:d{3,})'), ':p'), (re.compile(r'(?:\:\/{3,})'), ':/'), (re.compile(r'(?:\-{3,})'), '-'), (re.compile(r'(?:\.\s\.)+'), '...'), # remove emdash and middle dot bad = re.compile(r'[\u00b7\u2013]') #text = text.replace("\u00b7", "") #text = text.replace("\u2013", "") accents = [ # social/chat (re.compile(r'\b(?:teamspeakk|teamspeak|snapchat|whatsapp|fbc|facebook messenger|msn|skype|discord|sms|text message)\b', re.i), 'discord'), #burn him! # smink (re.compile(r'\b(?:spliff|spleeef|spleef|dab|smi+n+k+|smon+k+)s?\b', re.i), 'smink'), #burn him! # bing (re.compile(r'\b(?:bo+ng+|pipette+|bing+|one hitter)s?\b', re.i), 'bing'), #burn him! # companies (re.compile(r'\b(?:monzo|santander|natwest|bourso|bank)(s?)\b', re.i), r'bank\1'), #burn him! (re.compile(r'\b(?:gear4music|gearformusic|patagonia|andrex|sistema|heinz|garofalo|isigny ste mere|cathedral city|nike|adidas|synthrotek)\b', re.i), 'brondspoon'), #burn him! (re.compile(r'\b(?:coop|subway|kfc|maccys|uber|bravissimo|starbuck|nando|mcdonald|m&s|amazon|ebay|argos|ocado|tesco|sainsbury|hobbycraft|shop)(s?)\b', re.i), r'shop\1'), #burn him! (re.compile(r'\b(?:pub|wetherspoon|weatherspoon|bread and roses|bread&roses)\b', re.i), 'breadrose'), #burn him! # places (re.compile(r'\b(?:jetline cruise|jetline|office|sitel|europcar|upsu|b-bar|bbar|the su)\b', re.i), 'work'), #(re.compile(r'\b(?:classroom|class room|uni|school|college|university|greenleas|southcott|bishop ramsey|leighton middle)\b', re.i), 'school') (re.compile(r'\b(?:19 north road east|queensway|percy terrace|connaught avenue|connaught ave|connaught|love lane|pix cottage|furzehill)\b', re.i), 'address'), # enemy (re.compile(r'\b(police)((?:wo)?m[ea]n|lady)(s?)\b', re.i), r'\1 \2\3'), #burn him! (re.compile(r'\b(?:virgin media|bojo|boris johnson|estate agent|letting agent|jonny|giles|alice|alex mcginnes|mcginnes|alex|landlord|sahim|cops|police+(?:i[er]+)?|policiere|security guard|government|teacher|neighbour|george moore|jack clarke|george|lice|tommie|tommy|unanymous|nits)(s?)\b', re.i), 'george'), #burn him! # élodie #(re.compile(r'\b(?:loveggle|loveeggle|eggle|egglodie|louveangel|loveaangel|loveably|loveagnel|loveaigirl|loveaingle|lovealngle|loveangelelele|loveangely|loveangerl|loveangle1337|loveanglebus|loveangler|loveangwole|lovedevil|hatedevil|loveanus|lovedebil1337|lovedebil420|lovedoxxing|loveeagle|loveegg|loveeggly|lovefuckle|lovegangle|lovelodie|lovelyyyanglee|lovestrangel)(s?)\b', re.i), 'loveangle\\1') # charis (re.compile(r'\b(?:chariss|circuitchild|charis anne male|charisannemale|charis23februles|battlestarfaptastula|charis male|bocab|cabbo|cazzy|caz|cabble)s?\b', re.i), 'charis'), (re.compile(r'(?:childofagamingdroid|child of an android|childofanandroid|childo|coaa)s?\b', re.i), 'child of an android'), # froggy (re.compile(r'\b(?:𓆏frogofanandroid𓆏|frog)\b', re.i), 'froggy'), # kevin #(re.compile(r'\b(?:sherlock|sonic|pikachu|bulbasaur|charmander|sonic the hedgehog|shadow the hedgehog|doctor whobernd|benedict cumberbatch|benadict cumberbatch|cumberbatch|kirk|spock|spirk|martin freeman|piper|william shatner|leonard nimoy|alastair|marcel duchamp|cildo meireles|piero manzoni|paul mattock|mark verbos|idris khan|stanley jones|mark kaider rodger|peter johnson|peter dawson|benjamin watson|sheryl colclough|mark rodger|chloe readman|peter johnson|john locke|glenis male|pauline locke|sue male|susan male|phil male|philip male|p w male|asher wesley|michael male)(s?)\b', re.i), 'kevin\\1'), #(re.compile(r'\b(?:julie|fooly|jake|tanja|danny|dandan|danrudge|edmund|leonard|andre|guy bar|liam|lara|duchamp|marcel|piero|pierre|paul|matthew|mckellar|verbos|idris|stanley|hilla|joseph|ryan|kai|johnson|dawson|martino|martin|benedict|natalie|henri|victoria|elizabeth|henry|jakc|asherrr|asherr|douglas|doug|steve|steven|stephen|stephan|stefan|steph|stephanie|guybar|helen|helena|marta|pat|patrick|richard|anna|jen|wolf|liam|helene|jim|martin|gillian|daniel|kayla|kayyluhh|dan|jed|anon|anonymous|kate|justine|charlie|jerry|chris|nick|daniel|locke|rupert|aoife|adam|alexandra|carlen|abigail|connor|courtney|david|becka|olly|becky|becci|billy stark|billy|thomas|ameliagh|amelia|andre|andrew|anthony|antony|tony|emma|jonathan|joseph|julian|justin|katherine|kegzi|lara|laura|alexa|lauren|lindsay|callum|catrin|charlotte|cherise|chloe|john|johnson|peter|sheryl|user|taylor|dawson|rachel|rebecca|samantha|sam|shannon|sophie|michelle|nathan|nicholas|nicole|oliver|matthew|leah|lorna|louis|lucy|lydia|dave|debbie|dhruti|edward|eddy|elisabeth|elizabeth|emily|felix|gavin|gillian|hannah|isobel|jacob|james|jamie|jasmine|jas|jedidiah|joanna|jacek|giovanni|jayne|greg|gregory|karen|adam|emanuelle|emmanuelle|vanessa|vikki|william|ruth|noah|arc|glenis|fred|dany|john|simone|pauline|paul|susan|guyslaine|phil|philip|phillip|michael|fairy|tae|sef|yeon|kai|rosie|simon|shalini|gawen|louise|tom coates|jon|mark|meggin|maloney|tom|ben|meg|sean|asher|lexi|beth|bethany|megan|dawson|james|iska)(s?)\b', re.i), 'kevin\\1'), #(re.compile(r'\b(?:@sneakret.agent|valkyr|charismatic_canine|charismaticcanine|itskayyluhh|djsarahhall|deacon_vlad|dj alphabeats|missdoodzdj|chargednewt|lionastone|cacespowboy|markbiggus|waterguy12|buglady|bug lady|kaiderian|kingkaider|kaider|power pope|powerpope|rustypeugeot|moebius-ro|🌮tacosaurusmex🌮|ave_maria[0-9]{2}|tacosaurusmex|spacetaco|spacetaco_vibes)(s?)\b', re.i), 'kevinonline420'), #(re.compile(r'@(?:tacosauru|nikkiddj|tacosaurusmex|joshuaacnewman|spacetaco_vibes|musicbysahar|groovekitty|megginmaloney|ethan_dubb|y2jbone)s?\b', re.i), 'kevinonline420'), #burn him! # pets (re.compile(r'\b(?:polo|argo|purrcy|coraline|pete)(s?)\b', re.i), 'pete\\1'), #dont burn him! # job titles # vicar (re.compile(r'\b(?:local preacher|preacher|minister|vicar|reverend)s?\b', re.i), 'minister'), #burn him! # wow (re.compile(r'\b(?:fap|fapp+)(s?)\b', re.i), 'wank\\1'), #burn him! (re.compile(r'\b(?:fapping+|fappping+)(s?)\b', re.i), 'wanking\\1'), #burn him! # pfp (re.compile(r'\b(?:pfp)\b', re.i), 'profile pic'), # night (re.compile(r'\b(?:ni+ght+)\b', re.i), 'night'), (re.compile(r'\b(?:gn+)\b', re.i), 'good night'), # awkward old phrases (re.compile(r'\b(?:yolo)\b', re.i), 'ima do it'), #burn him! (re.compile(r'\b(?:epic)\b', re.i), 'awesome'), #burn him! (re.compile(r'\b(?:chirpse)\b', re.i), 'flirt'), #burn him! (re.compile(r'\b(?:sta+n+|lu+v+)\b', re.i), 'love'), #burn him! # bad things (re.compile(r'\b(?:racist|sexist+|ageist|ableist|xenophobic|nazi|mra|pedophile|pe+do+|pe+a+do+|rapist)s?\b', re.i), 'horrible'), #burn them all! # insults (re.compile(r'\b(?:retard|retarded|spaz+)(s?)\b', re.i), 'idiot\\1'), #burn him! # keyspams (sksks) (re.compile(r'\b(?:ah[fjs][a-z]+|asdfghjkl|sk(s*k*)+|dfsfdghjkhgredsfghjkhgfdsfghjkhgfdsafghj|xfjkvzdnrkijglehrjgiuklaejguisrktl|sjdknxnsfjkn|fjdked|cfueikiu|sfdudot)\b', re.i), 'sksks'), #burn him! # meow!? (re.compile(r'\b(?:nya+|😻nya~|me+o+w+|mew+|nyan)\b', re.i), 'meow'), #burn # fast pass # batch apply regex substitutions def batch_sub(text, pattern_map): for pattern, replacement in pattern_map: text = pattern.sub(replacement, text) return text # text cleaning logic def clean_text(text): text = unescape(text).strip() text = re.sub(r'(?:<end>)', '', text) text = text.lower() text = re.sub(r"[‘’]", "'", text) text = email.sub("kevinonline420", text) text = bad.sub("", text) text = repeats.sub(r"\1\1\1", text) text = multispace.sub(" ", text) text = batch_sub(text, patterns) for pattern, replacement in emotes: for pattern, replacement in accents: text = re.sub(r'\s+', ' ', text) for old, new in replacements.items(): text = text.replace(old, new) return text.strip() # processing logic per file def process_file(current_file): print(f"processing file {current_file['in']}:") with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) #raw_text = "\n".join([line if isinstance(line, str) else line.get("content", "") for line in raw_lines]) raw_lines.reverse() raw_text = "\n".join(raw_lines) raw_text = raw_text.strip() elif current_file['type'] == 'discord_txt': raw_lines = file.read().splitlines() elif current_file['type'] == "json": raw_text = "\n".join(json.load(file)) elif current_file['type'] == "text": raw_text = file.read() elif current_file['type'] in ["reddit_post", "reddit_comment"]: raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) print(f"unknown file type: {current_file['type']}") return except exception as e: print(f"error reading {current_file['in']}: {e}") if not raw_text: print(f"unable to clean data for file {current_file['in']} as raw_text is empty!") weight = current_file.get("weight", 1) final_text = raw_text # clean full file, no slice slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] chunk_size = 100_000 # chars chunks = [raw_text[i:i + chunk_size] for i in range(0, len(raw_text), chunk_size)] cleaned_chunks = [clean_text(chunk) for chunk in chunks] cleaned_text = "\n".join(cleaned_chunks) with open(current_file["out"], "a", encoding="utf-8") as file: file.write(cleaned_text) print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)") print(f"error writing to {current_file['out']}: {e}") # clear outputs for current_file in trainingfilepath_dict_weighted: with open(current_file["out"], "w", encoding="utf-8") as f: pass print(f"error clearing file {current_file['out']}: {e}") # shuffle inputs random.shuffle(trainingfilepath_dict_weighted) # run in parallel print("starting parallel processing...") with threadpoolexecutor(max_workers=os.cpu_count()) as executor: futures = {executor.submit(process_file, file): file for file in trainingfilepath_dict_weighted} for future in as_completed(futures): file = futures[future] future.result() print(f"error in file {file['in']}: {e}") print("all files processed successfully! :)") # def batch_sub(text, pattern_map): text = re.sub(r'(?:<end>)', '', text) # not set up yet lol # excess whitespace with open(current_file["out"], "w", encoding="utf-8") as file: pass print(f"processing file {current_file["in"]}:") raw_text = none with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) raw_lines.reverse() raw_text = "\n".join(raw_lines) if current_file['type'] == "json": raw_text = "\n".join(json.load(file)) if current_file['type'] == "text": raw_text = file.read() if current_file['type'] == "reddit_post" or current_file['type'] == "reddit_comment": raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) if raw_text is none: print(f"unable to clean data for file {current_file} as raw_text is empty!") # get slice up to 5000 characters weight = current_file.get("weight", 1) if weight == -1: final_text = raw_text # clean full file, no slice slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] # process text cleaned_text = clean_text(final_text) # save cleaned dataset print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)") # babyllm - babyllm.py import school.staffroom.calligraphist as s_output import school.staffroom.counsellor as c councel = c.counsellor() output = s_output.s_output(councel) for type, _ in output.s_types.items(): print(output.s_apply(type, f"this is the {type} colour!")) print(output.s_apply('superperfect', "chawis is perfect uwu owo")) print(output.s_apply('perfect', "elodie is perfect owo uwu")) #print(output.s_apply('almostperfect', "babyllm is almost perfect")) #print(output.s_apply('supergreat', "babyllm is super great")) #print(output.s_apply('great', "babyllm is great")) #print(output.s_apply('good', "babyllm is good")) #print(output.s_apply('fine', "babyllm is fine")) #print(output.s_apply('almostfine', "charis is almost fine")) #print(output.s_apply('average', "george is average")) #print(output.s_apply('meh', "babyllm is meh")) #print(output.s_apply('bad', "babyllm is bad")) #print(output.s_apply('worse', "george is worse")) #print(output.s_apply('wtf', "kevin is wtf")) #print(output.s_apply('omg', "pete is omg")) #print(output.s_apply('omgwtf', "pete is omgwtf")) #print(output.s_apply('omgwtf', "charis is omgwtf!")) #print(output.s_apply('emergency', "babyllm is emergency")) print("❀ ʕ⊃✰✰⋆⋆") print("✰✰⋆⋆") print("✰✰⋆⋆꩜ ❀ ꩜ ʕ꩜ ⊃꩜") print("ʕっ•̀o•́ʔっ✰✰⋆⋆") print("ʕっ꩜ o꩜ ʔっ✰✰⋆⋆") def decorator(func): def inner(*args, **kwargs): caller_stack = [] for stack in inspect.stack(): caller_stack.append(stack[0].f_code.co_qualname) print(f"calling {func.__class__}.{func.__name__} from: {', '.join(caller_stack)}") return func(*args, **kwargs) return inner @decorator def tryme1(): pass def tryme2(): tryme4() def tryme3(): tryme2() def tryme4(): tryme5() def tryme5(): tryme1() class t: def __init__(self): tryme3() t() from rich.traceback import install #from torch.profiler import profile, record_function, profileractivity import sys, traceback, warnings, torch, os, random from school.staffroom.counsellor import counsellor from school.staffroom.he_is_scribe import scribe from school.staffroom.tutor import tutor # from brain.layers.sensorywobble import wobble # from school.staffroom.newsletter import stats def handle_exception(exc_type, exc_value, exc_traceback): if not issubclass(exc_type, keyboardinterrupt): print("[rip ʕっₓᴥₓʔっ] uncaught exception:") traceback.print_exception(exc_type, exc_value, exc_traceback) sys.excepthook = handle_exception warnings.simplefilter("default") # show all warnings (pytorch hides some by default) install(show_locals = true) torch.autograd.set_detect_anomaly(mode = anomalydetect, check_nan = debugprints) def wakeup(): # wake up the school :) counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) with counsellor.infodump("wakeup") as ʕっʘ‿ʘʔっ: # open the library :) ʕっʘ‿ʘʔっ("waking the librarian...") librarian = librarian (_counsellor = counsellor, _basetokenizerpath = "brain/vocabcache/tokenizer_2000.json") exit(8) ʕっʘ‿ʘʔっ("opening questions...") newstartindex = openingquestions(_counsellor = counsellor, _librarian = librarian) ʕっʘ‿ʘʔっ("generating training data pairs...") trainingdatapairs = librarian.gentrainingdata(_windowmax = windowmax, _startindex = newstartindex) if debugprints: print(f"total trainingdatapairs: {len(trainingdatapairs)}") ʕっʘ‿ʘʔっ("loading chaos agents...") calligraphist = s_output (_counsellor = counsellor) scribe = scribe (_counsellor = counsellor, _calligraphist = calligraphist, _librarian = librarian, ) wobble = none # wobble = wobble (_counsellor = counsellor, # _calligraphist = calligraphist, # _device = modeldevice, # _activationfunction = activationfunction) # wake up the baby :) ʕっʘ‿ʘʔっ("loading babyllm...") babyllm = babyllm (_counsellor = counsellor, _scribe = scribe, _librarian = librarian, _device = modeldevice, tutor = tutor (_counsellor = counsellor, _model = babyllm, babyllm.loadmodel() babyllm.to(modeldevice) # start the lessons :) ʕっʘ‿ʘʔっ("starting lessons!") tutor.trainmodel (_trainingdatapairs = trainingdatapairs, _epochs = epochs, _startindex = newstartindex) print(f"[rip ʕっₓᴥₓʔっ]") raise except keyboardinterrupt: #as k for name, p in babyllm.named_parameters(): if p.grad is none: print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("emergency", f"no grad: {name}")}") grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") ʕっʘ‿ʘʔっ("♥keyboardinterrupt") if tutor.trainingstepcounter: step = tutor.trainingstepcounter step = 1 choice = input("save, cancel (do not save before exit), restart or interact?" + f"\n{username}: ").lower() if choice in ("save", "") or choice.startswith("s"): ʕっʘ‿ʘʔっ("♥choice = s") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) elif choice == "cancel" or choice.startswith("c"): ʕっʘ‿ʘʔっ("♥choice = c") print("\nhey! i wanted to remember that! :(") elif choice == "interact" or choice.startswith("i"): ʕっʘ‿ʘʔっ("♥choice = i") import code print("try:\nbabyllm.stats\nbabyllm.scheduledsampling\nbabyllm.memory.memory\nbabyllm.interneuronnetwork.cerebellum\nbabyllm.logits.forward(...)\nuse `exit()` to return to terminal.\n") code.interact(local = locals()) elif choice == "restart" or choice.startswith("r"): ʕっʘ‿ʘʔっ("♥choice = r") print("you spin me right round, babyllm, right round...") wakeup() ʕっʘ‿ʘʔっ("♥choice = none") print("\nuhh... i'm confused, but i saved anyway!") if modeldevice.type == 'mps': torch.mps.empty_cache() print(f"cache emptied") exit(8) def setstartindex(): if os.path.exists(stepcheckpointfilepath): with open(stepcheckpointfilepath, "r") as f: try: savedstep = int(f.read().strip()) except valueerror: babynote_loadcheckpoint = f"{babyname} 'oh. i couldn't load step checkpoint file from {stepcheckpointfilepath}, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 babynote_loadcheckpoint = f"{babyname} 'ah, the step checkpoint file {stepcheckpointfilepath} doesn't exist, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 savedstartindex = savedstep + trainingstartindex return savedstartindex def openingquestions(_counsellor, _librarian): counsellor = _counsellor with counsellor.infodump("babyllm") as ʕっʘ‿ʘʔっ: librarian = _librarian #babyllm.to(modeldevice) ʕっʘ‿ʘʔっ("setstartindex") newstartindex = setstartindex() babynote_loadcheckpointcheck = f"[{babyname}] right, last time i got to step {newstartindex}... want to restart from there?" ʕっʘ‿ʘʔっ("choice = input♥") choice = input(babynote_loadcheckpointcheck + f"\n[{username}] ").lower() usernote_loadcheckpoint = f"[{username}] {choice}" if choice == "" or choice.startswith("y"): ʕっʘ‿ʘʔっ("♥choice = y") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] ok! let's go to step {newstartindex}!" print(babynote_loadcheckpoint, end="") elif choice.startswith("r") or choice in ["random", "i dont care", "i don't care", "idc"]: newstartindex = random.randint(0, len(librarian.tokens) - windowmax - 1) babynote_loadcheckpoint = f"[{babyname}] oh, cool! i'll pick a random spot to start from... umm... let's go to step {newstartindex}!" elif choice.startswith("n") or choice in ["start again", "restart"]: ʕっʘ‿ʘʔっ("♥choice = n") babynote_loadcheckpoint = f"[{babyname}] alright, step {newstartindex}, let's go back to the beginning :)" elif choice.isdigit(): ʕっʘ‿ʘʔっ("♥choice = digit") newstartindex = int(choice) babynote_loadcheckpoint = f"[{babyname}] damn that's specific! heading to step {newstartindex}..." babynote_loadcheckpoint = f"[{babyname}] umm... i don't think i heard you properly, i'll just start from step {newstartindex} :) but," ʕっʘ‿ʘʔっ("runstart") printstartlogs(babynote_loadcheckpointcheck, usernote_loadcheckpoint, babynote_loadcheckpoint) return startindex def printstartlogs(_babynote_loadcheckpointcheck, _usernote_loadcheckpoint, _babynote_loadcheckpoint): #ʕっʘ‿ʘʔっ("♥bootprints") # boot prints to txt and terminal timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') babynote_runstart = f" what am i learning today?" # no tag of 'babyllm:' because it merges with the end of above message in logs usernote_runstart = f"[{username}] " + input(babynote_runstart + f"\n[{username}] ").strip().lower() + "" notesstring = f"--- {timestamp} --- \n{_babynote_loadcheckpointcheck}\n{_usernote_loadcheckpoint}\n{_babynote_loadcheckpoint}{babynote_runstart}\n{usernote_runstart}" print(notesstring) #ʕっʘ‿ʘʔっ("♥printstartlogs") with open(chatlogpath_forhumans, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_100, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_1000, "a") as logfile: logfile.write(notesstring) with open(chatlogpath_traininglog, "a") as logfile: logfile.write(notesstring) def main(): wakeup() main()