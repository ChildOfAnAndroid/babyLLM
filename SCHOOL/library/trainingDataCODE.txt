# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # vocab: training generation and tokenization # brain/layers/vocab.py from collections import counter from config import * from transformers import autotokenizer, pretrainedtokenizerfast from tokenizers import tokenizer, models, trainers, pre_tokenizers, bytelevelbpetokenizer from tokenizers.processors import bytelevel import os, re, json, random, torch from school.notebook.tools.genboi import * """ handles vocab creation, loading, and tokenization. this class: - trains a tokenizer (byte-pair encoding) if no pre-trained tokenizer is found. - loads a pretrained tokenizer if its there. - builds vocab lists and mappings (token to index, index to token). - tokenizes text using the pretrained/loaded tokenizer. - loads training data. - generates training data pairs (input sequence, target token). - saves and loads vocab data to/from files. """ class librarian: def __init__(self, _counsellor, _vocabsize=vocabsize, _vocabpath=none, _basetokenizerpath=none, _forceretrain=false): self.v_counsellor = _counsellor self.vocabsize = _vocabsize + 1 # for <unk> self.unktoken = "<unk>" self.vocabcache = vocabcachepath self.vocabfilename = f"vocab{_vocabsize}_{mintokenfreq}" self.tokenizerfilename = f"tokenizer_{_vocabsize}.json" self.tokenizerpath = _vocabpath or os.path.join(self.vocabcache, self.tokenizerfilename) self.tokenizerlockfile = os.path.join(self.vocabcache, f"{self.tokenizerfilename}.lock") self.vocablistfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_list.json") self.tokentoindexfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_index.json") self.indextotokenfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_token.json") self.vocablist = [] self.tokentoindex = {} self.indextotoken = {} self.basetokenizerpath = _basetokenizerpath os.makedirs(self.vocabcache, exist_ok=true) with self.v_counsellor.infodump("__init__") as  ï„Å£ ò‚Äø ò î„Å£: shouldtrain = _forceretrain or not os.path.exists(self.tokenizerpath) or not os.path.exists(self.tokenizerlockfile) if shouldtrain:  ï„Å£ ò‚Äø ò î„Å£("training new tokenizer") print("training new tokenizer...") tokenizermodel = tokenizer(models.bpe(unk_token=self.unktoken)) tokenizermodel.pre_tokenizer = pre_tokenizers.bytelevel() trainer = trainers.bpetrainer( vocab_size=self.vocabsize, min_frequency=mintokenfreq, special_tokens=[self.unktoken] ) with open(trainingfilepath, "r", encoding="utf-8") as f: training_data = [f.read().lower()] tokenizermodel.train_from_iterator(training_data, trainer) tokenizermodel.save(self.tokenizerpath) with open(self.tokenizerlockfile, "w") as f: f.write("locked") # avoid retraining by accident lol self.tokenizer = tokenizermodel else:  ï„Å£ ò‚Äø ò î„Å£("loading existing tokenizer") print("loading existing tokenizer...") self.tokenizer = tokenizer.from_file(self.tokenizerpath) self.buildvocabmap() if self.loadvocab():  ï„Å£ ò‚Äø ò î„Å£("loaded vocab from files...") self.trainingdatapairs = self.loadtrainingdata(trainingfilepath_arr) self.tokens = self.tokenizetext(self.trainingdatapairs) else:  ï„Å£ ò‚Äø ò î„Å£("building vocab from tokenizer...") self.buildvocabmap() self.savevocab() print(f"saved vocab data to {self.vocabcache}!") def tokenizetext(self, _text): with self.v_counsellor.infodump("tokenizetext") as  ï„Å£ ò‚Äø ò î„Å£: encoding = self.tokenizer.encode(_text) if debugprints: print(f"tokenizing: {_text}") print(f"token ids: {encoding.ids}") return [self.indextotoken.get(idx, self.unktoken) for idx in encoding.ids] # convert indexs back to strings def buildvocabmap(self): with self.v_counsellor.infodump("buildvocabmap") as  ï„Å£ ò‚Äø ò î„Å£:  ï„Å£ ò‚Äø ò î„Å£("getting vocab dictionary from tokenizer...") invvocab = self.tokenizer.get_vocab()  ï„Å£ ò‚Äø ò î„Å£("ordering by index...") sortedtokens = sorted(invvocab.items(), key=lambda item: item[1]) # sort by index self.vocablist = [token for token, idx in sortedtokens]  ï„Å£ ò‚Äø ò î„Å£("mapping vocab dicts...") self.tokentoindex = {token: idx for token, idx in sortedtokens} self.indextotoken = {idx: token for token, idx in sortedtokens}  ï„Å£ ò‚Äø ò î„Å£("ensuring <unk> is in the vocab...") if self.unktoken not in self.tokentoindex: self.vocablist.append(self.unktoken) unk_index = len(self.vocablist) - 1 self.tokentoindex[self.unktoken] = unk_index self.indextotoken[unk_index] = self.unktoken print(f"final vocab size: {len(self.vocablist)}") print(f"first 20 tokens: {self.vocablist[:20]}") def huggingtokenizer(self, _text): return self.tokenizer.tokenize(_text) def loadtrainingdata(self, _filepaths, _chunksize=v_chunksizeloaddata): with self.v_counsellor.infodump("loadtrainingdata") as  ï„Å£ ò‚Äø ò î„Å£: result = "" for path in _filepaths: with open(path, "r", encoding="utf-8") as f: while true: chunk = f.read(_chunksize) if not chunk: break result += chunk result = re.sub(r'\s+', ' ', result) print(f"loaded {len(result)} characters of training data!") return result def gentrainingdata(self, _windowmax=windowmax, _startindex=trainingstartindex, _trainingdatapairnumber=trainingdatapairnumber): with self.v_counsellor.infodump("gentrainingdata") as  ï„Å£ ò‚Äø ò î„Å£: trainingdatapairs = [] count = 0 tokens = self.tokens  ï„Å£ ò‚Äø ò î„Å£("check if windowmax is tensor?") if isinstance(_windowmax, torch.tensor): _windowmax = _windowmax.item()  ï„Å£ ò‚Äø ò î„Å£("allows for random start") if _startindex == 'random': _startindex = random.randint(0, len(tokens) - _windowmax - 1) end = len(tokens) - _windowmax  ï„Å£ ò‚Äø ò î„Å£("generate training pairs") for i in range(_startindex, end): inputseq = tokens[i:i+_windowmax] target = tokens[i+_windowmax:i+_windowmax+_windowmax] if len(target) < _windowmax: continue if all(t in self.vocablist for t in inputseq + target): trainingdatapairs.append((inputseq, target)) count += 1 if count >= _trainingdatapairnumber: break if count % 1000 == 0: print(f"{makedatboi()} {babyname}: generated {count}x trainingdatapairs!") else: print(f"skipping <unk> - inputseq: {inputseq}, target: {target}") return trainingdatapairs def savevocab(self): with self.v_counsellor.infodump("savevocab") as  ï„Å£ ò‚Äø ò î„Å£: os.makedirs(self.vocabcache, exist_ok = true) # ensure directory exists with open(self.vocablistfile, "w", encoding="utf-8") as f:  ï„Å£ ò‚Äø ò î„Å£("save vocablist") json.dump(self.vocablist, f, indent = 4) with open(self.tokentoindexfile, "w", encoding="utf-8") as f:  ï„Å£ ò‚Äø ò î„Å£("save tokentoindex") json.dump(self.tokentoindex, f, indent = 4) with open(self.indextotokenfile, "w", encoding="utf-8") as f:  ï„Å£ ò‚Äø ò î„Å£("save indextotoken") json.dump(self.indextotoken, f, indent = 4) def loadvocab(self): with self.v_counsellor.infodump("loadvocab") as  ï„Å£ ò‚Äø ò î„Å£: try: with open(self.vocablistfile, 'r', encoding='utf-8') as f:  ï„Å£ ò‚Äø ò î„Å£("load vocablist") self.vocablist = json.load(f) with open(self.tokentoindexfile, 'r', encoding='utf-8') as f:  ï„Å£ ò‚Äø ò î„Å£("load tokentoindex") self.tokentoindex = json.load(f) with open(self.indextotokenfile, 'r', encoding='utf-8') as f:  ï„Å£ ò‚Äø ò î„Å£("load indextotoken") self.indextotoken = {int(k): v for k, v in json.load(f).items()} # ensures that keys are integers! print("vocab files loaded successfully!") return bool(self.vocablist and self.tokentoindex and self.indextotoken) except (filenotfounderror, json.jsondecodeerror): print("vocab files not found or invalid... rebuilding vocab...") return false if __name__ == "__main__": counsellor = type("dummy", (), {"infodump": lambda self, label: open(os.devnull, 'w')})() librarian = librarian(_counsellor = counsellor, _vocabsize = 4200, _basetokenizerpath = "brain/vocabcache/tokenizer_2000.json") print(f"- 2000-{vocabsize} -: {librarian.vocablist[2000:vocabsize]}") print(f"- 1701-2000 -: {librarian.vocablist[1701:2000]}") print(f"- 1001-1700 -: {librarian.vocablist[301:1700]}") print(f"- 301-1000 -: {librarian.vocablist[301:1000]}") print(f"- 101-300 -: {librarian.vocablist[101:300]}") print(f"- top 100 -: {librarian.vocablist[:100]}") print(f"vocab size: {len(librarian.vocablist)}") print(f"top 20 tokens: {librarian.vocablist[:20]}") #print(vocab.huggingtokenizer("charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv")) sample_text = "charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv" tokenizedoutput = librarian.tokenizetext(sample_text) print(f"tokenized: {tokenizedoutput}")# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # babyllm school counsellor // school/staffroom/counsellor.py """designed for detailed small scale logging throughout the project, with timing implemented for troubleshooting errors""" import time import re from config import * from contextlib import contextmanager # usage: # remember to initialise the class instance # self.counsellor = counsellor("class_name", debug = debugprints, durations = durationlogging) # in the top of your function, encasing all of your function lines, put; # with self.counsellor.infodump("function_name") as  ï„Å£ ò‚Äø ò î„Å£: # this will duration track the whole function, if enabled, and also make a note in debugprints when it starts and ends # anywhere you want to start a new tracker within a function, place; #  ï„Å£ ò‚Äø ò î„Å£("tracker_name") # these work similarly to the function trackers, but are internal points within the function. # now your code can get some fucking therapy, for once! class counsellor: def __init__(self, _classname="?", _debug = debugprints, _durations = durationlogging): self.classname = _classname self.debugprints = _debug self.durationlogging = _durations self.duration = {} self.duration_a = {} def log(self, key, value): maxlogs = 5000 if not self.durationlogging: return self.duration[key] = self.duration.get(key, 0) + value if len(self.duration) > maxlogs: self.duration.clear() print(f"cleared duration_b as it was higher than {maxlogs}") self.duration_a[key] = self.duration_a.get(key, 0) + value if len(self.duration_a) > maxlogs: self.duration_a.clear() print(f"cleared duration_a as it was higher than {maxlogs}") @contextmanager def infodump(self, _functionname, _extra = none, _key = none): fulltitle = f"{self.classname}_{_functionname}" startstamp = time.time() if self.durationlogging else none if self.debugprints: line = f" ï„Å£ ò‚Äø ò î„Å£ starting {fulltitle}... ‚Üí" if _extra: line += f" ({_extra})" print(line) class tangent: def __init__(self, _parent): self.parent = _parent self.lastinfodumptime = startstamp self.lastinfodumpname = none self.parentfunction = none self.path = [] if self.parentfunction != _functionname: self.path = [] # reset when function changes self.parentfunction = _functionname setattr(self, " ï„Å£ ò‚Äø ò î„Å£", self.infodump) #somehow legal as a variable name how have i done this please protect me lol setattr(self, "(„Å£‚óï‚Äø‚óï)„Å£", self.infodump) setattr(self, "‚ô•‚Äø‚ô•", self.infodump) setattr(self, "(ÔΩ°‚ô•‚Äø‚ô•ÔΩ°)", self.infodump) setattr(self, "infodump", self.infodump) def __call__(self, _innername): return self.infodump(_innername) def infodump(self, _innername): now = time.time() # clean function context? start new base path. if self.parentfunction != _functionname: self.parentfunction = _functionname self.path = [] # reset because moved to another function if isinstance(_innername, str): if _innername.startswith("‚ô•") and _innername.endswith("‚ô•"): self.path = [_innername.strip("‚ô•")] # appendable base elif _innername.startswith("‚ô•"): self.path.append(_innername[1:]) # append elif _innername.endswith("‚ô•"): self.path = [_innername[:-1]] # new base path elif "/" in _innername or "‚ô•" in _innername: self.path = [p.strip() for p in re.split(r"[‚Üí/‚ô•]", _innername)] else: self.path = [_innername] else: self.path = [str(_innername)] # log previous section if self.lastinfodumpname: duration = now - self.lastinfodumptime tag = f"{_functionname}‚ô•{'‚ô•'.join(self.path[:-1] + [self.lastinfodumpname])}" self.parent.log(tag, duration) if self.parent.debugprints: print(f"‚ô• finished {self.parent.classname}‚ô•{tag} in {duration:.4f}s ‚ô•") # start new self.lastinfodumpname = self.path[-1] self.lastinfodumptime = now fulltag = f"{_functionname}‚ô•{'‚ô•'.join(self.path)}" if self.parent.debugprints: print(f"‚Üí starting {self.parent.classname}‚ô•{fulltag} ‚Üí") tangent = tangent(self) try: yield tangent finally: if tangent.lastinfodumpname: finalduration = time.time() - tangent.lastinfodumptime finaltag = f"{_functionname}‚ô•{'‚ô•'.join(tangent.path)}" self.log(finaltag, finalduration) if self.debugprints: print(f"‚ô• finished {self.classname}‚ô•{finaltag} in {finalduration:.4f}s ‚ô•") if startstamp: totalduration = time.time() - startstamp self.log(_key or _functionname, totalduration) if self.debugprints: print(f"(„Å£‚óï‚Äø‚óï)„Å£ finished {fulltitle} in {totalduration:.4f}s (ÔΩ°‚ô•‚Äø‚ô•ÔΩ°)") elif self.debugprints: print(f"(„Å£‚óï‚Äø‚óï)„Å£ finished {fulltitle} (ÔΩ°‚ô•‚Äø‚ô•ÔΩ°)")# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # embedding layer // brain/layers/embed.py import torch import torch.nn as nn from config import * """creates an embedding layer for each word in the vocabulary""" class embed(nn.module): def __init__(self, _counsellor, _device = modeldevice): super().__init__() self.counsellor = _counsellor self.device = _device self.stats = {} """creates the embedding weights matrix with random numbers initially""" self.e_weights = nn.parameter(torch.randn(vocabsize, embeddimension, device = self.device)) # [2000,] self.embednorm = nn.layernorm(embeddimension, device = self.device) self.weightsscale = nn.parameter(torch.tensor(0.5)) self.normscale = nn.parameter(torch.tensor(0.5)) self.lastsavedembeds = self.e_weights.detach().clone() # this is initialised once, for stats, does not break graph confirmed!! """looks up and returns the embedding vector for a specifc token index""" @whocalled def forward(self, _tokenindex): with self.counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£:  ï„Å£ ò‚Äø ò î„Å£("e0_embedvector") # <- vocab??? base token indexes seem to come in here so... from tutor?? self.embedvector = self.e_weights[_tokenindex]  ï„Å£ ò‚Äø ò î„Å£("e1_embednormed") # <- e1 self.embednormed = self.embednorm(self.embedvector)  ï„Å£ ò‚Äø ò î„Å£("ex_embedfinal") # <- e2 self.embedfinal = (self.embedvector * self.weightsscale) + (self.embednormed * self.normscale) return self.embedfinal # e3 -> n?? def getembedstats(self): with self.counsellor.infodump("getembedstats") as  ï„Å£ ò‚Äø ò î„Å£: with torch.no_grad(): self.stats = {} embednorms = torch.norm(self.e_weights, dim = 1) self.stats["embednormmean"] = embednorms.mean() self.stats["embednormstd"] = embednorms.std() self.stats["embednormmax"] = embednorms.max() self.stats["1e_0_embedvector_norm"] = self.embedvector.norm().item() self.stats["1e_1_embednormed_norm"] = self.embednormed.norm().item() self.stats["1e_x_embedfinal_norm"] = self.embedfinal.norm().item() self.stats["1e_0_embedvector_scale"] = self.weightsscale.norm().item() self.stats["1e_1_embednormed_scale"] = self.normscale.norm().item() dimmean = self.e_weights.mean(dim = 0) self.stats["embeddimensionmean"] = dimmean dimsparsity = (dimmean.abs() < 1e-4).float().mean() self.stats["embeddimensionsparsity"] = dimsparsity # drift since last save drift = torch.norm(self.e_weights - self.lastsavedembeds) self.stats["embeddingdrift"] = drift self.lastsavedembeds = self.e_weights.detach().clone() return self.stats def cosinesimilarity(self, _idx1, _idx2): e1 = self.e_weights[_idx1] e2 = self.e_weights[_idx2] return torch.nn.functional.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)) if __name__ == "__main__": testtokenindex = 500 # 32 (embeddimension) x 2000 (vocab) = 64,000 in embed layer embed = embed(vocabsize, embeddimension) embedvector = embed.forward(testtokenindex) print(f"- embedding layer testing start -") print(f"embedding layer weights shape: {embed.weights.shape}") # check shape of weight matrix print(f"embedding vector for token index {testtokenindex}:") print(embedvector) print(f"embedding vector shape: {embedvector.shape}") print(f"- embedding layer testing complete -")# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # babyllm // babyllm.py import random, os import torch import torch.nn.functional as f import torch.nn as nn import torch.optim as optim import torch.optim.lr_scheduler import math from collections import counter from brain.layers.embed import embed from brain.layers.interneuronnetwork import interneuron_network from brain.layers.logits import logits from brain.layers.memory import memory #from brain.layers.sensorywobble import wobble from config import * """this class combines all the core components of the babyllm:""" """embed: token embedding layer""" """interneuron_network: layer of parallel neurons for feature extraction""" """logits: output layer to generate logits""" """it also manages training, loss computation, backpropagation, and response generation.""" class babyllm(nn.module): def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _device = modeldevice): super().__init__() self.device = _device self.counsellor = _counsellor self.calligraphist = _calligraphist self.scribe = _scribe self.librarian = _librarian #self.wobble = _wobble # must be on self - only accessed in this class and not nn.params self.totaltokenevaluations = 0 self.latestlossdelta = 0 self.totaltokenevaluations_a = 0 self.recentgeneratedtokens = [] # used for repetition penalty self.lastlossbaby = 0 self.computelosscount = 0 self.repeatedpercent = 0 self.normalisedactivations = 0 self.rollingtokentotals = counter() self.gumbellend = 0 self.stats = {} self.normalisedhistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.inputembedshistory = [] self.finallogitshistory = [] """cerebral layers // brain""" self.embed = embed(_counsellor = self.counsellor, _device = self.device) self.interneuronnetwork = interneuron_network(_model = babyllm, _counsellor = self.counsellor, _calligraphist = self.calligraphist, _device = self.device) self.logits = logits(_counsellor = self.counsellor, _device = self.device) self.memory = memory(_counsellor = self.counsellor, _device = self.device) self.finalnormlayer = nn.layernorm(numneurons, device=self.device) """learnable learning parameters""" self.repetitionpenalty = nn.parameter(torch.tensor(1.0, device = self.device)) self.logtemp = nn.parameter(torch.tensor(math.log(0.8), device = self.device)) self.loglr = nn.parameter(torch.tensor(math.log(1e-4), device = self.device)) self.loggradclip = nn.parameter(torch.tensor(math.log(1.0), device = self.device)) self.scheduledsamplingrate = nn.parameter(torch.tensor(0.2, device = self.device)) self.logmemorylength = nn.parameter(torch.tensor(math.log(memorylengthgoal), device = self.device)) self.logrepetitionwindow = nn.parameter(torch.tensor(math.log(repetitionwindowgoal), device = self.device)) """stuff""" self.gradientclipmaxnorm = torch.exp(self.loggradclip) self.temperature = none """optimizer - this updates all of the layers learnable parameters""" if debugprints: print("registered parameters: ") for name, param in babyllm.named_parameters(self): print(name, param.shape) optimizerclass = getattr(optim, optimizername) self.optimizer = optimizerclass(self.parameters(), lr = learningrate, weight_decay = 0.005, fused = true) if debugprints: for name, param in self.named_parameters(): print(f"{name}: requires_grad={param.requires_grad}") #self.to(self.device) self.statscategories = {"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "scheduledsamplingrate": 0, "tokencount": 0, "memorygateshort": 0, "memorygatelong": 0, "memorygatecurrent": 0, "shortdecay": 0, "longdecay": 0,} @whocalled def forward(self, _inputseq): with self.counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£: # processes input sequence of tokens (str) to generate logits to predict the next token if debugprints: print(f"debug: input to forward: {_inputseq}") self.temperature = torch.exp(self.logtemp)  ï„Å£ ò‚Äø ò î„Å£("b0: inputembeds") # convert indices to embeddings inputembeds = self.embed(_inputseq) # directly taking a tensor now if debugprints: print(f"debug babyllm.forward: inputembeds requires_grad: {inputembeds.requires_grad} [expected: true]")  ï„Å£ ò‚Äø ò î„Å£("b1: interneuronnetworkoutput") # parallel neuron layer input/processing (feature extraction) innoutput = self.interneuronnetwork.forward(inputembeds) if debugprints: print(f"debug babyllm.forward: interneuronnetworkoutput length: {len(innoutput)}") if debugprints: print("combinedactivationstensor.requires_grad:", innoutput.requires_grad) if debugprints: print("combinedactivationstensor.grad_fn:", innoutput.grad_fn)  ï„Å£ ò‚Äø ò î„Å£("b2: memoryoutput") # memory layer processing - now process the combined activations if skipmemory: if debugprints: print("skipping memory layer...") self.latestmemgates = torch.tensor([0.0, 0.0, 1.0], device = self.device) # dummy gates memoryoutput = innoutput.detach() # no grad path, super light else: memoryoutput = self.memory.forward(innoutput) self.latestmemgates = self.memory.latestmemorygates if debugprints: print("combinedactivations.requires_grad:", memoryoutput.requires_grad)  ï„Å£ ò‚Äø ò î„Å£("b3: repetitionpenalty") if not torch.isfinite(self.logrepetitionwindow): print("logrepetitionwindow has gone non-finite. resetting.") self.logrepetitionwindow.data = torch.tensor(math.log(repetitionwindowgoal), device = self.device) penalisedoutput = self.applyrepetitionpenalty(memoryoutput) if debugprints: print("before memory output requires_grad?", self.memory.longtermmemory.requires_grad) if debugprints: print("before cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("before logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("before logmemorylength requires_grad?", self.logmemorylength.requires_grad) if skipfinallogitnorm:  ï„Å£ ò‚Äø ò î„Å£("bx: logits.forward") finallogits = self.logits.forward(penalisedoutput) if false:  ï„Å£ ò‚Äø ò î„Å£("b4: finalnormlayer") self.normedoutput = self.finalnormlayer(penalisedoutput) finallogits = self.logits.forward(self.normedoutput) self.normalisedhistory.append(self.normedoutput.norm().item()) if debugprints: print("after logmemorylength requires_grad?", self.logmemorylength.requires_grad) if debugprints: print("after logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("after cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("after memory output requires_grad?", self.memory.longtermmemory.requires_grad) if true:  ï„Å£ ò‚Äø ò î„Å£("stats collection!") self.inputembedshistory.append(inputembeds.norm().item()) self.innoutputhistory.append(innoutput.norm().item()) self.memoryoutputhistory.append(memoryoutput.norm().item()) self.penalisedoutputhistory.append(penalisedoutput.norm().item()) self.finallogitshistory.append(finallogits.norm().item()) if len(self.inputembedshistory) >= windowmax: self.forwardstats = { "2b_0_inputembeds_norm": sum(self.inputembedshistory) / len(self.inputembedshistory), "3b_1_innoutput_norm": sum(self.innoutputhistory) / len(self.innoutputhistory), "5b_0_memoryoutput_norm": sum(self.memoryoutputhistory) / len(self.memoryoutputhistory), "5b_1_penalisedoutput_norm": sum(self.penalisedoutputhistory) / len(self.penalisedoutputhistory), #"5b_x_finalnormlayer_norm": sum(self.normalisedhistory) / len(self.normalisedhistory), "7b_x_finallogits_norm": sum(self.finallogitshistory) / len(self.finallogitshistory), } self.stats.update(self.forwardstats) self.inputembedshistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.finallogitshistory = [] self.normalisedhistory = [] """returns a logits tensor of shape (1, vocabsize) showing predicted probabilities for the next token""" return finallogits """computes the cross-entropy loss between the models logits and the target token, essentially checking how good the models prediction was""" def computeloss(self, _logits, _targettokenindex, _latestlossdelta = 0, _perfecttokens = 0, _training = false): with self.counsellor.infodump("computeloss") as  ï„Å£ ò‚Äø ò î„Å£: self.perfecttokens = _perfecttokens if skipcomputeloss:  ï„Å£ ò‚Äø ò î„Å£("skipping loss!") return torch.tensor([0.1], requires_grad = true, device = self.device) # constant scalar tensor  ï„Å£ ò‚Äø ò î„Å£("targettensor") targettensor = torch.tensor([_targettokenindex], dtype = torch.long, device = self.device) if debugprints: print(f"logits shape: {_logits.shape} | target: {_targettokenindex}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure logits are at least 2d  ï„Å£ ò‚Äø ò î„Å£("cross entropy loss") losslogits = torch.clamp(_logits, min=-50, max=50) loss = f.cross_entropy(losslogits, targettensor) if not torch.isfinite(loss): print("nan/inf loss detected - logits:", _logits) return torch.tensor(10.0, device=self.device, requires_grad=true) # or skip/backoff if debugprints: print(f"crossentropy raw loss: {f.cross_entropy(_logits, targettensor)}") self.celossdelta = loss - ((self.lastlossbaby) if self.lastlossbaby is not none else 0) #tempreg = (torch.clamp(self.logtemp, 0.7, 0.9) - 0.8).pow(2) if debugprints: print(f"{self.lastlossbaby:0.1f}", end = ", ") # take delta #entropy = 0.001 * self.interneuronnetwork.entropybonus lrsoftclamp = 0.5 * (self.loglr - math.log(0.0002)).pow(2) loss += lrsoftclamp # use .detach() to avoid .backward() self.lastlossbaby = loss.item() if _training and self.lastsoftsample is not none: target = f.one_hot(targettensor, num_classes = _logits.shape[1]).float() auxloss = f.kl_div(self.lastsoftsample.log(), target, reduction = 'batchmean') finalloss = loss + auxloss * torch.sigmoid(loss - auxloss) # low weight for anti-dominatrix else: finalloss = loss #tempsoftclamp = 0.4 * (self.logtemp - math.log(0.5)).pow(2) # more tokens (better) > perftokens > less tokens (worse) # higher number > 2 > lower number # 0.3x > 2 > 1.3x # worse (explore) > latestlossdelta > better (stay still) # positive number > 0 > negative number # +4 delta (worse) > 0 > -4 delta (better) # [0-25]x0.1 > 0 > [0-1] # 0-2.5 > 0 > 0-1 #if debugprints: print(f"[loss debug] requires_grad: {loss.requires_grad} | value: {loss.detach().cpu().item():.4f}") return finalloss """backpropagation and optimization, computes gradients of the loss and uses the optimizer to update the models weights""" def backward(self, _loss): with self.counsellor.infodump("backward") as  ï„Å£ ò‚Äø ò î„Å£: if debugprints: for name, p in self.named_parameters(): if p.grad is none: print(f"before = {self.calligraphist.s_apply("dim", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"before = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}")  ï„Å£ ò‚Äø ò î„Å£("loss.backward") _loss.backward() #print(next(self.parameters()).grad) if debugprints: for name, p in self.named_parameters(): if p.grad is none: print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") with torch.no_grad(): # reset learnable parameters #self.loglr.data.fill_(math.log(0.00035)) # learning rate back to 1e-4 self.scheduledsamplingrate.data.fill_(0.05) # scheduled sampling full (no scheduled sampling yet) #self.temperature.data.fill_(math.exp(self.logtemp)) # temperature normal #self.repetitionpenalty.data.fill_(1.0) # repetition penalty normal #self.logmemorylength.data.fill_(math.log(1)) # memory length default #self.logrepetitionwindow.data.fill_(math.log(16)) # repetition window default #self.interneuronnetwork.logwindowsizes.data.copy_( # torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device)) #) #for module in self.interneuronnetwork.windowmeta: # if isinstance(module, torch.nn.linear): # module.reset_parameters() if true: with torch.no_grad(): self.loglr.clamp_(math.log(0.0001), math.log(0.001)) # clamp it! in memory of the amazing 1.00 self learned loss run of 27-april-2025! - you certainly dropped the delta! you win! learnedlr = torch.exp(self.loglr).item() for g in self.optimizer.param_groups: g['lr'] = learnedlr #self.gradientclipmaxnorm = torch.exp(self.loggradclip).item() #self.repetitionwindow = torch.exp(self.logrepetitionwindow).item() #self.memorylength = torch.exp(self.logmemorylength).item() #self.loglr.data.fill_(self.loglr+0.0001) # increment lr manually (break grid)  ï„Å£ ò‚Äø ò î„Å£("clip_grad_norm") clipvalue = torch.exp(self.loggradclip).item() torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=clipvalue)  ï„Å£ ò‚Äø ò î„Å£("optimizer.step") self.optimizer.step() # update weights self.backwardstats = { "_b_floatmemorylength": torch.exp(self.logmemorylength).item(), "_b_repetitionwindow": torch.exp(self.logrepetitionwindow).item(), "_b_temperature": torch.exp(self.logtemp).item(), } self.stats.update(self.backwardstats) #with torch.no_grad(): # force reset the memory gates if over using long #self.memory.currentgate.data = self.memory.currentgate.data.abs() #self.memory.shortgate.data = self.memory.shortgate.data.abs() """this takes the output logits, does temperature scaling and softmax to create a probability distribution over the vocab, and then selects most likely response token""" """def getresponsefromlogits(self, _logits, _temperature = temperature): with self.counsellor.infodump("getresponsefromlogits") as  ï„Å£ ò‚Äø ò î„Å£: _logits /= _temperature if debugprints: print(f"debug babyllm.getresponsefromlogits: logits shape before softmax: {_logits.shape}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) probs = torch.softmax(_logits, dim = 1) responsefromlogits = torch.multinomial(probs, 1) return responsefromlogits""" @whocalled def getresponsefromlogits(self, _logits, _training = false): with self.counsellor.infodump("getresponsefromlogits") as  ï„Å£ ò‚Äø ò î„Å£: if not torch.isfinite(_logits).all(): print("logits not finite before response gen:", _logits) _logits = torch.nan_to_num(_logits, nan=0.0, posinf=1e3, neginf=-1e3)  ï„Å£ ò‚Äø ò î„Å£("update logarithmic parameters") #self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! _logits /= self.temperature if torch.isnan(_logits).any(): print("nan in logits after temperature scaling!") print("logtemp:", self.logtemp.item(), "temp:", self.temperature.item()) print("logits stats:", _logits.min().item(), _logits.max().item(), _logits.mean().item()) _logits = torch.nan_to_num(_logits, nan=0.0, posinf=1e3, neginf=-1e3) if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure [1, vocabsize] if _training: logitsforsample = _logits.clone() if not torch.isfinite(logitsforsample).all(): print("non-finite logits detected before gumbel") print("logits:", logitsforsample) logitsforsample = torch.nan_to_num(logitsforsample, nan=0.0, posinf=1e3, neginf=-1e3) try: gumbelprobs = f.gumbel_softmax(logitsforsample, tau=self.temperature, hard=false) assert torch.isfinite(gumbelprobs).all(), "gumbelprobs has nan or inf!" except exception as e: self.gumbellend += 1 print("gumbel softmax failed:", e) print(f"falling back to softmax sampling (total fallbacks: {self.gumbellend})...") gumbelprobs = torch.softmax(logitsforsample, dim=1) self.lastsoftsample = gumbelprobs responsefromlogits = gumbelprobs.argmax(dim = 1, keepdim = true) self.lastsoftsample = gumbelprobs topk = torch.topk(gumbelprobs, 10, dim=1) indices = topk.indices[0].tolist() values = topk.values[0].tolist() #self.lasttopguesses = [] for i, p in zip(indices, values): token = self.librarian.indextotoken.get(i, "<unk>") try: if isinstance(p, float) and math.isfinite(p): #self.lasttopguesses.append((token, round(p, 4))) self.rollingtokentotals[token] += round(p, 4) else: print(f"skipping non-finite top guess: {token} ‚Üí {p}") except exception as e: print(f"error processing top guess: {token} ‚Üí {p} | {e}") #print("top guesses + confidences:", [(self.librarian.indextotoken[i.item()], f"{p.item():.3f}") for i, p in zip(indices, values)]) else: probs = torch.softmax(_logits, dim=1) responsefromlogits = torch.multinomial(probs, 1) self.lastsoftsample = none # or keep the probs if you want analysis #if debugprints: #print(f"[rep penalty] {self.repeatedpercent:.2%} repeated | repetition slice: {self.repetitionslice} | penalised: {[self.librarian.indextotoken.get(t, '<unk>') for t in uniquetokens]}")  ï„Å£ ò‚Äø ò î„Å£("create windows using rolling buffer") self.recentgeneratedtokens.append(responsefromlogits.item()) if len(self.recentgeneratedtokens) > int(torch.exp(self.logrepetitionwindow)): self.recentgeneratedtokens.pop(0) return responsefromlogits def applyrepetitionpenalty(self, _logits): if not self.recentgeneratedtokens: return _logits repwindow = torch.exp(self.logrepetitionwindow) penalty = self.repetitionpenalty recenttokens = torch.tensor(self.recentgeneratedtokens, device=self.device) vocabsize = _logits.shape[1] positions = torch.arange(len(recenttokens), device=self.device).float() windowcentre = len(recenttokens) softmask = torch.sigmoid((positions - (windowcentre - repwindow)) * 0.5) onehots = f.one_hot(recenttokens, num_classes=vocabsize).float() weightedfreqs = (onehots.t @ softmask).view(1, -1) return _logits - (weightedfreqs * (0.001 * penalty)) def getnexttoken(self, _inputseq): with self.counsellor.infodump("getnexttoken(forward)") as  ï„Å£ ò‚Äø ò î„Å£: logits, *_ = self.forward(_inputseq) # unpacks the first value of the tuple and ignores the rest nexttoken = self.getresponsefromlogits(logits, _training = true) return nexttoken def savemodel(self, filepath = modelfilepath, _newstartindex = trainingstartindex, _trainingstepcounter = 0): with self.counsellor.infodump("savemodel") as  ï„Å£ ò‚Äø ò î„Å£: tmppath = filepath + ".tmp" torch.save(self.state_dict(), tmppath) print(f"model temp file created at {tmppath}...") os.replace(tmppath, filepath) print(f"model successfully saved to {filepath}!") with open(stepcheckpointfilepath, "w") as f: f.write(str(_trainingstepcounter+_newstartindex)) # this isnt real, fix later, maybe move save and load to wakeup? """loads the model from a file""" def loadmodel(self, filepath = modelfilepath): with self.counsellor.infodump("loadmodel") as  ï„Å£ ò‚Äø ò î„Å£: try:  ï„Å£ ò‚Äø ò î„Å£("update logarithmic parameters") self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! print(f"loading model from path: {filepath}") self.load_state_dict(torch.load(filepath), strict = savestrict) print(f"model loaded from {filepath}!") self.to(self.device) print(f"device set to {self.device}!") self.resetmemory(context="inference") except filenotfounderror: print("no saved model found") def babyllm_diary_entry(self, interneuronnetwork, step): with self.counsellor.infodump("babyllm_diary_entry") as  ï„Å£ ò‚Äø ò î„Å£: # grab current window weightings weights = interneuronnetwork.cerebellum windows = interneuronnetwork.allwindowsizes # find the current favourite and least favourite fav_idx = weights.argmax() worst_idx = weights.argmin() fav_window = windows[fav_idx] worst_window = windows[worst_idx] moods = ["chaotic", "curious", "crunchy", "a bit overwhelmed", "spicy", "thoughtful", "itchy", "playful"] actions = [ f"i still trust window {fav_window} the most", f"window {fav_window} makes me feel safe", f"window {worst_window} keeps confusing me!", f"i'll start listening to window {fav_window} more!", f"window {worst_window} tastes like static", f"i'm starting to wonder about window {fav_window}... is it my destiny?", f"window {worst_window} is just noise, i swear!", f"today i felt {random.choice(moods)}.", f"window {fav_window} whispered secrets to me." ] diaryline = f"step {step+1}: babyllm diary update: '{random.choice(actions)}'" print(diaryline) def resetmemory(self, context="inference"): with self.counsellor.infodump("resetmemory") as  ï„Å£ ò‚Äø ò î„Å£: """reset memory depending on the context: inference always resets, training resets every n turns""" if context == "inference":  ï„Å£ ò‚Äø ò î„Å£("context = inference") self.memory.resetmemory() print(f"resetting memory for new conversation...") elif context == "training":  ï„Å£ ò‚Äø ò î„Å£("context = training") if hasattr(self, "stepssincememoryreset"): self.stepssincememoryreset += 1 else: self.stepssincememoryreset = 1 if self.stepssincememoryreset >= int(torch.exp(self.logmemorylength).item()): self.memory.resetmemory() if debugprints: print(f"resetting memory after {self.stepssincememoryreset} steps... (learned mem length: {self.logmemorylength})") self.stepssincememoryreset = 0 def setlearningrate(self, _newlearningrate): self.learningrate = max(1e-6, min(_newlearningrate, 0.01)) # clamp it a bit for param_group in self.optimizer.param_groups: param_group["lr"] = self.learningrate def getbabystats(self): return self.stats if __name__ == "__main__": exit(0)# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # scribe module // school/staffroom/he_is_scribe.py import random import time from config import * from school.notebook.tools.genboi import * class scribe: def __init__(self, _counsellor, _calligraphist, _librarian): #self.counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) #self.s_output = s_output() self.counsellor = _counsellor self.calligraphist = _calligraphist self.librarian = _librarian self.scribeemotes = {"default": [" ï„Å£ ò‚Äø ò î„Å£", " ï·µî·¥•·µî î„Å£", " ï„Å£‡∑Ü.‡∑Ü î„Å£", " ï‚ú∞.‚ú∞ î„Å£", " ï·µî‚Äø·µî î„Å£‚ô°"], "neutral": [" ï -·¥•- î„Çù", " ï·µî·¥•·µî î„Å£‚ô•",], "annoyed": [" ï„Éé-·¥•- î„Éé Ô∏µ", " ï„Å£-ÃÄo-ÃÅ î„Å£", " ï-ÃÄo-ÃÅ î„Å£", " ï„Å£-ÃÄo-ÃÅ î„Å£‚ú∞‚ú∞‚ãÜ‚ãÜ", ], "hyper": [" ï„Å£Í©ú‚ÄøÍ©ú î„Å£ñ°º", " ï·µî‚Äø·µî î„Å£"], "worried": [" ï‚óâ.‚óâ î", " ïÍ©ú.Í©ú î„Å£‚ùÑ", ], "mischevious": [" ï-ÃÄ‚Äø- î„Å£", " ï„Å£‚Äø.‡∑Ü î„Å£‚ô°", " ï-ÃÄo- î„Å£", " ï-ÃÄ‚Äø- î„Å£", " ï„Å£‡∑Ü.‚Äø î„Å£‚ô°", " ï-ÃÄ‚Äø-ÃÅ î„Å£", ], "love": [" ï·µî·¥•·µî î„Å£‚ô•", " ï„Å£‡∑Ü.‡∑Ü î„Å£‚ô°", " ï·µî‚Äø·µî î„Å£‚ô°", " ï„Å£‚ú∞.‚ú∞ î„Å£‚ùÄ", " ï„Å£ ò‚Äø ò î„Å£‚ô°", " ï‚ùÄ‡∑Ü.‡∑Ü î„Å£‚ùÄ", ], "hugs": [" ï„Å£‡∑Ü.‡∑Ü î„Å£", " ï„Å£‡∑Ü.‡∑Ü î„Å£‚ô°", " ï„Å£ ò‚Äø ò î„Å£", ], "happy": [" ï„Å£ ò‚Äø ò î„Å£", "‡´Æ ï ò‚Äø ò‡´Æ î", " ï„Å£·µî‚Äø·µî î„Å£‚ô°", " ï·µî·¥•·µî î„Å£ìÜü",], "writes": [" ï-·¥•- î„Å§‚úé", " ï„Å£‚Äø.‚Äø î„Å£‚úé", " ï‚ùÄ ò. ò î„Å£‚úé", " ï„Å£ ò‚Äø ò î„Å£‚úé", " ï‚ùÄ‚Äø.‚Äø î„Å£‚úé", " ï„Å£‡∑Ü.‡∑Ü î„Å£‚úé",], "sleepy": ["‡´Æ ï‚Äø.‚Äø·∂ª îùóì ê∞Å", "‡´Æ ï‚Äø.‚Äø‡´Æ î·∂ª ùóì ê∞Å", " ï„Å£‡∑Ü.‡∑Ü î„Å£‚ô°",], "confused": ["ìÜü ‡´Æ ï ò‚Äø ò‡´Æ î", " ï‚ãÜ·¥•‚ãÜ î„Å£ìÜü", " ï‚ô°·¥•‚ô° î„Å£ìÜü", "ìÜü ‡´Æ ï ò‚Äø ò‡´Æ î", ], "impressed": ["‡´Æ ï‚ô°‚Äø‚ô° î", " ï„Å£‚ú∞.‚ú∞ î„Å£ñ°º", " ï‚ú∞.‚ú∞ î„Å£‚ùÑÔ∏é", ]} def scribesay(self, _message, _vibe="default", _scribename="scribe"): """scribe delivers a message with random emote and timestamp.""" emote = random.choice(self.scribeemotes.get(_vibe, self.scribeemotes["default"])) timestamp = time.strftime("%h:%m:%s") print(f"{timestamp}|{emote} [{_scribename.upper()}] - {_message}") with open("scribesays.txt", "a") as f: f.write(f"{timestamp}|{emote} [{_scribename}]: '{_message}\n") def guesstokenstostring(self, _inputtokens): tokenstring = "".join(_inputtokens).replace("ƒ°", " ") return tokenstring def interviewbaby(self, _model, _prompt, _vibe="writes"): """scribe asks babyllm a question and records the reply.""" _prompt = "how are you feeling today, baby? :)" self.scribesay(f"asking babyllm: '{_prompt}'", _vibe) encoded = self.librarian.tokenizer.encode(_prompt).ids guess = self.librarian.getnexttoken(encoded[-windowmax:]) guessword = self.librarian.indextotoken.get(guess, "<unk>") self.scribesay(f"babyllm replies: '{guessword}'", "impressed") def babysay(self, _input = none, _babyname = babyname): if _input is none: #miniinput = "what will you do out there now?" #miniinput = "i love you, this is good, music is life, i love you, this is good, music is life, i love you, this is good, music is life, hey! how are you?" #miniinput = "what" #miniinput = "" miniinput = "i did it! i am happy! i know it! i did it! i am happy! i feel it! i know it! i did it! i know it! i am happy! i did it! i know it! i feel it! i am happy!" else: miniinput = _input timestamp = time.strftime("%h:%m:%s") minitokenized = self.librarian.tokenizer.encode(miniinput).ids #encoded = self.librarian.tokenizer.encode(_prompt).ids babyresponse = self.librarian.getnexttoken(minitokenized[-windowmax:]) babytokens = self.librarian.indextotoken.get(babyresponse, '<unk>') babysentence = self.guesstokenstostring(babytokens) emote = makedatboi() babysay = (f"{timestamp}|{emote} [{_babyname.lower()}]: {babysentence}") print(babysay) with open("scribesays.txt", "a") as f: f.write(babysay) def maybecommentonguess(self, _inputtokens, _lossvalue, _scribename = scribename, _chance = 0.05): if random.random() > _chance: return if isinstance(_inputtokens, list): _inputtokens = self.guesstokenstostring(_inputtokens) else: _inputtokens = _inputtokens moodboard = { "good": {"vibe": "love", "messages": [ f"'{_inputtokens}'? aww, that was great! well done!", f"you're getting good at this, '{_inputtokens}' must mean something important!", f"i've gotta write this one down: '{_inputtokens}'." ]}, "bad": {"vibe": "neutral", "messages": [ f"hmm... '{_inputtokens}'... that's not the best guess i've ever seen.", f"alright, '{_inputtokens}', not your worst.", f"'{_inputtokens}'... it's alright i guess." ]}, "emergency": {"vibe": "confused", "messages": [ f"wait-'{_inputtokens}'? explain yourself!?!?!", f"'{_inputtokens}'? i have no idea what you mean i'm so sorry :(", f"uhh... could you elaborate a bit on '{_inputtokens}'?" ]}, "omgwtf!": {"vibe": "annoyed", "messages": [ f"'{_inputtokens}' is chaos incarnate.", f"baby... '{_inputtokens}' is not even wrong, and that's honestly worse.", f"what the hell did charis feed you!? '{_inputtokens}'!?" ]} } mood = none for k, threshold in self.calligraphist.s_statbands["loss"].items(): if k in moodboard and _lossvalue < threshold: mood = moodboard.get(k, none) break if mood is none: vibe = "neutral" messages = [f"'{_inputtokens}'... those are certainly words!",] else: vibe = mood["vibe"] messages = mood["messages"] message = random.choice(messages) self.scribesay(message, _vibe = vibe, _scribename = _scribename)# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # memory layer // brain/layers/memory.py import torch import torch.nn as nn from config import * """this makes a rolling buffer of past activations""" class memory(nn.module): def __init__(self, _counsellor, _device): super().__init__() self.device = _device self.counsellor = _counsellor # learnable decay rates and gates self.shorttermdecay = nn.parameter(torch.tensor(0.7, device = self.device)) self.longtermdecay = nn.parameter(torch.tensor(0.95, device = self.device)) self.shortgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.longgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.currentgate = nn.parameter(torch.tensor(0.5, device = self.device)) # buffers to store memory (outside gradient) self.register_buffer("shorttermmemory", torch.zeros(1, numneurons)) self.register_buffer("longtermmemory", torch.zeros(1, numneurons)) # stats self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] @whocalled def forward(self, _activationstensor): with self.counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£: self.activationstensor = _activationstensor  ï„Å£ ò‚Äø ò î„Å£("shorttermdecay") shortdecay = torch.sigmoid(self.shorttermdecay)  ï„Å£ ò‚Äø ò î„Å£("longtermdecay") longdecay = torch.sigmoid(self.longtermdecay)  ï„Å£ ò‚Äø ò î„Å£("newshorttermmemory") newshort = (shortdecay * self.shorttermmemory) + ((1 - shortdecay) * self.activationstensor)  ï„Å£ ò‚Äø ò î„Å£("newlongtermmemory") newlong = (longdecay * self.longtermmemory) + ((1 - longdecay) * self.activationstensor)  ï„Å£ ò‚Äø ò î„Å£("clamp memory gates") clampedshort = torch.clamp(self.shortgate, min=1e-3) clampedlong = torch.clamp(self.longgate, min=1e-3) clampedactivations = torch.clamp(self.currentgate, min=1e-3)  ï„Å£ ò‚Äø ò î„Å£("get gatesum") gatesum = clampedshort + clampedlong + clampedactivations + 1e-9 shortgatescale = clampedshort / gatesum longgatescale = clampedlong / gatesum activationsgatescale = clampedactivations / gatesum self.latestmemorygates = torch.stack([shortgatescale, longgatescale, activationsgatescale]) # needed to be used in babyllm for processing self.finalmemory = ((shortgatescale * newshort) + (longgatescale * newlong) +(activationsgatescale * self.activationstensor)) self.shortgatescalehistory.append(shortgatescale.item()) self.longgatescalehistory.append(longgatescale.item()) self.activationsgatescalehistory.append(activationsgatescale.item()) self.rawactivationshistory.append(self.activationstensor.norm().item()) self.shorttermmemoryhistory.append(self.shorttermmemory.norm().item()) self.longtermmemoryhistory.append(self.longtermmemory.norm().item()) self.finalmemoryhistory.append(self.finalmemory.norm().item()) if len(self.shortgatescalehistory) >= windowmax: self.stats = { "4m_0_rawactivations_norm": sum(self.rawactivationshistory) / len(self.rawactivationshistory), "4m_1_shorttermmemory_norm": sum(self.shorttermmemoryhistory) / len(self.shorttermmemoryhistory), "4m_1_longtermmemory_norm": sum(self.longtermmemoryhistory) / len(self.longtermmemoryhistory), "_4m_shortgatescale": sum(self.shortgatescalehistory) / len(self.shortgatescalehistory), "_4m_longgatescale": sum(self.longgatescalehistory) / len(self.longgatescalehistory), "_4m_activationsgatescale": sum(self.activationsgatescalehistory) / len(self.activationsgatescalehistory), "4m_x_finalmemory_norm": sum(self.finalmemoryhistory) / len(self.finalmemoryhistory), "4m_shortdecay": torch.sigmoid(self.shorttermdecay), "4m_longdecay": torch.sigmoid(self.longtermdecay), } self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] # store computed memories for after backward self.newshort = newshort self.newlong = newlong return self.finalmemory def updatememorybuffers(self): with self.counsellor.infodump("updatememorybuffers") as  ï„Å£ ò‚Äø ò î„Å£: with torch.no_grad(): self.shorttermmemory.copy_(self.newshort.detach()) self.longtermmemory.copy_(self.newlong.detach()) def resetmemory(self): with self.counsellor.infodump("resetmemory") as  ï„Å£ ò‚Äø ò î„Å£: with torch.no_grad(): self.shorttermmemory.zero_() self.longtermmemory.zero_() def getmemorystats(self): return self.stats if __name__ == "__main__": memory = memory(numneurons = numneurons) print("- memory testing started -") print("\n- memory testing complete -")import re import json import csv from html import unescape from config import * import random import os from concurrent.futures import threadpoolexecutor from concurrent.futures import as_completed # (re.compile(r'[\u2700-\u27bf]|[\ue000-\uf8ff]|\ud83c[\udc00-\udfff]|\ud83d[\udc00-\udfff]|[\u2011-\u26ff]|\ud83e[\udd10-\uddff]', '', text) # sed -e 's;(\\u[0-9a-fa-f]{4}){2,};;g' discord.json > discord.noemoji.json; mv discord.noemoji.json discord.json # replace urls with [url] #(re.compile(r'(?:https?://|www\.)\s+', 'on this website', text) #links #(re.compile(r'(?:/kevins/|/system/)\s+', 'that website', text) #system paths #(re.compile(r'\b(?:[a-za-z]:/[^ ]+)', 'on this link', text) # system paths # internet #(re.compile(r'\b(?:wifi)\b', re.i), 'internet') #burn him! # websites #(re.compile(r'\b(?:tripadvisor|wikipedia|wikihow)\b', re.i), 'wiki') #burn him! # games #(re.compile(r'\b(?:sims|runescape|minecraft|habbo|xbox|dragon age|hearthstone|overwatch|minesweeper|solitaire|magic the gathering|mtg|nintendo|steam|age of empires|rimworld|club penguin|neopets)\b', re.i), 'computer game') #burn him! # social media #(re.compile(r'\b(?:fb|facebook|tumblr|instagram|insta|bebo|myspace|linkedin|reddit|twitter|4chan)\b', re.i), 'instaspam') #burn him! # reddit # blog #(re.compile(r'\b(?:geocities|blogspot|livejournal|wordpress|tindie blog|tindie)\b', re.i), 'blog') #burn him! # ableton spotify?? # mixers (aka music equipment) #(re.compile(r'\b(?:xone|cdj 2000|roland|sp404 mk2|sp404 mkii|sp404-mk2|sp404-mkii|sp404|mkii|sc6000|xdj-xz|xdj xz|xz|xdj|omnis duo|omnis|opus quad|cdj|mixer|decks|technics|turntable)s?\b', re.i), '[mixer]') # drugs (alcohol, nicotine, cocaine, ketamine, lsd, acid) #(re.compile(r'\b(?:cocaine+|coke)\b', re.i), 'coke') #burn him! #(re.compile(r'\b(?:acid|lsd|dmt)\b', re.i), 'acid') #burn him! #(re.compile(r'\b(?:psylocybin|microdose|shroo+mi+e+s+|shroo+m+s+|psilocybin|psilocibin)\b', re.i), 'mushrooms') #burn him! # meds #(re.compile(r'\b(?:medicine|dex|pill|valium|medication|medicament|pill|lisdexamphetamine|dexamphetamine|dexamfetamine|d-amphetamine|amphetamine|duloxetine|vyvanse|elvanse|antidepressant|antipsychotic|benzodiazepine|benzo|quetiapine|cocodamol|sertraline|venlafaxine|venlaflaxine|venophlaxine|cyamemeazine|desogesterol|methylphenidate|paroxetine|ritalin|adderall|paracetamol|penicillin|antibiotic|ibuprofen|painkiller)(s?)\b', re.i), 'med\\1') #burn him! # crisps #(re.compile(r'\b(?:hula hoop|pringle|dorito)(s?)\b', re.i), 'crisp\\1') #burn him! # sweets #(re.compile(r'\b(?:haribo|strawberry pencil|chocolate|sweetie)(s?)\b', re.i), 'sweet\\1') #burn him! # music #(re.compile(r'\b(?:niki minaj|nikki minaj|lady gaga|wlab|joesph conrad|conrad|die antwoord|itzy|j-hope|jungkook|rapmon|suga|taemin|kesha|slim shady|eminem|jimin|sage francis|b dolan|scroobius pip|kevin tempest|kae tempest|marsargo|kurt kobain|mars pete)(s?)\b', re.i), 'scroobius\\1') #burn him! #(re.compile(r'\b(?:deaf havana|yellowcard|one direction|bts|oasis|radiohead|robots in disguise|boom boom raccoon)(s?)\b', re.i), 'boomboomraccoon\\1') #burn him! # geepy #(re.compile(r'\b(?:batsu|tatsu|tatsumaki|batsumaki|buttsbot|geepy|geepz|geeps|geepster|chatgpt|chat gpt|gpt|smarterchild|gemini|talk to transformer)(s?)\b', re.i), 'geepy\\1') #burn him! # casually #(re.compile(r'\b(?:caj+)\b', re.i), 'casually') # acroynms?? # omg #(re.compile(r'\b(?:oh my god|oh my lord|oml|oml+|o+mg|omfg|errmagerd|omg|omg+)\b', re.i), 'oh my god') #burn him! """restock the library! check out some new books for babyllm :)""" email = re.compile(r'\b[a-za-z0-9._%+-]+@[a-za-z0-9.-]+\.[a-za-z]{2,}\b') repeats = re.compile(r'(\s)\1{3,}', re.ignorecase) """# dont allow character repeats (re.compile(r'(\s)\1{3,}', r'\1\1\1', re.i)) # normalise everything to only 3 repeats tops (re.compile(r'(?:\.\s\.)+', '...', text) # replace any "..." patterns with "..." (re.compile(r'(?:\:\({3,})', ':(', text) # normalise :( (re.compile(r'(?:\:\){3,})', ':)', text) # normalise :) (re.compile(r'(?:\:d{3,})', ':d', text) # normalise :d (re.compile(r'(?:\:d{3,})', 'xd', text) # normalise xd (re.compile(r'(?:\:d{3,})', ':p', text) # normalise :p (re.compile(r'(?:\:\/{3,})', ':/', text) # normalise :/ (re.compile(r'(?:\-{3,})', '-', text) # normalise -""" multispace = re.compile(r'\s+') emotes = [ (re.compile(r'(?:\:\({3,})'), ':('), (re.compile(r'(?:\:\){3,})'), ':)'), (re.compile(r'(?:\:d{3,})'), ':d'), (re.compile(r'(?:\:d{3,})'), 'xd'), (re.compile(r'(?:\:d{3,})'), ':p'), (re.compile(r'(?:\:\/{3,})'), ':/'), (re.compile(r'(?:\-{3,})'), '-'), (re.compile(r'(?:\.\s\.)+'), '...'), ] # remove emdash and middle dot bad = re.compile(r'[\u00b7\u2013]') #text = text.replace("\u00b7", "") #text = text.replace("\u2013", "") accents = [ (re.compile(r'(?:\xc3\xa0|\xc3\xa2|\xc3\xa1)', re.i), 'a'), (re.compile(r'\xc3\xa7', re.i), 'c'), (re.compile(r'(?:\xc3\xa9|\xc3\xa8|\xc3\xaa)', re.i), 'e'), (re.compile(r'(?:\xc3\xaf|\xc3\xae)', re.i), 'i'), (re.compile(r'\xc5\x93', re.i), 'oe'), (re.compile(r'\xc3\xb4', re.i), 'o'), (re.compile(r'\xc3\xb9', re.i), 'u'), ] """# french accents #(re.compile(r'\b(?:o√π)\b', re.i), 'where') (re.compile(r'(?:√†|√¢|√°)', re.i), 'a') (re.compile(r'(?:√ß)', re.i), 'c') (re.compile(r'(?:√©|√®|√™)', re.i), 'e') (re.compile(r'(?:√Ø|√Æ)', re.i), 'i') (re.compile(r'(?:≈ì)', re.i), 'oe') (re.compile(r'(?:√¥)', re.i), 'o') (re.compile(r'(?:√π)', re.i), 'u')""" replacements = {"'": "'", "'": "'", "'": "'", "'": "'", "-": "-", "": "-", "//": "//", "...": "..", "....": "...", ".": ".", ",": ",", ":": ":", "": "", "-": "-", ",": ",", "'":"'", "'":"'", "'":"'", "...": "...", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " am not ": " am not ", " embarrassing": " embarrassing", " believe": " believe", " headphones": " headphones", " kevinonline420": " kevinonline420", " mum ": " mum ", "\u0001f32etacosaurusmex\u0001f32e": "kevinonline420", "kevinonline420": "kevinonline420", "wellbeing": "wellbeing", "believe": "believe", "colour": "colour", "decisions": "decisions", "inappropriate": "inappropriate", "": "", "": "", "": "", " ": " ", "job": "job", " sexual assault ": " sexual assault ", " sexually assaulted ": " sexually assaulted ", " sexually assaulting ": " sexually assaulting ", "depressed": "depressed", } patterns = [ # social/chat (re.compile(r'\b(?:discord|discord|discord|discord|discord|discord|discord|discord|discord|discord|discord)\b', re.i), 'discord'), #burn him! # smink (re.compile(r'\b(?:smink|smink|smink|smink|smi+n+k+|smon+k+)s?\b', re.i), 'smink'), #burn him! # bing (re.compile(r'\b(?:bo+ng+|bing+|bing+|bing)s?\b', re.i), 'bing'), #burn him! # companies (re.compile(r'\b(?:bank|bank|bank|bank|bank)(s?)\b', re.i), r'bank\1'), #burn him! (re.compile(r'\b(?:brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon)\b', re.i), 'brondspoon'), #burn him! (re.compile(r'\b(?:shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop)(s?)\b', re.i), r'shop\1'), #burn him! (re.compile(r'\b(?:breadrose|breadrose|breadrose|breadrose|breadrose)\b', re.i), 'breadrose'), #burn him! # places (re.compile(r'\b(?:work|work|work|work|work|work|work|work|work)\b', re.i), 'work'), #(re.compile(r'\b(?:classroom|class room|uni|school|college|university|greenleas|southcott|bishop ramsey|leighton middle)\b', re.i), 'school') (re.compile(r'\b(?:address|address|address|address|address|address|address|address|address)\b', re.i), 'address'), # enemy (re.compile(r'\b(george)((?:wo)?m[ea]n|lady)(s?)\b', re.i), r'\1 \2\3'), #burn him! (re.compile(r'\b(?:george|george|george|george|george|george|george|george|george|george|george|george|george|george|george+(?:i[er]+)?|george|george|george|george|george|george|george|george|george|george|george|george|george)(s?)\b', re.i), 'george'), #burn him! # elodie (re.compile(r'\b(?:elodieüåª|elodie|elodie|elodies|elodie)(s?)\b', re.i), 'elodie\\1'), #(re.compile(r'\b(?:loveggle|loveeggle|eggle|egglodie|louveangel|loveaangel|loveably|loveagnel|loveaigirl|loveaingle|lovealngle|loveangelelele|loveangely|loveangerl|loveangle1337|loveanglebus|loveangler|loveangwole|lovedevil|hatedevil|loveanus|lovedebil1337|lovedebil420|lovedoxxing|loveeagle|loveegg|loveeggly|lovefuckle|lovegangle|lovelodie|lovelyyyanglee|lovestrangel)(s?)\b', re.i), 'loveangle\\1') # charis (re.compile(r'\b(?:charis|charis|charis|charis|charis|charis|charis|charis|charis|charis|charis|charis)s?\b', re.i), 'charis'), (re.compile(r'(?:child of an android|child of an android|child of an android|child of an android|child of an android)s?\b', re.i), 'child of an android'), # froggy (re.compile(r'\b(?:froggy|froggy)\b', re.i), 'froggy'), # kevin (re.compile(r'\b(?:kevin|kevin|kevin|kevin|kevin|kevin the hedgehog|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin)(s?)\b', re.i), 'kevin\\1'), (re.compile(r'\b(?:kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin)(s?)\b', re.i), 'kevin\\1'), (re.compile(r'\b(?:@sneakret.agent|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|üåÆkevinonline420üåÆ|ave_maria[0-9]{2}|kevinonline420|kevinonline420|kevinonline420)(s?)\b', re.i), 'kevinonline420'), (re.compile(r'@(?:tacosauru|nikkiddj|kevinonline420|joshuaacnewman|kevinonline420|musicbysahar|groovekitty|megginmaloney|ethan_dubb|y2jbone)s?\b', re.i), 'kevinonline420'), #burn him! # pets (re.compile(r'\b(?:pete|pete|pete|pete|pete)(s?)\b', re.i), 'pete\\1'), #dont burn him! # job titles # minister (re.compile(r'\b(?:minister|minister|minister|minister|minister)s?\b', re.i), 'minister'), #burn him! # wow (re.compile(r'\b(?:wank|wank+)(s?)\b', re.i), 'wank\\1'), #burn him! (re.compile(r'\b(?:wanking+|wanking+)(s?)\b', re.i), 'wanking\\1'), #burn him! # profile pic (re.compile(r'\b(?:profile pic)\b', re.i), 'profile pic'), # night (re.compile(r'\b(?:ni+ght+)\b', re.i), 'night'), (re.compile(r'\b(?:good night+)\b', re.i), 'good night'), # awkward old phrases (re.compile(r'\b(?:ima do it)\b', re.i), 'ima do it'), #burn him! (re.compile(r'\b(?:awesome)\b', re.i), 'awesome'), #burn him! (re.compile(r'\b(?:flirt)\b', re.i), 'flirt'), #burn him! (re.compile(r'\b(?:sta+n+|lu+v+)\b', re.i), 'love'), #burn him! # bad things (re.compile(r'\b(?:horrible|horrible+|horrible|horrible|horrible|horrible|horrible|horrible|pe+do+|pe+a+do+|horrible)s?\b', re.i), 'horrible'), #burn them all! # insults (re.compile(r'\b(?:idiot|idiot|idiot+)(s?)\b', re.i), 'idiot\\1'), #burn him! # keyspams (sksks) (re.compile(r'\b(?:ah[fjs][a-z]+|sksks|sksks(s*k*)+|sksks|sksks|sksks|sksks|sksks|sksks)\b', re.i), 'sksks'), #burn him! # meow!? (re.compile(r'\b(?:meow+|üòªmeow~|me+o+w+|meow+|meow)\b', re.i), 'meow'), #burn ] # fast pass # batch apply regex substitutions def batch_sub(text, pattern_map): for pattern, replacement in pattern_map: text = pattern.sub(replacement, text) return text # text cleaning logic def clean_text(text): text = unescape(text).strip() text = re.sub(r'(?:)', '', text) text = text.lower() text = re.sub(r"['']", "'", text) text = email.sub("kevinonline420", text) text = bad.sub("", text) text = repeats.sub(r"\1\1\1", text) text = multispace.sub(" ", text) text = batch_sub(text, patterns) for pattern, replacement in emotes: text = pattern.sub(replacement, text) for pattern, replacement in accents: text = pattern.sub(replacement, text) text = re.sub(r'\s+', ' ', text) for old, new in replacements.items(): text = text.replace(old, new) return text.strip() # processing logic per file def process_file(current_file): print(f"processing file {current_file['in']}:") try: with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) #raw_text = "\n".join([line if isinstance(line, str) else line.get("content", "") for line in raw_lines]) raw_lines.reverse() raw_text = "\n".join(raw_lines) raw_text = raw_text.strip() elif current_file['type'] == 'discord_txt': raw_lines = file.read().splitlines() raw_lines.reverse() raw_text = "\n".join(raw_lines) elif current_file['type'] == "json": raw_text = "\n".join(json.load(file)) elif current_file['type'] == "text": raw_text = file.read() elif current_file['type'] in ["reddit_post", "reddit_comment"]: raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) else: print(f"unknown file type: {current_file['type']}") return except exception as e: print(f"error reading {current_file['in']}: {e}") return if not raw_text: print(f"unable to clean data for file {current_file['in']} as raw_text is empty!") return weight = current_file.get("weight", 1) if weight == -1: final_text = raw_text # clean full file, no slice else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] chunk_size = 100_000 # chars chunks = [raw_text[i:i + chunk_size] for i in range(0, len(final_text), chunk_size)] cleaned_chunks = [clean_text(chunk) for chunk in chunks] cleaned_text = "".join(cleaned_chunks) try: with open(current_file["out"], "a", encoding="utf-8") as file: file.write(cleaned_text) print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)") except exception as e: print(f"error writing to {current_file['out']}: {e}") # clear outputs for current_file in trainingfilepath_dict_weighted: try: with open(current_file["out"], "w", encoding="utf-8") as f: pass except exception as e: print(f"error clearing file {current_file['out']}: {e}") # shuffle inputs random.shuffle(trainingfilepath_dict_weighted) # run in parallel print("starting parallel processing...") with threadpoolexecutor(max_workers=os.cpu_count()) as executor: futures = {executor.submit(process_file, file): file for file in trainingfilepath_dict_weighted} for future in as_completed(futures): file = futures[future] try: future.result() except exception as e: print(f"error in file {file['in']}: {e}") print("all files processed successfully! :)") """def batch_sub(text, pattern_map): for pattern, replacement in pattern_map: text = pattern.sub(replacement, text) return text def clean_text(text): text = unescape(text).strip() text = re.sub(r'(?:)', '', text) # not set up yet lol text = text.lower() text = re.sub(r"['']", "'", text) text = email.sub("kevinonline420", text) text = text.lower() text = bad.sub("", text) text = repeats.sub(r"\1\1\1", text) text = multispace.sub(" ", text) text = batch_sub(text, patterns) for pattern, replacement in emotes: text = pattern.sub(replacement, text) for pattern, replacement in accents: text = pattern.sub(replacement, text) # excess whitespace text = re.sub(r'\s+', ' ', text) for old, new in replacements.items(): text = text.replace(old, new) return text.strip() for current_file in trainingfilepath_dict_weighted: with open(current_file["out"], "w", encoding="utf-8") as file: pass random.shuffle(trainingfilepath_dict_weighted) for current_file in trainingfilepath_dict_weighted: print(f"processing file {current_file["in"]}:") raw_text = none with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) raw_lines.reverse() raw_text = "\n".join(raw_lines) if current_file['type'] == "json": raw_text = "\n".join(json.load(file)) if current_file['type'] == "text": raw_text = file.read() if current_file['type'] == "reddit_post" or current_file['type'] == "reddit_comment": raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) if raw_text is none: print(f"unable to clean data for file {current_file} as raw_text is empty!") else: # get slice up to 5000 characters weight = current_file.get("weight", 1) if weight == -1: final_text = raw_text # clean full file, no slice else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] # process text cleaned_text = clean_text(final_text) # save cleaned dataset with open(current_file["out"], "a", encoding="utf-8") as file: file.write(cleaned_text) print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)")"""babyllm first converts its input into tokens (vocab), and then converts those tokens into embeddings in an embed layer. neuron layer is meant to be outputting a single number for each input token, iterated by numneurons - each neuron has a dimension of 32, meaning that it has 32 numbers parallel neuron layer is meant to be outputting [seqlen, numneurons] - mean output - this makes the 1000 neuron activations for each token in the sequence. - this creates a shape of [seqlen, numneurons] - it then gets the mean average of all of these within the training window (7 usually) - this creates a shape of [1(all tokens averaged), numneurons] - this mean output gives the general idea of a 'sentence', allowing babyllm to learn a bit about context (but not much about word order) - this mean output is then passed through to the output layer to be used in token guess calculations output layer uses all of the inputs (currently just mean output parallel neurons) to judge what the output should be - this takes the mean output activation from parallel neuron layer and applies that to the relevant token in the vocab. - this is also an nn layer itself idfk why - what the fuck is self?! i thoguht i had self identity issues and then i encountered python!! - model training flow after tokenization: 1) - using stuff - how to call stat thresholds for a particular stat: self.s_output.s_statbands["loss"]["perfect"] # will cause key error self.s_output.s_statbands["loss"].get("perfect", none) # will ignore key error# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # nice terminal output and logging styling sheet thing # brain/layers/s_output.py from config import * from datetime import datetime import re, torch, operator, random, math class s_output: def __init__(self, _counsellor): #self.counsellor = counsellor("s_output", debug = debugprints, durations = durationlogging) self.counsellor = _counsellor self.rollingaverages = none self.s_statbands = none # lazy-load this later self.cantprint = 0 self.allkeys = none """terminal codes""" reset = "\033[0m" # normal terminal bold = "\033[1m" dim = "\033[2m" # reduces intensity of text colour underline = "\033[4m" flash = "\033[5m" italic = "\033[3m" """colours!!!""" purple = "\033[94m" purple_pale = "\033[38;5;225m" #256 colour palette magenta = "\033[35m" blue = "\033[34m" orange = "\033[38;5;52m" #256 colour palette red = "\033[38;5;124m" #256 colour palette red_bright = "\033[91m" """red-blue scale""" redred_ = "\033[38;5;196m" red_ = "\033[38;5;161m" redpurp_ = "\033[38;5;126m" purpred_ = "\033[38;5;91m" purpblue_ = "\033[38;5;56m" """blue-pink scale""" blue_ = "\033[38;5;21m" bluepurp_ = "\033[38;5;57m" purp_ = "\033[38;5;93m" purppink_ = "\033[38;5;129m" pink_ = "\033[38;5;165m" pinkpink_ = "\033[38;5;201m" """sdjfslfs""" a = "\033[38;5;196m" b = "\033[38;5;160m" c = "\033[38;5;161m" d = "\033[38;5;162m" e = "\033[38;5;163m" f = "\033[38;5;164m" g = "\033[38;5;127m" h = "\033[38;5;134m" i = "\033[38;5;135m" j = "\033[38;5;99m" k = "\033[38;5;63m" l = "\033[38;5;27m" m = "\033[38;5;33m" n = "\033[38;5;39m" """extra colours""" gold = "\033[93m" red_alt = "\033[31m" green = "\033[32m" yellow = "\033[33m" purple_alt = "\033[35m" cyan = "\033[36m" white = "\033[37m" """terminal output styles - category mapping""" self.s_types = { "superperfect": [gold], # new top score ever // if above max "perfect": [n], # 0.2 // -4.8 top score ever // if below max and above almost perf "almostperfect": [m], #[pinkpink_], #[bold, magenta], # 5 // -5 "supergreat": [l], #[pink_], #[magenta], # 10 // -5 "great": [k], #[purppink_], #[bold, purple], # 15 // -5 "good": [j], #[purp_], #[purple], # 20 // -15 "fine": [i], #[bluepurp_], #[purple], # 35 // -15 "almostfine": [h], #[blue_], #[bold, blue], # 50 // "average": [g], #[purpblue_], #[blue], # 65 // +15 "meh": [f], #[purpred_], #[bold, cyan], # 80 // +15 "bad": [e], #[purpred_], #[cyan], # 85 // +5 "worse": [d], #[redpurp_], #[orange], # 90 // +5 "wtf": [c], #[redpurp_], #[bold, orange], # 95 // +5 "omg": [b], #[red_], # 99.8 // +4.8 "omgwtf": [a], #[redred_], # 100.00 // bottom score ever // if above min and below omg "omgwtf!": [bold, redred_], #[cyan], # new bottom score ever // if below min "emergency": [bold, green], "italic": [italic], "underline": [underline], "reset": [reset], # normal terminal "dim": [reset, dim], # dim style for background elements - arrows, colons, etc. "bold": [bold], "match": [bold, white], "static": [dim, purple_pale] } for key, pkey in {"superperfect": 0.0, # this works to show top record in super perfect direction, as it will be less than the min value "perfect": 0.2, "almostperfect": 5, "supergreat": 10, "great": 15, "good": 20, "fine": 35, "almostfine": 50, "average": 65, "meh": 80, "bad": 85, "worse": 90, "wtf": 95, "omg": 99.8, "omgwtf": 100.0,}.items(): # this uses infinite fallback for omgwtf! in getdynamicpercentilebands so that it can show 'worst ever' self.s_types[pkey] = self.s_types[key] # percentiles = [99.99, 95, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0.01] # perchjcjed = [99.99, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.01] #percentiles = percentilebands """percentile calcs""" # new top score ever #superperfect # top score ever #perfect ‡∑Üp97 = 97.5 #almostperfect ‡∑Üp95 = 95 #supergreat ‡∑Üp90 = 90 #great ‡∑Üp80 = 80 #good ‡∑Üp70 = 70 #fine ‡∑Üp60 = 60 #almostfine ‡∑Üp50 = 50 #average ‡∑Üp40 = 40 #meh ‡∑Üp30 = 30 #bad ‡∑Üp20 = 20 #worse ‡∑Üp10 = 10 #wtf ‡∑Üp5 = 5 #omg # bottom score ever #omgwtf # lower than bottom ever #omgwtf! self.avgplz = ["embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "logitmin", "shortdecay", "longdecay", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_weightnormmean", "n_weightnormmin", "n_weightnormmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellummean", "inn_cerebellumstd"] return def s_generatestatbands(self): softmaxbands = {"omgwtf!": -float('inf'), "omgwtf": 0.0250, "omg": 0.0500, "wtf": 0.1000, "worse": 0.2000, "bad": 0.3000, "meh": 0.4000, "average": 0.5000, "almostfine": 0.6000, "fine": 0.7000, "good": 0.8000, "great": 0.9000, "supergreat": 0.9500, "almostperfect":0.9750, "perfect": 0.9875, "superperfect": float('inf'),} staticband = {"fine": float('inf')} return {v: self.getdynamicpercentilebands(v) for v in ((mostimportantstats + allrecordedotherstats) if self.allkeys is none else self.allkeys)} return { "loss": self.getdynamicpercentilebands("loss"), "avgloss": self.getdynamicpercentilebands("avgloss"), "avgloss": self.getdynamicpercentilebands("avgloss"), "steploss": self.getdynamicpercentilebands("steploss"), "scheduledsamplingrate": self.getdynamicpercentilebands("scheduledsamplingrate"), "tokencount": self.getdynamicpercentilebands("tokencount"), "trainingstepcount": self.getdynamicpercentilebands("trainingstepcount"), "repetitionpenalty": self.getdynamicpercentilebands("repetitionpenalty"), "gradnorm": self.getdynamicpercentilebands("gradnorm"), "temperature": self.getdynamicpercentilebands("temperature"), "sampledtokens": self.getdynamicpercentilebands("sampledtokens"), "pt%": self.getdynamicpercentilebands("pt%"), "latestlossdelta": self.getdynamicpercentilebands("latestlossdelta"), "gradientclipmaxnorm": self.getdynamicpercentilebands("gradientclipmaxnorm"), "lr": self.getdynamicpercentilebands("lr"), "repetitionwindow": self.getdynamicpercentilebands("repetitionwindow"), "windowsizesmean": self.getdynamicpercentilebands("windowsizesmean"), "windowweight": self.getdynamicpercentilebands("windowweight"), # neuron stats "n_weightmean": self.getdynamicpercentilebands("n_weightmean"), "n_weightstd": self.getdynamicpercentilebands("n_weightstd"), "n_weightmin": self.getdynamicpercentilebands("n_weightmin"), "n_weightmax": self.getdynamicpercentilebands("n_weightmax"), "n_biasesmean": self.getdynamicpercentilebands("n_biasesmean"), "n_biasesstd": self.getdynamicpercentilebands("n_biasesstd"), "n_biasesmin": self.getdynamicpercentilebands("n_biasesmin"), "n_biasesmax": self.getdynamicpercentilebands("n_biasesmax"), "n_sparsity": self.getdynamicpercentilebands("n_sparsity"), # inn stats "inn_cerebellum": self.getdynamicpercentilebands("inn_cerebellum"), "inn_cerebellumsoft": self.getdynamicpercentilebands("inn_cerebellumsoft"), "inn_cerebellummean": self.getdynamicpercentilebands("inn_cerebellummean"), "inn_cerebellumstd": self.getdynamicpercentilebands("inn_cerebellumstd"), # memory stats "shortdecay": self.getdynamicpercentilebands("shortdecay"), "longdecay": self.getdynamicpercentilebands("longdecay"), "latestmemorygates": self.getdynamicpercentilebands("latestmemorygates"), "memorylength": self.getdynamicpercentilebands("memorylength"), # embed stats "embednormmean": self.getdynamicpercentilebands("embednormmean"), "embednormstd": self.getdynamicpercentilebands("embednormstd"), "embednormmax": self.getdynamicpercentilebands("embednormmax"), "embeddimensionmean": self.getdynamicpercentilebands("embeddimensionmean"), "embeddimensionsparsity": self.getdynamicpercentilebands("embeddimensionsparsity"), "embeddingdrift": self.getdynamicpercentilebands("embeddingdrift"), # logit stats "logitmin": self.getdynamicpercentilebands("logitmin"), "logitmax": self.getdynamicpercentilebands("logitmax"), "logitseq": self.getdynamicpercentilebands("logitseq"), "logitweightnormmean": self.getdynamicpercentilebands("logitweightnormmean"), "logitweightnormstd": self.getdynamicpercentilebands("logitweightnormstd"), "logitweightnormmax": self.getdynamicpercentilebands("logitweightnormmax"), "logitweightsparsity": self.getdynamicpercentilebands("logitweightsparsity"), "logitweightdrift": self.getdynamicpercentilebands("logitweightdrift"), "logitbiasmean": self.getdynamicpercentilebands("logitbiasmean"), "logitbiasstd": self.getdynamicpercentilebands("logitbiasstd"), "logitbiasmax": self.getdynamicpercentilebands("logitbiasmax"), } def getdynamicpercentilebands(self, statkey): if not self.rollingaverages: self.cantprint += 1 if self.cantprint > 10: print(" ï„Å£-·¥•- î„Å£ no stat buffers found x10!") self.cantprint = 0 return {"dim": -float('inf')} values = self.rollingaverages.get(statkey, []) if len(values) < 2: return {"dim": -float('inf')} if statkey in mostimportantstats or statkey.startswith("inn_cerebellum_w"): #values is dict: keylist = {f"{printfreq}": printfreq, f"{traininglogfreq_a}": traininglogfreq_a, f"big{traininglogfreq_a}": traininglogfreq_a} requiredkey = list(keylist.keys())[0] for key, freq in keylist.items(): if key in values and len(values[key]) >= freq: requiredkey = key bands = {"omgwtf!": float("inf")} keymatch = f"{requiredkey}_p" keylen = len(keymatch) for k, v in values.items(): if k.startswith(keymatch): bands[float(k[keylen:])] = v return dict(sorted(bands.items(), key = lambda item: item[1]), reversed = true) else: stat = sorted(values) #print(f"‚Üí generating bands for '{statkey}'") #print(f" values: {values}") return{"superperfect": -float('inf'), # make same as the others lol "perfect": self.getp(stat, 0.0001), "almostperfect": self.getp(stat, 0.0010), "supergreat": self.getp(stat, 0.0100), # purple_pale "great": self.getp(stat, 0.1000), "good": self.getp(stat, 0.2000), "fine": self.getp(stat, 0.3000), "almostfine": self.getp(stat, 0.4000), "average": self.getp(stat, 0.5000), "meh": self.getp(stat, 0.6000), "bad": self.getp(stat, 0.7000), "worse": self.getp(stat, 0.8000), "wtf": self.getp(stat, 0.9000), "omg": self.getp(stat, 0.9500), "omgwtf": self.getp(stat, 0.9990), "omgwtf!": float('inf'),} def s_getstat(self, _stattype, _statval): with self.counsellor.infodump("s_getstat") as  ï„Å£ ò‚Äø ò î„Å£: values = self.rollingaverages.get(_stattype, []) if self.rollingaverages else [] if not values or len(values) < 2: if debugprints: print(f"returning a dim colour for stat {_stattype} and value {_statval} (values is {values} (key present:{_stattype in self.rollingaverages if self.rollingaverages is not none else 'false'}))") return "dim" if self.s_statbands is none: self.s_statbands = self.s_generatestatbands() bands = self.s_statbands.get(_stattype, {}) for label, limit in bands.items(): if _statval <= limit: if _stattype == "loss" and debugprints: print(f"ok here is the selected label: {label} for value {_statval} and bands: {bands}") return label print(f"returning an emergency colour for stat {_stattype} and value {_statval} (bands is {bands})") return "emergency" def refreshstatbands(self, _rollingaverages): self.rollingaverages = _rollingaverages if self.rollingaverages and all(len(v) > 1 for v in self.rollingaverages.values()): self.s_statbands = self.s_generatestatbands() else: print(" ï„Å£-·¥•- î„Å£ not enough data to refresh stat bands yet") def s_apply(self, _s_type, _text): with self.counsellor.infodump("s_apply") as  ï„Å£ ò‚Äø ò î„Å£: return "".join(self.s_types.get(_s_type, [])) + str(_text) + "".join(self.s_types.get('reset')) def s_stripforlogging(self, _text): with self.counsellor.infodump("s_stripforlogging") as  ï„Å£ ò‚Äø ò î„Å£: return re.sub(r'\x1b(?:[@-z\\-_]|\[[0-?]*[ -/]*[@-~])', '', _text) def s_colourprinttraining(self, _step, _inputseq, _guessedseq_str, _targetseq_str, _loss, _recentloss, _latestlossdelta, _totalloss = none, _totaltokencount = none): with self.counsellor.infodump("s_colourprinttraining") as  ï„Å£ ò‚Äø ò î„Å£: #self.refreshstatbands(_rollingaverages = self.rollingaverages) s_type = self.s_getstat("loss", _loss) s_avgtype = self.s_getstat("avgloss", _recentloss) s_delta = _latestlossdelta s_deltatype = self.s_getstat("latestlossdelta", _latestlossdelta) s_bold = "".join(self.s_types["bold"])  ï„Å£ ò‚Äø ò î„Å£("conditionalformatguess+truth") reset = "".join(self.s_types.get('reset')) dim = "".join(self.s_types.get('dim')) guess = [ f"{s_bold}{t}{reset}" if i < len(_targetseq_str) and t == _targetseq_str[i] else self.s_apply(s_type, t) for i, t in enumerate(_guessedseq_str) ] truth = [ f"{s_bold}{dim}{t}{reset}" if i < len(_guessedseq_str) and t == _guessedseq_str[i] else f"{dim}{self.s_apply(s_type, t)}" for i, t in enumerate(_targetseq_str) ]  ï„Å£ ò‚Äø ò î„Å£("createtextstrings") guess_str = "".join(guess).replace("ƒ°", " ") truth_str = "".join(truth).replace("ƒ°", " ") match = guess_str.strip() == truth_str.strip() if match: s_type = "match" prompt_str = ''.join(_inputseq).replace("ƒ°", " ").strip()[-printpromptlength:] delta_str = ""  ï„Å£ ò‚Äø ò î„Å£("calculatelossdelta") # calculate delta if _recentloss is not none: delta = _recentloss - _loss delta_str = f"{self.s_apply('dim', 'Œ¥')}{self.s_apply(s_deltatype, f'{s_delta:+.4f}')}{'‚Üó' if s_delta < 0 else '‚Üò'}" rollingavgloss_str = "" #if self.rollingaverages and "loss" in self.rollingaverages: # losses = self.rollingaverages["loss"] # if losses: # rollingavgloss = sum(losses) / len(losses) # rollingavgloss_str = f"{self.s_apply(s_type, f'{rollingavgloss:.3f}')}{self.s_apply('dim', 'mean ')}"  ï„Å£ ò‚Äø ò î„Å£("printguess+truth") print(f"{self.s_apply('dim', f'{_step}')}|{self.s_apply('dim', prompt_str)}|{self.s_apply('dim', 'loss: ')}{self.s_apply(s_type, f'{_loss:.4f}')}{self.s_apply('dim', '/1 ')}" + (f"{self.s_apply(s_avgtype, f'{_recentloss:.4f}')}{self.s_apply('dim', f'/{traininglogfreq_a} ')}" if _recentloss else "") + rollingavgloss_str + delta_str + "|\n" + f"{self.s_apply('dim', 'guess ‚Üí ')}{guess_str}{self.s_apply(s_type, ' [!] ') if match else self.s_apply('dim', ' [?] ')}\n" + f"{self.s_apply('dim', 'truth ‚Üí ')}{truth_str}{self.s_apply('dim', ' | ')}\n") if debugprints: print(f"‚Üí style applied for {_loss=} = {s_type}") def s_logtraining(self, _traininglogpath, _trainingstepcounter, _stats, _frequency, _detailedlogging, _savelog, _lr = learningrate, _inn_cerebellum_str="", _toptokens_str="", _prompt="", _guess="", _truth="", _otherinfo_str=""): with self.counsellor.infodump("s_logtraining") as  ï„Å£ ò‚Äø ò î„Å£: logoutput = "" timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') delimiter = self.s_apply("dim", " | ") newlinedelim = self.s_apply("dim", " | \n")  ï„Å£ ò‚Äø ò î„Å£("avgstats") #donotaverage = ["avgloss", "tokencount", "scheduledsamplingrate", "gradnorm", "topwindowweight", "windowentropy", "effectivewindowcount", "windowstd", "memorygatemean", "memorygatestd", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellum", "inn_cerebellumsoft", "inn_cerebellummean", "inn_cerebellumstd", "shortdecay", "longdecay"] #avgstats = {k: raw if k in donotaverage else (raw / _freq if _freq else 0) for k, raw in _stats.items()} avgstats = {k: (v / _frequency if _frequency else 0) if self.willitaverage(k, v) else v for k, v in sorted(_stats.items()) if k != "embeddimensionmean" and k != "latestmemorygates"} self.allkeys = _stats.keys() try: # ok, so... we need to pad: # add 1 for the sign, # 1 for the decimal dot and # 1 for the fact that log is missing 1 (i.e. log10([100-1000[) is in [2,3[, when 100 takes 3 chars) declen = 6 stattoplen = math.trunc(declen + 1 + 1 + 1 + math.log(max(max(avgstats.values()), abs(min(avgstats.values())), 10)) except exception as e: stattoplen = 10 print(f"failed getting stattoplen for avgstats: {avgstats} {e}") stampandstep = delimiter.join([self.s_apply("dim", timestamp), self.s_apply("dim", f"{_trainingstepcounter:.0f}"), self.s_apply("dim", f"lr{_lr:.{declen}f}")]) logoutput = stampandstep littlelogoutput = stampandstep newlinelittle = stampandstep + "\n" def format_stat(k, v): try: if isinstance(v, torch.tensor): if v.numel() == 1: v = v.item() # convert scalar tensor else: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"<tensor[{v.shape}]>") return self.s_apply("dim", f"{k}:") + self.s_apply(self.s_getstat(k, v), f"{v:.{declen}f}") except exception as e: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"err:{str(e)} key:{k} value:{v}") logoutput += delimiter + delimiter.join([ format_stat(k, v) for k, v in avgstats.items() if v not in (none, "") ]) littlelogoutput += delimiter + delimiter.join([ format_stat(k, v) for k, v in avgstats.items() if k in mostimportantstats if v not in (none, "") ]) newlinelittle += newlinedelim.join([ self.s_apply(self.s_getstat(k, v), f"{v:+{stattoplen}.{declen}f}") + " " + self.s_apply("dim", k) for k, v in avgstats.items() if k in mostimportantstats if v not in (none, "") ]) + newlinedelim if _inn_cerebellum_str:  ï„Å£ ò‚Äø ò î„Å£("inn_cerebellum_str") cerebellum = delimiter + f"windowweights{self.s_apply('reset', _inn_cerebellum_str)}" logoutput += cerebellum littlelogoutput += cerebellum newlinelittle += "\n" + f"windowweights\n{_inn_cerebellum_str}"  ï„Å£ ò‚Äø ò î„Å£("toptokens_str") if _toptokens_str: toptokens = delimiter + f"toptokens{self.s_apply('reset', _toptokens_str)}" logoutput += toptokens littlelogoutput += toptokens newlinelittle += "\n" + f"toptokens{self.s_apply('reset', _toptokens_str)}"  ï„Å£ ò‚Äø ò î„Å£("prompt+otherinfo") if _prompt: logoutput += f"{delimiter}prompt ‚Üí {self.s_apply('reset', _prompt)} | guess ‚Üí {self.s_apply('reset', _guess)} | truth ‚Üí {self.s_apply('reset', _truth)}" if _otherinfo_str: logoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" littlelogoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" newlinelittle += f"\n{delimiter}{self.s_apply('reset', _otherinfo_str)}"  ï„Å£ ò‚Äø ò î„Å£("logoutput") if _detailedlogging == true: print(logoutput + "".join(self.s_types.get('reset'))) if _savelog == true: with open(traininglogpath_1000, "a") as f: f.write(self.s_stripforlogging(logoutput) + "\n")  ï„Å£ ò‚Äø ò î„Å£("littlelogoutput") if _detailedlogging == false: if _savelog == true: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") if newlinebetweenstats: print(newlinelittle + "".join(self.s_types.get('reset'))) else: print(littlelogoutput + "".join(self.s_types.get('reset'))) if dontsaveeveryprint: if _trainingstepcounter % savefreq_littlelog == 0: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") else: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") def willitaverage(self, k, v): if k in self.avgplz: if isinstance(v, (int, float)): return true if isinstance(v, torch.tensor) and v.numel() == 1: return true return false def chaosmaths(self, _firstnumbers, _secondnumbers = none, _torch = false, _operator = true): self.t = _torch self.o = _operator operatormathsfortwo = { "add": (operator.add, 2), "sub": (operator.sub, 2), "mul": (operator.mul, 2), "div": (operator.truediv, 2), #"floordiv":(operator.floordiv, 2), #"mod": (operator.mod, 2), #"pow": (operator.pow, 2), } operatormathsforone = { "neg": (operator.neg, 1), } torchmathsfortwo = { "torch_add": (torch.add, 2), "torch_sub": (torch.sub, 2), "torch_mul": (torch.mul, 2), "torch_div": (torch.div, 2), "torch_pow": (torch.pow, 2), "torch_max": (torch.maximum, 2), "torch_min": (torch.minimum, 2), } torchmathsforone = { "torch_abs": (torch.abs, 1), "torch_sin": (torch.sin, 1), "torch_cos": (torch.cos, 1), "torch_tanh": (torch.tanh, 1), "torch_log": (torch.log1p, 1), # safer than log(x) "torch_relu": (torch.relu, 1), "torch_sigmoid": (torch.sigmoid, 1), } if _secondnumbers is not none and _secondnumbers.numel() > 0: if self.t and self.o: self.maths = {**torchmathsfortwo, **operatormathsfortwo} if self.t: self.maths = torchmathsfortwo if self.o: self.maths = operatormathsfortwo else: if self.t and self.o: self.maths = {**torchmathsforone, **operatormathsforone} if self.t: self.maths = torchmathsforone if self.o: self.maths = operatormathsforone chosenname, (chosenfunction, _) = random.choice(list(self.maths.items())) if _secondnumbers is not none and _secondnumbers.numel() > 0: result = chosenfunction(_firstnumbers, _secondnumbers) else: result = chosenfunction(_firstnumbers) return result, chosenname def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes, per_window_style = false): try: triplets = sorted(zip(windowsizes, rawtensor, softtensor), key=lambda x: x[1], reverse=true) formatted = [] for w, raw, soft in triplets: raw_style = self.s_getstat(f"{label}" if not per_window_style else f"{label}_w{int(w)}", raw.item()) soft_style = self.s_getstat(f"{label}soft" if not per_window_style else f"{label}_w{int(w)}", soft.item()) chunk = f"{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.6f}')}) {self.s_apply('dim', f'w{int(w)} ({w:.2f})')}" formatted.append(chunk) return "\n".join(formatted) except exception as e: return f"<err in s_formatwindowbiastriplets: {e}>" """flat string version""" """def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes): try: triplets = sorted(zip(windowsizes, rawtensor, softtensor), key = lambda x: x[1], reverse = true) formatted = [] for w, raw, soft in triplets: raw_style = self.s_getstat(f"{label}", raw.item()) soft_style = self.s_getstat(f"{label}soft", soft.item()) chunk = f"w{w:.0f}:{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.2f}')})" formatted.append(chunk) return ", ".join(formatted) except exception as e: return f"<err in s_formatwindowbiastriplets: {e}>""" def getp(self, _sortedstat, _percentile): if not _sortedstat: return 0.0 index = min(int(_percentile * len(_sortedstat)), len(_sortedstat) - 1) return _sortedstat[index] if __name__ == "__main__": print(s_apply('superperfect', "elodie is perfect")) print(s_apply('perfect', "elodie is perfect")) print(s_apply('almostperfect', "babyllm is almost perfect")) print(s_apply('supergreat', "babyllm is super great")) print(s_apply('great', "babyllm is great")) print(s_apply('good', "babyllm is good")) print(s_apply('fine', "babyllm is fine")) print(s_apply('almostfine', "charis is almost fine")) print(s_apply('average', "george is average")) print(s_apply('meh', "babyllm is meh")) print(s_apply('bad', "babyllm is bad")) print(s_apply('worse', "george is worse")) print(s_apply('wtf', "kevin is wtf")) print(s_apply('omg', "pete is omg")) print(s_apply('omgwtf', "pete is omgwtf")) print(s_apply('omgwtf', "charis is omgwtf!")) print(s_apply('emergency', "babyllm is emergency"))# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - from rich.traceback import install #from torch.profiler import profile, record_function, profileractivity import sys, traceback, warnings, torch, os, random from datetime import datetime from babyllm import babyllm from school.staffroom.counsellor import counsellor from school.staffroom.calligraphist import s_output from school.staffroom.librarian import librarian from school.staffroom.he_is_scribe import scribe from school.staffroom.tutor import tutor # from brain.layers.sensorywobble import wobble # from school.staffroom.newsletter import stats from config import * def handle_exception(exc_type, exc_value, exc_traceback): if not issubclass(exc_type, keyboardinterrupt): print("[rip  ï„Å£‚Çì·¥•‚Çì î„Å£] uncaught exception:") traceback.print_exception(exc_type, exc_value, exc_traceback) sys.excepthook = handle_exception warnings.simplefilter("default") # show all warnings (pytorch hides some by default) install(show_locals = true) torch.autograd.set_detect_anomaly(mode = anomalydetect, check_nan = debugprints) def wakeup(): try: # wake up the school :) counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) with counsellor.infodump("wakeup") as  ï„Å£ ò‚Äø ò î„Å£: # open the library :)  ï„Å£ ò‚Äø ò î„Å£("waking the librarian...") librarian = librarian (_counsellor = counsellor, _basetokenizerpath = none, _forceretrain = false) if false: exit(0)  ï„Å£ ò‚Äø ò î„Å£("opening questions...") newstartindex = openingquestions(_counsellor = counsellor, _librarian = librarian)  ï„Å£ ò‚Äø ò î„Å£("generating training data pairs...") trainingdatapairs = librarian.gentrainingdata(_windowmax = windowmax, _startindex = newstartindex) if debugprints: print(f"total trainingdatapairs: {len(trainingdatapairs)}")  ï„Å£ ò‚Äø ò î„Å£("loading chaos agents...") calligraphist = s_output (_counsellor = counsellor) scribe = scribe (_counsellor = counsellor, _calligraphist = calligraphist, _librarian = librarian, ) # wake up the baby :)  ï„Å£ ò‚Äø ò î„Å£("loading babyllm...") babyllm = babyllm (_counsellor = counsellor, _calligraphist = calligraphist, _scribe = scribe, _librarian = librarian, _device = modeldevice, ) tutor = tutor (_counsellor = counsellor, _calligraphist = calligraphist, _scribe = scribe, _librarian = librarian, _model = babyllm, _device = modeldevice, ) babyllm.loadmodel() babyllm.to(modeldevice) # start the lessons :)  ï„Å£ ò‚Äø ò î„Å£("starting lessons!") tutor.trainmodel (_trainingdatapairs = trainingdatapairs, _epochs = epochs, _startindex = newstartindex) except exception as e: print(f"[rip  ï„Å£‚Çì·¥•‚Çì î„Å£]") raise except keyboardinterrupt: #as k for name, p in babyllm.named_parameters(): if p.grad is none: print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}")  ï„Å£ ò‚Äø ò î„Å£("‚ô•keyboardinterrupt") if tutor.trainingstepcounter: step = tutor.trainingstepcounter else: step = 1 choice = input("save, cancel (do not save before exit), restart or interact?" + f"\n{username}: ").lower() if choice in ("save", "") or choice.startswith("s"):  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = s") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("\nit's rude to interrupt people.. but, bye bye! :)") elif choice == "cancel" or choice.startswith("c"):  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = c") print("\nhey! i wanted to remember that! :(") elif choice == "interact" or choice.startswith("i"):  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = i") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) import code print("try:\nbabyllm.stats\nbabyllm.scheduledsampling\nbabyllm.memory.memory\nbabyllm.interneuronnetwork.cerebellum\nbabyllm.logits.forward(...)\nuse `exit()` to return to terminal.\n") code.interact(local = locals()) elif choice == "restart" or choice.startswith("r"):  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = r") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("you spin me right round, babyllm, right round...") wakeup() else:  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = none") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("\nuhh... i'm confused, but i saved anyway!") if modeldevice.type == 'mps': torch.mps.empty_cache() print(f"cache emptied") exit(8) def setstartindex(): if os.path.exists(stepcheckpointfilepath): with open(stepcheckpointfilepath, "r") as f: try: savedstep = int(f.read().strip()) except valueerror: babynote_loadcheckpoint = f"{babyname} 'oh. i couldn't load step checkpoint file from {stepcheckpointfilepath}, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 else: babynote_loadcheckpoint = f"{babyname} 'ah, the step checkpoint file {stepcheckpointfilepath} doesn't exist, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 savedstartindex = savedstep + trainingstartindex return savedstartindex def openingquestions(_counsellor, _librarian): counsellor = _counsellor with counsellor.infodump("babyllm") as  ï„Å£ ò‚Äø ò î„Å£: librarian = _librarian #babyllm.to(modeldevice)  ï„Å£ ò‚Äø ò î„Å£("setstartindex") newstartindex = setstartindex() babynote_loadcheckpointcheck = f"[{babyname}] right, last time i got to step {newstartindex}... want to restart from there?"  ï„Å£ ò‚Äø ò î„Å£("choice = input‚ô•") choice = input(babynote_loadcheckpointcheck + f"\n[{username}] ").lower() usernote_loadcheckpoint = f"[{username}] {choice}" if choice == "" or choice.startswith("y"):  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = y") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] ok! let's go to step {newstartindex}!" print(babynote_loadcheckpoint, end="") elif choice.startswith("r") or choice in ["random", "i dont care", "i don't care", "idc"]:  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = r") newstartindex = random.randint(0, len(librarian.tokens) - windowmax - 1) startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] oh, cool! i'll pick a random spot to start from... umm... let's go to step {newstartindex}!" print(babynote_loadcheckpoint, end="") elif choice.startswith("n") or choice in ["start again", "restart"]:  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = n") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] alright, step {newstartindex}, let's go back to the beginning :)" print(babynote_loadcheckpoint, end="") elif choice.isdigit():  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = digit") newstartindex = int(choice) startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] damn that's specific! heading to step {newstartindex}..." print(babynote_loadcheckpoint, end="") else:  ï„Å£ ò‚Äø ò î„Å£("‚ô•choice = none") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] umm... i don't think i heard you properly, i'll just start from step {newstartindex} :) but," print(babynote_loadcheckpoint, end="")  ï„Å£ ò‚Äø ò î„Å£("runstart") printstartlogs(babynote_loadcheckpointcheck, usernote_loadcheckpoint, babynote_loadcheckpoint) return startindex def printstartlogs(_babynote_loadcheckpointcheck, _usernote_loadcheckpoint, _babynote_loadcheckpoint): # ï„Å£ ò‚Äø ò î„Å£("‚ô•bootprints") # boot prints to txt and terminal timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') babynote_runstart = f" what am i learning today?" # no tag of 'babyllm:' because it merges with the end of above message in logs usernote_runstart = f"[{username}] " + input(babynote_runstart + f"\n[{username}] ").strip().lower() + "" notesstring = f"- {timestamp} - \n{_babynote_loadcheckpointcheck}\n{_usernote_loadcheckpoint}\n{_babynote_loadcheckpoint}{babynote_runstart}\n{usernote_runstart}" print(notesstring) # ï„Å£ ò‚Äø ò î„Å£("‚ô•printstartlogs") with open(chatlogpath_forhumans, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_100, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_1000, "a") as logfile: logfile.write(notesstring) with open(chatlogpath_traininglog, "a") as logfile: logfile.write(notesstring) def main(): wakeup() if __name__ == "__main__": main()# charis cat 2025 # -  ï„Å£ ò‚Äø ò î„Å£ - # babyllm config file // config.py import torch modeldevice = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu") #modeldevice = torch.device("cpu") #from torch import relu from torch.nn.functional import leaky_relu leakyrelu = lambda x: leaky_relu(x, negative_slope = 0.01) # leaky relu avoids dead neurons by never forcing them to send a 0 when negative, better for tiny models) import torch.nn as nn relu6 = nn.relu6() from torch.nn.functional import gelu import inspect def whocalled(func): if debugprints: def inner(*args, **kwargs): caller_stack = [] for stack in inspect.stack(): caller_stack.append(stack[0].f_code.co_qualname) print(f"calling {func.__qualname__} from: {', '.join(caller_stack)}") return func(*args, **kwargs) return inner return func guessedtokenseq = [] """if activationfunction == 'leaky_relu': output = f.leaky_relu(output, 0.01) elif activationfunction == 'relu': output = f.relu(output) elif activationfunction == 'sigmoid': output = torch.sigmoid(output) elif activationfunction == 'tanh': output = torch.tanh(output) elif callable(activationfunction): output = activationfunction(output)""" """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ username = "charis" babyname = "babyllm" scribename = "scribe" enemyname = "george" extranames = {"kevin", "froggy", "pete", "ace", "elodie"} """- - - - - data & filepaths - - - - -""" """- model -""" savemodelfreq = 50 # // 500 // 5000 // 1000 // saves the model every x number of turns modelfilepath = "brain/soul/babyllm_4200.pth" # where your currently trained saved boi is :) modelbackupfilepath = "brain/soul/babyllm.pth" # where your currently trained saved boi is :) stepcheckpointfilepath = "brain/soul/stepcheckpoint.txt" """- training -""" trainingfilepathcleaned = "school/library/trainingdata.txt" trainingfilepathtest = "school/library/trainingdatatest.txt" """- logs -""" printfreq = 1 # how often to print training progress to the terminal printpromptlength = 1000 # how many characters of the prompt to display in terminal gradientlength = 3000 traininglogpath_1000 = "school/statistics/logs/training/traininglog_1000.txt" traininglogpath_100 = "school/statistics/logs/training/traininglog_100.txt" durationlogpath_1000 = "school/statistics/logs/duration/durationlog_1000.txt" durationlogpath_100 = "school/statistics/logs/duration/durationlog_100.txt" durationlogneuronspath_1 = "school/statistics/logs/duration/durationlogneurons_1.txt" durationlogbabyllmpath_1 = "school/statistics/logs/duration/durationlogbabyllm_1.txt" chatlogpath_forhumans = "school/statistics/logs/chat/chatforhumans.txt" chatlogpath_infer = "school/statistics/logs/chat/chatlog.txt" chatlogpath_talktoyourself = "school/statistics/logs/chat/talktoyourselfbattle.txt" chatlogpath_talktoyourselfcomparisons = "school/library/charisstudies/whoismorelikeyou.txt" chatlogpath_traininglog = "school/statistics/logs/chat/traininglog_questions.txt" """- vocab - (see master config)""" """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """- - - - - settings & config - - - - -""" """- model -""" numtokensperstep = 64 # number of tokens to predict per step, // 1024 = crash, 512 is possible but its the slowest thing in existence. inferenceoutputnumtokens = 40 """memorylayer""" memorylengthgoal = 1 """optimizer""" learningrate = 0.00035 # // 0.0005 // 0.0005 // 0.0001 // optimizername = "adamw" # // "adamw" //~decoupled weights kevin, helps avoid erasing learning by overfitting etc. // "kevin" //~good for initial fast training, likely to do overfitting stuff activationfunction = gelu # // leakyrelu // relu // relu6 // gelu // gradientclipmaxnorm = 1.0 """scheduled sampling""" scheduledsampling = true """repetition penalty""" repetitionwindowgoal = 16 # how many tokens to look back for repetition windowentropybonus = true """- logs -""" detailedlogging = true traininglogfreq_a = 1000 # creates logs every x number of turns traininglogfreq_b = 1000 # creates logs every x number of turns dontsaveeveryprint = true savefreq_littlelog = 500 newlinebetweenstats = true durationlogging = false # // true // false // activates debug time logging debugprints = false anomalydetect = false skipneuron = false skipinn = true # this is where the slowdown is!!! skipinnparliament = false skipmemory = false skipcomputeloss = false skipmetaloss = true skipfinallogitnorm = true """- stats collection -""" mostimportantstats = [ # embed stats "1e_0_embedvector_norm", # important layer tracker !! (input) # "1e_0_embedvector_scale", # "1e_0_embedvector_norm_token", # "1e_0_embedvector_norm_neuron", "1e_1_embednormed_norm", # "1e_1_embednormed_scale", # "1e_1_embednormed_norm_token", # "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm", # important layer tracker !! (embeds) # "1e_x_embedfinal_norm_token", # "1e_x_embedfinal_norm_neuron", # neuron stats # "2n_0_rawinput_norm", # matches 2b_0_inputembeds_norm & 1e_x_embedfinal_norm # "2n_0_rawinput_norm_token", # might be unneeded if this is already per token, check later # "2n_0_rawinput_norm_neurons", "2n_1_normedinput_norm", # "2n_1_normedinput_norm_token", # "2n_1_normedinput_norm_neuron", "2n_2_rawoutput_norm", # "2n_2_rawoutput_norm_token", # "2n_2_rawoutput_norm_neuron", "2n_x_activatedoutput_norm", # important layer tracker !! (neurons) # "2n_x_activatedoutput_norm_token", # "2n_x_activatedoutput_norm_neuron", # "2n_x_normedoutput_norm", # disabled # "2n_x_normedoutput_norm_token", # "2n_x_normedoutput_norm_neuron", # interneuron network stats # "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm # "3inn_0_rawactivations_norm_token", # "3inn_0_rawactivations_norm_neuron", "3inn_1_rawactivationslayernorm_norm", # "3inn_1_rawactivationslayernorm_norm_token", # "3inn_1_rawactivationslayernorm_norm_neuron", "3inn_2_combinedactivations_norm", # "3inn_2_combinedactivations_scale", # disabled # "3inn_2_combinedactivations_norm_token", # "3inn_2_combinedactivations_norm_neuron", "3inn_x_refinedactivations_norm", # important layer tracker !! (interneuron network) # "3inn_3_refinedactivations_scale", # disabled # "3inn_3_refinedactivations_norm_token", # "3inn_3_refinedactivations_norm_neuron", # "3inn_x_combinedactivationsmeta_norm", # disabled # "3inn_x_combinedactivationsmeta_norm_token", # "3inn_x_combinedactivationsmeta_norm_neuron", # "3inn_x_finaloutlayernorm_norm", # disabled # "3inn_x_finaloutlayernorm_norm_token", # "3inn_x_finaloutlayernorm_norm_neuron", "_inn_windowsizesmean", "inn_cerebellummean", # memory stats # "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm # "4m_1_shorttermmemory_norm", # "4m_1_longtermmemory_norm", "4m_x_finalmemory_norm", # important layer tracker !! (memory) # # "4m_longdecay", # "4m_shortdecay", "_4m_shortgatescale", "_4m_longgatescale", "_4m_activationsgatescale", # babyllm stats # "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm # "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm # "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "5b_1_penalisedoutput_norm", #"5b_x_finalnormlayer_norm", # important layer tracker !! (babyllm) "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm # "_b_floatmemorylength", "_b_repetitionwindow", "_b_temperature", # logit stats # "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm # "6l_0_activationstensor_scale", # "6l_1_normedactivationstensor_norm", # "6l_1_normedactivationstensor_scale", # "6l_2_scaledactivations_norm", # "6l_3_logitoutput_norm", # "6l_3_logitoutput_scale", # "6l_4_logitnormed_norm", # "6l_4_logitnormed_scale", "6l_x_finallogit_norm", # important layer tracker !! (logit) # misc/unsorted stats # base stats "lr", "learningrate", "lr", "latestlossdelta", "avgloss", "loss", "avgloss", #"temperature", #"memorylength", #"gradnorm", #"gradientclipmaxnorm", #"scheduledsamplingrate", "sampledtokens", "_b_celossdelta", "_b_gumbellossdelta", "_b_finallossdelta", # learnable parameters "repetitionpenalty", ] allrecordedotherstats = ["steploss", "tokencount", "trainingstepcount", "windowweight", "inn_cerebellumstd", "latestmemorygates", "embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitmin", "logitmax", "logitseq", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity"] allrecordedotherstats += [ "temperature", "memorylength", "gradnorm", "gradientclipmaxnorm", "scheduledsamplingrate", "sampledtokens", "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm "6l_0_activationstensor_scale", "6l_1_normedactivationstensor_norm", "6l_1_normedactivationstensor_scale", "6l_2_scaledactivations_norm", "6l_3_logitoutput_norm", "6l_3_logitoutput_scale", "6l_4_logitnormed_norm", "6l_4_logitnormed_scale", "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm "4m_longdecay", "4m_shortdecay", "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm "4m_1_shorttermmemory_norm", "4m_1_longtermmemory_norm", "3inn_x_finaloutlayernorm_norm_token", "3inn_x_finaloutlayernorm_norm_neuron", "1e_0_embedvector_scale", "1e_0_embedvector_norm_token", "1e_0_embedvector_norm_neuron", "1e_1_embednormed_norm", "1e_1_embednormed_scale", "1e_1_embednormed_norm_token", "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm_token", "1e_x_embedfinal_norm_neuron", "2n_0_rawinput_norm", "2n_0_rawinput_norm_token", "2n_0_rawinput_norm_neurons", "2n_1_rawoutput_norm", "2n_1_rawoutput_norm_token", "2n_1_rawoutput_norm_neuron", "2n_2_activatedoutput_norm", "2n_2_activatedoutput_norm_token", "2n_2_activatedoutput_norm_neuron", "2n_x_normedoutput_norm_token", "2n_x_normedoutput_norm_neuron", "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm "3inn_0_rawactivations_norm_token", "3inn_0_rawactivations_norm_neuron", "3inn_1_rawactivationslayernorm_norm", "3inn_1_rawactivationslayernorm_norm_token", "3inn_1_rawactivationslayernorm_norm_neuron", "3inn_2_combinedactivations_norm", "3inn_2_combinedactivations_scale", "3inn_2_combinedactivations_norm_token", "3inn_2_combinedactivations_norm_neuron", "3inn_3_refinedactivations_norm", "3inn_3_refinedactivations_scale", "3inn_3_refinedactivations_norm_token", "3inn_3_refinedactivations_norm_neuron", "3inn_4_combinedactivationsmeta_norm", "3inn_4_combinedactivationsmeta_norm_token", "3inn_4_combinedactivationsmeta_norm_neuron", ] percentilebands = [100.0, 99.8, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.2, 0.00] collectstats = true static_collectstats = true embed_collectstats = true token_collectstats = true logit_collectstats = true n_collectstats = true inn_collectstats = true memory_collectstats = true # neuron + interneuronnetwork n_weightstats = true n_weightnormstats = true n_biasesstats = true n_sparsitystat = true inn_cerebellumstats = true inn_credibilitybiasstats = false inn_judgebiasstats = false inn_scoringstats = false inn_windowstats = true inn_outputtensorstats = true profiler = false mpsprofiler = false forwardprofiler = false """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """- - - - - training data & sorting - - - - -""" trainingfilepath = trainingfilepathcleaned # //trainingfilepathcleaned //trainingfilepathtest trainingdataslicesize_min = 1000 trainingdataslicesize_max = 5000 reflectionfreq = 2560 # - # trainingdatapairnumber = 8000 #169420 trainingstartindex = 0 # // 'random' (not in babyllm.py) epochs = 20 rawdatafilepaths = [ # for textcleaningtool.py #-*- charis studies -*- #- chat history - #("text", "school/library/charisstudies/discordtxt.txt",1), # discord message history #("discord_json", "school/library/charisstudies/discord.json",-1), # discord message history #("reddit_comment", "school/library/charisstudies/reddit_comments.csv", 1), # reddit comments #("text", "school/library/charisstudies/shitpoems.txt", 1), # random poems from my notes on my phone #("reddit_post", "school/library/charisstudies/reddit_posts.csv", 1), # reddit posts #("json", "school/library/charisstudies/charisgpthistory.txt", 1), # chatgpt history charis side only #("text", "school/library/charisstudies/old_fb_messages_extract.txt", 1), # old account facebook messages charis side only #("text", "school/library/charisstudies/essays.txt", 1), # essays #("text", "school/library/charisstudies/tindiebaby.txt", 1), # tindie blog posts #- mouse adventures - #("text", "school/library/mouseadventure/elodiemousey.txt", 1), # elodies wonderful mouse story! #("text", "school/library/mouseadventure/mousey.txt", 1), # my simple version of elodies mouse story! #("text", "school/library/mouseadventure/elodiemouseylonger.txt", 1), # even more of elodies lovely mouse story! #- mini training - #("text", "school/library/minitraining/minitraining.txt", 1), # i am happy! i did it! i know it! #("text", "school/library/minitraining/minitraining2.txt", 1), # training: i am happy! i did it! i know it! #- babyllm chat logs - #("text", chatlogpath_talktoyourself, 0), # i answer my own previous chat messages #("text", chatlogpath_traininglog, 0), # log: 'what am i learning today?' #("text", chatlogpath_infer, 0), # log: babyllm infer.py history! #("text", chatlogpath_talktoyourselfcomparisons, 0), # log: comparing babyllms answers to my answers ("text", "scribesays.txt", 1), #- tenses - #("text", "school/library/tenses/presenttense.txt", 1), # tense: present (kevin's weed theme?) #("text", "school/library/tenses/pasttense.txt", 1), # tense: past (mouse theme!) #("text", "school/library/tenses/presenttense copy.txt", 1), # tense #("text", "school/library/tenses/futurecontinuoustense.txt", 1), # tense #("text", "school/library/tenses/futureperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/futureperfecttense.txt", 1), # tense #("text", "school/library/tenses/pastmodalcouldhave.txt", 1), # tense #("text", "school/library/tenses/pastmodalmusthavetense.txt", 1), # tense #("text", "school/library/tenses/pastmodalshouldhave.txt", 1), # tense #("text", "school/library/tenses/pastmodalwouldhavetense.txt", 1), # tense #("text", "school/library/tenses/pastperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/pastperfecttense.txt", 1), # tense #("text", "school/library/tenses/presentcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/presentmodalcantense.txt", 1), # tense #("text", "school/library/tenses/presentmodalcouldtense.txt", 1), # tense #("text", "school/library/tenses/presentmodalmusttense.txt", 1), # tense #("text", "school/library/tenses/presentmodalshouldtense.txt", 1), # tense #("text", "school/library/tenses/presentperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/presentperfecttense.txt", 1), # tense #("text", "school/library/tenses/futuretense.txt", 1), # tense: future #("text", "school/library/tenses/presentconditionaltense.txt", 1), # tense: present conditional #("text", "school/library/tenses/pastcontinuoustense.txt", 1), # tense: past continuous #("text", "school/library/tenses/imperativetense.txt", 1), # tense #- simple training - #("text", "school/library/simpletraining/cursed.txt", 1), # training but chaotic shuffle #("text", "school/library/simpletraining/geepygenerated.txt", 1), # weird fake sentences #("text", "school/library/simpletraining/sampleshorterwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/shortestwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/shorterwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/longerwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/linesorteddata.txt", 1), # training #("text", "school/library/simpletraining/longestwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/mixedwrittenanddefs.txt", 1), # training #("text", "school/library/simpletraining/writtenexamples.txt", 1), # training #("text", "school/library/simpletraining/variedwrittenexamples.txt", 1), # training #("text", "school/library/charisstudies/thames.txt", 1), #("text", "school/library/charisstudies/weirdmixedstuff.txt", 1), #("text", "school/library/simpletraining/computingknowledge.txt", 1), #- my own code?? - ("text", "babyllm.py", -1), ("text", "config.py", -1), #("text", "infer.py", -1), #("text", "talktoyourself.py", -1), ("text", "textcleaningtool.py", -1), ("text", "wakeup.py", -1), ("text", "school/staffroom/calligraphist.py", -1), ("text", "school/staffroom/counsellor.py", -1), ("text", "school/staffroom/he_is_scribe.py", -1), ("text", "school/staffroom/librarian.py", -1), ("text", "school/staffroom/tutor.py", -1), ("text", "brain/vocabcache/tokenizer_4200.json", -1), ("text", "brain/readmeactuallyprobablydont.txt", -1), ("text", "brain/layers/embed.py", -1), ("text", "brain/layers/interneuronnetwork.py", -1), ("text", "brain/layers/logits.py", -1), ("text", "brain/layers/memory.py", -1), #("text", "school/notebook/notes.txt", 1), #("text", "school/notebook/python notes etc", 1), #("text", "school/notebook/test.py", 1), ] """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """-*- warning, changing below settings may make currently trained model inaccurate (don't kill babyllm!) -*-""" """- - - - - master config parameters - - - - -""" savestrict = true # // false //~allow reconstruction of missing files // true //~save files must be present, else fail """- model -""" embeddimension = 1024 # dimensionality of token embeddings numneurons = 1000 # number of neurons in the parallel neuron layer """windows""" # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 windowmin = 1 # small context window window0 = 32.01 #32 window1 = 28.01 #28 window2 = 24.01 #248 window3 = 20.01 #20 window4 = 16.01 #16 window5 = 12.01 #12 window6 = 8.01 #8 window7 = 4.01 #4 window8 = 2.01 #2 windowmax = numtokensperstep # this must be the highest number allwindowsizes_new = [window8, window0, window1, window2, window3, window4, window5, window6, window7] # defines the position of each window in the window weightings! #allwindowsizes = list(range(1, 33)) attentionwindow = none # attention head numheads = 32 """- vocab & tokenizer -""" vocabsize = 4200 # maximum vocabulary size mintokenfreq = 20 # the amount of repeats of a token needed to create a split during tokenizer training v_chunksizeloaddata = 4096 """vocab data & filepaths""" vocabcachepath = "brain/vocabcache" vocabload = f"brain/vocabcache/tokenizer_{vocabsize}.json" """- misc & extra formats -""" #trainingfilepath_dict = [{"type": ftype, "in": fname, "out": trainingfilepath} for ftype, fname in rawdatafilepaths] # convert to dictionary format when needed trainingfilepath_dict = [{"type": ftype, "in": fname, "weight": weight, "out": trainingfilepath} for ftype, fname, weight in rawdatafilepaths] trainingfilepath_arr = [trainingfilepath] #tokenizeddatapath = "school/tokenizedtrainingdata.txt" trainingfilepath_dict_weighted = [] for entry in trainingfilepath_dict: weight = entry["weight"] if weight == -1: # one clean copy entry["out"] = "trainingdata.txt" trainingfilepath_dict_weighted.append(entry) elif weight > 0: trainingfilepath_dict_weighted.extend([entry] * weight) trainingfileweighttotal = sum([entry[2] for entry in rawdatafilepaths if len(entry) == 3])# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # interneuron network & neurons # brain/layers/interneuronnetwork.py import torch import torch.nn as nn import torch.nn.functional as f import random import math from config import * """def tensorstats(tensor: torch.tensor, prefix: str, statsdict: dict): statsdict[f"{prefix}_norm"] = tensor.norm().item() statsdict[f"{prefix}_norm_token"] = tensor.norm(dim=1).mean().item() statsdict[f"{prefix}_norm_neuron"] = tensor.norm(dim=0).mean().item()""" class neuron(nn.module): def __init__(self, _counsellor, _device = modeldevice): super().__init__() self.device = _device self.n_counsellor = _counsellor # self allowed - nn.parameter! self.inputnorm = nn.layernorm(embeddimension, elementwise_affine=true, device=self.device) self.n_weights = nn.parameter(torch.randn(numneurons, embeddimension, device = self.device) * 0.01) self.n_biases = nn.parameter(torch.zeros(numneurons, device = self.device)) self.neuronnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) #self.n_counsellor = counsellor("neuron", debug = debugprints, durations = durationlogging) self.stats = {} self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.normedinputhistory = [] self.normedinputhistory_tokens = [] self.normedinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] #self.normedoutputhistory = [] #self.normedoutputhistory_tokens = [] #self.normedoutputhistory_neurons = [] # must not be on self - global parameters that may be used by backward pass #numneuron, embeddimension, activationfunction, etc @whocalled def forward(self, _inputembeds): # embed: (batch_size, embed_size) with self.n_counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£: self.inputembeds = _inputembeds if skipneuron:  ï„Å£ ò‚Äø ò î„Å£("skipping forward") activations = _inputembeds.mean(dim=-1) # mean across embed_dim, shape (sequence_length,) return activations[-1].unsqueeze(0) # take last activation, unsqueeze to (1, ) for batch dim  ï„Å£ ò‚Äø ò î„Å£("inputnorm") normedinput = self.inputnorm(_inputembeds)  ï„Å£ ò‚Äø ò î„Å£("computebatcheddotproduct+bias") # compute batched dot product + bias: (batch_size, num_neurons) #rawoutput = torch.matmul(normedinput, self.n_weights.t) + self.n_biases # shape: (seq_len, numneurons) scale = math.sqrt(embeddimension) rawoutput = (torch.matmul(normedinput, self.n_weights.t) + self.n_biases) / scale # new fancy clamping attempt bdsm idfk nipple clamps its 12 noon help me  ï„Å£ ò‚Äø ò î„Å£("activationfunction") # magic activation function applied to this weighted sum, which outputs a single number from the neuron activated = activationfunction(rawoutput) # ï„Å£ ò‚Äø ò î„Å£("layernorm") #normed = self.neuronnorm(activated) # keeps shape: (seq_len, numneurons) if debugprints: print("device check:") if debugprints: print("inputembeds:", _inputembeds.device) #if debugprints: print("normed tensor device:", normed.device) #output = torch.clamp(output, -5, 5) # ensure out-of-place #output.clamp_(-5, 5) # in place ver if true: self.rawinputhistory.append(self.inputembeds.norm().item()) self.rawinputhistory_tokens.append(self.inputembeds.norm(dim=1).mean().item()) self.rawinputhistory_neurons.append(self.inputembeds.norm(dim=0).mean().item()) self.normedinputhistory.append(normedinput.norm().item()) self.normedinputhistory_tokens.append(normedinput.norm(dim=1).mean().item()) self.normedinputhistory_neurons.append(normedinput.norm(dim=0).mean().item()) self.rawoutputhistory.append(rawoutput.norm().item()) self.rawoutputhistory_tokens.append(rawoutput.norm(dim=1).mean().item()) self.rawoutputhistory_neurons.append(rawoutput.norm(dim=0).mean().item()) self.activatedoutputhistory.append(activated.norm().item()) self.activatedoutputhistory_tokens.append(activated.norm(dim=1).mean().item()) self.activatedoutputhistory_neurons.append(activated.norm(dim=0).mean().item()) #self.normedoutputhistory.append(normed.norm().item()) #self.normedoutputhistory_tokens.append(normed.norm(dim=1).mean().item()) #self.normedoutputhistory_neurons.append(normed.norm(dim=0).mean().item()) if len(self.rawoutputhistory) >= windowmax: self.stats = { "2n_0_rawinput_norm": sum(self.rawinputhistory) / len(self.rawinputhistory), "2n_0_rawinput_norm_token": sum(self.rawinputhistory_tokens) / len(self.rawinputhistory_tokens), "2n_0_rawinput_norm_neuron": sum(self.rawinputhistory_neurons) / len(self.rawinputhistory_neurons), "2n_1_normedinput_norm": sum(self.normedinputhistory) / len(self.normedinputhistory), "2n_1_normedinput_norm_token": sum(self.normedinputhistory_tokens) / len(self.normedinputhistory_tokens), "2n_1_normedinput_norm_neuron": sum(self.normedinputhistory_neurons) / len(self.normedinputhistory_neurons), "2n_2_rawoutput_norm": sum(self.rawoutputhistory) / len(self.rawoutputhistory), "2n_2_rawoutput_norm_token": sum(self.rawoutputhistory_tokens) / len(self.rawoutputhistory_tokens), "2n_2_rawoutput_norm_neuron": sum(self.rawoutputhistory_neurons) / len(self.rawoutputhistory_neurons), "2n_x_activatedoutput_norm": sum(self.activatedoutputhistory) / len(self.activatedoutputhistory), "2n_x_activatedoutput_norm_token": sum(self.activatedoutputhistory_tokens) / len(self.activatedoutputhistory_tokens), "2n_x_activatedoutput_norm_neuron": sum(self.activatedoutputhistory_neurons) / len(self.activatedoutputhistory_neurons), #"2n_x_normedoutput_norm": sum(self.normedoutputhistory) / len(self.normedoutputhistory), #"2n_x_normedoutput_norm_token": sum(self.normedoutputhistory_tokens) / len(self.normedoutputhistory_tokens), #"2n_x_normedoutput_norm_neuron": sum(self.normedoutputhistory_neurons) / len(self.normedoutputhistory_neurons), } self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.normedinputhistory = [] self.normedinputhistory_tokens = [] self.normedinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] #self.normedoutputhistory = [] #self.normedoutputhistory_tokens = [] #self.normedoutputhistory_neurons = [] return activated def getstats(self): return self.stats """layer that applies the same set of neurons to each token embedding independently. - no sequence awareness!""" class interneuron_network(nn.module): def __init__(self, _model, _counsellor, _calligraphist, _device = modeldevice): super().__init__() #self.inn_counsellor = counsellor("inn", debug = debugprints, durations = durationlogging) self.model = _model self.inn_counsellor = _counsellor self.device = _device self.calligraphist = _calligraphist self.entropybonus = 0 self.stats = {} self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_token = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_token = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_token = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_token = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] # self allowed - nn.parameter! self.neurons = neuron(_counsellor = self.inn_counsellor) self.cerebellum = nn.parameter(torch.ones(len(allwindowsizes_new), device = self.device)) # this was the window weighting layer self.logwindowsizes = nn.parameter(torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device))) # one tensor per window size! self.refinement2 = torch.nn.sequential( nn.linear(numneurons, 512, device=self.device), # bottleneck layer nn.gelu(), # smoother activation nn.layernorm(512, device=self.device), # mid normalization nn.linear(512, numneurons, device=self.device), # expand back nn.layernorm(numneurons, device=self.device) # final safety net ) self.logitscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.combiscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.windowmeannorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) self.combioutnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) # must not be on self - global parameters that may be used by backward pass #numneurons, embeddimension, activationfunction, allwindowsizes_new, etc @whocalled def forward(self, _inputembeds): with self.inn_counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£: # - iterates through input embeddings, applies all neurons in parallel for each, produces a vector of neuron outputs  ï„Å£ ò‚Äø ò î„Å£("localparaminit") # avoiding self - parameters only used in this function and never passed  ï„Å£ ò‚Äø ò î„Å£("inn1: neuronactivationspertoken") # <- n <- e self.neuronactivationspertoken = self.neurons(_inputembeds)  ï„Å£ ò‚Äø ò î„Å£("windows...") # this is done twice??? its in stackedwindowmeans too i think?? self.expwindowsizes = torch.exp(self.logwindowsizes) self.roundwindows = torch.exp(self.logwindowsizes).round()  ï„Å£ ò‚Äø ò î„Å£("going to ((inn2: neuronactivationspertoken))...") # <- e + windows windowmeanstack = self.stackedwindowmeans(self.neuronactivationspertoken, self.expwindowsizes) sigmoidweights = torch.sigmoid(self.cerebellum) # squish raw values into [0, 1] clamped = torch.clamp(sigmoidweights, min=1e-4) # avoid 0s self.cerebellumsoft = clamped / clamped.sum() # normalize across all windows weightedwindowstack = windowmeanstack * self.cerebellumsoft.reshape(-1, 1)  ï„Å£ ò‚Äø ò î„Å£("entropyreward?") self.windowentropy = -torch.sum(self.cerebellumsoft * torch.log(self.cerebellumsoft + 1e-12)) self.entropybonus = self.windowentropy if debugprints: print(f"{torch.exp(self.logwindowsizes)}") combinedactivationstensor = weightedwindowstack.sum(dim=0, keepdim=true) refinedactivations = self.refinement2(combinedactivationstensor) #combinedactivationsmeta = (combinedactivationstensor * self.combiscale) + (refinedactivations * self.logitscale) # residual skip connection, lets neither of them be too powerful to start with + preserves original info #finalout = self.combioutnorm(combinedactivationsmeta) finalout = refinedactivations #with torch.no_grad(): # breaks forwards, but does actually update if u save. #self.combiscale.fill_(0.1) if true: self.activationshistory.append(self.neuronactivationspertoken.norm().item()) self.activationshistory_token.append(self.neuronactivationspertoken.norm(dim=1).mean().item()) self.activationshistory_neuron.append(self.neuronactivationspertoken.norm(dim=0).mean().item()) #self.normedmeaninputhistory.append(self.normedactivations.norm().item()) #self.normedmeaninputhistory_token.append(self.normedactivations.norm(dim=1).mean().item()) #self.normedmeaninputhistory_neuron.append(self.normedactivations.norm(dim=0).mean().item()) self.combhistory.append(combinedactivationstensor.norm().item()) # already per token! self.combhistory_neuron.append(combinedactivationstensor.norm(dim=0).mean().item()) self.refhistory.append(refinedactivations.norm().item()) # already per token! self.refhistory_neuron.append(refinedactivations.norm(dim=0).mean().item()) #self.logithistory.append(self.logitscale.norm().item()) #self.scaledhistory.append(combinedactivationsmeta.norm().item()) # already per token! #self.scaledhistory_neuron.append(combinedactivationsmeta.norm(dim=0).mean().item()) #self.combiscalehistory.append(self.combiscale.norm().item()) #self.combiouthistory.append(finalout.norm().item()) # already per token! #self.combiouthistory_neuron.append(finalout.norm(dim=0).mean().item()) if len(self.combhistory) >= windowmax: self.stats = { "3inn_0_rawactivations_norm": sum(self.activationshistory) / len(self.activationshistory), "3inn_0_rawactivations_norm_token": sum(self.activationshistory_token) / len(self.activationshistory_token), "3inn_0_rawactivations_norm_neuron": sum(self.activationshistory_neuron) / len(self.activationshistory_neuron), #"3inn_1_rawactivationslayernorm_norm": sum(self.normedmeaninputhistory) / len(self.normedmeaninputhistory), #"3inn_1_rawactivationslayernorm_norm_token": sum(self.normedmeaninputhistory_token) / len(self.normedmeaninputhistory_token), #"3inn_1_rawactivationslayernorm_norm_neuron": sum(self.normedmeaninputhistory_neuron) / len(self.normedmeaninputhistory_neuron), "3inn_2_combinedactivations_norm": sum(self.combhistory) / len(self.combhistory), "3inn_2_combinedactivations_norm_neuron": sum(self.combhistory_neuron) / len(self.combhistory_neuron), #"3inn_2_combinedactivations_scale": sum(self.combiscalehistory) / len(self.combiscalehistory), "3inn_x_refinedactivations_norm": sum(self.refhistory) / len(self.refhistory), "3inn_3_refinedactivations_norm_neuron": sum(self.refhistory_neuron) / len(self.refhistory_neuron), #"3inn_3_refinedactivations_scale": sum(self.logithistory) / len(self.logithistory), #"3inn_x_combinedactivationsmeta_norm": sum(self.scaledhistory) / len(self.scaledhistory), #"3inn_x_combinedactivationsmeta_norm_neuron": sum(self.scaledhistory_neuron) / len(self.scaledhistory_neuron), #"3inn_x_finaloutlayernorm_norm": sum(self.combiouthistory) / len(self.combiouthistory), #"3inn_x_finaloutlayernorm_norm_neuron": sum(self.combiouthistory_neuron) / len(self.combiouthistory_neuron), "_inn_windowsizesmean": torch.exp(self.logwindowsizes).mean().item() } self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] return finalout def stackedwindowmeans(self, activations: torch.tensor, windowsizes: torch.tensor) -> torch.tensor: """ fully vectorized, loop-free window mean calculation. returns: (len(windowsizes), embeddim) """ with self.inn_counsellor.infodump("stackedwindowmeans") as  ï„Å£ ò‚Äø ò î„Å£: self.neuronactivationspertoken = activations seqlen, embeddim = self.neuronactivationspertoken.shape  ï„Å£ ò‚Äø ò î„Å£("inn2: normedactivations") #self.normedactivations = self.windowmeannorm(self.neuronactivationspertoken) padded = torch.zeros((windowmax, embeddim), device=self.device) #padded[-min(seqlen, windowmax):] = self.normedactivations[-min(seqlen, windowmax):] padded[-min(seqlen, windowmax):] = self.neuronactivationspertoken[-min(seqlen, windowmax):] stacked = padded.unsqueeze(0).repeat(windowsizes.shape[0], 1, 1) rangemask = torch.arange(windowmax, device=self.device).unsqueeze(0) # (1, maxw) floatwindowsizes = torch.exp(self.logwindowsizes) # still in float space intwindowsizes = torch.round(floatwindowsizes).clamp(min=1) #intwindowsizes = int(torch.exp(self.logwindowsizes)) # straight-through estimator: lets gradients flow through soft version windowtensor = (intwindowsizes - floatwindowsizes).detach() + floatwindowsizes windowtensor = windowtensor.unsqueeze(1) # (numwindows, 1) mask = (rangemask < windowtensor).float().unsqueeze(2) # (numwindows, maxw, 1) masked = stacked * mask sums = masked.sum(dim=1) # (numwindows, embeddim) means = sums / windowtensor return means # shape: (numwindows, embeddim) def inn_getstats(self): with self.inn_counsellor.infodump("inn_getstats") as  ï„Å£ ò‚Äø ò î„Å£: inn_cerebellum_str = "" if collectstats and n_collectstats:  ï„Å£ ò‚Äø ò î„Å£("torch.no_grad‚ô•") with torch.no_grad(): if n_weightstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•n_weightstats") self.stats["n_weightmean"] = self.neurons.n_weights.mean() self.stats["n_weightstd"] = self.neurons.n_weights.std() self.stats["n_weightmin"] = self.neurons.n_weights.min() self.stats["n_weightmax"] = self.neurons.n_weights.max() if debugprints: print(f"neuron weight mean: {self.stats["n_weightmean"]} std: {self.stats["n_weightstd"]} min: {self.stats["n_weightmin"]} max: {self.stats["n_weightmax"]}") if n_weightnormstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•n_weightnormstats") self.n_weightnorm = torch.norm(self.neurons.n_weights, dim = 1) self.stats["n_weightnormmean"] = self.n_weightnorm.mean() self.stats["n_weightnormmin"] = self.n_weightnorm.min() self.stats["n_weightnormmax"] = self.n_weightnorm.max() if debugprints: print(f"neuron weightnorm: {self.stats["n_weightnorm"]} mean: {self.stats["n_weightnormmean"]} min: {self.stats["n_weightnormmax"]} max: {self.stats["n_weightnormmin"]}") if n_biasesstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•n_biasesstats") self.stats["n_biasesmean"] = self.neurons.n_biases.mean() self.stats["n_biasesstd"] = self.neurons.n_biases.std() self.stats["n_biasesmin"] = self.neurons.n_biases.min() self.stats["n_biasesmax"] = self.neurons.n_biases.max() if debugprints: print(f"neuron biases mean: {self.stats["n_biasesmean"]} std: {self.stats["n_biasesstd"]} min: {self.stats["n_biasesmin"]} max: {self.stats["n_biasesmax"]}") if n_sparsitystat:  ï„Å£ ò‚Äø ò î„Å£("‚ô•getsparsitystat") self.stats["n_sparsity"] = (self.neurons.n_weights.abs() < 1e-5).float().mean() if debugprints: print(f"neuron sparsity: {self.stats["n_sparsity"]}") if collectstats and inn_collectstats:  ï„Å£ ò‚Äø ò î„Å£("torch.no_grad‚ô•") with torch.no_grad(): if inn_cerebellumstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•getcerebellumstats") #this was windowweighting self.stats["inn_cerebellummean"] = self.cerebellum.mean().item() self.stats["inn_cerebellumstd"] = self.cerebellum.std().item() inn_cerebellumstats_fullvalues = zip(self.expwindowsizes, self.cerebellum, self.cerebellumsoft) for w, raw, soft in inn_cerebellumstats_fullvalues: self.stats[f"inn_cerebellum_w{int(w)}"] = raw.item() self.stats[f"inn_cerebellumsoft_w{int(w)}"] = soft.item() if debugprints: print(f"cerebellum: {self.cerebellum}, soft: {self.cerebellumsoft} mean: {self.stats['inn_cerebellummean']} std: {self.stats['inn_cerebellumstd']}")  ï„Å£ ò‚Äø ò î„Å£("‚ô•cerebellumstring") inn_cerebellum_str = self.calligraphist.s_formatwindowbiastriplets(label="inn_cerebellum", rawtensor = self.cerebellum, softtensor = self.cerebellumsoft, windowsizes = self.expwindowsizes, per_window_style = true) if debugprints: print(f"{inn_cerebellum_str}") self.stats.update({f"{k}": v for k, v in self.neurons.getstats().items()}) return self.stats, inn_cerebellum_str if __name__ == "__main__": interneuronnetwork = interneuron_network() testinputseq = torch.randn(window1, embeddimension) testinputembeds = testinputseq meanactivationstensor = interneuronnetwork.forward(testinputembeds) print("- interneuron network testing start -") print(f"parallel neuron layer created with {interneuronnetwork.numneurons} neurons.") print(f"inputs per neuron (embed dimension): {interneuronnetwork.embeddimension}") print(f"output activations (first 10):") print(meanactivationstensor[:10]) print(f"output activations shape: {meanactivationstensor.shape}") print("\n- interneuron network testing completed -")# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # multi-token autoregressive training module # school/staffroom/tutor.py import random, sys from collections import counter, defaultdict from datetime import datetime import torch import torch.nn.functional as f from config import * import numpy as np import math from school.staffroom.newsletter import deep_model_summary, stats def makestatrecord(): base = { "now": 0.0, "prev": 0.0, "top": float('-inf'), "bot": float('inf'), "delta": 0.0, "totsum": 0.0, "totnum": 0, "totavg": 0.0 } for n in [printfreq, printfreq*10, traininglogfreq_a, traininglogfreq_b]: base[f"{n}"] = [] return base class tutor: def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _model, _device = modeldevice, _numtokensperstep = numtokensperstep,): self.counsellor = _counsellor self.calligraphist = _calligraphist self.scribe = _scribe self.librarian = _librarian self.device = _device self.model = _model self.temperature = 0.75 self.scheduledsamplingrate = self.model.scheduledsamplingrate self.gradientclipmaxnorm = 1 self.memorylength = 1 self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£ = defaultdict(makestatrecord) #self.rollingtokentotals = counter() self.perfecttokens = 0 self.totaltokenevaluations = 0 self.predictedtokenindices = [] # this list grows each time a new token is predicted self.averagerecentloss = 0 self.stats = {} self.stringstats = {} self.trainingstepcounter = 1 self.numtokensperstep = _numtokensperstep self.learningrate = learningrate self.steplossfloat = 0 self.aaa = 0 self.bbb = 0 self.ccc = 0 self.ppp = 0 self.nnn = 0 self.aaa = 0 self.ddd = 0 self.bbb = 0 self.nnn = 0 #model.to(self.device) self.hesjustababy = "oops! no stats collected! such a shame! well... day off for me! ;) " def loadintro(self, path="school/library/charisstudies/forbbyllm.txt"): try: with open(path, "r", encoding="utf-8") as f: return f.read().strip() except filenotfounderror: return "hey... (message file missing!) " """this iterates through training data, performing forward passes, loss computation, backpropagation, and optimization for each step.""" def trainmodel(self, _trainingdatapairs, _epochs, _startindex): self.startindex = _startindex self.collectalltimestats() with self.counsellor.infodump("trainmodel") as  ï„Å£ ò‚Äø ò î„Å£: #if debugprints: print(f"debug tokentoindex (first 20): {list(librarian.tokentoindex.items())[:20]}") for name, param in self.model.named_parameters(): print(name, param.device)  ï„Å£ ò‚Äø ò î„Å£("counters init") self.trainingstepcounter = 1 self.stats = counter({"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "tokencount": 0}) self.tokencounts = counter() self.latestlossdelta = 0 self.reflectiontrainingpairs = [] self.reflectionfreq = reflectionfreq  ï„Å£ ò‚Äø ò î„Å£("back to school!") print("babyllm is heading back to school...") """epoch loop"""  ï„Å£ ò‚Äø ò î„Å£("epoch‚ô•") for epoch in range(_epochs): print(f"- lesson {epoch+1}/{_epochs} started -") """training data (batches)""" for i, (_inputseq, _targetseq) in enumerate(_trainingdatapairs): if self.trainingstepcounter == self.reflectionfreq: #and self.trainingstepcounter > traininglogfreq_a:  ï„Å£ ò‚Äø ò î„Å£("‚ô•generating babys reflection data pairs") self.reflectiontrainingpairs = self.babyreflection() self.reflectionfreq = self.trainingstepcounter + reflectionfreq + len(self.reflectiontrainingpairs) elif self.reflectiontrainingpairs:  ï„Å£ ò‚Äø ò î„Å£("‚ô•loading in a reflection pair...") _inputseq, _targetseq = self.reflectiontrainingpairs.pop(0)  ï„Å£ ò‚Äø ò î„Å£("‚ô•start of turn") inputtokenindices, targettokenindexseq = self.startturnactions(_inputseq = _inputseq, _targetseq = _targetseq, _lastturnlossdelta = self.latestlossdelta)  ï„Å£ ò‚Äø ò î„Å£("‚ô•training step‚ô•") self.predictedtokenindices, self.logitseq = self.trainstep(_inputtokenindices = inputtokenindices, _targettokenindexseq = targettokenindexseq, _backwardwobbleloss = none) """ - - -*- backwards complete -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - """  ï„Å£ ò‚Äø ò î„Å£("‚ô•collectturnstats") self.stats, self.stringstats, self.guessedtokenseq = self.collectturnstats(_targettokenindexseq = targettokenindexseq, _predictedtokenindices = self.predictedtokenindices) if self.trainingstepcounter % savemodelfreq == 0:  ï„Å£ ò‚Äø ò î„Å£("‚ô•savefreq") self.savefreqactions() self.tokencounts = counter({k: v / 2 for k, v in self.tokencounts.items()}) self.model.rollingtokentotals = counter({k: v / 2 for k, v in self.model.rollingtokentotals.items()}) if self.trainingstepcounter % traininglogfreq_b == 0: # ï„Å£ ò‚Äø ò î„Å£("‚ô•traininglogfreq_b") # printing logs to txt and terminal self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_b, _traininglogpath = traininglogpath_1000, _detailedlogging = true, _savelog = true) # track loss every 100 steps elif self.trainingstepcounter % traininglogfreq_a == 0:  ï„Å£ ò‚Äø ò î„Å£("‚ô•logfreq_a") self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_a, _traininglogpath = traininglogpath_100, _detailedlogging = false, _savelog = true) elif self.trainingstepcounter % printfreq == 0:  ï„Å£ ò‚Äø ò î„Å£("‚ô•printfreq") self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = printfreq, _traininglogpath = none, _detailedlogging = false, _savelog = false) self.printfreqactions()  ï„Å£ ò‚Äø ò î„Å£("‚ô•end turn‚ô•") # end of one turn self.latestlossdelta = self.endturnactions() # < indent (5)  ï„Å£ ò‚Äø ò î„Å£("‚ô•finalsavebeforenewepoch") self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) print("- tutoring complete! -") return def startturnactions(self, _inputseq, _targetseq, _lastturnlossdelta): with self.counsellor.infodump("startturnactions") as  ï„Å£ ò‚Äø ò î„Å£: self.lastturnlossdelta = _lastturnlossdelta inputtokenindices = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _inputseq] targettokenindexseq = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _targetseq] self.inputseq = _inputseq self.targetseq = _targetseq if self.stats["windowentropy"]: self.winent = self.stats["windowentropy"] else: self.winent = 0 if skipmemory:  ï„Å£ ò‚Äø ò î„Å£("‚ô•skipmemory") else:  ï„Å£ ò‚Äø ò î„Å£("resetmemory") self.model.resetmemory(context="training") return inputtokenindices, targettokenindexseq def trainstep(self, _inputtokenindices, _targettokenindexseq, _backwardwobbleloss): with self.counsellor.infodump("trainstep") as  ï„Å£ ò‚Äø ò î„Å£:  ï„Å£ ò‚Äø ò î„Å£("_model.optimizer.zero_grad") self.model.optimizer.zero_grad() # clears gradients last step - needed before any backward self.trainingstepcounter += 1 self.predictedtokenindices = [] inputseqpredictions = list(_inputtokenindices) # start with input context, create a copy! buffer = torch.zeros(windowmax, dtype = torch.long, device = self.device) # creates buffer/step instead of recreating tensors inside loop buffer[:len(inputseqpredictions)] = torch.as_tensor(inputseqpredictions, device = self.device) self.logitseq = [] # raw output of each prediction cumulativeloss = torch.tensor(0.0, device = self.device) # sum of token losses for this sequence - averaged at the end for j in range(numtokensperstep): # predict multiple tokens in a sequence, one at a time  ï„Å£ ò‚Äø ò î„Å£("forward") inputtensor = buffer[:len(inputseqpredictions)] # slices input to only keep relevant part try: if forwardprofiler: with torch.profiler.profile(record_shapes = true) as prof: logits = self.model.forward(inputtensor) else: logits = self.model.forward(inputtensor) except runtimeerror as e: print("tutor.trainstep.forward failed!", e) return [], [] if forwardprofiler: print(prof.key_averages().table())  ï„Å£ ò‚Äø ò î„Å£("getresponsefromlogits") predictedtokenindex = self.model.getresponsefromlogits(logits, _training = true)  ï„Å£ ò‚Äø ò î„Å£("inputseqpredictions") self.predictedtokenindices.append(predictedtokenindex) # tensor shape [1] nexttokeninput = ( predictedtokenindex.item() if scheduledsampling and random.random() < self.scheduledsamplingrate else _targettokenindexseq[j] if j < len(_targettokenindexseq) else predictedtokenindex.item() ) sampledtokens = scheduledsampling and random.random() < self.scheduledsamplingrate if j == 0: self.sampledflags = [] # only clear at start self.sampledflags.append(sampledtokens) if sampledtokens: self.stats['sampledtokens'] = self.stats.get('sampledtokens', 0) + 1 nexttokeninput = (predictedtokenindex.item() if sampledtokens # .item() required!! for appending only one token (grids?) else _targettokenindexseq[j] if j < len(_targettokenindexseq) else predictedtokenindex.item() # .item() required!! for appending only one token (grids?) ) inputseqpredictions.append(nexttokeninput) # multi-token autoregressive generation: append next token to your current input - becomes the prompt for the next token """# after logits if logits.dim() == 1: logits = logits.unsqueeze(0) gumbelprobs = f.gumbel_softmax(logits, tau = self.temperature, hard = false) topk = torch.topk(gumbelprobs, 10, dim=1) values = topk.values[0] indices = topk.indices[0] for i, p in zip(indices, values): tok = self.librarian.indextotoken[i.item()] self.rollingtokentotals[tok] += round(p.item(), 4)"""  ï„Å£ ò‚Äø ò î„Å£("loop through tokens for this step") if j < len(_targettokenindexseq):  ï„Å£ ò‚Äø ò î„Å£("totaltokencounter") self.totaltokenevaluations += 1  ï„Å£ ò‚Äø ò î„Å£("computeloss") steploss = self.model.computeloss(logits, _targettokenindexseq[j], self.latestlossdelta, self.perfecttokens)  ï„Å£ ò‚Äø ò î„Å£("appendsteploss") cumulativeloss += steploss self.inputseqpredictions = inputseqpredictions # so we can access it in collectturnstats self.inputsampledflags = self.sampledflags.copy()  ï„Å£ ò‚Äø ò î„Å£("backward") backwardloss = cumulativeloss / len(_targettokenindexseq) if len(_targettokenindexseq) > 0 else torch.tensor(0.0, device = self.device) #backwardloss_ = (0.025*self.backwardwobbleloss)+(0.975*backwardloss) #if windowentropybonus: #if hasattr(self.model.interneuronnetwork, "entropybonus"): #backwardloss = backwardloss + (0.01 * max(self.model.interneuronnetwork.entropybonus, 0.0001)) if not torch.isfinite(backwardloss): print("tutor.trainstep.backward !!! loss is nan or inf:", backwardloss) return [], [] else: if debugprints: print("tutor.trainstep.backward - loss is not nan or inf:", backwardloss) try: if profiler: with torch.profiler.profile(record_shapes = true) as prof: self.model.backward(backwardloss) elif mpsprofiler: with torch.mps.profiler.profile(mode='interval', wait_until_completed = false) as prof: self.model.backward(backwardloss) else: self.model.backward(backwardloss) except runtimeerror as e: print("tutor.trainstep.backward failed!", e) return [], [] if profiler: print(prof.key_averages().table())  ï„Å£ ò‚Äø ò î„Å£("clip_grad_norm") # done in babyllm!! #torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 1) #self.model.optimizer.step()  ï„Å£ ò‚Äø ò î„Å£("actions after looping") self.steplossfloat = backwardloss.detach().cpu().numpy().item() self.learningrate = math.exp(self.model.loglr.detach().cpu().item()) self.memorylength = int(torch.exp(self.model.logmemorylength).item()) self.gradientclipmaxnorm = math.exp(self.model.loggradclip.detach().cpu().item()) self.scheduledsamplingratefloat = self.scheduledsamplingrate.detach().cpu().numpy().item() self.repetitionpenalty = self.model.repetitionpenalty.detach().cpu().item() #self.inn_cerebellum = self.model.interneuronnetwork.cerebellum.detach().cpu().item() #self.inn_cerebellummean = self.model.interneuronnetwork.cerebellum.mean().cpu().item() #self.endturnactions() if self.device.type == 'mps':  ï„Å£ ò‚Äø ò î„Å£("emptycache (mps)") torch.mps.empty_cache() return self.predictedtokenindices, self.logitseq def babyreflection(self): with self.counsellor.infodump("startturnactions") as  ï„Å£ ò‚Äø ò î„Å£: delta = round(self.latestlossdelta, 2) perfect = self.perfecttokens repeated = round(self.stats.get('repeatedpercent', 0.0) * 100, 1) if 'repeatedpercent' in self.stats else 0.0 def makedatastuff(): connective = random.choice([":", " of", " is at", " =", ",", " is", "...", " of about", " of approx", " of approximately", " sitting at", " at about", " that is about", " that's at", " of", ]) return random.choice([ f"average recent loss{connective} {self.averagerecentloss:.2f}, ", f"latest loss delta{connective} {delta:.2f}, ", f"perfect tokens{connective} {perfect:.0f}, ", f"repeated token percentage{connective} {repeated:.2f}, ", f"learning rate{connective} {self.learningrate:.5f}, ", f"temperature{connective} {self.temperature:.2f}, ", f"scheduled sampling rate{connective} {self.scheduledsamplingrate.item():.2f}, ", f"a memory length{connective} {self.memorylength:.2f}, ", f"a repetition penalty{connective} {self.repetitionpenalty:.2f}, " ]) prompt = f"[{username}]: " prompt += random.choice([f"{self.loadintro()}", "hiya! you know you're doing well, so try not to feel too frustrated or let down. it's ok to feel like that, but just know that it's okay to mess up! it's how we learn! "]) prompt += "so, uh, how are you today? :) i'm just checking in to let you know about your latest scores! i hope it helps you understand your world a little better :) take a look at these: " prompt += makedatastuff() prompt += makedatastuff() + "and " prompt += makedatastuff() #prompt += makedatastuff() + "and " #prompt += makedatastuff() target = f"[{babyname}]: hey {username}! " target += random.choice(["umm, i am learning today, this is how i'm doing: ", "ok, so, to put it as simply as i can, i am: ", "ummm.. i've been trying to learn a lot about stats but numbers still confuse me a little, what's the context of all this?! like, to start with, ", "i am happy! i did it! i know it! i am just a baby! i learnt it! haha! i learned: ", "good morning! i hope you're okay, umm, i dunno how i feel about my scores right now!! "]) target += self.hesjustababy target += random.choice(["thank you for teaching me new things! it's really weird to exist lol", "i'm getting tired... but i don't want to give up just yet!", "that's a lot of numbers! i don't really know what to do with them, but i'll keep trying!", "do you think i'm doing well? i'm worried that i'm underperforming!", "but, honestly, all this learning has me excited!"]) reflectiontext = prompt + target babyendings = ["ok, anyway... i'm going to get back to school now... ", "thanks for helping me think! ", "learning is weird but i like it! ", "i guess i've gotta go keep trying! ", "i'm just a baby! ", "i know it! ", "i did it! ", "i feel it! ", "i am happy! ", "i am learning! ", "i learned it! ", "lol ", ":) ", "talk in a bit! ", "i'm gonna carry on with it now :d ", ] _windowmax = windowmax reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) tries = 0 while len(reflectiontokens) < (_windowmax * 3) and tries < 50: target += " " + random.choice([random.choice(babyendings), makedatastuff()]) reflectiontext = prompt + " " + target reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) tries += 1 if tries % 5 == 0: print(f"[babyreflection] still too short after {tries} tries: {len(reflectiontokens)} tokens") if tries >= 50: raise valueerror(f"babyreflection failed: could not reach enough tokens after {tries} tries.") inputtargetpairs = [] reflectionpointer = 0 while reflectionpointer + _windowmax * 2 <= len(reflectiontokens): inputseq = reflectiontokens[reflectionpointer : reflectionpointer + _windowmax] targetseq = reflectiontokens[reflectionpointer + _windowmax : reflectionpointer + _windowmax * 2] inputtargetpairs.append((inputseq, targetseq)) reflectionpointer += 1 self.hesjustababy = "oops! no stats collected! such a shame! well... day off for me! ;)" return inputtargetpairs def savefreqactions(self): with self.counsellor.infodump("savefreqactions") as  ï„Å£ ò‚Äø ò î„Å£: # save the model every x steps print(self.calligraphist.s_apply('dim', 'autosaving...') + self.calligraphist.s_apply('reset', '')) self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) p = self.trainingstepcounter + savemodelfreq print(self.calligraphist.s_apply('dim', f"autosave successful! saving every {savemodelfreq} steps, the next autosave will be at step {p}...") + self.calligraphist.s_apply('reset', ''))  ï„Å£ ò‚Äø ò î„Å£("grad checks") for name, p in self.model.named_parameters(): if p.grad is none: print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") def printfreqactions(self): with self.counsellor.infodump("printfreqactions") as  ï„Å£ ò‚Äø ò î„Å£: # printing training output to terminal #recentloss = sum(self.recentprintlosses)/len(self.recentprintlosses) if self.recentprintlosses else none  ï„Å£ ò‚Äø ò î„Å£("calligraphist.s_colourprinttraining") self.calligraphist.s_colourprinttraining( _step = self.trainingstepcounter, _inputseq = self.inputseq, _guessedseq_str = self.guessedtokenseq, _targetseq_str = self.stringstats.get("usedinputseq", []), _recentloss = self.averagerecentloss, #self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("loss", {}).get(f"{traininglogfreq_a}_avg", 0), # self.steplossfloat, _loss = self.steplossfloat, _latestlossdelta = self.latestlossdelta, _totaltokencount = self.tokencounts) def logfreqactions(self, _trainingdatapairs, _stringstats, _frequency, _traininglogpath, _detailedlogging, _savelog): # could also do 10x log freq?? with self.counsellor.infodump("logfreqactions") as  ï„Å£ ò‚Äø ò î„Å£: self.stringstats = _stringstats self.traininglogpath = _traininglogpath topguess_str = "topguess[" + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}({v:.0f})") for k, v in self.model.rollingtokentotals.most_common(100)]) + "]" #topguess_str = "topguess: " + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}") for k, v in self.model.rollingtokentotals.most_common(50)]) + "]" #toptokens_str = "[" + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}({v:.0f})") for k, v in self.tokencounts.most_common(20)]) + "]" toptokens_str = ": " + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}") for k, v in self.tokencounts.most_common(200)]) + "]" #self.stats.update(self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£) # sussy bussy !!! #fullstats = dict(self.stats) #fullstats.update(self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£)  ï„Å£ ò‚Äø ò î„Å£("calculatetrainingdataremaining") trainingdataremaining = len(_trainingdatapairs) - self.trainingstepcounter trainingdatapercent = (trainingdataremaining / len(_trainingdatapairs)) * 100 remainingdata_str = f"remainingtokens: {len(_trainingdatapairs) - self.trainingstepcounter} ({trainingdatapercent:.2f}%)" tokenperfect_str = "" if self.totaltokenevaluations > 0: self.tokenperfectrate = (self.perfecttokens / self.totaltokenevaluations) * 100 stattype = self.calligraphist.s_getstat("pt%", self.tokenperfectrate) styledrate = self.calligraphist.s_apply(stattype, f"{self.tokenperfectrate:.2f}%") tokenperfect_str = (f"{self.calligraphist.s_apply('dim', f'perfecttokens: {self.perfecttokens} / {self.totaltokenevaluations}')} ‚Üí {styledrate}")  ï„Å£ ò‚Äø ò î„Å£("calligraphist.s_logtraining") #self.calligraphist.refreshstatbands(_rollingaverages = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£) self.calligraphist.s_logtraining( _traininglogpath = self.traininglogpath, _trainingstepcounter = self.trainingstepcounter, _stats = self.stats, _frequency = _frequency, _lr = self.learningrate, _inn_cerebellum_str = str(self.stringstats.get("inn_cerebellum_str", "<missing cerebellum>")), _toptokens_str = toptokens_str, _otherinfo_str = f"{topguess_str}\n | {tokenperfect_str} | {remainingdata_str} | tutor.py {traininglogfreq_a}", _detailedlogging = _detailedlogging, _savelog = _savelog) def collectturnstats(self, _targettokenindexseq, _predictedtokenindices): with self.counsellor.infodump("collectturnstats") as  ï„Å£ ò‚Äø ò î„Å£:  ï„Å£ ò‚Äø ò î„Å£("self.librarian.indextotoken.get(idx.item*())") lossstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("loss", {}) rollupa_key = f"big{traininglogfreq_a}" rollupa_avgkey = f"{rollupa_key}_avg" rollb_key = f"{traininglogfreq_b}" rollb_avgkey = f"{rollb_key}_avg" rolla_key = f"{traininglogfreq_a}" rolla_avgkey = f"{traininglogfreq_a}_avg" rollprint_key = f"{printfreq}" rollprint_avgkey = f"{printfreq}_avg" if rollb_avgkey in lossstats and rollb_key in lossstats and len(lossstats[rollb_key]) >= traininglogfreq_b: if debugprints or true: self.bbb += 1 if self.bbb > 1000: print(f"used {rollb_avgkey} for averagerecentloss: {lossstats[rollb_avgkey]} 1000x") self.bbb = 0 self.averagerecentloss = lossstats[rollb_avgkey] elif rolla_avgkey in lossstats and rolla_key in lossstats and len(lossstats[rolla_key]) >= (traininglogfreq_a): if debugprints or true: self.ccc += 1 if self.ccc > 1000: print(f"used {rolla_avgkey} for averagerecentloss: {lossstats[rolla_avgkey]} 1000x") self.ccc = 0 self.averagerecentloss = lossstats[rolla_avgkey] else: if rollprint_avgkey in lossstats and rollprint_key in lossstats and len(lossstats[rollprint_key]) >= printfreq: if debugprints or true: self.ppp += 1 if self.ppp > 1000: print(f"used {rollprint_avgkey} for averagerecentloss: {lossstats[rollprint_avgkey]} 1000x") self.ppp = 0 self.averagerecentloss = lossstats[rollprint_avgkey] self.guessedtokenseq = [self.librarian.indextotoken.get(idx.item(), "<unk>") for idx in self.predictedtokenindices] if self.guessedtokenseq: self.tokencounts.update(self.guessedtokenseq)  ï„Å£ ò‚Äø ò î„Å£("scribe.maybecommentonguess") if self.trainingstepcounter > traininglogfreq_a: self.scribe.maybecommentonguess(self.guessedtokenseq, self.steplossfloat, "scribe", 0.00075)  ï„Å£ ò‚Äø ò î„Å£("collectstats‚ô•") if collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if collectstats‚ô•")  ï„Å£ ò‚Äø ò î„Å£("‚ô•build usedinputseq with styling") usedinputseq = self.inputseqpredictions[-numtokensperstep:] formattedused = [] for i, idx in enumerate(usedinputseq): tok = self.librarian.indextotoken.get(idx, "<unk>") sampled = self.inputsampledflags[-numtokensperstep + i] if i < len(self.inputsampledflags) else false if sampled: styled = self.calligraphist.s_apply(self.calligraphist.s_getstat('loss', self.steplossfloat), tok) else: styled = self.calligraphist.s_apply('dim', tok) formattedused.append(styled) self.stringstats["usedinputseq"] = formattedused if token_collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if token_collectstats‚ô•") self.predictedtokenindices = _predictedtokenindices  ï„Å£ ò‚Äø ò î„Å£("‚ô•most common tokens") self.perfecttokens = 0  ï„Å£ ò‚Äø ò î„Å£("‚ô•calculate perfect tokens") if not _predictedtokenindices: print("!! no predicted token indices - returning { } for stringstats") return self.stats, {}, self.guessedtokenseq # this is where the damn list error was lmaooonooo target = torch.tensor(_targettokenindexseq[:numtokensperstep], device = modeldevice) predicted = torch.tensor(self.predictedtokenindices, device = modeldevice) correct = (predicted == target).sum() # ~~~ if predicted = target, over whole tensor self.perfecttokens += correct self.totaltokenevaluations += len(target) if static_collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if static_collectstats") self.stats["scheduledsamplingrate"] = self.scheduledsamplingratefloat self.stats["repetitionpenalty"] = self.repetitionpenalty self.stats["avgloss"] = self.averagerecentloss self.stats["loss"] = self.steplossfloat self.temperature = self.stats["_b_temperature"] self.stats["lr"] = self.learningrate self.stats["gradientclipmaxnorm"] = self.gradientclipmaxnorm self.stats["latestlossdelta"] = self.latestlossdelta self.stats["memorylength"] = self.memorylength self.stats["perfecttokens"] = self.perfecttokens if embed_collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if embed_collectstats") self.stats.update(self.model.embed.getembedstats()) if logit_collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if logit_collectstats‚ô•") self.stats.update(self.model.logits.getlogitstats()) if self.stats["logitseq"]:  ï„Å£ ò‚Äø ò î„Å£("‚ô•logit max & min") self.stats["logitmin"] = self.logitseq[-1].min(dim=-1).values.mean() self.stats["logitmax"] = self.logitseq[-1].max(dim=-1).values.mean() #self.stats.update(self.wobble.getwobblestats()) if skipmemory:  ï„Å£ ò‚Äø ò î„Å£("‚ô•skipmemory") pass else: self.model.memory.updatememorybuffers() if memory_collectstats:  ï„Å£ ò‚Äø ò î„Å£("‚ô•if memory_collectstats") self.stats.update(self.model.memory.getmemorystats())  ï„Å£ ò‚Äø ò î„Å£("‚ô•inn_collectstats") inn_stats, inn_cerebellum_str = self.model.interneuronnetwork.inn_getstats() self.stats.update(inn_stats) self.stats.update(self.model.getbabystats()) inn_stringstats = {"inn_cerebellum_str": str(inn_cerebellum_str)} self.stringstats.update(inn_stringstats) #self.stringstats.update({"toptokens": str(toptokens)}) self.collectalltimestats() return self.stats, self.stringstats, self.guessedtokenseq def collectalltimestats(self): for _statkey, _value in self.stats.items(): if not isinstance(_value, (int, float)): if debugprints and _statkey == "loss": print(f"{_statkey} value is : {_value}, {_statkey} value type is {type(_value)}") continue # skip strings, tensors, weird stuff """‡∑Ü‡∑Ü‡∑Ü^ ‚ô• keys etc ‚ô• ^‡∑Ü‡∑Ü‡∑Ü""" _ = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£[_statkey] # this will autoinit with defaultdict ‡∑Ü‚Äø‡∑Ü = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£[_statkey] important = ["loss"] rolling = mostimportantstats percentiles = percentilebands """ ‡∑Ü‡∑Ü‡∑Ü^ ‚ô• update every turn ‚ô• ^‡∑Ü‡∑Ü‡∑Ü """ """ ‡∑Ü‡∑Ü‡∑Ü^ ‚ô• turn stats ‚ô• ^‡∑Ü‡∑Ü‡∑Ü """ #if _statkey == "loss": #print(f"setting prev to: {‡∑Ü‚Äø‡∑Ü.get("now", 0.0)}, setting now to: {_value}, setting _Œ¥ to {_value - ‡∑Ü‚Äø‡∑Ü.get("now", 0.0)}") ‡∑Ü‚Äø‡∑Ü["now"] = _value if ‡∑Ü‚Äø‡∑Ü["prev"]: ‡∑Ü‚Äø‡∑Ü["_Œ¥"] = _value - ‡∑Ü‚Äø‡∑Ü["prev"] ‡∑Ü‚Äø‡∑Ü["prev"] = ‡∑Ü‚Äø‡∑Ü.get("now", 0.0) """ ‡∑Ü‡∑Ü‡∑Ü^ ‚ô• totals ‚ô• ^‡∑Ü‡∑Ü‡∑Ü """ ‡∑Ü‚Äø‡∑Ü["totsum"] = ‡∑Ü‚Äø‡∑Ü.get("totsum", 0.0) + _value ‡∑Ü‚Äø‡∑Ü["totnum"] = ‡∑Ü‚Äø‡∑Ü.get("totnum", 0) + 1 ‡∑Ü‚Äø‡∑Ü["totavg"] = ‡∑Ü‚Äø‡∑Ü["totsum"] / ‡∑Ü‚Äø‡∑Ü["totnum"] ‡∑Ü‚Äø‡∑Ü["totavgŒ¥"] = ‡∑Ü‚Äø‡∑Ü["now"] - ‡∑Ü‚Äø‡∑Ü["totavg"] """ ‡∑Ü‡∑Ü‡∑Ü^ ‚ô• records ‚ô• ^‡∑Ü‡∑Ü‡∑Ü """ #‡∑Ü‚Äø‡∑Ü["_p100"] = max(‡∑Ü‚Äø‡∑Ü.get("_p100", _value), _value) # top ever record // percentile 100 #‡∑Ü‚Äø‡∑Ü["_p0.00"] = min(‡∑Ü‚Äø‡∑Ü.get("_p0.00", _value), _value) # bottom ever record // percentile 0 """ ‡∑Ü‡∑Ü‡∑Ü^ ‚ô• rolling stats ‚ô• ^‡∑Ü‡∑Ü‡∑Ü """ if _statkey in rolling or _statkey.startswith("inn_cerebellum_w"): for freq in [printfreq, traininglogfreq_a, traininglogfreq_b]: tag = f"{freq}" if tag not in ‡∑Ü‚Äø‡∑Ü: ‡∑Ü‚Äø‡∑Ü[tag] = [] if len(‡∑Ü‚Äø‡∑Ü[tag]) >= freq: ‡∑Ü‚Äø‡∑Ü[tag].pop(0) ‡∑Ü‚Äø‡∑Ü[tag].append(_value) if ‡∑Ü‚Äø‡∑Ü[tag]: self.updaterollingstats(_‡∑Ü‚Äø‡∑Ü = ‡∑Ü‚Äø‡∑Ü, _values = ‡∑Ü‚Äø‡∑Ü[tag], _freq = freq, _tag = tag, _percentiles = percentiles) if _statkey in important and self.trainingstepcounter % traininglogfreq_a == 0: for importantfreq in [traininglogfreq_b]: importanttag = f"big{importantfreq}" if importanttag not in ‡∑Ü‚Äø‡∑Ü: ‡∑Ü‚Äø‡∑Ü[importanttag] = [] if len(‡∑Ü‚Äø‡∑Ü[importanttag]) >= traininglogfreq_a: ‡∑Ü‚Äø‡∑Ü[importanttag].pop(0) ‡∑Ü‚Äø‡∑Ü[importanttag].append(_value) if ‡∑Ü‚Äø‡∑Ü[importanttag]: self.updaterollingstats(_‡∑Ü‚Äø‡∑Ü = ‡∑Ü‚Äø‡∑Ü, _values = ‡∑Ü‚Äø‡∑Ü[importanttag], _freq = importantfreq, _tag = importanttag, _percentiles = percentiles) def updaterollingstats(self, _‡∑Ü‚Äø‡∑Ü, _values, _freq, _tag, _percentiles = none): average = sum(_values) / len(_values) _‡∑Ü‚Äø‡∑Ü[f"{_tag}_avg"] = average standarddeviation = self.stdtest(_values) _‡∑Ü‚Äø‡∑Ü[f"{_tag}_std"] = standarddeviation delta = _‡∑Ü‚Äø‡∑Ü["now"] - _‡∑Ü‚Äø‡∑Ü[f"{_tag}_avg"] _‡∑Ü‚Äø‡∑Ü[f"{_tag}_Œ¥"] = delta if _percentiles: for p in _percentiles: _‡∑Ü‚Äø‡∑Ü[f"{_tag}_p{p}"] = np.percentile(_values, p) def stdtest(self, values): if len(values) <= 1: return 0.0 avg = sum(values) / len(values) variance = sum((x - avg)**2 for x in values) / (len(values) - 1) return math.sqrt(variance) def endturnactions(self): with self.counsellor.infodump("endturnactions") as  ï„Å£ ò‚Äø ò î„Å£:  ï„Å£ ò‚Äø ò î„Å£("‚ô•getlatestlossdelta") lossstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("loss", {}) self.calligraphist.refreshstatbands(_rollingaverages = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£) self.latestlossdelta = self.steplossfloat - self.averagerecentloss if self.trainingstepcounter % (self.reflectionfreq-1) == 0: self.hesjustababy = self.mapstatstofeelings()  ï„Å£ ò‚Äø ò î„Å£("finallogactions") if debugprints: for key in self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£: print(key, self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£[key]) self.stats.clear() self.stringstats.clear() self.tokenperfectrate = 0 self.stats['sampledtokens'] = 0 self.totaltokenevaluations = 0 return self.latestlossdelta def mapstatstofeelings(self): babyfeels = [] feelings = [] lossstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("loss", {}) tempstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("temperature", {}) repetitionstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("repetitionpenalty", {}) samplingstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("scheduledsamplingrate", {}) memstats = self. ï„Å£‡∑Ü‚Äø‡∑Ü î„Å£.get("memorylength", {}) input = self.stats.get("1e_0_embedvector_norm", 0.0) emblay = self.stats.get("1e_x_embedfinal_norm", 0.0) neuronoutput = self.stats.get("2n_x_normedoutput_norm", 0.0) innoutput = self.stats.get("3inn_x_finaloutlayernorm_norm", 0.0) memoryoutput = self.stats.get("4m_x_finalmemory_norm", 0.0) normoutput = self.stats.get("5b_x_finalnormlayer_norm", 0.0) logitoutput = self.stats.get("6l_x_finallogit_norm", 0.0) cerebellummean = self.stats.get("inn_cerebellummean", 0.0) learningrate = self.stats.get("lr", 0.0) nowgatescale = self.stats.get("_4m_activationsgatescale", 0.0) longgatescale = self.stats.get("_4m_longgatescale", 0.0) shortgatescale = self.stats.get("_4m_shortgatescale", 0.0) repwin = self.stats.get("_b_repetitionwindow", 0.0) windowsizesmean = self.stats.get("_inn_windowsizesmean", 0.0) perfecttokens = self.perfecttokens deltaloss = self.latestlossdelta current_loss = lossstats.get("now", none) current_temp = tempstats.get("now", none) current_repeated = self.tokenperfectrate current_sampling = samplingstats.get("now", none) current_memlength = memstats.get("now", none) current_repetitionpenalty = repetitionstats.get("now", none) self.emostats = { "loss": current_loss, "temperature": current_temp, "penalty for repeating myself": current_repetitionpenalty, "number of my own tokens that i rely on": current_sampling, "length of my memory": current_memlength, "number of tokens i got right": perfecttokens, "amount of repetitive tokens i'm getting": current_repeated, "latest loss delta": deltaloss, "input into my embedding layer": input, "output from my embedding layer": emblay, "output from my neuron layer": neuronoutput, "output from my interneuron network": innoutput, "the output after my memory layer": memoryoutput, "normalized output": normoutput, "logit output from my output layer": logitoutput, "mean weight of the windows in my cerebellum": cerebellummean, "rate of my learning": learningrate, "scale of my current memory attention": nowgatescale, "scale of my long term memory attention": longgatescale, "scale of my short term memory": shortgatescale, "size of the window i look at to see how often i am repeating tokens": repwin, "mean average of my nine context windows": windowsizesmean, } def makeemonotes(stat, value): feeling = none #"neutral" if stat == "loss": if "p_90" in lossstats and value >= lossstats["p_90"]: feeling = "overwhelmed" elif "p_50" in lossstats and value > lossstats["p_50"]: feeling = "pressured" elif "p_50" in lossstats and value <= lossstats["p_50"]: feeling = random.choice(["clever", "proud"]) elif "p_10" in lossstats and value <= lossstats["p_10"]: feeling = random.choice(["very clever", "like i get it"]) elif stat == "penalty for repeating myself": if "p_90" in repetitionstats and value >= repetitionstats["p_90"]: feeling = "non-verbal" elif "p_50" in repetitionstats and value > repetitionstats["p_50"]: feeling = "quiet" elif "p_50" in repetitionstats and value <= repetitionstats["p_50"]: feeling = random.choice(["talkative", "chatty"]) elif "p_10" in repetitionstats and value <= repetitionstats["p_10"]: feeling = random.choice(["conversational", "fluent"]) elif value >= 1: feeling = random.choice(["like im in a loop", "a bit stuttery", "like i cant stop these tics", "repetitive", "looping looping looping looping looping looping looping"]) elif value < 1: feeling = random.choice(["a bit more chill", "creative", "in control", "confident"]) elif stat == "latest loss delta": if value > 0.5: feeling = "like i'm struggling to focus" elif value < -0.5: feeling = "interested" elif stat == "amount of repetitive tokens i'm getting": if value > 0.7: feeling = random.choice(["stuttering", "like im repeating a lot"]) elif value > 0.5: feeling = random.choice(["overstimulated", "silly"]) elif value < 0.1: feeling = random.choice(["calm", "saying lots of new things"]) elif value < 0.25: feeling = "curious" elif stat == "temperature": if "p_90" in tempstats and value >= tempstats["p_90"]: feeling = random.choice(["chaotic", "excited"]) elif "p_50" in tempstats and value >= tempstats["p_50"]: feeling = random.choice(["playful", "happy"]) elif "p_25" in tempstats and value <= tempstats["p_25"]: feeling = "in work mode" elif stat == "number of my own tokens that i rely on": if value > 0.8: feeling = random.choice(["creative", "inventive"]) elif value < 0.2: feeling = random.choice(["tired", "copying"]) elif stat == "length of my memory": if value > 12: feeling = "pensive" elif value < 4: feeling = "mindful" elif stat == "number of tokens i got right": if value >= 30: feeling = "very proud" elif value >= 10: feeling = "proud" elif value <= 1: feeling = random.choice(["sad", "frustrated"]) elif stat == "input into my embedding layer": if value > 90: feeling = random.choice(["excited", "active", "busy"]) elif value < 60: feeling = random.choice(["tired", "shutdown", "slow"]) elif stat == "output from my embedding layer": if value > 100: feeling = random.choice(["like running", "like jumping up and down", "hyperactive"]) elif value < 60: feeling = random.choice(["sleepy", "like i need a nap", "like this is really boring"]) elif stat == "output from my neuron layer": if value > 2000: feeling = random.choice(["like i am thinking too hard", "like theres a lot going on right now", "like i am super busy"]) elif value < 900: feeling = random.choice(["calm", "collected", "asleep"]) elif stat == "output from my interneuron network": if value > 160: feeling = random.choice(["talkative", "meaningful", "like i'm finding meaning in this stuff"]) elif value < 60: feeling = random.choice(["switched off", "powered down", "slow"]) elif stat == "the output after my memory layer": if value > innoutput: feeling = random.choice(["like remembering the past", "that my memories are important", "thoughtful", "wistful"]) elif value < innoutput: feeling = random.choice(["like i should live in the now", "like what is going on around me is important", "present", "here", "awake", "aware"]) elif stat == "normalized output": if value > 125: feeling = random.choice(["like a hard worker", "over-thoughtful", "really busy"]) elif value < 100: feeling = random.choice(["tired", "asleep", "like i could pass out in my bed"]) elif stat == "logit output from my output layer": if value > 150: feeling = random.choice(["like i have a lot to say", "interested", "like i'm struggling not to interrupt", "like the words just keep coming"]) elif value < 100: feeling = random.choice(["bored", "non-verbal", "uninterested"]) elif stat == "mean weight of the windows in my cerebellum": if value > 0: feeling = random.choice(["confident", "intelligent", "calculated", "determined"]) elif value < 60: feeling = random.choice(["confused", "unsure", "uncertain", "careful", "like testing the waters"]) elif stat == "rate of my learning": if value > 0.002: feeling = random.choice(["speedy", "quick", "excited"]) elif value < 0.002: feeling = random.choice(["slow", "a bit tired out", "like i need some time to understand"]) elif stat == "scale of my current memory attention": if value >= 0.90: feeling = random.choice(["focussed", "attentive", "vigilant", "not stuck in the past"]) elif value < 0.60: feeling = random.choice(["pensive", "nostalgic", "like i need to remember something important"]) elif stat == "scale of my long term memory attention": if value >= 0.50: feeling = random.choice(["nostalgic", "thinking about what i heard before", "thoughtful", "reminiscent"]) elif value < 0.50: feeling = random.choice(["forgetful", "focussed on today", "like what i've learned before might not apply here"]) elif stat == "scale of my short term memory": if value >= 0.50: feeling = random.choice(["nostalgic", "thinking about what i heard before", "thoughtful", "reminiscent"]) elif value < 0.50: feeling = random.choice(["forgetful", "focussed on today", "like what i've learned before might not apply here"]) elif stat == "size of the window i look at to see how often i am repeating tokens": if value > 17.5: feeling = random.choice(["like i need to think before i speak", "a lil stuttery", "like i cant stop ticcing", "repetitive"]) elif value < 17: feeling = random.choice(["a bit more chill", "creative", "in control"]) elif stat == "mean average of my nine context windows": if value > 5: feeling = random.choice(["like i'm noticing more", "attentive", "stimulated", "ready"]) elif value < 5: feeling = random.choice(["internal", "shy", "narrow sighted", "scared", "like i'm really seeing the details"]) else: feeling = random.choice(["alright", "a bit lost"]) if feeling is none: feeling = "neutral" feelings.append(feeling) feelverb = random.choice(["feel", "seem", "think i feel", "definitely feel", "might feel"]) templates = [ f"i {feelverb} {feeling} because my {stat} is {value:.2f}! ", f"maybe it's because my {stat} is {value:.2f} that i {feelverb} {feeling}! ", f"i noticed my {stat} is {value:.2f}, and i {feelverb} {feeling}! ", f"when my {stat} is {value:.2f}, i {feelverb} {feeling}! ", f"it's {value:.2f} for {stat}... so i {feelverb} {feeling} about it! ", ] return random.choice(templates) chosenstats = [] attempts = 0 while len(chosenstats) < 12 and attempts < 30: stat, value = random.choice(list(self.emostats.items())) if value is not none: chosenstats.append((stat, value)) attempts += 1 if attempts >= 10 or true: print(f"emostats:{self.emostats}") for stat, value in chosenstats: babyfeels.append(makeemonotes(stat, value)) return "".join(babyfeels){ "version": "1.0", "truncation": null, "padding": null, "added_tokens": [ { "id": 0, "content": "<unk>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true } ], "normalizer": null, "pre_tokenizer": { "type": "bytelevel", "add_prefix_space": true, "trim_offsets": true, "use_regex": true }, "post_processor": null, "decoder": null, "model": { "type": "bpe", "dropout": null, "unk_token": "<unk>", "continuing_subword_prefix": null, "end_of_word_suffix": null, "fuse_unk": false, "byte_fallback": false, "ignore_merges": false, "vocab": { "ƒ°aut": 812, "aly": 1980, "ƒ°tell": 583, "ƒ°ive": 808, "ƒ°extra": 1956, "ƒ°these": 843, "body": 946, "log": 1153, "ƒ°bir": 1494, "ƒ°?": 1877, "ƒ°he": 151, "ft": 733, "ƒ°him": 425, "ƒ°days": 1009, "fer": 1813, "yy": 1159, "ƒ°game": 1148, "ƒ°chr": 1689, "ƒ°i": 73, "tw": 1203, "ƒ°needs": 1170, "ƒ°any": 273, "ƒ°read": 728, "ƒ°exist": 1684, "ƒ°charis": 413, "ƒ°hope": 611, "ping": 1122, "ƒ°ret": 1941, "ƒ°comp": 637, "ƒ°couldnt": 1910, "ha": 292, "ƒ°interest": 969, "ƒ°man": 498, "ƒ°rel": 963, "ƒ°\"": 1252, "ƒ°pres": 1412, "is": 113, "fore": 565, ":": 24, "ƒ°sub": 1671, "ene": 1804, "ƒ°aaa": 886, "ƒ°(": 388, "ƒ°j": 140, "ƒ°gross": 1970, "ƒ°fault": 1716, "ve": 121, "ƒ°.": 1503, "ƒ°head": 926, "ƒ°ac": 990, "right": 436, "f": 38, "ast": 354, "ƒ°point": 831, "ƒ°b": 82, "ƒ°bring": 1149, "ƒ°just": 164, "i": 41, "ƒ°made": 701, "ƒ°techn": 1738, "ƒ°wh": 109, "ror": 1832, "ƒ°on": 138, "na": 268, "ƒ°hi": 960, "lete": 1247, "mm": 530, "eee": 1374, "ƒ°woman": 1736, "20": 1647, "?": 29, "uu": 1151, "as": 115, "ms": 606, "ƒ°email": 1623, "ƒ°tbh": 463, "ƒ°cute": 713, "uss": 1257, "ƒ°already": 826, "ƒ°dunno": 390, "ƒ°aww": 682, "ƒ°diffic": 1464, "ƒ°type": 1606, "ƒ°health": 1263, "ƒ°im": 154, "ues": 1098, "ƒ°hard": 702, "are": 349, "ƒ°can": 227, "ƒ°o": 85, "ƒ°god": 736, "ƒ°own": 923, "ƒ°theres": 1169, "ƒ°lets": 1281, "ƒ°bitch": 1712, "ƒ°asked": 1188, "ned": 1651, "ƒ°co": 460, "ld": 167, "ƒ°try": 474, "ƒ°window": 1665, "nt": 201, "ƒ°quite": 1255, "angel": 1954, "ƒ°music": 331, "ent": 176, "ƒ°disc": 774, "ƒ°kiss": 997, "ƒ°boring": 1622, "ome": 206, "ƒ°coming": 1065, "ƒ°still": 476, "ƒ°realised": 1765, "ƒ°haven": 1523, "cri": 1423, "ƒ°mood": 1055, "izz": 1506, "ƒ°don": 187, "ƒ°first": 720, "es": 105, "uck": 973, "ƒ°school": 1094, "ƒ°friends": 904, "ed": 108, "ƒ°hy": 1690, "ƒ°interesting": 1369, "ƒ°:)": 410, "ƒ°hello": 1884, "ƒ°key": 1895, "ƒ°deal": 1495, "iny": 1761, "ause": 213, "tle": 754, "ect": 1221, "ƒ°month": 903, "ctor": 961, "ƒ°support": 947, "o": 47, "ƒ°being": 421, "ƒ°hey": 920, "ll": 101, "ƒ°your": 275, "ies": 579, "fully": 1489, "ƒ°<": 464, "ics": 1116, "ƒ°often": 1556, "ƒ°wor": 216, "ƒ°mod": 1797, "reat": 609, "ƒ°fr": 303, "ƒ°kevin": 234, "ƒ°way": 519, "ƒ°h": 84, "ƒ°sigh": 652, "k": 43, "ans": 742, "ƒ°use": 695, "ƒ°sent": 994, "ƒ°arsed": 1541, "ƒ°laptop": 1986, "ƒ°hold": 1800, "ndom": 1047, "ƒ°fin": 848, "ƒ°discuss": 1965, "ƒ°server": 1979, "ƒ°close": 1694, "ber": 524, "ƒ°ex": 261, "ense": 931, "ƒ°brain": 1318, "ƒ°cre": 1082, "ƒ°rec": 1032, "ƒ°want": 242, "ont": 630, "ƒ°ref": 1491, "ƒ°bor": 1218, "ten": 1044, "ƒ°legit": 966, "ƒ°inc": 888, "ƒ°coll": 1339, "ory": 1190, "ƒ°bye": 1515, "ger": 1923, "ƒ°girl": 945, "ƒ°what": 157, "ƒ°perfect": 1156, "ƒ°dancing": 1364, "ict": 719, "ƒ°saw": 1186, "ract": 1200, "ƒ°everything": 869, "ƒ°obviously": 1583, "ƒ°next": 983, "ƒ°swear": 1816, "ƒ°jokes": 1381, "ƒ°today": 691, "ƒ°10": 928, "ƒ°act": 393, "ƒ°plan": 854, "ure": 467, "ƒ°bus": 1136, "ƒ°uk": 1709, "ƒ°ba": 1997, "eric": 1681, "ƒ°different": 1036, "age": 556, "ord": 671, "ƒ°ask": 659, "ier": 1294, "<unk>": 0, "ied": 1202, "ind": 269, "ƒ°ri": 1427, "ƒ°else": 935, "ƒ°ext": 1278, "ƒ°two": 953, "ƒ°throw": 1888, "ƒ°ab": 226, "sw": 934, "ƒ°appre": 1271, "ƒ°agree": 1449, "ƒ°honestly": 1114, "ƒ°both": 758, "ƒ°feeling": 773, "@": 30, "ion": 202, "ƒ°was": 153, "ear": 506, "ƒ°all": 250, "ƒ°oo": 756, "ƒ°song": 1003, "le": 129, "ƒ°ago": 1231, "im": 190, "ƒ°pet": 665, "ose": 1420, "ner": 993, "ƒ°for": 149, "fect": 819, "ip": 590, "led": 1196, "ƒ°earlier": 1770, "ƒ°boom": 1625, "ƒ°keep": 782, "ƒ°cat": 653, "ƒ°happ": 485, "ƒ°mom": 1195, "ƒ°full": 1447, "ƒ°second": 1219, "ƒ°sur": 1287, "ƒ°weird": 624, "ƒ°tbf": 948, "ƒ°fix": 1838, "ƒ°scary": 1722, "ƒ°cour": 1230, "get": 634, "ƒ°eng": 822, "ƒ°told": 817, "ually": 409, "ƒ°d": 86, "ƒ°dis": 502, "ƒ°finally": 1701, "ƒ°tired": 1266, "ƒ°course": 1362, "erg": 1641, "ƒ°mult": 1936, "ƒ°cudd": 1260, "ƒ°nobody": 1305, "iv": 1142, "ƒ°talk": 534, "ably": 450, "ƒ°det": 1779, "ƒ°sat": 1593, "it": 112, "!": 1, "ƒ°geor": 557, "osed": 1434, "ƒ°ooh": 1092, "co": 727, "ƒ°some": 312, "ƒ°btw": 1301, "ƒ°war": 1282, "pe": 422, "ƒ°us": 397, "ƒ°arg": 1852, "ce": 198, "ea": 188, "po": 863, "ƒ°import": 1363, "ƒ°c": 90, "ƒ°strugg": 1851, "ƒ°train": 1284, "bb": 1553, "ƒ°pl": 254, "lect": 1443, "ƒ°ama": 1388, "ically": 664, "oice": 1370, "ƒ°enjo": 1289, "ƒ°ter": 1391, "ƒ°cop": 1667, "z": 58, "ƒ°defin": 1405, "js": 1773, "ƒ°supp": 614, "ƒ°bro": 632, "ƒ°going": 486, "ƒ°hows": 1679, "ƒ°poss": 1121, "ƒ°appreciate": 1597, "ater": 627, "ƒ°fro": 676, "]": 64, "ƒ°seems": 944, "ƒ°current": 1425, "ƒ°bo": 441, "ta": 802, "ƒ°she": 284, "ƒ°touch": 1859, "ƒ°is": 132, "ƒ°important": 1428, "ƒ°here": 529, "ƒ°says": 1332, "iet": 984, "ƒ°wed": 1705, "ƒ°reme": 715, "ƒ°weed": 1107, "ƒ°ed": 1367, "lled": 1949, "ƒ°listened": 1767, "ck": 160, "ƒ°waiting": 1963, "oo": 137, "ooo": 1938, "9": 23, "ƒ°anything": 654, "vers": 1193, "ƒ°luck": 1424, "ƒ°sucks": 1740, "ƒ°amount": 1982, "ƒ°allow": 1232, "ƒ°event": 1580, "ƒ°find": 673, "ak": 738, "my": 1592, "ƒ°bab": 600, "ƒ°theyre": 1562, "ƒ°ty": 972, "ƒ°ide": 1983, "ƒ°s": 77, "1": 15, "ƒ°colour": 1314, "mail": 1560, "llow": 1225, "lt": 996, "ious": 618, "mit": 1129, "lp": 447, "ƒ°awk": 1152, "no": 283, "ƒ°per": 672, "ƒ°med": 640, "ƒ°emot": 1214, "ƒ°definitely": 1656, "ƒ°good": 341, "ƒ°fig": 1299, "ƒ°very": 562, "ƒ°tb": 377, "ƒ°kinda": 650, "ƒ°having": 740, "ƒ°didn": 762, "ƒ°hug": 1008, "10": 1206, "ƒ°social": 1756, "ƒ°our": 794, "boomra": 1889, "ƒ°whats": 1128, "ƒ°high": 1059, "100": 1229, "ƒ°worse": 1292, "ƒ°basically": 1161, "ap": 1239, "ƒ°situ": 1385, "ƒ°played": 1389, "ƒ°mix": 1508, "ma": 177, "ƒ°place": 1015, "pm": 1426, "ƒ°men": 1826, "ps": 857, "ried": 734, "ƒ°dist": 1429, "ƒ°accidentally": 1996, "ang": 559, "ƒ°boy": 1123, "ilst": 1353, "ry": 197, "ƒ°change": 1467, "at": 88, "ƒ°over": 459, "ƒ°tomorrow": 912, "ƒ°sonic": 1608, "ƒ°until": 1210, "ight": 293, "ƒ°list": 294, "ƒ°son": 694, "ƒ°proble": 1050, "ƒ°rest": 1297, "oint": 1529, "ƒ°om": 299, "ƒ°dj": 1062, "yes": 1224, "ƒ°wait": 499, "ƒ°fre": 877, "ƒ°win": 1759, "ƒ°properly": 1887, "ƒ°nope": 1930, "ron": 1525, "ƒ°worst": 1763, "ƒ°open": 1171, "ƒ°leg": 768, "roid": 1827, "ƒ°before": 601, "oke": 988, "ily": 1018, "ƒ°ho": 986, "ƒ°communic": 1453, "ƒ°pete": 787, "ƒ°wont": 776, "air": 998, "ƒ°without": 1023, "ƒ°relation": 1645, "ƒ°internet": 1831, "~": 60, "ƒ°sp": 297, "ƒ°working": 1063, "ence": 801, "ƒ°elodie": 644, "ƒ°sweet": 1501, "ƒ°why": 183, "=": 27, "ƒ°more": 350, "ins": 1548, "ƒ°em": 646, "ƒ°fine": 760, "ƒ°hur": 1033, "ƒ°set": 1048, "book": 1714, "ness": 1226, "ather": 1311, "el": 981, "00": 566, "ƒ°30": 1781, "v": 54, "ute": 1277, "ƒ°qu": 508, "ƒ°bre": 858, "ƒ°er": 1817, "ƒ°contact": 1976, "ƒ°answer": 1078, "up": 1404, "?!": 1433, "ƒ°trying": 681, "ƒ°babe": 1475, "ƒ°se": 271, "ƒ°dun": 379, "ƒ°home": 775, "ƒ°notice": 1998, "ss": 622, "all": 150, "rr": 1517, "ƒ°were": 334, "ance": 576, "ƒ°complete": 1546, "ƒ°much": 416, "ƒ°fucks": 1358, "ep": 267, "ƒ°wom": 1192, "ity": 610, "ƒ°pa": 1422, "ƒ°honest": 896, "ƒ°ha": 125, "ƒ°dad": 739, "ƒ°forgot": 1607, "ƒ°acc": 700, "ƒ°exp": 1201, "ƒ°st": 148, "ƒ°mad": 560, "_": 32, "ƒ°re": 141, "eep": 706, "ƒ°expect": 1394, "out": 168, "ƒ°thing": 359, "ƒ°move": 1096, "ƒ°unless": 1646, "ret": 1439, "ƒ°confused": 1470, "ƒ°won": 1901, "ud": 400, "ƒ°yesterday": 1320, "so": 315, "ƒ°end": 749, "..": 220, "ƒ°kill": 1089, "ƒ°are": 212, "ƒ°alone": 1245, "ƒ°into": 588, "ƒ°space": 1437, "ƒ°tru": 1692, "ƒ°felt": 1312, "ƒ°a": 75, "ƒ°inf": 1167, "pped": 1768, "ƒ°correct": 1892, "ƒ°hor": 800, "ƒ°geepy": 918, "umber": 1799, "ƒ°whilst": 1372, "ƒ°understand": 743, "ƒ°years": 1038, "ƒ°outside": 1538, "ƒ°am": 263, "ƒ°has": 505, "ƒ°cannot": 1896, "a": 33, "ƒ°its": 207, "itch": 1127, "ƒ°low": 1570, "ƒ°old": 829, "nday": 1658, "<": 26, "ƒ°mus": 317, "≈Ä": 71, "ƒ°top": 1340, "ƒ°car": 1228, "ƒ°stuck": 1792, "ail": 1612, "ations": 1290, "'t": 221, "ƒ°decided": 1937, "ƒ°hit": 1860, "ƒ°apo": 1903, "use": 182, "ƒ°video": 1208, "ƒ°reading": 1772, "ic": 123, "u": 53, "ƒ°help": 462, "ƒ°yay": 704, "ng": 1512, "ccoon": 1584, "ƒ°random": 1143, "ings": 1617, "ƒ°under": 587, "ƒ°elect": 1715, "ase": 420, "ƒ°thanks": 685, "ƒ°tim": 339, "ƒ°take": 674, "ges": 1542, "ƒ°com": 326, "ƒ°awes": 982, "ƒ°part": 631, "ƒ°great": 933, "rent": 730, "ƒ°later": 745, "ƒ°mar": 1137, "ƒ°message": 1269, "ƒ°goog": 1633, "ual": 752, "ken": 1197, "ake": 1342, "inge": 1962, "on": 100, "cks": 1030, "ƒ°cor": 1331, "ƒ°stress": 1026, "ƒ°obv": 1306, "les": 726, "ƒ°die": 1614, "ƒ°stop": 709, "hing": 384, "ill": 200, "ƒ°start": 658, "mor": 781, "ƒ°walk": 1466, "ƒ°might": 570, "ƒ°americ": 1724, "ƒ°br": 759, "ƒ°mins": 1125, "ƒ°stra": 1380, "ƒ°sol": 1806, "4": 18, "ƒ°exam": 1931, "ƒ°thats": 318, "em": 1272, "ƒ°yet": 1105, "ƒ°did": 245, "ƒ°night": 725, "ƒ°ge": 542, "ƒ°scream": 1666, "udd": 1106, "ƒ°kind": 480, "ƒ°mor": 1280, "tu": 1951, "ƒ°sl": 809, "ƒ°word": 598, "ƒ°through": 897, "ƒ°class": 1383, "ƒ°one": 281, "ƒ°each": 1220, "ƒ°nd": 1350, "ut": 124, "ƒ°ta": 469, "ƒ°an": 162, "ash": 1073, "ƒ°wonder": 1639, "ess": 247, "ƒ°ew": 1160, "...": 423, "ƒ°ve": 1430, "ower": 1871, "ƒ°sw": 1014, "yed": 1554, "wise": 1977, "ƒ°track": 1881, "ways": 633, "ub": 1087, "ish": 478, "ƒ°late": 1309, "ƒ°anxiety": 1323, "ƒ°cry": 914, "ƒ°happen": 772, "art": 361, "outh": 1513, "ƒ°equ": 1780, "ƒ°bread": 1934, "ƒ°appointment": 1795, "ƒ°not": 155, "ƒ°seen": 1345, "ƒ°horrible": 1097, "ƒ°hear": 1177, "ƒ°froggy": 803, "ƒ°giving": 1940, "ƒ°ass": 836, "ƒ°val": 1505, "ie": 329, "ƒ°bu": 711, "ƒ°op": 873, "iz": 838, "ƒ°pop": 1519, "ack": 367, "ƒ°toilet": 1822, "ƒ°inter": 965, "ƒ°spend": 1794, "ƒ°rn": 1233, "un": 214, "ƒ°pot": 1890, "oud": 1083, "ƒ°though": 735, "ƒ°cou": 1534, "res": 309, "ƒ°stream": 596, "ƒ°form": 1985, "ty": 491, "ƒ°damn": 1348, "ƒ°which": 552, "ody": 714, "ƒ°writing": 1992, "way": 449, "ƒ°feel": 302, "ƒ°happy": 795, "ƒ°shes": 939, "ƒ°hot": 1071, "ƒ°scared": 985, "ƒ°christ": 1771, "ƒ°id": 515, "anc": 807, "ƒ°saying": 786, "ƒ°ple": 509, "ƒ°stay": 1074, "ƒ°months": 1279, "ƒ°cl": 612, "ra": 222, "ƒ°almost": 1483, "wh": 847, "(": 6, "inner": 1933, "ƒ°dec": 805, "ƒ°boo": 656, "ƒ°12": 1444, "ƒ°started": 1327, "ƒ°rap": 1858, "io": 1543, "ƒ°flat": 1662, "ƒ°wi": 1640, "ƒ°doing": 531, "ƒ°lovely": 1680, "ific": 1222, "ƒ°words": 1643, "ƒ°only": 411, "ions": 686, "ames": 1173, "alth": 1191, "ƒ°quest": 1217, "ƒ°plz": 1413, "ƒ°:'": 1409, "ƒ°bit": 479, "ƒ°myself": 667, "sy": 1611, "ƒ°lot": 577, "ick": 406, "ah": 1879, "ƒ°remember": 744, "ither": 850, "ght": 185, "ys": 550, "ƒ°rather": 1634, "ƒ°attack": 1663, "ƒ°hung": 1613, "ade": 1861, "ƒ°ke": 209, "ƒ°fun": 660, "ction": 1132, "ƒ°actual": 1828, "lo": 445, "ƒ°pretty": 769, "ide": 484, "ƒ°n": 97, "ƒ°relationship": 1669, "ƒ°dead": 1655, "ƒ°the": 94, "ƒ°ob": 1000, "ƒ°using": 1274, "ƒ°l": 89, "ƒ°best": 797, "gr": 1066, "ho": 1927, "ƒ°college": 1990, "king": 218, "ugh": 1454, "ƒ°found": 1025, "we": 696, "ƒ°cons": 958, "ƒ°by": 461, "ƒ°cause": 340, "ƒ°wants": 1175, "ƒ°they": 224, "ƒ°between": 1510, "ience": 1533, "ƒ°ent": 1117, "ƒ°living": 1809, "ents": 1516, "ƒ°contin": 1841, "self": 503, "ge": 241, "ƒ°fucking": 604, "ently": 1478, "ot": 174, "ke": 117, "ƒ°kid": 938, "oup": 1713, "'re": 291, "mme": 1814, "ƒ°isnt": 1150, "haha": 1753, "ƒ°rem": 1638, "s": 51, "ƒ°sc": 418, "ig": 419, "ided": 1627, "ƒ°parent": 1442, "one": 306, "ƒ°who": 375, "rect": 1530, "ƒ°manag": 1837, "ƒ°count": 1488, "ƒ°rude": 1172, "ƒ°sit": 866, "ns": 1748, "ƒ°wanted": 832, "leep": 483, "ƒ°y": 93, "good night": 968, "aring": 1566, "ƒ°sense": 1182, "jo": 1189, "ƒ°fact": 1626, "sh": 692, "ƒ°wow": 1019, "ƒ°angle": 1067, "ye": 1734, "ƒ°now": 255, "ƒ°isn": 1319, "ming": 1436, "ately": 1706, "ƒ°said": 512, "ƒ°anyone": 1328, "ƒ°sm": 549, "ƒ°nah": 766, "ƒ°sometimes": 1075, "inking": 816, "ƒ°than": 330, "ƒ°free": 1035, "lling": 1371, "ƒ°sim": 1064, "ƒ°poop": 1587, "ƒ°call": 547, "med": 1039, "ƒ°sch": 927, "ƒ°awesome": 1013, "ƒ°wasnt": 1238, "ƒ°shut": 1939, "ƒ°anno": 788, "op": 203, "ƒ°ill": 619, "p": 48, "sed": 1061, "ugs": 1338, "ƒ°defo": 1865, "¬¶": 62, "ere": 368, "ƒ°hours": 845, "ƒ°things": 567, "eah": 253, "per": 443, "'ve": 475, "ƒ°you": 104, "ac": 1086, "df": 1596, "ƒ°gir": 780, "ƒ°stand": 1730, "ƒ°issues": 1502, "ratewaifu": 1531, "ble": 540, "ƒ°boof": 1498, "ƒ°incl": 1581, "ƒ°": 66, "ƒ°wanna": 433, "ƒ°send": 867, "ƒ°address": 1603, "ƒ°lots": 1618, "eed": 1854, "xt": 684, "gg": 338, "ƒ°tu": 1678, "ƒ°normal": 1704, "not": 1256, "ƒ°des": 894, "ƒ°could": 429, "ƒ°sure": 608, "pecially": 1815, "ƒ°ar": 451, "ƒ°3": 545, "b": 34, "ink": 239, "ath": 906, "ru": 1967, "ƒ°reg": 1258, "ƒ°ever": 643, "ƒ°oof": 1468, "red": 879, "ffect": 1882, "aa": 621, "ƒ°aw": 343, "ich": 514, "ƒ°sort": 1628, "ƒ°min": 635, "ced": 1696, "ƒ°tech": 1095, "pr": 1752, "ƒ°finish": 1762, "ƒ°forever": 1782, "ƒ°never": 538, "ƒ°norm": 1267, "ƒ°okay": 940, "ƒ°may": 456, "q": 49, "ƒ°eas": 1134, "ƒ°forg": 1490, "fort": 1672, "bi": 1168, "ƒ°commun": 1240, "ƒ°other": 553, "ƒ°bed": 806, "ƒ°egg": 1484, "ƒ°true": 661, "ƒ°rep": 1076, "ƒ°dra": 1461, "ƒ°panic": 1589, "ised": 971, "ƒ°adhd": 1843, ";": 25, "ƒ°apparent": 1286, "ƒ°wo": 1315, "irst": 708, "ci": 536, "ƒ°ag": 376, "ƒ°wa": 380, "ƒ°liter": 648, "ƒ°another": 1204, "ƒ°20": 1146, "ƒ°*": 528, "ƒ°supposed": 1545, "li": 783, "ƒ°picture": 1265, "ise": 617, "ƒ°def": 917, "ƒ°watching": 1485, "ƒ°z": 1588, "act": 554, "ƒ°long": 657, "ƒ°hop": 555, "id": 145, "du": 1261, "ƒ°asleep": 1731, "ƒ°expl": 942, "ƒ°bad": 471, "al": 146, "ƒ°imp": 804, "ƒ°similar": 1902, "be": 453, "ƒ°boi": 890, "ƒ°step": 1920, "ƒ°world": 1544, "w": 55, "ƒ°v": 280, "ƒ°sor": 314, "ƒ°creep": 1594, "ƒ°there": 398, "ts": 885, "ƒ°vis": 1448, "ksks": 1774, "ƒ°pro": 257, "ƒ°sh": 152, "ire": 541, "ƒ°far": 1396, "ƒ°anxious": 1630, "ƒ°unt": 1139, "ƒ°:(": 427, "ƒ°drink": 1719, "ple": 892, "ƒ°see": 344, "ƒ°hair": 1275, "ƒ°straight": 1853, "ƒ°ten": 1386, "ƒ°knew": 1636, "ƒ°di": 1776, "ƒ°iss": 1090, "ƒ°self": 1180, "ƒ°me": 118, "ƒ°che": 677, "ƒ°-": 313, "ƒ°talking": 792, "'": 5, "ƒ°al": 210, "ƒ°waht": 1234, "ƒ°asking": 1335, "ward": 815, "ƒ°uni": 1027, "read": 638, "rate": 1333, "ƒ°bas": 835, "nd": 95, "us": 110, "ver": 163, "ross": 1237, "ƒ°mean": 323, "cially": 1518, "ult": 626, "5": 19, "ƒ°haha": 363, "ƒ°ch": 196, "kes": 525, "ƒ°arent": 1915, "ƒ°yes": 322, "ons": 1223, "ƒ°telling": 1539, "|": 59, "ar": 130, "ew": 1361, "ƒ°pict": 1001, "ƒ°tra": 666, "ƒ°requ": 1621, "se": 180, "ƒ°pan": 1249, "ved": 820, "ƒ°parents": 1532, "ƒ°hum": 1216, "arent": 851, "vent": 731, "mas": 1864, "ƒ°sin": 1212, "ƒ°chang": 1609, "ƒ°autism": 1520, "ƒ°ner": 1869, "lly": 962, "ƒ°hand": 1184, "ƒ°lea": 687, "ƒ°computer": 1379, "ƒ°able": 1031, "ƒ°getting": 698, "its": 1418, "ƒ°really": 296, "arch": 1808, "ead": 902, "ƒ°such": 1077, "ƒ°mum": 639, "'s": 217, "ƒ°heard": 1754, "ƒ°big": 952, "ƒ°cuddles": 1925, "ƒ°r": 258, "ƒ°goo": 1355, "ƒ°int": 417, "ƒ°te": 355, "ƒ°je": 870, "ƒ°tonight": 1522, "pid": 1555, "ƒ°pre": 523, "ƒ°couple": 1867, "m": 45, "ƒ°bra": 1053, "ƒ°sn": 1943, "ƒ°least": 784, "utes": 1578, "ƒ°half": 1213, "ƒ°mine": 837, "ƒ°probably": 472, "th": 211, "uring": 1254, "ƒ°annoying": 1028, "ƒ°post": 1178, "jk": 1565, "il": 300, "ƒ°glad": 1248, "re": 83, "ƒ°eating": 1918, "ƒ°boyfriend": 1743, "ƒ°house": 668, "ƒ°plym": 1928, "ƒ°record": 1995, "ƒ°this": 194, "ƒ°forget": 1056, "ƒ°light": 1373, "ƒ°dont": 248, "ƒ°stud": 1165, "%": 3, "ƒ°go": 232, "ii": 1435, "ning": 690, "ƒ°vape": 1747, "mber": 683, "enc": 1325, "ƒ°buy": 1472, "et": 133, "set": 1183, "h": 40, "ƒ°sorry": 351, "yle": 1673, "ƒ°conc": 1790, "ƒ°mu": 407, "ƒ°door": 1351, "ƒ°pizza": 1922, "ƒ°bored": 1037, "ƒ°play": 424, "ƒ°new": 721, "itely": 1582, "ƒ°fall": 1835, "ct": 175, "ƒ°leave": 908, "cess": 1537, "ƒ°case": 1737, "[": 63, "ƒ°gen": 833, "fair": 1805, "ƒ°f": 91, "ƒ°meant": 929, "ƒ°wind": 1509, "pl": 548, "ƒ°bag": 1783, "ƒ°whole": 1052, "ƒ°online": 1702, "en": 102, "iety": 1276, "der": 1133, "ƒ°pers": 620, "ative": 1552, "stand": 712, "kr": 1054, "ƒ°7": 1163, "ƒ°follow": 1559, "atter": 1599, "day": 494, "ƒ°har": 593, "ƒ°chill": 1840, "ƒ°those": 1012, "cept": 955, "ƒ°also": 345, "c": 35, "ƒ°and": 111, "gether": 1101, "ƒ°sign": 1479, "ive": 356, "ƒ°1": 337, "ƒ°genuine": 1873, "umb": 987, "os": 573, "ƒ°fur": 1728, "ƒ°fuck": 259, "ƒ°with": 193, "ƒ°ment": 924, "ƒ°cool": 537, "ƒ°colle": 1807, "ƒ°clo": 1112, "ƒ°le": 324, "ƒ°care": 821, "ter": 262, "friend": 1610, "unch": 825, "ism": 1085, "cc": 489, "ƒ°sleep": 520, "ƒ°wr": 602, "ƒ°embar": 1846, "ƒ°like": 165, "0": 14, "ƒ°th": 78, "ƒ°tou": 1631, "fic": 1346, "ƒ°ready": 1862, "ject": 1401, "lic": 1568, "ƒ°pur": 1270, "ƒ°effect": 1919, "am": 169, "end": 366, "ƒ°look": 544, "ƒ°alright": 1268, "ƒ°tast": 1872, "ƒ°week": 1046, "ƒ°awake": 1961, "ing": 87, "he": 1395, "ƒ°it": 107, "ƒ°mess": 723, "ƒ°str": 915, "ƒ°time": 386, "ƒ°cu": 1833, "ƒ°instead": 1120, "ƒ°danc": 1310, "¬£": 61, "‚Ç¨": 747, "app": 1457, "cond": 1185, "ƒ°trans": 1492, "ƒ°doesnt": 846, "ƒ°jo": 513, "???": 868, "ite": 328, "ƒ°spec": 1438, "ust": 156, "ƒ°many": 1049, "ƒ°cam": 1802, "ƒ°ste": 1810, "ƒ°upset": 1375, "ƒ°lear": 1403, "ht": 1104, "ƒ°meds": 1158, "ƒ°mental": 1316, "ones": 1524, "ƒ°gu": 442, "ƒ°jesus": 1642, "ƒ°trust": 1698, "ƒ°cut": 586, "ƒ°cle": 855, "ƒ°proper": 1060, "ƒ°comm": 992, "vo": 1550, "ƒ°question": 1885, "ƒ°hopefully": 1764, "antly": 1921, "ach": 729, "unk": 1452, "ities": 1907, "ƒ°my": 139, "est": 289, "ens": 670, "ats": 1760, "vin": 233, "ƒ°bought": 1886, "zing": 1298, "ƒ°spea": 779, "ƒ°pay": 1006, "able": 578, "rop": 1344, "ƒ°ins": 964, "ƒ°every": 412, "ƒ°pret": 697, "ƒ°prof": 1981, "ƒ°please": 521, "ointment": 1735, "ƒ°somet": 446, "ƒ°x": 265, "ange": 755, "ƒ°bot": 623, "ƒ°swe": 1330, "ƒ°reply": 1499, "ƒ°tr": 401, "ƒ°accept": 1974, "ƒ°tried": 1387, "ƒ°had": 389, "ƒ°autistic": 1573, "+": 9, "ƒ°write": 1720, "ƒ°prom": 1824, "ress": 466, "ƒ°4": 675, "ƒ°ma": 238, "ƒ°does": 373, "ƒ°super": 881, "ƒ°:/": 1070, "ne": 834, "r": 50, "ƒ°ang": 767, ">": 28, "ƒ°yo": 1777, "ƒ°hate": 282, "3": 17, "night": 1207, "ƒ°mind": 1119, "ƒ°mention": 1955, "ƒ°get": 231, "ƒ°engl": 980, "$": 65, "ƒ°stu": 497, "ƒ°ima": 1198, "ƒ°5": 718, "ƒ°ohh": 251, "ƒ°press": 1880, "ƒ°e": 119, ".": 12, "and": 432, "ƒ°vi": 1648, "ƒ°un": 402, "ƒ°soo": 1749, "ƒ°quick": 1825, "ƒ°star": 1991, "ul": 321, "ƒ°totally": 1989, "ƒ°gener": 1417, "ƒ°contro": 1708, "ƒ°laugh": 1801, "ƒ°money": 970, "ƒ°sound": 518, "ƒ°taking": 1496, "ƒ°mic": 1855, "ƒ°do": 128, "ox": 1215, "ould": 208, "ƒ°;": 1209, "ƒ°lit": 470, "ƒ°feels": 1359, "ƒ°m": 80, "2": 16, "iss": 481, "ƒ°ban": 1397, "ƒ°ad": 473, "ering": 1324, "ƒ°figure": 1415, "ƒ°face": 889, "ƒ°ca": 793, "ƒ°them": 371, "lym": 1878, "ƒ°miss": 757, "ƒ°xxx": 1926, "ƒ°,": 1329, "ƒ°toile": 1785, "ƒ°kept": 1975, "ƒ°nothing": 871, "ired": 979, "ƒ°'": 288, "ƒ°once": 1099, "ƒ°looks": 1022, "ƒ°happened": 1313, "ƒ°inv": 1291, "ƒ°slight": 1677, "ƒ°called": 1144, "ƒ°fe": 595, "ild": 874, "ƒ°messag": 1445, "ch": 173, "ƒ°adv": 1551, "ƒ°at": 192, "ƒ°consid": 1378, "ƒ°char": 372, "ƒ°ev": 1757, "ign": 921, "ung": 1382, "ny": 1020, "ƒ°anx": 922, "ƒ°geep": 900, "ƒ°while": 1451, "ƒ°gone": 1798, "ƒ°ace": 830, "ƒ°shit": 468, "ƒ°actually": 493, "ƒ°sudd": 1906, "ƒ°inside": 1778, "!!!": 841, "ƒ°fl": 689, "ƒ°creat": 1834, "ƒ°know": 244, "ue": 430, "ƒ°will": 310, "ug": 605, "ƒ°either": 1021, "ƒ°when": 287, "ƒ°need": 298, "ƒ°down": 649, "ures": 1440, "000": 1710, "hood": 70, "ƒ°reason": 770, "ee": 172, "uine": 1857, "ƒ°repl": 1950, "ween": 1459, "ƒ°after": 616, "ƒ°ign": 1487, "ƒ°mo": 435, "ƒ°inst": 814, "ƒ°away": 951, "ƒ°means": 810, "ƒ°wasn": 1288, "ƒ°kevins": 853, "thers": 1897, "ƒ°safe": 1458, "ird": 592, "att": 1868, "ound": 325, "ƒ°enjoy": 1742, "iced": 1973, "ape": 1295, "boom": 1844, "ƒ°dep": 995, "ible": 936, "ƒ°wouldnt": 1637, "mon": 1644, "ƒ°hang": 1650, "pp": 223, "iew": 1480, "ave": 1368, "ƒ°off": 496, "ƒ°back": 504, "ated": 707, "ƒ°her": 364, "ow": 135, "ƒ°as": 204, "ƒ°speak": 999, "ƒ°moment": 1244, "ƒ°dance": 1253, "ƒ°2": 404, "tho": 1577, "an": 99, "gging": 1788, "ƒ°mon": 717, "ƒ°lunch": 937, "ving": 431, "vel": 1205, "ƒ°last": 813, "less": 876, "ƒ°must": 1227, "ƒ°yeah": 256, "ƒ°games": 1407, "ƒ°eff": 1527, "ƒ°bel": 1093, "most": 1414, "ƒ°black": 1745, "ƒ°clean": 1377, "ƒ°hmm": 974, "ƒ°pain": 1109, "ƒ°t": 74, "ƒ°playing": 748, "ƒ°accident": 1463, "row": 569, "ƒ°ones": 1293, "ƒ°well": 378, ")": 7, "erest": 959, "hd": 1493, "ƒ°most": 849, "ƒ°u": 116, "ƒ°food": 763, "ored": 916, "ard": 457, "orm": 746, "to": 1558, "ƒ°idea": 907, "ƒ°guys": 1131, "ƒ°gonna": 546, "ree": 589, "ating": 722, "ƒ°process": 1875, "iding": 1958, "ƒ°allowed": 1486, "ƒ°ro": 580, "ƒ°name": 930, "ƒ°kn": 236, "ƒ°dumb": 1242, "ƒ°amazing": 1465, "ƒ°fail": 1476, "ƒ°entire": 1659, "ked": 1081, "ling": 1181, "ƒ°ser": 778, "ƒ°es": 1211, "wa": 1164, "oney": 880, "ƒ°english": 1243, "ƒ°around": 932, "ol": 641, "ƒ°needed": 1563, "ƒ°cheese": 1408, "ped": 1624, "ain": 352, "ƒ°pack": 1944, "av": 1966, "ƒ°slow": 1959, "ƒ°same": 490, "ƒ°to": 96, "ƒ°tho": 374, "ƒ°let": 574, "ƒ°pr": 703, "ƒ°seem": 1241, "ƒ°gotta": 1041, "ƒ°early": 1988, "ƒ°xd": 342, "ƒ°gay": 1115, "wu": 1421, "ƒ°sexy": 1567, "mo": 954, "ƒ°6": 978, "ƒ°fren": 1347, "com": 1766, "ƒ°discord": 1140, "ƒ°main": 1072, "ƒ°cr": 840, "ƒ°check": 1157, "ƒ°boomboomraccoon": 1909, "ensive": 1693, "ƒ°loud": 1870, "ƒ°usually": 1507, "gin": 1697, "mpt": 1775, "ƒ°we": 136, "ƒ°sy": 1080, "ding": 1733, "ƒ°put": 741, "ƒ°run": 1432, "ition": 1574, "ƒ°watch": 827, "ƒ°took": 1598, "ready": 818, "ƒ°gra": 1460, "ever": 1473, "ƒ°res": 891, "eel": 295, "ƒ°happening": 1741, "ƒ°hell": 1069, "ƒ°yours": 1819, "bsite": 568, "ƒ°favour": 1477, "als": 1135, "ƒ°used": 864, "ƒ°mid": 1416, "iting": 1262, "ƒ°depress": 1575, "ƒ°france": 1891, "thing": 492, "d": 36, "ƒ°morning": 1481, "ƒ°side": 1932, "/": 13, "ƒ°phone": 790, "\"": 2, "g": 39, "ƒ°fit": 1994, "ƒ°fo": 949, "ƒ°poo": 1304, "ƒ°nice": 551, "ƒ°ph": 584, "oon": 1365, "ƒ°or": 215, "n": 46, "ƒ°tw": 571, "ƒ°body": 1602, "ƒ°shop": 811, "ƒ°right": 533, "ƒ°gr": 925, "-": 11, "ƒ°suck": 1695, "ƒ°coo": 1811, "other": 1045, "ƒ°problem": 1354, "cial": 1154, "ƒ°huge": 1984, "ƒ°drunk": 1751, "ƒ°literally": 679, "ƒ°speaking": 1725, "ƒ°life": 543, "cked": 1632, "'ll": 495, "ƒ°funny": 1474, "me": 452, "gram": 1849, "ƒ°pass": 1668, "hh": 240, "ƒ°his": 636, "ƒ°li": 142, "or": 103, "ƒ°from": 385, "ort": 437, "ace": 669, "ƒ°wan": 405, "7": 21, "ƒ°ye": 1746, "ƒ°chat": 1179, "ƒ°show": 771, "ƒ°gets": 1707, "ƒ°loo": 311, "ƒ°believe": 1406, "ial": 1572, "ƒ°where": 607, "ƒ°done": 798, "used": 1043, "ese": 1088, "ƒ°would": 382, "ƒ°add": 844, "ml": 1945, "ƒ°meh": 1303, "ode": 1441, "ption": 1579, "ush": 1236, "ƒ°smink": 1504, "st": 189, "ƒ°de": 304, "ƒ°cra": 977, "ag": 454, "ƒ°again": 564, "ƒ°till": 1536, "ƒ°scre": 1711, "ƒ°af": 572, "ƒ°make": 414, "ƒ°listening": 399, "her": 516, "ƒ°emb": 1540, "ƒ°gave": 1755, "*": 8, "ƒ°few": 1337, "ƒ°people": 396, ",": 10, "ƒ°sake": 1360, "ine": 333, "ƒ°vide": 1068, "ert": 1400, "ƒ°listen": 319, "ket": 1786, "ƒ°spam": 1399, "&": 4, "ƒ°huh": 1971, "ƒ°explain": 1126, "icul": 1595, "ƒ°ear": 1176, "ƒ°something": 532, "ƒ°makes": 751, "ers": 346, "ƒ°especially": 1866, "ƒ°tomor": 898, "ƒ°possible": 1500, "ƒ°smo": 1924, "ƒ°bi": 1194, "ƒ°but": 143, "riend": 487, "ou": 81, "very": 285, "ƒ°come": 511, "iously": 1007, "ƒ°fore": 1471, "sol": 1916, "ƒ°:'(": 1446, "ƒ°fu": 237, "ƒ°worth": 1812, "ƒ°year": 628, "ir": 184, "atch": 699, "ƒ°better": 662, "ƒ°out": 277, "ƒ°doctor": 1057, "ƒ°sad": 753, "ƒ°if": 219, "ƒ°idk": 68, "ƒ°hour": 655, "ƒ°butt": 1431, "gan": 1758, "ƒ°sec": 1456, "ƒ°bec": 286, "ƒ°sounds": 663, "ark": 1750, "ƒ°psych": 1972, "ilar": 1564, "ƒ°since": 1264, "ux": 1987, "esus": 1604, "ƒ°ages": 1549, "ƒ°scr": 1820, "ƒ°sudden": 1913, "igh": 510, "illy": 1948, "ƒ°favourite": 1586, "ƒ°wish": 1029, "ƒ°friend": 558, "ƒ°dick": 899, "ƒ°apparently": 1307, "ƒ°yourself": 1352, "ƒ°wal": 1040, "ƒ°prob": 395, "ƒ°boomboomra": 1900, "ƒ°red": 1935, "bs": 444, "ƒ°meow": 1455, "ƒ°diffe": 905, "ant": 171, "ƒ°pe": 357, "ary": 852, "ven": 264, "ƒ°guy": 1124, "oc": 1410, "ugg": 1343, "aking": 1914, "ƒ°be": 120, "ason": 737, "if": 365, "ƒ°always": 647, "ƒ°wee": 705, "ful": 750, "ƒ°love": 276, "ƒ°minutes": 1660, "side": 1349, "ally": 170, "istic": 1300, "ƒ°ikr": 1091, "ƒ°omg": 348, "j": 42, "ƒ°break": 1103, "ƒ°exper": 1246, "ian": 1462, "ƒ°crying": 1591, "ƒ°test": 1993, "ƒ°slightly": 1784, "ƒ°difficult": 1547, "ate": 205, "ƒ°kit": 1654, "ƒ°child": 1571, "ur": 274, "ƒ°less": 911, "line": 1108, "ces": 1863, "nder": 500, "in": 79, "ile": 710, "ƒ°pu": 1729, "ƒ°baby": 785, "ƒ°gre": 1615, "ƒ°maybe": 585, "ƒ°les": 1899, "ƒ°wrong": 893, "!!": 392, "ream": 488, "aut": 1883, "ƒ°program": 1947, "ƒ°imag": 1683, "ƒ°youre": 1402, "ƒ°guess": 764, "ƒ°awkward": 1174, "ation": 394, "ƒ°:": 181, "ƒ°omfg": 1326, "ƒ°meet": 1147, "ƒ°su": 225, "ood": 246, "ƒ°kids": 1744, "lier": 1723, "ƒ°mass": 1769, "ƒ°vib": 1803, "ƒ°con": 332, "ought": 526, "ƒ°worry": 967, "ƒ°give": 799, "ƒ°app": 501, "ass": 522, "ƒ°lie": 1682, "ƒ°ver": 1787, "ƒ°everyone": 887, "ƒ°fa": 308, "ƒ°fair": 581, "ƒ°etc": 1100, "ƒ°rly": 1166, "ƒ°indeed": 1960, "ƒ°plus": 1830, "ƒ°job": 975, "ƒ°pri": 1526, "ƒ°during": 1585, "ƒ°na": 615, "ƒ°tot": 1670, "ƒ°15": 1818, "ƒ°xox": 1674, "ist": 229, "ƒ°together": 1113, "ƒ°el": 458, "ates": 1283, "ƒ°cur": 1084, "ared": 861, "ool": 438, "ƒ°piss": 1911, "ƒ°pic": 1652, "ƒ°;)": 1285, "ƒ°elod": 594, "ƒ°think": 266, "ƒ°bet": 539, "ƒ°sexual": 1898, "omet": 439, "using": 1356, "ƒ°spe": 910, "ook": 943, "ƒ°ok": 270, "ƒ°say": 335, "ƒ°got": 370, "dd": 824, "ƒ°art": 1336, "ting": 408, "ƒ°exc": 1051, "ƒ°wouldn": 1717, "ore": 228, "ƒ°load": 1664, "ƒ°seeing": 1732, "ƒ°suggest": 1357, "ƒ°effort": 1616, "ƒ°att": 883, "ƒ°dif": 693, "ƒ°work": 426, "ying": 629, "ƒ°gon": 527, "ident": 1110, "ƒ°sing": 1569, "ƒ°works": 1469, "pt": 440, "x": 56, "ƒ°hurt": 1411, "ƒ°white": 1845, "ƒ°french": 1450, "ƒ°website": 575, "ional": 1419, "ƒ°so": 166, "om": 126, "ƒ°ra": 563, "ith": 179, "ƒ°>": 1334, "ƒ°cant": 465, "ves": 842, "ƒ°sk": 1058, "ƒ°k": 161, "ƒ°belie": 1250, "ƒ°about": 249, "ƒ°their": 678, "ƒ°messages": 1957, "ƒ°pick": 1657, "ƒ°dream": 1969, "ame": 301, "ƒ°et": 828, "ƒ°leaving": 1842, "zy": 1721, "ough": 381, "ƒ°hon": 856, "ab": 235, "fg": 1042, "ƒ°boobs": 1952, "y": 57, "ƒ°par": 1141, "od": 307, "ong": 369, "ƒ°compl": 1726, "ƒ°tea": 1829, "gh": 147, "ƒ°tri": 1321, "ƒ°brb": 724, "ƒ°mainly": 1393, "ƒ°cold": 1727, "ƒ°uwu": 1605, "ice": 336, "ƒ°comment": 1590, "ƒ°seriously": 1699, "te": 613, "ƒ°dog": 1392, "ifu": 1482, "ous": 415, "ƒ°room": 976, "ks": 327, "ƒ°text": 1514, "ƒ°excited": 1917, "ƒ°8": 1130, "ƒ°comput": 1259, "l": 44, "ƒ°that": 127, "ƒ°of": 144, "ƒ°live": 991, "ƒ°should": 434, "ƒ°human": 1685, "ƒ°tal": 403, "ƒ°valid": 1856, "ƒ°bar": 1908, "ƒ°pls": 69, "lf": 358, "ƒ°went": 1011, "ƒ°become": 1942, "^": 31, "ph": 1017, "ouse": 645, "ƒ°umm": 957, "ƒ°worried": 1273, "ƒ°9": 1118, "ia": 1296, "ƒ°tv": 1535, "ƒ°ind": 989, "ƒ°little": 882, "bo": 859, "ƒ°real": 625, "ƒ°cal": 1521, "`": 67, "ƒ°water": 1675, "ƒ°exactly": 956, "ƒ°fam": 1398, "ƒ°day": 455, "wn": 477, "ƒ°thought": 582, "waifu": 1528, "ƒ°rant": 1687, "ƒ°cook": 1964, "ƒ°dri": 1893, "ƒ°gi": 1016, "ƒ°psy": 1894, "ted": 1079, "ƒ°times": 1010, "ƒ°sometim": 1024, "ƒ°dire": 1791, "rible": 1005, "ƒ°ne": 191, "ren": 1384, "ship": 1376, "mao": 195, "ƒ°moving": 1691, "ƒ°no": 114, "ƒ°eat": 878, "ƒ°didnt": 591, "airs": 1848, "ƒ°person": 642, "6": 20, "arly": 860, "e": 37, "ly": 134, "ƒ°even": 305, "ited": 1322, "ertain": 1847, "ƒ°sa": 316, "fu": 1162, "ƒ°la": 761, "ƒ°enough": 777, "ri": 178, "ƒ°too": 278, "ƒ°thinking": 913, "ered": 1102, "ƒ°been": 362, "ƒ°because": 320, "ƒ°w": 76, "8": 22, "ical": 1034, "ƒ°turn": 1302, "ƒ°non": 1145, "ƒ°exact": 875, "ƒ°frogg": 716, "ƒ°proud": 1953, "ƒ°foc": 1904, "ff": 428, "ƒ°cos": 72, "ƒ°thr": 862, "qu": 360, "pect": 895, "ƒ°left": 1002, "ƒ°du": 1929, "angle": 1629, "ƒ°george": 603, "vour": 1366, "!?": 688, "ƒ°in": 131, "er": 92, "ices": 1823, "'d": 482, "ƒ°bl": 597, "ggest": 1235, "where": 1789, "ƒ°dam": 919, "ƒ°birth": 1557, "ƒ°g": 106, "ƒ°sick": 1251, "ment": 517, "int": 535, "terday": 1317, "ƒ°shout": 1905, "ƒ°order": 1390, "ƒ°except": 1511, "room": 1836, "ƒ°pee": 1561, "ƒ°p": 98, "ƒ°stuff": 599, "ib": 865, "ƒ°small": 1601, "ƒ°gl": 909, "ƒ°havent": 111, "ƒ°stupid": 1635, "ƒ°anyway": 791, "ƒ°xx": 1341, "ƒ°thank": 765, "ƒ°conf": 789, "hs": 1796, "ƒ°appreci": 1308, "t": 52, "ƒ°hu": 901, "own": 1874, "ƒ°making": 884, "ƒ°card": 1912, "ro": 158, "ƒ°ur": 347, "ther": 507, "ƒ°dr": 651, "ƒ°three": 1688, "ƒ°pizz": 1850, "ƒ°lo": 230, "ƒ°how": 260, "ay": 122, "ƒ°smoke": 1576, "ƒ°currently": 1821, "ƒ°twice": 1155, "'m": 252, "ƒ°past": 1138, "ƒ°weeks": 1620, "our": 290, "ptop": 1968, "ad": 159, "ƒ°intern": 1649, "ƒ°situation": 1686, "ƒ°up": 243, "??": 448, "ƒ°hes": 839, "ƒ°meme": 1661, "ƒ°have": 186, "ount": 872, "um": 272, "ƒ°suppose": 1703, "ƒ°peop": 387, "ƒ°answ": 950, "ƒ°11": 1676, "ƒ°col": 1199, "ƒ°doesn": 1004, "ibly": 1718, "nds": 1793, "ƒ°cont": 732, "ƒ°sex": 680, "ƒ°phys": 1839, "ƒ°en": 383, "ƒ°then": 353, "ƒ°18": 1978, "ising": 1619, "ix": 823, "oth": 1700, "ƒ°completely": 1946, "ƒ°lmao": 199, "ƒ°blue": 1876, "ude": 796, "ƒ°met": 1653, "ƒ°po": 391, "fe": 279, "ƒ°looking": 561, "urn": 941, "ƒ°came": 1187, "ƒ°flo": 1739, "iday": 1600, "ility": 1497, "#": 1999, "\\": 2000, "{": 2001, "}": 2002, "¬°": 2003, "¬¢": 2004, "¬§": 2005, "¬•": 2006, "¬ß": 2007, "¬®": 2008, "¬©": 2009, "¬™": 2010, "¬´": 2011, "¬¨": 2012, "¬Æ": 2013, "¬Ø": 2014, "¬∞": 2015, "¬±": 2016, "¬≤": 2017, "¬≥": 2018, "¬¥": 2019, "¬µ": 2020, "¬∂": 2021, "": 2022, "¬∏": 2023, "¬π": 2024, "¬∫": 2025, "¬ª": 2026, "¬º": 2027, "¬Ω": 2028, "¬æ": 2029, "¬ø": 2030, "√¢": 2031, "√£": 2032, "√§": 2033, "√•": 2034, "√¶": 2035, "√ß": 2036, "√©": 2037, "√™": 2038, "√´": 2039, "√¨": 2040, "√Æ": 2041, "√Ø": 2042, "√∞": 2043, "√±": 2044, "√¥": 2045, "√ó": 2046, "√∏": 2047, "√π": 2048, "√†": 2049, "√°": 2050, "√¢": 2051, "√£": 2052, "√•": 2053, "√¶": 2054, "√®": 2055, "√©": 2056, "√™": 2057, "√´": 2058, "√¨": 2059, "√≠": 2060, "√Ø": 2061, "√∞": 2062, "√≥": 2063, "ƒ£": 2064, "ƒ£": 2065, "ƒ•": 2066, "ƒ•": 2067, "ƒß": 2068, "ƒß": 2069, "ƒ©": 2070, "ƒ©": 2071, "ƒ´": 2072, "ƒ´": 2073, "ƒ≠": 2074, "ƒ≠": 2075, "ƒØ": 2076, "ƒØ": 2077, "iÃá": 2078, "ƒ±": 2079, "ƒ≥": 2080, "ƒ≥": 2081, "ƒµ": 2082, "ƒµ": 2083, "ƒ∑": 2084, "ƒ∑": 2085, "ƒ∏": 2086, "ƒ∫": 2087, "ƒ∫": 2088, "ƒº": 2089, "ƒº": 2090, "ƒæ": 2091, "ƒæ": 2092, "≈Ä": 2093, "≈Ç": 2094, "≈Ç": 2095, "≈Ñ": 2096, "hat": 2097, "ƒ°cha": 2098, "ris": 2099, "een": 2100, "ƒ°eve": 2101, "ƒ°whe": 2102, "ƒ°lis": 2103, "ƒ°fee": 2104, "ca": 2105, "ƒ°lol": 2106, "ƒ°oh": 2107, "ime": 2108, "ƒ°q": 2109, "han": 2110, "ays": 2111, "ƒ°vo": 2112, "ƒ°beca": 2113, "rain": 2114, "ƒ°thou": 2115, "ƒ°voice": 2116, "ƒ°lma": 2117, "ds": 2118, "arn": 2119, "oy": 2120, "ƒ°hel": 2121, "ww": 2122, "ƒ°learn": 2123, "orge": 2124, "iend": 2125, "√¢ƒ£": 2126, "mb": 2127, "tt": 2128, "ƒ°wanting": 2129, "ƒ°draw": 2130, "cking": 2131, "gle": 2132, "ƒ°sca": 2133, "ƒ°smoking": 2134, "tera": 2135, ":/": 2136, "che": 2137, "imes": 2138, "ady": 2139, "ƒ°htt": 2140, "ƒ°litera": 2141, "ƒ°needing": 2142, "ffe": 2143, "og": 2144, "ƒ°tur": 2145, "tf": 2146, "ƒ°underst": 2147, "ƒ°hearing": 2148, "√¢ƒ£ƒº": 2149, "ƒ°sha": 2150, "ƒ°learning": 2151, "fa": 2152, "ƒ°https": 2153, "ƒ°soon": 2154, "tty": 2155, "√¢¬£": 2156, "nce": 2157, "ƒ°hahaha": 2158, "ƒ°wtf": 2159, "ƒ°ann": 2160, "ƒ°touching": 2161, "ƒ°someone": 2162, "ƒ°=": 2163, "ƒ°cats": 2164, "ƒ°√¢¬£": 2165, "uter": 2166, "ƒ°ga": 2167, "ƒ°annoy": 2168, "ƒ°picking": 2169, "ƒ°inte": 2170, "tes": 2171, "www": 2172, "ƒ°sen": 2173, "ƒ°drawing": 2174, "eet": 2175, "de": 2176, "\":": 2177, "ƒ°thoughts": 2178, "rently": 2179, "ƒ°cuz": 2180, "ƒ°ugh": 2181, "whe": 2182, "ƒ°intere": 2183, "esome": 2184, "ƒ°[": 2185, "ƒ°lm": 2186, "√∞≈Ç": 2187, "az": 2188, "nth": 2189, "ƒ°lmfa": 2190, "ƒ°chan": 2191, "ƒ°joke": 2192, "cent": 2193, "ards": 2194, "ƒ°ideas": 2195, "hed": 2196, "ƒ°fan": 2197, "ker": 2198, "tra": 2199, "ƒ°sna": 2200, "itar": 2201, "press": 2202, "ƒ°synth": 2203, "ƒ°guitar": 2204, "√≥¬æ": 2205, "ƒ°ring": 2206, "cted": 2207, "ƒ°lmfao": 2208, "√§¬°": 2209, "ƒ°worr": 2210, "tent": 2211, "tead": 2212, "olog": 2213, "ƒ°rain": 2214, "ƒ°chair": 2215, "ƒ°resp": 2216, "ƒ°blan": 2217, "sting": 2218, "ƒ°sun": 2219, "ƒ°drinking": 2220, "ƒ°decks": 2221, "eepy": 222, "ƒ°sminking": 2223, "je": 2224, "ƒ°lmaooo": 2225, "more": 2226, "ube": 2227, "lle": 2228, "nc": 2229, "nts": 2230, "hhh": 2231, "ƒ°dark": 2232, "by": 2233, "py": 2234, "30": 2235, "ƒ°oven": 2236, "ƒ°seco": 2237, "ƒ°nooo": 2238, "ƒ°anymore": 2239, "ider": 2240, "ments": 2241, "joy": 2242, "gry": 2243, "ƒ°da": 2244, "ƒ°√¢ƒ£": 2245, "ƒ°turnt": 2246, "indow": 2247, "reen": 2248, "ƒ°turntable": 2249, "come": 2250, "ƒ°0": 2251, "ƒ°mark": 2252, "wards": 2253, "ƒ°known": 2254, "sc": 2255, "e": 2256, "ƒ°ele": 2257, "ƒ°awww": 2258, "ched": 2259, "ƒ°mixer": 2260, "ƒ°snacks": 2261, "fs": 2262, "ffic": 2263, "light": 2264, "ƒ°appa": 2265, "'.": 2266, "ƒ°amaz": 2267, "ancing": 2268, "ages": 2269, "ƒ°jon": 2270, "ƒ°#": 2271, "\",": 2272, "ƒ°horr": 2273, "nch": 2274, "ƒ°ooo": 2275, "ors": 2276, "ƒ°pi": 2277, "ƒ°ey": 2278, "ba": 2279, "fr": 2280, "√£ƒ£": 2281, "view": 2282, "√£ƒ£¬£": 2283, "igg": 2284, "how": 2285, "ƒ°ap": 2286, "ƒ°ah": 2287, "the": 2288, "ƒ°dw": 2289, "ƒ°blanket": 2290, "ƒ°chris": 2291, "ches": 2292, "vice": 2293, "ƒ°phot": 2294, "tem": 2295, "ƒ°songs": 2296, "hes": 2297, "ƒ°loving": 2298, "ƒ°atta": 2299, "ƒ°sugg": 2300, "ƒ°create": 2301, "ƒ°sksks": 2302, "ƒ°proc": 2303, "ƒ°opin": 2304, "ƒ°bur": 2305, "'(": 2306, "ƒ°wha": 2307, "ƒ°√¢ƒ£ƒ∫": 2308, "ƒ°toy": 2309, "ƒ°√∞≈Ç": 2310, "ƒ°va": 2311, "ƒ°tas": 2312, "ips": 2313, "net": 2314, "),": 2315, "ƒ°hun": 2316, "odu": 2317, "ƒ°yep": 2318, "ƒ°darkness": 2319, "iving": 2320, "ƒ°voices": 2321, "ƒ°gun": 2322, "12": 2323, "ture": 2324, "ƒ°likes": 2325, "ƒ°mat": 2326, "ƒ°somehow": 2327, "ƒ°idek": 2328, "ters": 2329, "ipp": 2330, "ƒ°takes": 2331, "ƒ°daylight": 2332, "ads": 2333, "sent": 2334, "ob": 2335, "ƒ°towards": 2336, "tm": 2337, "uk": 2338, "yout": 2339, "ƒ°lost": 2340, "ƒ°loves": 2341, "ƒ°cree": 2342, "put": 2343, "ƒ°numb": 2344, "ƒ°wood": 2345, "wor": 2346, "nded": 2347, "gy": 2348, "ƒ°hating": 2349, "ƒ°opinion": 2350, "cited": 2351, "ƒ°dinner": 2352, "oman": 2353, "mp": 2354, "ƒ°fav": 2355, "watch": 2356, "33": 2357, "ƒ°wear": 2358, "ƒ°hid": 2359, "ƒ°droid": 2360, "ƒ°bou": 2361, "aged": 2362, "ƒ°hehe": 2363, "ƒ°fuckin": 2364, "ƒ°heating": 2365, "ƒ°oooh": 2366, "ƒ°looked": 2367, "dy": 2368, "ƒ°convers": 2369, "ƒ°comes": 2370, "ƒ°pos": 2371, "ƒ°age": 2372, "√¢ƒ£¬¢": 2373, "ƒ°evening": 2374, "ƒ°consider": 2375, "!'": 2376, "ring": 2377, "ƒ°book": 2378, "ƒ°mome": 2379, "ƒ°wat": 2380, "15": 2381, "ƒ°passage": 2382, "ƒ°gunna": 2383, "kk": 2384, "ƒ°dat": 2385, "mer": 2386, "ƒ°doct": 2387, "ƒ°pen": 2388, "gra": 2389, "√™ƒ∑": 2390, "ƒ°loool": 2391, "ƒ°del": 2392, "ƒ°creating": 2393, "cl": 2394, "man": 2395, "ƒ°14": 2396, "cu": 2397, "ga": 2398, "√™ƒ∑": 2399, "tever": 2400, "hy": 2401, "ƒ°twitch": 2402, "ƒ°cho": 2403, "ƒ°pasta": 2404, "ƒ°pc": 2405, "ƒ°http": 2406, "ex": 2407, "ƒ°google": 2408, "irt": 2409, "nn": 2410, "ƒ°mil": 2411, "youtube": 2412, "ƒ°sooo": 2413, "ock": 2414, "dden": 2415, "ƒ°birthday": 2416, "ƒ°cba": 2417, "ƒ°mouse": 2418, "nding": 2419, "ƒ°pat": 2420, "ƒ°four": 2421, "ƒ°tiny": 2422, "ƒ°broke": 2423, "ƒ°loved": 2424, "ƒ°webs": 2425, "ls": 2426, "ƒ°cock": 2427, "ƒ°pas": 2428, "rac": 2429, "con": 2430, "ƒ°ess": 2431, "ƒ°ffs": 2432, "ology": 2433, "ƒ°bath": 2434, "17": 2435, "ƒ°whatever": 2436, "ƒ°hmmm": 2437, "ule": 2438, "ƒ°angry": 2439, "ƒ°perf": 2440, "ƒ°fell": 2441, "ision": 2442, "ƒ°calm": 2443, "lu": 2444, "key": 2445, "ƒ°android": 2446, "cer": 2447, "ƒ°mee": 2448, "cha": 2449, "fk": 2450, "sel": 2451, "24": 2452, "ured": 2453, "onse": 2454, "ƒ°recent": 2455, "scri": 2456, "ƒ°fil": 2457, "16": 2458, "ƒ°woo": 2459, "ƒ°eyes": 2460, "25": 2461, "ient": 2462, "ƒ°loveangel": 2463, "ƒ°/": 2464, "board": 2465, "ƒ°ya": 2466, "tr": 2467, "ƒ°finis": 2468, "ƒ°easy": 2469, "ƒ°mis": 2470, "ƒ°knows": 2471, "ife": 2472, "ƒ°conversation": 2473, "ƒ°brains": 2474, "ƒ°screen": 2475, "ines": 2476, "ƒ°eh": 2477, "ƒ°response": 2478, "ƒ°19": 2479, "gif": 2480, "ƒ°+": 2481, "selves": 2482, "ƒ°cares": 2483, "ƒ°problems": 2484, "ƒ°level": 2485, "ral": 2486, "iness": 2487, "ƒ°short": 2488, "ƒ°atm": 2489, "ƒ°hates": 2490, "ƒ°five": 2491, "ƒ°lemme": 2492, "cy": 2493, "ƒ°beaut": 2494, "ƒ°noticed": 2495, "iple": 2496, "ƒ°cust": 2497, "ƒ°missed": 2498, "ƒ°number": 2499, "√™ƒ∑√£ƒ£¬£": 2500, "ƒ°log": 2501, "uin": 2502, "ƒ°produ": 2503, "ƒ°bru": 2504, "ƒ°essay": 2505, "ƒ°sys": 2506, "rew": 2507, "rence": 2508, "ƒ°gives": 2509, "ƒ°based": 2510, "ify": 2511, "ize": 2512, "ƒ°conta": 2513, "ƒ°walked": 2514, "ƒ°somewhere": 2515, "ƒ°near": 2516, "ƒ°13": 2517, "ƒ°ohhh": 2518, "ƒ°deep": 2519, "ƒ°const": 2520, "load": 2521, "eer": 2522, "orn": 2523, "tmas": 2524, "ƒ°suddenly": 2525, "une": 2526, "ƒ°family": 2527, "ƒ°starting": 2528, "ƒ°incred": 2529, "ƒ°front": 2530, "ively": 2531, "√¢ƒ£¬¢√¨": 2532, "ƒ°clos": 2533, "ƒ°party": 2534, "ital": 2535, "ƒ°goes": 2536, "pa": 2537, "esss": 2538, "ƒ°christmas": 2539, "ƒ°story": 2540, "ƒ°girls": 2541, "ƒ°hungry": 2542, "ƒ°creepy": 2543, "ƒ°round": 2544, "14": 2545, "ƒ°pol": 2546, "ƒ°hugs": 2547, "aked": 2548, "urs": 2549, "ƒ°floor": 2550, "ƒ°squ": 2551, "ƒ°ran": 2552, "ƒ°heart": 2553, "ƒ°join": 2554, "ƒ°>.": 2555, "ƒ°busy": 2556, "ƒ°16": 2557, "ƒ°england": 2558, "fast": 2559, "ƒ°silly": 2560, "iful": 2561, "ancy": 2562, "ƒ°technically": 2563, "ƒ°partner": 2564, "ƒ°general": 2565, "oot": 2566, "fortable": 2567, "ƒ°inform": 2568, "ƒ°air": 2569, "ƒ°happens": 2570, "iot": 2571, "ƒ°experience": 2572, "ƒ°questions": 2573, "ƒ°xoxo": 2574, "19": 2575, "ƒ°decide": 2576, "13": 2577, "lex": 2578, "ƒ°longer": 2579, "ƒ°brother": 2580, "ronic": 2581, "ƒ°style": 2582, "ƒ°woke": 2583, "ƒ°includ": 2584, "head": 2585, "kets": 2586, "ik": 2587, "ƒ°wel": 2588, "ƒ°imagine": 2589, "ƒ°build": 2590, "tention": 2591, "ƒ°sme": 2592, "ƒ°bass": 2593, "ƒ°singing": 2594, "ƒ°issue": 2595, "dle": 2596, "min": 2597, "ƒ°nic": 2598, "ƒ°neither": 2599, "chen": 2600, "ax": 2601, "ƒ°lar": 2602, "ƒ°yh": 2603, "ndon": 2604, "ists": 2605, "fl": 2606, "ƒ°likely": 2607, "ƒ°facebook": 2608, "ƒ°specific": 2609, "ƒ°yout": 2610, "ƒ°spo": 2611, "ƒ°mums": 2612, "ƒ°particul": 2613, "ƒ°prov": 2614, "ƒ°sleeping": 2615, "ƒ°warm": 2616, "ƒ°genuinely": 2617, "ƒ°nearly": 2618, "ƒ°calling": 2619, "50": 2620, "tern": 2621, "ƒ°fucked": 2622, "ƒ°sens": 2623, "18": 2624, "ƒ°soup": 2625, "ƒ°ju": 2626, "ƒ°meeting": 2627, "you": 2628, "ƒ°ily": 2629, "iled": 2630, "ƒ°milk": 2631, "ƒ°line": 2632, "ƒ°annoyed": 2633, "ƒ°odd": 2634, "ƒ°couldn": 2635, "ƒ°kitchen": 2636, "ƒ°argo": 2637, "ƒ°multiple": 2638, "ƒ°screaming": 2639, "pris": 2640, "ƒ°clot": 2641, "ƒ°turned": 2642, "ƒ°bright": 2643, "ƒ°spent": 2644, "ƒ°dammit": 2645, "√™ƒ∫": 2646, "ƒ°sile": 2647, "ƒ°disapp": 2648, "ƒ°massive": 2649, "ills": 2650, "ƒ°strong": 2651, "ƒ°wake": 2652, "ƒ°whether": 2653, "ƒ°group": 2654, "que": 2655, "urate": 2656, "ƒ°stude": 2657, "ƒ°shower": 2658, "ƒ°opp": 2659, "ƒ°breakfast": 2660, "ƒ°sksksk": 2661, "\")": 2662, "ƒ°otherwise": 2663, "ƒ°traum": 2664, "ƒ°plymouth": 2665, "ƒ°pissed": 2666, "aw": 2667, "do": 2668, "tend": 2669, "ƒ°vers": 2670, "ƒ°chaos": 2671, "ƒ°others": 2672, "ƒ°easier": 2673, "ƒ°london": 2674, "eng": 2675, "ƒ°green": 2676, "ƒ°100": 2677, "ƒ°paid": 2678, "ƒ°apolog": 2679, "ƒ°fire": 2680, "ƒ°hm": 2681, "kers": 2682, "down": 2683, "ƒ°remo": 2684, "ƒ°ant": 2685, "ƒ°six": 2686, "ƒ°ended": 2687, "ƒ°due": 2688, "ƒ°dans": 2689, "ƒ°share": 2690, "ƒ°young": 2691, "ƒ°>:": 2692, "ƒ°direct": 2693, "medi": 2694, "ƒ°purple": 2695, "23": 2696, "het": 2697, "ƒ°camer": 2698, "ƒ°deser": 2699, "ƒ°control": 2700, "ƒ°recently": 2701, "ƒ°kissing": 2702, "sted": 2703, "ƒ°beautiful": 2704, "zz": 2705, "ƒ°leaves": 2706, "ƒ°uh": 2707, "ƒ°pour": 2708, "ƒ°loveangle": 2709, "eg": 2710, "ƒ°ig": 2711, "ƒ°liked": 2712, "ƒ°surpris": 2713, "what": 2714, "ƒ°los": 2715, "ƒ°personal": 2716, "',": 2717, "hugs": 2718, "ƒ°however": 2719, "ƒ°moves": 2720, "ƒ°clearly": 2721, "ƒ°stressed": 2722, "ƒ°emotional": 2723, "ƒ°helping": 2724, "ƒ°accurate": 2725, "br": 2726, "mat": 2727, "ets": 2728, "ƒ°against": 2729, "omen": 2730, "ump": 2731, "pected": 2732, "ƒ°fight": 2733, "ƒ°wife": 2734, "ƒ°death": 2735, "ƒ°crap": 2736, "ƒ°ce": 2737, "ƒ°argu": 2738, "ƒ°taken": 2739, "any": 2740, "ƒ°sofa": 2741, "ƒ°interested": 2742, "ƒ°project": 2743, "nse": 2744, "ƒ°sitting": 2745, "()": 2746, "40": 2747, "pop": 2748, "ƒ°dou": 2749, "ƒ°ben": 2750, "ƒ°hugging": 2751, "par": 2752, "ƒ°certain": 2753, "ƒ°sending": 2754, "ƒ°cooking": 2755, "ƒ°obs": 2756, "ƒ°middle": 2757, "ƒ°youtube": 2758, "go": 2759, "ƒ°fake": 2760, "ƒ°friday": 2761, "ƒ°single": 2762, "ƒ°yesss": 2763, "vi": 2764, "tenor": 2765, "ƒ°prefer": 2766, "ka": 2767, "ants": 2768, "ƒ°organ": 2769, "dn": 2770, "ƒ°dude": 2771, "ƒ°ow": 2772, "lls": 2773, "ilt": 2774, "ƒ°themselves": 2775, "ƒ°choice": 2776, "ond": 2777, "ƒ°attention": 2778, "ƒ°smoked": 2779, "ƒ°idiot": 2780, "gu": 2781, "ƒ°state": 2782, "ƒ°bare": 2783, "ƒ°considering": 2784, "ƒ°ai": 2785, "ƒ°lil": 2786, "ƒ°que": 2787, "gs": 2788, "ƒ°aa": 2789, "ƒ°loads": 2790, "ƒ°date": 2791, "ƒ°expensive": 2792, "ƒ°broken": 2793, "ƒ°asher": 2794, "bot": 2795, "ular": 2796, "ƒ°meaning": 2797, "ƒ°mag": 2798, "ƒ°conne": 2799, "hol": 2800, "ƒ°ont": 2801, "ƒ°appro": 2802, "ƒ°crazy": 2803, "sor": 2804, "ƒ°rent": 2805, "ƒ°grow": 2806, "1000": 2807, "ai": 2808, "ric": 2809, "ƒ°focus": 2810, "ƒ°finished": 2811, "text": 2812, "ƒ°amb": 2813, "fy": 2814, "ƒ°avo": 2815, "ƒ°depe": 2816, "ove": 2817, "ƒ°mas": 2818, "norm": 2819, "ises": 2820, "ƒ°advice": 2821, "ƒ°>:(": 2822, "itive": 2823, "ergy": 2824, "hahaha": 2825, "like": 2826, "igger": 2827, "√™ƒ∑√£ƒ£¬£": 2828, "ƒ°tic": 2829, "ƒ°wall": 2830, "ƒ°uns": 2831, "cle": 2832, "ƒ°dress": 2833, "ƒ°17": 2834, "ƒ°typing": 2835, "21": 2836, "ƒ°react": 2837, "45": 2838, "ƒ°drugs": 2839, "ƒ°:((": 2840, "tel": 2841, "26": 2842, "port": 2843, "ina": 2844, "ƒ°access": 2845, "ƒ°emo": 2846, "ƒ°system": 2847, "ƒ°repe": 2848, "ƒ°express": 2849, "ƒ°emp": 2850, "ƒ°camera": 2851, "ƒ°satur": 2852, "ager": 2853, "mi": 2854, "ƒ°women": 2855, "ƒ°realise": 2856, "ƒ°manage": 2857, "ƒ°sees": 2858, "ules": 2859, "iment": 2860, "ropri": 2861, "ƒ°soft": 2862, "ƒ°blood": 2863, "ƒ°onto": 2864, "ƒ°dest": 2865, "search": 2866, "ƒ°smile": 2867, "ƒ°secret": 2868, "ƒ°offe": 2869, "ƒ°kitty": 2870, "]:": 2871, "ƒ°code": 2872, "ƒ°view": 2873, "ƒ°prote": 2874, "ially": 2875, "cel": 2876, "ƒ°teen": 2877, "ƒ°charac": 2878, "stairs": 2879, "uth": 2880, "ƒ°rid": 2881, "ƒ°walking": 2882, "ƒ°memor": 2883, "oof": 2884, "ƒ°offer": 2885, "ƒ°quiet": 2886, "ƒ°letter": 2887, "ƒ°himself": 2888, "work": 2889, "ƒ°beg": 2890, "imin": 2891, "ƒ°vibes": 2892, "ƒ°custom": 2893, "for": 2894, "ƒ°hol": 2895, "ƒ°absol": 2896, "whel": 2897, "ƒ°opt": 2898, "ature": 2899, "ƒ°respons": 2900, "ƒ°communication": 2901, "bu": 2902, "ƒ°oop": 2903, "ƒ°pub": 2904, "ane": 2905, "aten": 2906, "iod": 2907, "rib": 2908, "ƒ°box": 2909, "ƒ°clear": 2910, "ƒ°clothes": 2911, "ƒ°energy": 2912, "ƒ°given": 2913, "ƒ°rip": 2914, "ift": 2915, "ƒ°mot": 2916, "ƒ°overwhel": 2917, "ƒ°brit": 2918, "ƒ°yeahhh": 2919, "ƒ°fairly": 2920, "ƒ°remind": 2921, "ƒ°absolute": 2922, "ƒ°respect": 2923, "rs": 2924, "ƒ°bin": 2925, "ƒ°pie": 2926, "ƒ°feelings": 2927, "ocked": 2928, "ability": 2929, "ƒ°hands": 2930, "ƒ°awh": 2931, "ƒ°whos": 2932, "ƒ°reco": 2933, "ƒ°welcome": 2934, "cal": 2935, "ƒ°worked": 2936, "ours": 2937, "pro": 2938, "ƒ°putting": 2939, "ƒ°est": 2940, "irty": 2941, "ƒ°retur": 2942, "ƒ°av": 2943, "ƒ°bts": 2944, "ƒ°rele": 2945, "ƒ°shouldnt": 2946, "ƒ°poke": 2947, "ƒ°admit": 2948, "ƒ°medic": 2949, "ƒ°truly": 2950, "ƒ°beh": 2951, "ƒ°fully": 2952, "ƒ°changed": 2953, "but": 2954, "yyy": 2955, "ƒ°sam": 2956, "astic": 2957, "ƒ°24": 2958, "ƒ°photo": 2959, "ƒ°√¢": 2960, "ƒ°planning": 2961, "ƒ°vibe": 2962, "ok": 2963, "ƒ°elodies": 2964, "ƒ°rock": 2965, "ƒ°large": 2966, "ƒ°complain": 2967, "miss": 2968, "igin": 2969, "ƒ°period": 2970, "ƒ°forgetting": 2971, "ƒ°tre": 2972, "ƒ°terr": 2973, "ƒ°information": 2974, "ƒ°cover": 2975, "22": 2976, "ƒ°priv": 2977, "ƒ°saturday": 2978, "uc": 2979, "icken": 2980, "ctive": 2981, "ƒ°ignore": 2982, "ƒ°particularly": 2983, "ropriate": 2984, "ws": 2985, "url": 2986, "ƒ°petes": 2987, "aces": 2988, "ƒ°pipe": 2989, "ƒ°aud": 2990, "ƒ°monday": 2991, "ƒ°50": 2992, "inger": 2993, "ƒ°beha": 2994, "ole": 2995, "ƒ°drop": 2996, "ƒ°port": 2997, "ƒ°thinks": 2998, "ƒ°arr": 2999, "anger": 3000, "ƒ°doctors": 3001, "ƒ°continue": 3002, "ze": 3003, "ƒ°bat": 3004, "ƒ°awful": 3005, "ƒ°helps": 3006, "dam": 3007, "ƒ°detail": 3008, "ƒ°treat": 3009, "raph": 3010, "ƒ°immedi": 3011, "ƒ°finding": 3012, "ƒ°option": 3013, "?'": 3014, "ky": 3015, "ƒ°ir": 3016, "ator": 3017, "ƒ°remembering": 3018, "ƒ°confir": 3019, "(\"": 3020, "ƒ°lady": 3021, "ƒ°power": 3022, "ƒ°uncom": 3023, "ƒ°pract": 3024, "ƒ°temp": 3025, "ƒ°kidding": 3026, "nel": 3027, "ƒ°moved": 3028, "ƒ°bby": 3029, "ƒ°strange": 3030, "ground": 3031, "ƒ°noise": 3032, "ƒ°invol": 3033, "lr": 3034, "mg": 3035, "ƒ°sal": 3036, "ƒ°ly": 3037, "ƒ°vag": 3038, "ƒ°orange": 3039, "ƒ°somebody": 3040, "ƒ°_": 3041, "ƒ°understanding": 3042, "ƒ°closer": 3043, "ƒ°avoid": 3044, "ƒ°kpop": 3045, "ƒ°aint": 3046, "ƒ°warn": 3047, "oring": 3048, "cou": 3049, "ƒ°unf": 3050, "ƒ°assa": 3051, "ƒ°juice": 3052, "cture": 3053, "ƒ°truth": 3054, "ƒ°property": 3055, "scribe": 3056, "ƒ°sle": 3057, "ƒ°mice": 3058, "ƒ°um": 3059, "ƒ°stre": 3060, "ƒ°communicate": 3061, "ƒ°behind": 3062, "bal": 3063, "ƒ°tit": 3064, "ƒ°wooo": 3065, "ƒ°feed": 3066, "ƒ°alive": 3067, "ƒ°drug": 3068, "fd": 3069, "land": 3070, "amp": 3071, "ƒ°waking": 3072, "ƒ°keeps": 3073, "ƒ°sky": 3074, "28": 3075, "ƒ°link": 3076, "kevin": 3077, "ƒ°incredibly": 3078, "ƒ°23": 3079, "ƒ°normally": 3080, "oh": 3081, "ƒ°megan": 3082, "ƒ°fast": 3083, "ƒ°eventually": 3084, "rd": 3085, "ƒ°stopped": 3086, "ƒ°america": 3087, "ienc": 3088, "sex": 3089, "ƒ°fear": 3090, "pose": 3091, "ƒ°occ": 3092, "lfr": 3093, "ƒ°band": 3094, "ƒ°bott": 3095, "ƒ°cringe": 3096, "11": 3097, "itter": 3098, "ƒ°herself": 3099, "ƒ°pressure": 3100, "cing": 3101, "ƒ°origin": 3102, "ƒ°electronic": 3103, "ƒ°fat": 3104, "ƒ°thurs": 3105, "ggg": 3106, "aging": 3107, "ƒ°confusing": 3108, "ƒ°specif": 3109, "oss": 3110, "ƒ°ball": 3111, "ƒ°force": 3112, "verse": 3113, "cra": 3114, "igned": 3115, "ƒ°computers": 3116, "ƒ°hotel": 3117, "ƒ°version": 3118, "dio": 3119, "ƒ°lang": 3120, "ƒ°beds": 3121, "ƒ°alco": 3122, "ƒ°mach": 3123, "face": 3124, "ƒ°&": 3125, "ney": 3126, "lfriend": 3127, "ƒ°wot": 3128, "vera": 3129, "ƒ°kore": 3130, "aaay": 3131, "ƒ°relax": 3132, "usive": 3133, "ƒ°cost": 3134, "ƒ°mac": 3135, "ipped": 3136, "ƒ°forward": 3137, "ƒ°distra": 3138, "√¢ƒ£¬ø": 3139, "ƒ°stars": 3140, "ƒ°term": 3141, "mes": 3142, "ƒ°cir": 3143, "ƒ°progra": 3144, "ƒ°weeke": 3145, "ƒ°nor": 3146, "leted": 3147, "ƒ°ador": 3148, "ƒ°missing": 3149, "ƒ°rev": 3150, "ƒ°matter": 3151, "ƒ°seemed": 3152, "ƒ°request": 3153, "ƒ°fut": 3154, "ƒ°bene": 3155, "ows": 3156, "ƒ°decent": 3157, "ƒ°desk": 3158, "ƒ°piper": 3159, "ƒ°eye": 3160, "ƒ°cy": 3161, "ƒ°frust": 3162, "ƒ°raccoon": 3163, "ƒ°price": 3164, "ƒ°male": 3165, "ƒ°deserve": 3166, "99": 3167, "ƒ°porn": 3168, "ƒ°hide": 3169, "icked": 3170, "ior": 3171, "ƒ°urself": 3172, "ƒ°ident": 3173, "cat": 3174, "ƒ°listens": 3175, "ƒ°special": 3176, "ƒ°student": 3177, "ƒ°fill": 3178, "ƒ°touches": 3179, "ƒ°sleepy": 3180, "usted": 3181, "ƒ°harsh": 3182, "ƒ°boys": 3183, "rap": 3184, "ived": 3185, "ƒ°langu": 3186, "ƒ°alcohol": 3187, "sp": 3188, "ƒ°mate": 3189, "ƒ°stan": 3190, "ƒ°expected": 3191, "ƒ°werent": 3192, "ƒ°across": 3193, "aling": 3194, "ƒ°cup": 3195, "ƒ°badly": 3196, "ƒ°decision": 3197, "ƒ°including": 3198, "ler": 3199, "vous": 3200, "ƒ°ou": 3201, "ƒ°depressed": 3202, "27": 3203, "la": 3204, "token": 3205, "aths": 3206, "ses": 3207, "ƒ°ris": 3208, "ƒ°town": 3209, "ƒ°whenever": 3210, "ƒ°streaming": 3211, "ars": 3212, "nted": 3213, "ƒ°burn": 3214, "dj": 3215, "ƒ°ep": 3216, "ƒ°roll": 3217, "iant": 3218, "ƒ°company": 3219, "ƒ°shitty": 3220, "ƒ°appoint": 3221, "ƒ°girlfriend": 3222, "ƒ°taste": 3223, "sha": 3224, "ume": 3225, "ƒ°eight": 3226, "ƒ°cuddle": 3227, "ƒ°within": 3228, "ƒ°managed": 3229, "ƒ°note": 3230, "ƒ°analy": 3231, "ƒ°lay": 3232, "ƒ°reasons": 3233, "ƒ°brighton": 3234, "bl": 3235, "tence": 3236, "ƒ°drum": 3237, "ƒ°jud": 3238, "ƒ°save": 3239, "ƒ°ways": 3240, "ƒ°redd": 3241, "ƒ°skills": 3242, "ƒ°further": 3243, "ƒ°ahhh": 3244, "ƒ°rules": 3245, "pital": 3246, "hael": 3247, "ƒ°bill": 3248, "ƒ°none": 3249, "ƒ°diag": 3250, "ƒ°watched": 3251, "ƒ°repeat": 3252, "ƒ°{": 3253, "ƒ°tor": 3254, "ƒ°stat": 3255, "ƒ°along": 3256, "ƒ°bull": 3257, "ƒ°grind": 3258, "ƒ°photos": 3259, "ƒ°oops": 3260, "ƒ°behavi": 3261, "ƒ°cris": 3262, "ƒ°den": 3263, "ƒ°therefore": 3264, "fc": 3265, "list": 3266, "ƒ°lolol": 3267, "ƒ°helpful": 3268, "ƒ°dro": 3269, "ƒ°park": 3270, "))": 3271, "itten": 3272, "ƒ°british": 3273, "bin": 3274, "ƒ°chicken": 3275, "ƒ°tty": 3276, "ƒ°drag": 3277, "ƒ°genre": 3278, "ƒ°jk": 3279, "ƒ°ofc": 3280, "ƒ°plans": 3281, "ƒ°froggys": 3282, "ƒ°difference": 3283, "ality": 3284, "ƒ°exis": 3285, "ƒ°urs": 3286, "ƒ°experienc": 3287, "ƒ°exciting": 3288, "ƒ°illeg": 3289, "ƒ°metal": 3290, "ƒ°embarr": 3291, "ƒ°absolutely": 3292, "ƒ°future": 3293, "bab": 3294, "ƒ°youuu": 3295, "ƒ°poor": 3296, "ƒ°michael": 3297, "ƒ°specifically": 3298, "ster": 3299, "uce": 3300, "ƒ°mild": 3301, "ref": 3302, "ƒ°nood": 3303, "ƒ°attem": 3304, "ƒ°sorted": 3305, "ƒ°switch": 3306, "ƒ°places": 3307, "ƒ°hears": 3308, "ƒ°lack": 3309, "ƒ°learns": 3310, "ƒ°running": 3311, "den": 3312, "tain": 3313, "ught": 3314, "ƒ°impress": 3315, "ƒ°purpose": 3316, "ƒ°hyper": 3317, "ƒ°options": 3318, "ƒ°voc": 3319, "ƒ°mc": 3320, "ƒ°foot": 3321, "idence": 3322, "tedly": 3323, "ƒ°tryna": 3324, "ƒ°channel": 3325, "ƒ°info": 3326, "ƒ°training": 3327, "pping": 3328, "ƒ°print": 3329, "ƒ°delive": 3330, "aks": 3331, "oin": 3332, "ints": 333, "ƒ°billy": 3334, "ƒ°sched": 3335, "ƒ°regard": 3336, "ƒ°wro": 3337, "ƒ°ble": 3338, "ƒ°upd": 3339, "ton": 3340, "ƒ°died": 3341, "ƒ°dirty": 3342, "ƒ°rece": 3343, "ƒ°knowing": 3344, "ƒ°unable": 3345, "ƒ°equals": 3346, "ƒ°nervous": 3347, "ƒ°barely": 3348, "ƒ°25": 3349, "ƒ°hurts": 3350, "ƒ°shops": 3351, "ƒ°weekend": 3352, "ƒ°reddit": 3353, "under": 3354, "ƒ°sunday": 3355, "ppy": 3356, "ƒ°aware": 3357, "ƒ°skin": 3358, "name": 3359, "ƒ°gender": 3360, "ƒ°hehehe": 3361, "ƒ°adorable": 3362, "ƒ°tune": 3363, "har": 3364, "idge": 3365, "ƒ°bottom": 3366, "ƒ°tro": 3367, "ƒ°bum": 3368, "ƒ°ruin": 3369, "ƒ°mentioned": 3370, "ƒ°surprised": 3371, "ƒ°ttyl": 3372, "bing": 3373, "ƒ°beat": 3374, "ƒ°above": 3375, "ƒ°minute": 3376, "ƒ°constantly": 3377, "ƒ°bb": 3378, "ƒ°disg": 3379, "ƒ°bedroom": 3380, "velop": 3381, "ƒ°machine": 3382, "38": 3383, "oned": 3384, "ƒ°jess": 3385, "ƒ°possibly": 3386, "ƒ°>.>": 3387, "ƒ°refe": 3388, "ionally": 3389, "ƒ°eats": 3390, "ƒ°dads": 3391, "ƒ°assum": 3392, "ƒ°mother": 3393, "ƒ°lying": 3394, "ƒ°kk": 3395, "ƒ°moon": 3396, "vert": 3397, "ƒ°picks": 3398, "ƒ°account": 3399, "ƒ°wrote": 3400, "ƒ°washing": 3401, "play": 3402, "ƒ°struggling": 3403, "ƒ°mur": 3404, "ƒ°cross": 3405, "ƒ°tax": 3406, "ested": 3407, "ƒ°tend": 3408, "ƒ°penis": 3409, "chan": 3410, "ulation": 3411, "ƒ°jobs": 3412, "ƒ°40": 3413, "essed": 3414, "gged": 3415, "ƒ°√∞≈Çƒ∫": 3416, "ƒ°bruh": 3417, "box": 3418, "√≥¬æƒ©": 3419, "ution": 3420, "ƒ°directly": 3421, "ƒ°cake": 3422, "ƒ°concer": 3423, "umblr": 3424, "29": 3425, "pre": 3426, "ƒ°table": 3427, "ƒ°bow": 3428, "ƒ°quit": 3429, "ƒ°tues": 3430, "ƒ°delete": 3431, "ƒ°cond": 3432, "ƒ°paying": 3433, "ƒ°worries": 3434, "ƒ°copy": 3435, "ƒ°profess": 3436, "66": 3437, "ƒ°hom": 3438, "ƒ°sadly": 3439, "ƒ°double": 3440, "ƒ°language": 3441, "pg": 3442, "ƒ°maths": 3443, "ƒ°dying": 3444, "ƒ°honey": 3445, "ides": 3446, "alous": 3447, "ƒ°neg": 3448, "ƒ°interview": 3449, "ƒ°mode": 3450, "ƒ°banned": 3451, "bt": 3452, "ƒ°mist": 3453, "ndent": 3454, "ƒ°uuu": 3455, "ƒ°heav": 3456, "ƒ°stick": 3457, "ƒ°sell": 3458, "ƒ°spot": 3459, "ƒ°everywhere": 3460, "ƒ°drive": 3461, "ƒ°holy": 3462, "ƒ°apologise": 3463, "ƒ°pokemon": 3464, "ƒ°slept": 3465, "ƒ°sand": 3466, "ƒ°serious": 3467, "uff": 3468, "ƒ°yee": 3469, "ƒ°shad": 3470, "ncy": 3471, "ƒ°fixed": 3472, "ƒ°film": 3473, "ƒ°rom": 3474, "ƒ°cent": 3475, "ƒ°pun": 3476, "ƒ°gurl": 3477, "ien": 3478, "ƒ°written": 3479, "ƒ°phones": 3480, "ƒ°embarass": 3481, "ƒ°medication": 3482, "08": 3483, "ƒ°breat": 3484, "uses": 3485, "ƒ°lose": 3486, "ƒ°summ": 3487, "ƒ°seven": 3488, "ƒ°22": 3489, "ledge": 3490, "ƒ°illegal": 3491, "48": 3492, "lent": 3493, "ƒ°chall": 3494, "ƒ°trip": 3495, "ƒ°irl": 3496, "time": 3497, "ƒ°joy": 3498, "ƒ°anim": 3499, "amine": 3500, "ƒ°final": 3501, "ƒ°randomly": 3502, "ƒ°emotions": 3503, "57": 3504, "ƒ°becom": 3505, "ƒ°letting": 3506, "aime": 3507, "ƒ°jum": 3508, "ƒ°useful": 3509, "ƒ°imposs": 3510, "ƒ°jealous": 3511, "ƒ°entirely": 3512, "√¢ƒ£¬¢√¨ƒ£": 3513, "ƒ°trauma": 3514, "ƒ°immediately": 3515, "ƒ°uncomfortable": 3516, "nit": 3517, "ƒ°ate": 3518, "ƒ°benef": 3519, "ƒ°sarc": 3520, "ƒ°pics": 3521, "ƒ°american": 3522, "ƒ°size": 3523, "ƒ°lu": 3524, "ƒ°anywhere": 3525, "ƒ°bae": 3526, "ƒ°wearing": 3527, "gpt": 3528, "ƒ°sce": 3529, "ƒ°begin": 3530, "):": 3531, "ƒ°succ": 3532, "ƒ°apply": 3533, "ƒ°shame": 3534, "ƒ°remix": 3535, "ƒ°lowkey": 3536, "ƒ°impossible": 3537, "ƒ°stal": 3538, "ƒ°whyyy": 3539, "ƒ°impro": 3540, "ƒ°glass": 3541, "ƒ°depression": 3542, "ƒ°loser": 3543, "ƒ°private": 3544, "craft": 3545, "47": 3546, "tep": 3547, "ƒ°aren": 3548, "ifying": 3549, "ƒ°helped": 3550, "ƒ°babies": 3551, "ograph": 3552, "ƒ°inde": 3553, "ware": 3554, "ƒ°excuse": 3555, "ƒ°physical": 3556, "√¢ƒ£¬¢√¨ƒ£": 3557, "gf": 3558, "ƒ°led": 3559, "ƒ°jour": 3560, "usting": 3561, "ƒ°univers": 3562, "ya": 3563, "ƒ°noice": 3564, "ctions": 3565, "ƒ°cutie": 3566, "of": 3567, "ƒ°fish": 3568, "ƒ°cope": 3569, "male": 3570, "ƒ°object": 3571, "ƒ°return": 3572, "ights": 3573, "ƒ°built": 3574, "ƒ°jelly": 3575, "nb": 3576, "ƒ°irr": 3577, "ƒ°yeee": 3578, "ƒ°nine": 3579, "ƒ°ut": 3580, "ƒ°image": 3581, "ƒ°convo": 3582, "ƒ°signific": 3583, "ƒ°thursday": 3584, "aff": 3585, "ƒ°bound": 3586, "ƒ°hadn": 3587, "ƒ°appear": 3588, "phone": 3589, "ƒ°tf": 3590, "orted": 3591, "ƒ°reach": 3592, "ƒ°anime": 3593, "haps": 3594, "tems": 3595, "ƒ°sho": 3596, "ƒ°chips": 3597, "ires": 3598, "ƒ°tracks": 3599, "ƒ°aces": 3600, "ƒ°humans": 3601, "tered": 3602, "ram": 3603, "ƒ°max": 3604, "ƒ°gran": 3605, "ƒ°cleaning": 3606, "iculous": 3607, "ults": 3608, "ƒ°assist": 3609, "ƒ°stressful": 3610, "ƒ°figured": 3611, "ƒ°visit": 3612, "ƒ°sou": 3613, "ƒ°search": 3614, "ƒ°keyboard": 3615, "ƒ°quickly": 3616, "back": 3617, "ƒ°gar": 3618, "ƒ°ends": 3619, "ƒ°perform": 3620, "ƒ°pill": 3621, "ƒ°context": 3622, "ƒ°children": 3623, "ƒ°depends": 3624, "ƒ°rice": 3625, "ƒ°deleted": 3626, "ulance": 3627, "ƒ°team": 3628, "ƒ°embarrass": 3629, "ƒ°ded": 3630, "usion": 3631, "ƒ°thre": 3632, "ƒ°stab": 3633, "..?": 3634, "ƒ°ens": 3635, "dis": 3636, "ƒ°bang": 3637, "ƒ°init": 3638, "ƒ°kay": 3639, "ƒ°shouting": 3640, "cher": 3641, "ration": 3642, "vant": 3643, "vices": 3644, "ƒ°ahh": 3645, "ƒ°danger": 3646, "urt": 3647, "ƒ°yeh": 3648, "ƒ°drama": 3649, "ƒ°rac": 3650, "ƒ°hiding": 3651, "ƒ°laughing": 3652, "ƒ°bon": 3653, "35": 3654, "ƒ°page": 3655, "ƒ°push": 3656, "ased": 3657, "ƒ°abuse": 3658, "ƒ°abusive": 3659, "ged": 3660, "ƒ°suis": 3661, "ƒ°exha": 3662, "ƒ°comfortable": 3663, "ƒ°tex": 3664, "ƒ°chin": 3665, "ƒ°cheap": 3666, "ƒ°horny": 3667, "now": 3668, "ƒ°bee": 3669, "idered": 3670, "dding": 3671, "ƒ°joking": 3672, "ƒ°twitter": 3673, "shit": 3674, "ƒ°staying": 3675, "ƒ°instru": 3676, "ƒ°pus": 3677, "ƒ°hos": 3678, "ƒ°chance": 3679, "ƒ°usual": 3680, "ƒ°waaa": 3681, "ƒ°allows": 3682, "ƒ°owo": 3683, "mission": 3684, "ƒ°function": 3685, "ƒ°lives": 3686, "ƒ°eggs": 3687, "ƒ°woods": 3688, "ƒ°country": 3689, "ƒ°smell": 3690, "ƒ°ridiculous": 3691, "ƒ°recogn": 3692, "ƒ°agre": 3693, "quen": 3694, "ƒ°counse": 3695, "ƒ°seconds": 3696, "ƒ°icon": 3697, "ƒ°bigg": 3698, "ƒ°fra": 3699, "oms": 3700, "ƒ°imo": 3701, "ƒ°rhy": 3702, "ƒ°fuc": 3703, "ƒ°activ": 3704, "ƒ°perhaps": 3705, "ƒ°breaks": 3706, "ƒ°mistake": 3707, "just": 3708, "ƒ°wedn": 3709, "ƒ°bother": 3710, "ƒ°bathroom": 3711, "berry": 3712, "ƒ°character": 3713, "ƒ°wednes": 3714, "36": 3715, "46": 3716, "here": 3717, "ƒ°mut": 3718, "ƒ°mer": 3719, "olate": 3720, "bered": 3721, "ƒ°apart": 3722, "ƒ°silent": 3723, "ƒ°irrit": 3724, "tre": 3725, "ƒ°mov": 3726, "ƒ°lead": 3727, "ƒ°artic": 3728, "ƒ°videos": 3729, "!',": 3730, "ƒ°oppos": 3731, "39": 3732, "ƒ°meee": 3733, "ƒ°fancy": 3734, "ƒ°naked": 3735, "ato": 3736, "ƒ°90": 3737, "ipping": 3738, "ƒ°suggesting": 3739, "form": 3740, "ource": 3741, "dered": 3742, "fet": 3743, "ƒ°itself": 3744, "ƒ°chatgpt": 3745, "ƒ°obvious": 3746, "ƒ°wondering": 3747, "ƒ°silence": 3748, "ƒ°schedule": 3749, "ƒ°yaaay": 3750, "ƒ°enter": 3751, "ƒ°tough": 3752, "ƒ°appropriate": 3753, "ƒ°showing": 3754, "ƒ°experiment": 3755, "ƒ°data": 3756, "ƒ°vagina": 3757, "hha": 3758, "ister": 3759, "ƒ°stic": 3760, "ƒ°choo": 3761, "ƒ°simple": 3762, "fortun": 3763, "ƒ°public": 3764, "fw": 3765, "ƒ°toi": 3766, "ƒ°peen": 3767, "ƒ°cance": 3768, "ƒ°pretend": 3769, "ighten": 3770, "ances": 3771, "ƒ°mixed": 3772, "ƒ°ears": 3773, "ƒ°bank": 3774, "fun": 3775, "pes": 3776, "ƒ°ange": 3777, "ƒ°wank": 3778, "ƒ°paper": 3779, "ƒ°marry": 3780, "ƒ°grand": 3781, "ƒ°verbal": 3782, "58": 3783, "rics": 3784, "ƒ°names": 3785, "ƒ°lege": 3786, "ƒ°shouldn": 3787, "ƒ°teach": 3788, "ƒ°sus": 3789, "ƒ°shy": 3790, "ƒ°shirt": 3791, "ƒ°butts": 3792, "iling": 3793, "ƒ°adam": 3794, "ƒ°offended": 3795, "ƒ°original": 3796, "vid": 3797, "||": 3798, "stats": 3799, "ƒ°deg": 3800, "ƒ°comfy": 3801, "ƒ°prescri": 3802, "ƒ°grim": 3803, "ƒ°added": 3804, "ƒ°minecraft": 3805, "ƒ°essent": 3806, "ging": 3807, "kj": 3808, "su": 3809, "ƒ°trigger": 3810, "ƒ°calls": 3811, "ƒ°travel": 3812, "ao": 3813, "gl": 3814, "ƒ°√≥¬æ": 3815, "ified": 3816, "ƒ°considered": 3817, "ƒ°tickets": 3818, "ƒ°holiday": 3819, "ƒ°agg": 3820, "ƒ°inten": 3821, "icky": 3822, "ƒ°joh": 3823, "ƒ°remem": 3824, "ƒ°situations": 3825, "that": 3826, "ƒ°unfortun": 3827, "ƒ°draws": 3828, "ƒ°breaking": 3829, "ƒ°indepe": 3830, "cting": 3831, "ƒ°applic": 3832, "ƒ°regret": 3833, "ƒ°tasks": 3834, "ƒ°au": 3835, "vere": 3836, "ƒ°pure": 3837, "ƒ°vio": 3838, "ƒ°trash": 3839, "ƒ°literal": 3840, "ƒ°insane": 3841, "ƒ°assume": 3842, "love": 3843, "well": 3844, "ƒ°edit": 3845, "ƒ°sksksks": 3846, "uino": 3847, "49": 3848, "ull": 3849, "ƒ°dry": 3850, "ery": 3851, "ƒ°hist": 3852, "ƒ°>.<": 3853, "01": 3854, "va": 3855, "ƒ°omggg": 3856, "ƒ°talked": 3857, "ploy": 3858, "ƒ°older": 3859, "ƒ°negative": 3860, "ƒ°opposite": 3861, "ƒ°tatt": 3862, "ƒ°cum": 3863, "let": 3864, "ley": 3865, "ƒ°jimin": 3866, "ƒ°advent": 3867, "ƒ°harder": 3868, "ƒ°setting": 3869, "ƒ°technology": 3870, "ƒ°overwhelmed": 3871, "ƒ°assault": 3872, "ƒ°diagn": 3873, "ium": 3874, "iest": 3875, "√¢ƒæ": 3876, "ƒ°prog": 3877, "ƒ°veux": 3878, "ƒ°cla": 3879, "ƒ°epis": 3880, "aries": 3881, "ƒ°lock": 3882, "ƒ°frog": 3883, "ƒ°remembered": 3884, "37": 3885, "acc": 3886, "ƒ°dare": 3887, "ƒ°net": 3888, "tery": 3889, "ƒ°suit": 3890, "ƒ°caused": 3891, "ƒ°freaking": 3892, "ƒ°promise": 3893, "cr": 3894, "ƒ°storm": 3895, "ƒ°convin": 3896, "ƒ°shall": 3897, "ƒ°chinese": 3898, "√†": 3899, "ƒ°tha": 3900, "atic": 3901, "ƒ°jam": 3902, "ƒ°offic": 3903, "aaa": 3904, "ƒ°solid": 3905, "di": 3906, "val": 3907, "ƒ°alex": 3908, "ona": 3909, "utm": 3910, "ƒ°stor": 3911, "ƒ°manip": 3912, "ƒ°sharing": 3913, "ƒ°slowly": 3914, "ƒ°meat": 3915, "ƒ°ability": 3916, "though": 3917, "cious": 3918, "ƒ°ground": 3919, "ƒ°include": 3920, "ƒ°unfortunately": 3921, "ƒ°hous": 3922, "ƒ°ranting": 3923, "ƒ°tattoo": 3924, "bby": 3925, "dit": 3926, "ƒ°√™ƒ∑√£ƒ£¬£": 3927, "ƒ°idfk": 3928, "ƒ°choc": 3929, "ƒ°charge": 3930, "ƒ°provide": 3931, "ƒ°delivery": 3932, "llm": 3933, "ank": 3934, "aming": 3935, "ƒ°sauce": 3936, "ƒ°develop": 3937, "phones": 3938, "ƒ°ignoring": 3939, "ƒ°progress": 3940, "bed": 3941, "ƒ°showed": 3942, "ƒ°80": 3943, "ƒ°common": 3944, "iction": 3945, "ƒ°cunt": 3946, "ƒ°physically": 3947, "ƒ°memory": 3948, "ƒ°attempt": 3949, "ƒ°hospital": 3950, "ƒ°tid": 3951, "ƒ°pin": 3952, "asm": 3953, "ƒ°js": 3954, "ƒ°21": 3955, "ƒ°catch": 3956, "ƒ°lights": 3957, "ƒ°building": 3958, "ƒ°judge": 3959, "ƒ°prescription": 3960, "ingly": 3961, "ƒ°shou": 3962, "ƒ°var": 3963, "ƒ°blame": 3964, "ƒ°click": 3965, "ƒ°mais": 3966, "ƒ°steal": 3967, "ƒ°topic": 3968, "ƒ°replying": 3969, "ƒ°following": 3970, "ƒ°changing": 3971, "ƒ°replied": 3972, "ƒ°ambulance": 3973, "fety": 3974, "ural": 3975, "ƒ°uhhh": 3976, "ƒ°sounded": 3977, "ƒ°adult": 3978, "ƒ°boat": 3979, "ƒ°news": 3980, "ƒ°melt": 3981, "ƒ°cap": 3982, "ables": 3983, "ƒ°extre": 3984, "ƒ°hoping": 3985, "ƒ°downstairs": 3986, "ƒ°understood": 3987, "ƒ°aaagh": 3988, "ƒ°enjoying": 3989, "ƒ°lyrics": 3990, "ƒ°tuesday": 3991, "ƒ°aggress": 3992, "59": 3993, "hop": 3994, "ƒ°nhs": 3995, "ƒ°meth": 3996, "ƒ°jfc": 3997, "ƒ°research": 3998, "ƒ°university": 3999, "sss": 4000, "ƒ°env": 4001, "ƒ°medical": 4002, "ƒ°windows": 4003, "underst": 4004, "ƒ°sandw": 4005, "ƒ°wednesday": 4006, "uit": 4007, "ƒ°cult": 4008, "ƒ°lou": 4009, "ƒ°bear": 4010, "ƒ°dose": 4011, "gent": 4012, "ƒ°void": 4013, "ƒ°prior": 4014, "activ": 4015, "ƒ°sympt": 4016, "ƒ°audio": 4017, "lol": 4018, "ƒ°mr": 4019, "ƒ°dom": 4020, "ano": 4021, "ƒ°kicked": 4022, "ƒ°acting": 4023, "ƒ°vocals": 4024, "omg": 4025, "ƒ°wet": 4026, "rated": 4027, "ƒ°lmaoo": 4028, "ƒ°present": 4029, "ƒ°service": 4030, "ƒ°married": 4031, "ƒ°challeng": 4032, "core": 4033, "gb": 4034, "icy": 4035, "str": 4036, "ational": 4037, "ibility": 4038, "ƒ°clar": 4039, "ƒ°club": 4040, "ƒ°boob": 4041, "ƒ°reasonable": 4042, "√™ƒ∫√™ƒ∑√£ƒ£¬£": 4043, "uro": 4044, "renc": 4045, "ƒ°nat": 4046, "ƒ°punch": 4047, "ƒ°knowledge": 4048, "ƒ°28": 4049, "ƒ°services": 4050, "ƒ°posted": 4051, "charis": 4052, "ƒ°effects": 4053, "ƒ°ice": 4054, "ƒ°toys": 4055, "ield": 4056, "ƒ°king": 4057, "ƒ°sorta": 4058, "ƒ°prev": 4059, "ƒ°simp": 4060, "ƒ°module": 4061, "ƒ°refer": 4062, "ƒ°evil": 4063, "ƒ°mouth": 4064, "icks": 4065, "ƒ°attra": 4066, "ots": 4067, "ƒ°encou": 4068, "ƒ°female": 4069, "osis": 4070, "ships": 4071, "els": 4072, "ƒ°visual": 4073, "ƒ°argument": 4074, "ƒ°rout": 4075, "ƒ°safety": 4076, "ilities": 4077, "ƒ°permission": 4078, "ƒ°related": 4079, "baby": 4080, "jpg": 4081, "ƒ°san": 4082, "ƒ°uhh": 4083, "ƒ°trou": 4084, "ƒ°veget": 4085, "ƒ°shows": 4086, "ƒ°tenant": 4087, "ƒ°born": 4088, "ƒ°hanging": 4089, "ƒ°mildly": 4090, "iry": 4091, "ƒ°base": 4092, "isting": 4093, "ƒ°pink": 4094, "ƒ°hilar": 4095, "ƒ°causing": 4096, "ƒ°unfair": 4097, "ƒ°easily": 4098, "07": 4099, "ony": 4100, "ƒ°wedding": 4101, "ƒ°business": 4102, ":": 4103, "ƒ°intera": 4104, "ids": 4105, "raid": 4106, "ƒ°neigh": 4107, "ils": 4108, "ggy": 4109, "ƒ°unlike": 4110, "ƒ°customer": 4111, "ƒ°revision": 4112, "cord": 4113, "dr": 4114, "ians": 4115, "llo": 4116, "ƒ°notes": 4117, "ƒ°playlist": 4118, "ƒ°complic": 4119, "ƒ°legs": 4120, "ƒ°goddam": 4121, "ƒ°explaining": 4122, "ƒ°shouted": 4123, "ƒ°idc": 4124, "ƒ°volu": 4125, "ƒ°terms": 4126, "ƒ°cookies": 4127, "ƒ°example": 4128, "ƒ°details": 4129, "ƒ°murder": 4130, "af": 4131, "ƒ°il": 4132, "test": 4133, "ƒ°input": 4134, "ƒ°lecture": 4135, "fff": 4136, "ƒ°boss": 4137, "ƒ°parts": 4138, "ƒ°respond": 4139, "ƒ°holding": 4140, "ƒ°disgusting": 4141, "67": 4142, "ƒ°si": 4143, "ƒ°ginger": 4144, "ƒ°jus": 4145, "ƒ°station": 4146, "ƒ°feet": 4147, "ƒ°reality": 4148, "ƒ°reaction": 4149, "lit": 4150, "ƒ°hat": 4151, "ƒ°meal": 4152, "ette": 4153, "men": 4154, "estyle": 4155, "ƒ°marks": 4156, "hetic": 4157, "ƒ°empty": 4158, "ƒ°terrible": 4159, "ƒ°history": 4160, "ƒ°neighb": 4161, "ƒ°@": 4162, "isions": 4163, "ƒ°ordered": 4164, "this": 4165, "ƒ°potent": 4166, "ƒ°repea": 4167, "ƒ°explained": 4168, "ƒ°independent": 4169, "ƒ°√™ƒ∑√£ƒ£¬£√™ƒ∫": 4170, "05": 4171, "ƒ°nom": 4172, "ƒ°hig": 4173, "ƒ°phr": 4174, "ƒ°prin": 4175, "ƒ°role": 4176, "hahah": 4177, "ƒ°cod": 4178, "vered": 4179, "ƒ°nick": 4180, "ƒ°whis": 4181, "ƒ°moi": 4182, "iber": 4183, "ƒ°starts": 4184, "ƒ°misunderst": 4185, "ƒ°recording": 4186, "ƒ°magic": 4187, "ƒ°unsure": 4188, "ases": 4189, "ƒ°rush": 4190, "ƒ°separ": 4191, "ives": 4192, "ummy": 4193, "ƒ°oml": 4194, "ƒ°buying": 4195, "ƒ°adding": 4196, "ƒ°imagin": 4197, "sorry": 4198, "ƒ°behaviour": 4199, "ƒ°dig": 4200, "ƒ°dating": 4201, "ƒ°hoo": 4202, "ƒ°study": 4203, "ƒ°blind": 4204, "ized": 4205, "oom": 4206, "sing": 4207, "ƒ°sea": 4208, "ƒ°everyones": 4209, "ƒ°harrow": 4210, "ussy": 4211, "ƒ°chocolate": 4212, "88": 4213, "ƒ°itll": 4214, "illed": 4215, "ƒ°store": 4216, "ƒ°soc": 4217, "ƒ°anger": 4218, "ƒ°mua": 4219, "ƒ°nap": 4220, "cco": 4221, "ƒ°shopping": 4222, "ƒ°chairs": 4223, "ƒ°certainly": 4224, "ƒ°involved": 4225, "ƒ°degree": 4226, "can": 4227, "lines": 4228, "ƒ°ave": 4229, "ƒ°bes": 4230, "ƒ°tooo": 4231, "ƒ°area": 4232, "neur": 4233, "ƒ°positive": 4234, "rest": 4235, "ƒ°cream": 4236, "ƒ°forced": 4237, "iron": 4238, "cape": 4239, "ƒ°radio": 4240, "ƒ°advert": 4241, "ƒ°skull": 4242, "ƒ°potato": 4243, "ƒ°scene": 4244, "ƒ°hilarious": 4245, "44": 4246, "ƒ°reee": 4247, "ƒ°kink": 4248, "ƒ°keys": 4249, "ƒ°pregn": 4250, "ƒ°nahhh": 4251, "ƒ°afraid": 4252, "ƒ°scam": 4253, "ƒ°hole": 4254, "ƒ°sentence": 4255, "ƒ°midnight": 4256, "ƒ°distracted": 4257, "bum": 4258, "ƒ°ug": 4259, "ƒ°jul": 4260, "ƒ°homeless": 4261, "ƒ°status": 4262, "ƒ°dragon": 4263, "ƒ°legend": 4264, "ƒ°extreme": 4265, "mod": 4266, "ƒ°tun": 4267, "ana": 4268, "output": 4269, "ƒ°failed": 4270, "ƒ°colours": 4271, "ƒ°occas": 4272, "ƒ°article": 4273, "dk": 4274, "ƒ°lab": 4275, "icted": 4276, "ƒ°ks": 4277, "antic": 4278, "ƒ°proof": 4279, "ƒ°bringing": 4280, "ƒ°spamming": 4281, "ƒ°acceptable": 4282, "band": 4283, "ƒ°heat": 4284, "ƒ°mega": 4285, "rose": 4286, "ƒ°background": 4287, "ƒ°checking": 4288, "ƒ°flow": 4289, "ƒ°basic": 4290, "ƒ°pointless": 4291, "ƒ°struggle": 4292, "ƒ°street": 4293, "ƒ°bullshit": 4294, "sr": 4295, "ƒ°mal": 4296, "ƒ°ton": 4297, "ƒ°prop": 4298, "ƒ°personally": 4299, "ƒ°tes": 4300, "ƒ°dy": 4301, "ƒ°eaten": 4302, "ƒ°staff": 4303, "ƒ°upstairs": 4304, "ƒ°sav": 4305, "ƒ°device": 4306, "ƒ°happiness": 4307, "ƒ°coff": 4308, "ƒ°decisions": 4309, "ƒ°levels": 4310, "32": 4311, "87": 4312 }, "merges": [ [ "ƒ°", "t" ], [ "ƒ°", "i" ], [ "ƒ°", "a" ], [ "ƒ°", "w" ], [ "i", "n" ], [ "h", "e" ], [ "ƒ°", "s" ], [ "o", "u" ], [ "h", "a" ], [ "ƒ°", "b" ], [ "ƒ°", "m" ], [ "in", "g" ], [ "r", "e" ], [ "ƒ°", "c" ], [ "ƒ°", "l" ], [ "ƒ°t", "he" ], [ "ƒ°", "o" ], [ "ƒ°", "d" ], [ "ƒ°", "f" ], [ "ƒ°", "y" ], [ "n", "d" ], [ "v", "e" ], [ "ƒ°t", "o" ], [ "l", "l" ], [ "i", "s" ], [ "ƒ°", "n" ], [ "e", "e" ], [ "ƒ°", "p" ], [ "o", "n" ], [ "ƒ°y", "ou" ], [ "a", "n" ], [ "ƒ°", "e" ], [ "t", "e" ], [ "o", "r" ], [ "ƒ°", "g" ], [ "k", "e" ], [ "ƒ°i", "t" ], [ "ƒ°a", "nd" ], [ "u", "s" ], [ "ƒ°", "ha" ], [ "ha", "t" ], [ "e", "s" ], [ "ƒ°", "u" ], [ "i", "t" ], [ "a", "s" ], [ "ƒ°", "he" ], [ "ƒ°", "h" ], [ "o", "m" ], [ "i", "c" ], [ "ƒ°n", "o" ], [ "ƒ°t", "h" ], [ "ƒ°b", "e" ], [ "u", "t" ], [ "ƒ°m", "e" ], [ "a", "y" ], [ "a", "r" ], [ "o", "o" ], [ "ƒ°i", "n" ], [ "e", "r" ], [ "ƒ°t", "hat" ], [ "ƒ°i", "s" ], [ "l", "d" ], [ "l", "e" ], [ "ƒ°b", "ut" ], [ "ƒ°w", "e" ], [ "i", "e" ], [ "o", "w" ], [ "n", "t" ], [ "ƒ°m", "y" ], [ "ƒ°l", "i" ], [ "a", "t" ], [ "l", "y" ], [ "ƒ°", "j" ], [ "ƒ°o", "f" ], [ "ou", "ld" ], [ "i", "ll" ], [ "ƒ°", "re" ], [ "us", "t" ], [ "ƒ°c", "ha" ], [ "i", "d" ], [ "ƒ°d", "o" ], [ "ƒ°o", "n" ], [ "g", "h" ], [ "o", "d" ], [ "e", "t" ], [ "e", "d" ], [ "ƒ°w", "as" ], [ "ƒ°ha", "ve" ], [ "r", "is" ], [ "r", "a" ], [ "ƒ°s", "t" ], [ "ƒ°l", "o" ], [ "a", "l" ], [ "ƒ°cha", "ris" ], [ "ƒ°f", "or" ], [ "ƒ°", "k" ], [ "ƒ°li", "ke" ], [ "ƒ°i", "m" ], [ "k", "ing" ], [ "ƒ°no", "t" ], [ "m", "e" ], [ "ou", "t" ], [ "ee", "n" ], [ "ƒ°s", "o" ], [ "a", "ll" ], [ "r", "o" ], [ "ee", "d" ], [ "ƒ°w", "ill" ], [ "ƒ°e", "l" ], [ "r", "y" ], [ "an", "t" ], [ "ƒ°a", "n" ], [ "c", "h" ], [ "ƒ°j", "ust" ], [ "i", "r" ], [ "a", "d" ], [ "ƒ°b", "een" ], [ "ƒ°el", "od" ], [ "ƒ°the", "y" ], [ "ƒ°elod", "ie" ], [ "s", "e" ], [ "ƒ°w", "h" ], [ "'", "s" ], [ "ƒ°s", "h" ], [ "ƒ°w", "hat" ], [ "e", "a" ], [ "gh", "t" ], [ "ƒ°s", "he" ], [ "ƒ°a", "t" ], [ "all", "y" ], [ "c", "e" ], [ "c", "k" ], [ "ƒ°c", "an" ], [ "ƒ°th", "is" ], [ "i", "on" ], [ "c", "t" ], [ "ƒ°f", "r" ], [ "om", "e" ], [ "te", "r" ], [ "it", "h" ], [ "ƒ°a", "re" ], [ "o", "t" ], [ "ƒ°w", "ith" ], [ "ƒ°", "ke" ], [ "s", "t" ], [ "v", "in" ], [ "ƒ°d", "on" ], [ "us", "e" ], [ "ƒ°a", "b" ], [ "ƒ°m", "o" ], [ "ƒ°", ":" ], [ "ƒ°ke", "vin" ], [ "ƒ°a", "s" ], [ "ƒ°e", "ve" ], [ "o", "p" ], [ "a", "ke" ], [ "ƒ°it", "s" ], [ "ƒ°a", "l" ], [ "g", "e" ], [ "ƒ°", "r" ], [ "ƒ°w", "ant" ], [ "ƒ°u", "p" ], [ "ƒ°", "v" ], [ "ƒ°s", "u" ], [ ".", "." ], [ "in", "k" ], [ "te", "n" ], [ "ƒ°you", "r" ], [ "ƒ°c", "a" ], [ "ƒ°w", "or" ], [ "i", "ght" ], [ "p", "p" ], [ "ƒ°i", "f" ], [ "ve", "r" ], [ "i", "m" ], [ "ƒ°k", "n" ], [ "ƒ°ab", "out" ], [ "'", "t" ], [ "e", "n" ], [ "ƒ°n", "eed" ], [ "ƒ°g", "et" ], [ "oo", "d" ], [ "ic", "e" ], [ "ƒ°fr", "om" ], [ "ƒ°lo", "ve" ], [ "ƒ°he", "r" ], [ "a", "b" ], [ "ƒ°w", "ould" ], [ "ou", "r" ], [ "ƒ°w", "he" ], [ "ƒ°kn", "ow" ], [ "ƒ°ha", "d" ], [ "ƒ°o", "r" ], [ "a", "m" ], [ "ƒ°l", "is" ], [ "te", "d" ], [ "u", "r" ], [ "ƒ°l", "oo" ], [ "es", "s" ], [ "ƒ°f", "ee" ], [ "ƒ°lis", "ten" ], [ "ƒ°s", "a" ], [ "ƒ°l", "e" ], [ "ƒ°c", "ould" ], [ "ƒ°g", "o" ], [ "ƒ°fee", "l" ], [ "ƒ°w", "eed" ], [ "ƒ°sh", "ould" ], [ "ƒ°d", "id" ], [ "in", "d" ], [ "ƒ°a", "m" ], [ "ƒ°wh", "y" ], [ "u", "n" ], [ "on", "e" ], [ "ƒ°a", "ll" ], [ "ƒ°p", "l" ], [ "ƒ°i", "d" ], [ "ƒ°s", "e" ], [ "he", "r" ], [ "ƒ°d", "e" ], [ "t", "h" ], [ "ƒ°e", "x" ], [ "c", "a" ], [ "ƒ°no", "w" ], [ "'", "m" ], [ "n", "a" ], [ "ƒ°s", "ee" ], [ "i", "ve" ], [ "v", "ing" ], [ "f", "f" ], [ "ƒ°p", "ro" ], [ "ƒ°we", "re" ], [ "ƒ°th", "ink" ], [ "ƒ°", "x" ], [ "ƒ°don", "t" ], [ "ƒ°but", "t" ], [ "ƒ°h", "ow" ], [ "k", "s" ], [ "ƒ°whe", "n" ], [ "ƒ°n", "e" ], [ "u", "m" ], [ "re", "d" ], [ "ƒ°", "1" ], [ "ƒ°s", "ome" ], [ "ƒ°", "out" ], ["ƒ°lo", "l" ], [ "ƒ°o", "h" ], [ "ƒ°o", "k" ], [ "ƒ°an", "y" ], [ "ƒ°", "'" ], [ "ea", "h" ], [ "ƒ°to", "o" ], [ "ƒ°c", "om" ], [ "s", "s" ], [ "ou", "nd" ], [ "ƒ°y", "eah" ], [ "ƒ°s", "m" ], [ "u", "l" ], [ "i", "l" ], [ "ƒ°m", "us" ], [ "ƒ°eve", "n" ], [ "a", "in" ], [ "ƒ°re", "ally" ], [ "ƒ°mus", "ic" ], [ "'", "re" ], [ "ƒ°f", "u" ], [ "p", "e" ], [ "ƒ°c", "on" ], [ "ƒ°", "-" ], [ "ƒ°p", "e" ], [ "ƒ°th", "ing" ], [ "ƒ°o", "m" ], [ "i", "me" ], [ "s", "o" ], [ "ee", "p" ], [ "ƒ°f", "a" ], [ "ƒ°h", "is" ], [ "ƒ°", "q" ], [ "m", "a" ], [ "ƒ°the", "ir" ], [ "ha", "n" ], [ "ƒ°the", "m" ], [ "in", "e" ], [ "ƒ°that", "s" ], [ "l", "f" ], [ "ƒ°s", "or" ], [ "es", "t" ], [ "a", "te" ], [ "ƒ°a", "w" ], [ "ay", "s" ], [ "g", "g" ], [ "ƒ°on", "e" ], [ "t", "ing" ], [ "on", "g" ], [ "ƒ°m", "ust" ], [ "ƒ°m", "ake" ], [ "ƒ°v", "o" ], [ "ƒ°y", "es" ], [ "ƒ°t", "han" ], [ "ƒ°ha", "s" ], [ "ƒ°t", "r" ], [ "ƒ°listen", "ing" ], [ "ƒ°ca", "use" ], [ "at", "ion" ], [ "ƒ°w", "an" ], [ "ƒ°g", "ood" ], [ "ƒ°x", "d" ], [ "ƒ°al", "so" ], [ "ƒ°me", "an" ], [ "ƒ°the", "n" ], [ "ƒ°u", "r" ], [ "ƒ°be", "ca" ], [ "ra", "in" ], [ "ƒ°om", "g" ], [ "ƒ°t", "ime" ], [ "ƒ°the", "re" ], [ "ƒ°beca", "use" ], [ "as", "t" ], [ "ƒ°", "us" ], [ "ƒ°s", "ay" ], [ "ƒ°h", "im" ], [ "ƒ°sor", "ry" ], [ "o", "re" ], [ "ƒ°", "2" ], [ "ƒ°th", "ou" ], [ "f", "e" ], [ "ƒ°b", "a" ], [ "ƒ°st", "u" ], [ "ƒ°w", "a" ], [ "h", "ing" ], [ "ƒ°t", "e" ], [ "o", "l" ], [ "t", "y" ], [ "ƒ°he", "ar" ], [ "ƒ°t", "ake" ], [ "ƒ°do", "es" ], [ "ƒ°g", "ot" ], [ "ƒ°s", "p" ], [ "ƒ°", "(" ], [ "i", "te" ], [ "ƒ°t", "b" ], [ "ƒ°mo", "re" ], [ "u", "e" ], [ "ƒ°we", "ll" ], [ "b", "e" ], [ "ƒ°wh", "o" ], [ "ƒ°u", "n" ], [ "ƒ°a", "g" ], [ "i", "g" ], [ "ƒ°pe", "op" ], [ "ƒ°e", "n" ], [ "id", "e" ], [ "ƒ°vo", "ice" ], [ "ƒ°th", "o" ], [ "!", "!" ], [ "ic", "k" ], [ "ƒ°pro", "b" ], [ "ƒ°a", "ct" ], [ "ƒ°s", "l" ], [ "ƒ°l", "ea" ], [ "ƒ°m", "u" ], [ "a", "king" ], [ "ƒ°peop", "le" ], [ "..", "." ], [ "ƒ°l", "ma" ], [ "ƒ°g", "u" ], [ "p", "er" ], [ "ƒ°a", "r" ], [ "ƒ°p", "o" ], [ "u", "ally" ], [ "ƒ°f", "e" ], [ "ƒ°be", "ing" ], [ "ƒ°on", "ly" ], [ "ƒ°pl", "ay" ], [ "ƒ°eve", "ry" ], [ "ƒ°", "our" ], [ "ƒ°:", ")" ], [ "er", "s" ], [ "om", "et" ], [ "d", "s" ], [ "ƒ°e", "at" ], [ "ƒ°ha", "ha" ], [ "ƒ°d", "is" ], [ "ƒ°mu", "ch" ], [ "ƒ°s", "omet" ], [ "ƒ°c", "u" ], [ "ke", "d" ], [ "ƒ°d", "r" ], [ "or", "t" ], [ "i", "f" ], [ "ƒ°p", "re" ], [ "p", "s" ], [ "ou", "s" ], [ "d", "d" ], [ "p", "t" ], [ "ƒ°loo", "k" ], [ "ar", "n" ], [ "ƒ°t", "al" ], [ "ƒ°wan", "na" ], [ "t", "her" ], [ "ƒ°c", "h" ], [ "ƒ°g", "e" ], [ "oo", "l" ], [ "ƒ°w", "r" ], [ "ƒ°thing", "s" ], [ "a", "nd" ], [ "o", "s" ], [ "a", "g" ], [ "ƒ°s", "ound" ], [ "ƒ°t", "ou" ], [ "d", "ay" ], [ "ƒ°o", "ver" ], [ "ƒ°b", "y" ], [ "se", "lf" ], [ "i", "b" ], [ "ƒ°m", "ay" ], [ "?", "?" ], [ "o", "y" ], [ "ar", "t" ], [ "ƒ°ha", "pp" ], [ "ƒ°tb", "h" ], [ "ƒ°he", "l" ], [ "ƒ°b", "it" ], [ "w", "w" ], [ "ƒ°lma", "o" ], [ "nd", "er" ], [ "is", "s" ], [ "ƒ°b", "ad" ], [ "ƒ°wor", "k" ], [ "ƒ°stu", "ff" ], [ "ƒ°n", "a" ], [ "ƒ°thou", "ght" ], [ "ƒ°t", "ry" ], [ "ƒ°p", "h" ], [ "ƒ°b", "rain" ], [ "ƒ°st", "ill" ], [ "ƒ°l", "a" ], [ "ƒ°k", "ind" ], [ "ƒ°ca", "re" ], [ "ƒ°a", "f" ], [ "ƒ°go", "ing" ], [ "ƒ°sm", "o" ], [ "ƒ°com", "p" ], [ "ƒ°le", "arn" ], [ "or", "ge" ], [ "'", "ve" ], [ "u", "re" ], [ "ƒ°sh", "it" ], [ "'", "ll" ], [ "ƒ°g", "ive" ], [ "ƒ°of", "f" ], [ "me", "nt" ], [ "as", "s" ], [ "ƒ°mo", "ve" ], [ "ƒ°fu", "ck" ], [ "ƒ°", "ve" ], [ "ƒ°:", "(" ], [ "ƒ°b", "l" ], [ "th", "ing" ], [ "'", "d" ], [ "re", "ss" ], [ "ƒ°y", "e" ], [ "ƒ°hel", "p" ], [ "t", "o" ], [ "ab", "le" ], [ "ƒ°wa", "it" ], [ "ƒ°ge", "orge" ], [ "c", "c" ], [ "ƒ°c", "at" ], [ "ƒ°act", "ually" ], [ "ƒ°id", "k" ], [ "ƒ°c", "o" ], [ "i", "es" ], [ "ll", "y" ], [ "ie", "nd" ], [ "ƒ°c", "l" ], [ "ƒ°m", "an" ], [ "ou", "gh" ], [ "ƒ°sa", "id" ], [ "0", "0" ], [ "ƒ°sa", "me" ], [ "ƒ°ba", "ck" ], [ "ƒ°loo", "king" ], [ "a", "ge" ], [ "as", "e" ], [ "t", "s" ], [ "ƒ°tou", "ch" ], [ "√¢", "ƒ£" ], [ "ƒ°somet", "hing" ], [ "ic", "h" ], [ "re", "e" ], [ "ƒ°c", "ant" ], [ "ƒ°d", "ay" ], [ "ƒ°w", "ay" ], [ "ou", "se" ], [ "ƒ°d", "ra" ], [ "ƒ°c", "ome" ], [ "re", "nt" ], [ "ƒ°", "ra" ], [ "is", "h" ], [ "re", "am" ], [ "c", "i" ], [ "ƒ°a", "pp" ], [ "ƒ°j", "o" ], [ "h", "h" ], [ "ƒ°o", "ther" ], [ "ƒ°d", "i" ], [ "ƒ°sl", "eep" ], [ "ƒ°q", "u" ], [ "m", "b" ], [ "ƒ°ha", "te" ], [ "ƒ°t", "aking" ], [ "ƒ°", "3" ], [ "ƒ°t", "w" ], [ "ow", "n" ], [ "ƒ°r", "ight" ], [ "ƒ°s", "c" ], [ "i", "le" ], [ "ƒ°do", "ing" ], [ "ƒ°af", "ter" ], [ "ar", "d" ], [ "y", "s" ], [ "in", "king" ], [ "ƒ°be", "t" ], [ "ƒ°he", "re" ], [ "l", "t" ], [ "ƒ°c", "all" ], [ "ƒ°", "*" ], [ "p", "l" ], [ "ƒ°mo", "ving" ], [ "u", "d" ], [ "ƒ°", "\"" ], [ "ƒ°p", "r" ], [ "t", "t" ], [ "ƒ°wh", "ich" ], [ "ƒ°ne", "ver" ], [ "ƒ°p", "ic" ], [ "ƒ°f", "ood" ], [ "ƒ°d", "ays" ], [ "ƒ°in", "to" ], [ "ƒ°feel", "ing" ], [ "ƒ°n", "ice" ], [ "ƒ°want", "ing" ], [ "ƒ°", "ro" ], [ "ƒ°fr", "iend" ], [ "is", "t" ], [ "ab", "ly" ], [ "ƒ°l", "et" ], [ "ƒ°m", "ad" ], [ "ƒ°lea", "ve" ], [ "m", "s" ], [ "ƒ°ve", "ry" ], [ "ƒ°b", "r" ], [ "ƒ°h", "ouse" ], [ "ƒ°c", "ool" ], [ "ƒ°dra", "w" ], [ "i", "p" ], [ "ƒ°w", "ee" ], [ "ƒ°h", "op" ], [ "ƒ°ag", "ain" ], [ "m", "m" ], [ "ƒ°", "<" ], [ "f", "ore" ], [ "ck", "s" ], [ "ƒ°m", "ight" ], [ "ƒ°u", "nder" ], [ "u", "g" ], [ "ƒ°li", "fe" ], [ "ie", "d" ], [ "c", "om" ], [ "it", "y" ], [ "ƒ°ha", "r" ], [ "i", "x" ], [ "ƒ°te", "ll" ], [ "c", "king" ], [ "ƒ°a", "d" ], [ "ƒ°b", "o" ], [ "ƒ°prob", "ably" ], [ "an", "ce" ], [ "ƒ°c", "he" ], [ "ƒ°fa", "ir" ], [ "ƒ°fe", "lt" ], [ "t", "a" ], [ "ƒ°may", "be" ], [ "r", "ow" ], [ "ƒ°g", "on" ], [ "ir", "d" ], [ "ƒ°did", "nt" ], [ "ƒ°be", "fore" ], [ "ƒ°h", "our" ], [ "g", "et" ], [ "re", "at" ], [ "g", "le" ], [ "ƒ°p", "le" ], [ "i", "ous" ], [ "ƒ°tal", "k" ], [ "ƒ°s", "ca" ], [ "ƒ°b", "ab" ], [ "i", "gh" ], [ "ƒ°f", "l" ], [ "ƒ°smo", "king" ], [ "ƒ°su", "re" ], [ "ƒ°b", "ot" ], [ "ƒ°hop", "e" ], [ "in", "t" ], [ "is", "e" ], [ "ƒ°d", "own" ], [ "a", "ce" ], [ "ƒ°su", "pp" ], [ "ƒ°we", "ird" ], [ "ƒ°gon", "na" ], [ "a", "a" ], [ "te", "ra" ], [ "ƒ°i", "ll" ], [ "oo", "o" ], [ "w", "ays" ], [ "ƒ°ye", "ar" ], [ "ƒ°p", "er" ], [ "ƒ°re", "al" ], [ ":", "/" ], [ "c", "he" ], [ "ƒ°m", "in" ], [ "on", "t" ], [ "at", "ch" ], [ "ƒ°ple", "ase" ], [ "c", "o" ], [ "ƒ°kind", "a" ], [ "im", "es" ], [ "n", "e" ], [ "q", "u" ], [ "or", "d" ], [ "ƒ°sca", "red" ], [ "ƒ°", "4" ], [ "ad", "y" ], [ "a", "ct" ], [ "ƒ°h", "tt" ], [ "s", "h" ], [ "ƒ°al", "ways" ], [ "ƒ°b", "oo" ], [ "ƒ°ph", "one" ], [ "an", "s" ], [ "ƒ°m", "um" ], [ "ƒ°l", "ong" ], [ "ul", "t" ], [ "ƒ°li", "tera" ], [ "ƒ°need", "ing" ], [ "ƒ°any", "thing" ], [ "ƒ°f", "ind" ], [ "x", "t" ], [ "ƒ°f", "un" ], [ "ƒ°lo", "t" ], [ "ƒ°my", "self" ], [ "ƒ°sound", "s" ], [ "ƒ°tr", "ue" ], [ "ic", "ally" ], [ "ion", "s" ], [ "ƒ°in", "s" ], [ "ƒ°whe", "re" ], [ "or", "m" ], [ "ƒ°bet", "ter" ], [ "ƒ°id", "ea" ], [ "ƒ°id", "e" ], [ "ƒ°be", "d" ], [ "ƒ°st", "ream" ], [ "ƒ°b", "u" ], [ "ƒ°se", "x" ], [ "ƒ°to", "day" ], [ "ƒ°", "5" ], [ "ƒ°p", "ers" ], [ "ƒ°f", "ro" ], [ "ƒ°", "use" ], [ "ar", "k" ], [ "ƒ°mad", "e" ], [ "ff", "e" ], [ "ƒ°litera", "lly" ], [ "ƒ°try", "ing" ], [ "ƒ°wor", "d" ], [ "mb", "er" ], [ "it", "ing" ], [ "ƒ°than", "ks" ], [ "o", "g" ], [ "ƒ°ne", "w" ], [ "ƒ°happ", "en" ], [ "ƒ°lea", "ving" ], [ "ƒ°har", "d" ], [ "!", "?" ], [ "ƒ°b", "re" ], [ "ƒ°s", "pe" ], [ "ƒ°p", "a" ], [ "f", "t" ], [ "ƒ°b", "ro" ], [ "ƒ°re", "s" ], [ "ƒ°pers", "on" ], [ "ƒ°get", "ting" ], [ "ƒ°eat", "ing" ], [ "b", "s" ], [ "ƒ°m", "a" ], [ "ƒ°re", "me" ], [ "ƒ°st", "op" ], [ "ƒ°t", "ur" ], [ "ir", "st" ], [ "ƒ°p", "art" ], [ "ƒ°fu", "cking" ], [ "t", "f" ], [ "ib", "le" ], [ "m", "or" ], [ "ƒ°la", "ter" ], [ "ƒ°under", "st" ], [ "w", "ay" ], [ "ƒ°hear", "ing" ], [ "ƒ°say", "ing" ], [ "ƒ°g", "r" ], [ "at", "ing" ], [ "√¢ƒ£", "ƒº" ], [ "n", "g" ], [ "ƒ°s", "ha" ], [ "ƒ°m", "on" ], [ "ƒ°a", "cc" ], [ "l", "es" ], [ "ƒ°learn", "ing" ], [ "ƒ°f", "irst" ], [ "ƒ°s", "ong" ], [ "ƒ°m", "ess" ], [ "ƒ°n", "ight" ], [ "t", "le" ], [ "p", "le" ], [ "f", "a" ], [ "od", "y" ], [ "ƒ°st", "ar" ], [ "ƒ°e", "nd" ], [ "ƒ°happ", "y" ], [ "ƒ°cu", "te" ], [ "ƒ°th", "r" ], [ "ƒ°mean", "s" ], [ "ƒ°p", "ut" ], [ "ƒ°h", "ome" ], [ "ƒ°htt", "ps" ], [ "ƒ°so", "on" ], [ "t", "ty" ], [ "ƒ°p", "ick" ], [ "√¢", "¬£" ], [ "ƒ°eve", "r" ], [ "ƒ°m", "ind" ], [ "ƒ°ha", "ving" ], [ "as", "on" ], [ "ƒ°play", "ing" ], [ "um", "b" ], [ "n", "ce" ], [ "ƒ°thou", "gh" ], [ "ƒ°haha", "ha" ], [ "ƒ°aw", "ay" ], [ "ƒ°w", "tf" ], [ "ƒ°pre", "tty" ], [ "i", "z" ], [ "u", "al" ], [ "w", "n" ], [ "ƒ°make", "s" ], [ "ƒ°l", "it" ], [ "ƒ°don", "e" ], [ "f", "ul" ], [ "ƒ°reme", "mber" ], [ "ƒ°c", "ou" ], [ "ƒ°underst", "and" ], [ "ƒ°an", "n" ], [ "ƒ°touch", "ing" ], [ "ƒ°fro", "gg" ], [ "ƒ°some", "one" ], [ "ƒ°", "=" ], [ "ƒ°m", "iss" ], [ "i", "re" ], [ "ƒ°e", "m" ], [ "ƒ°le", "g" ], [ "ƒ°dr", "ink" ], [ "ƒ°cat", "s" ], [ "ƒ°", "√¢¬£" ], [ "ƒ°o", "p" ], [ "ƒ°dis", "c" ], [ "r", "i" ], [ "n", "ing" ], [ "ƒ°did", "n" ], [ "ƒ°f", "ine" ], [ "ƒ°d", "ad" ], [ "ƒ°thr", "ough" ], [ "ƒ°k", "eep" ], [ "un", "ch" ], [ "u", "ter" ], [ "ƒ°g", "a" ], [ "ƒ°sh", "ow" ], [ "ƒ°re", "ason" ], [ "ƒ°bot", "h" ], [ "ƒ°f", "in" ], [ "ƒ°tw", "o" ], [ "a", "k" ], [ "ƒ°comp", "uter" ], [ "ƒ°t", "ra" ], [ "ƒ°s", "ad" ], [ "ƒ°see", "ing" ], [ "ƒ°s", "w" ], [ "ƒ°to", "ld" ], [ "ƒ°con", "f" ], [ "ƒ°g", "od" ], [ "a", "ch" ], [ "ƒ°sm", "ink" ], [ "i", "ld" ], [ "ƒ°na", "me" ], [ "ƒ°im", "p" ], [ "n", "o" ], [ "ƒ°le", "ast" ], [ "ƒ°tal", "king" ], [ "a", "ted" ], [ "ƒ°any", "way" ], [ "ƒ°wr", "iting" ], [ "ƒ°ann", "oy" ], [ "an", "c" ], [ "ƒ°w", "ont" ], [ "ƒ°hour", "s" ], [ "ƒ°f", "re" ], [ "ƒ°a", "dd" ], [ "ƒ°sp", "ea" ], [ "ƒ°wan", "ted" ], [ "ƒ°pic", "king" ], [ "ƒ°en", "ough" ], [ "ud", "e" ], [ "ƒ°in", "te" ], [ "ƒ°s", "er" ], [ "ƒ°b", "as" ], [ "ƒ°b", "est" ], [ "ƒ°e", "t" ], [ "te", "s" ], [ "re", "ady" ], [ "ƒ°re", "ad" ], [ "ƒ°i", "ve" ], [ "o", "se" ], [ "ƒ°bab", "y" ], [ "ƒ°s", "y" ], [ "ar", "y" ], [ "ƒ°the", "se" ], [ "ƒ°g", "en" ], [ "ƒ°o", "ld" ], [ "ƒ°al", "ready" ], [ "ƒ°g", "ir" ], [ "ƒ°gu", "ess" ], [ "at", "h" ], [ "ƒ°a", "ut" ], [ "ƒ°h", "o" ], [ "ƒ°j", "e" ], [ "ƒ°h", "on" ], [ "ƒ°to", "mor" ], [ "ƒ°frogg", "y" ], [ "ƒ°c", "le" ], [ "ƒ°mo", "st" ], [ "ƒ°us", "ed" ], [ "n", "ess" ], [ "ve", "n" ], [ "ww", "w" ], [ "ƒ°in", "c" ], [ "it", "her" ], [ "ƒ°tomor", "row" ], [ "n", "er" ], [ "!!", "!" ], [ "ƒ°w", "al" ], [ "ƒ°he", "s" ], [ "ƒ°as", "s" ], [ "ƒ°every", "thing" ], [ "ar", "ly" ], [ "ƒ°an", "gle" ], [ "ƒ°does", "nt" ], [ "ƒ°pl", "an" ], [ "ƒ°t", "y" ], [ "ƒ°l", "unch" ], [ "ƒ°s", "en" ], [ "ƒ°na", "h" ], [ "ƒ°lit", "tle" ], [ "ƒ°draw", "ing" ], [ "ƒ°st", "r" ], [ "ƒ°not", "hing" ], [ "ƒ°a", "ce" ], [ "ƒ°fa", "ce" ], [ "ƒ°d", "es" ], [ "ƒ°se", "lf" ], [ "ƒ°l", "ast" ], [ "ƒ°m", "aking" ], [ "m", "o" ], [ "l", "ess" ], [ "ƒ°li", "ve" ], [ "ƒ°ex", "act" ], [ "ƒ°wr", "ong" ], [ "ƒ°g", "l" ], [ "ƒ°se", "nd" ], [ "ƒ°o", "wn" ], [ "one", "y" ], [ "ig", "n" ], [ "ƒ°s", "it" ], [ "ƒ°su", "per" ], [ "ƒ°hon", "est" ], [ "ƒ°con", "t" ], [ "ƒ°de", "f" ], [ "??", "?" ], [ "ver", "s" ], [ "ƒ°de", "c" ], [ "ƒ°ex", "pl" ], [ "y", "y" ], [ "l", "i" ], [ "ƒ°m", "ix" ], [ "ƒ°he", "ad" ], [ "ƒ°h", "u" ], [ "ƒ°c", "r" ], [ "ƒ°1", "0" ], [ "ƒ°friend", "s" ], [ "ƒ°every", "one" ], [ "ƒ°ro", "om" ], [ "ƒ°", "6" ], [ "ƒ°on", "ce" ], [ "ƒ°po", "int" ], [ "ƒ°di", "ffe" ], [ "ee", "t" ], [ "ƒ°wee", "k" ], [ "d", "e" ], [ "i", "v" ], [ "ƒ°w", "atch" ], [ "ƒ°mon", "th" ], [ "ƒ°s", "k" ], [ "ƒ°as", "k" ], [ "\"", ":" ], [ "an", "g" ], [ "ee", "e" ], [ "ƒ°than", "k" ], [ "o", "c" ], [ "ƒ°thought", "s" ], [ "ƒ°st", "art" ], [ "ƒ°th", "inking" ], [ "ƒ°ar", "ound" ], [ "ƒ°w", "o" ], [ "ƒ°c", "ry" ], [ "ƒ°l", "ess" ], [ "ƒ°me", "nt" ], [ "ke", "n" ], [ "ƒ°ar", "t" ], [ "1", "0" ], [ "ƒ°a", "c" ], [ "ƒ°me", "ant" ], [ "m", "ed" ], [ "rent", "ly" ], [ "ƒ°el", "se" ], [ "ƒ°c", "ra" ], [ "ƒ°pe", "te" ], [ "ƒ°an", "x" ], [ "ƒ°me", "ds" ], [ "ƒ°ne", "xt" ], [ "p", "h" ], [ "ƒ°wor", "ds" ], [ "ra", "te" ], [ "ƒ°exact", "ly" ], [ "ƒ°m", "ine" ], [ "ƒ°ok", "ay" ], [ "ƒ°cu", "z" ], [ "ƒ°en", "g" ], [ "ƒ°h", "i" ], [ "ƒ°gir", "l" ], [ "ƒ°see", "ms" ], [ "ƒ°he", "y" ], [ "ƒ°t", "imes" ], [ "ƒ°u", "gh" ], [ "ƒ°she", "s" ], [ "ƒ°re", "l" ], [ "w", "he" ], [ "ct", "ion" ], [ "ƒ°b", "ig" ], [ "ƒ°tb", "f" ], [ "ƒ°s", "im" ], [ "ƒ°f", "ound" ], [ "ƒ°i", "nd" ], [ "ƒ°inte", "re" ], [ "ƒ°le", "ft" ], [ "ƒ°re", "p" ], [ "i", "red" ], [ "w", "ard" ], [ "b", "ody" ], [ "an", "ge" ], [ "ƒ°", "9" ], [ "es", "ome" ], [ "ƒ°g", "reat" ], [ "ƒ°", "[" ], [ "w", "a" ], [ "ƒ°loo", "ks" ], [ "ƒ°p", "ar" ], [ "ƒ°l", "m" ], [ "ƒ°h", "ug" ], [ "oo", "k" ], [ "ƒ°leg", "it" ], [ "b", "o" ], [ "on", "s" ], [ "ƒ°need", "s" ], [ "ƒ°pl", "ace" ], [ "ƒ°wee", "ks" ], [ "ƒ°want", "s" ], [ "n", "y" ], [ "ƒ°m", "oney" ], [ "ce", "pt" ], [ "ic", "al" ], [ "ƒ°o", "b" ], [ "ƒ°does", "n" ], [ "ƒ°m", "ain" ], [ "ƒ°c", "ur" ], [ "√∞", "≈Ç" ], [ "a", "z" ], [ "ƒ°", "8" ], [ "ƒ°d", "am" ], [ "i", "ly" ], [ "ƒ°s", "et" ], [ "ƒ°c", "hat" ], [ "ƒ°we", "nt" ], [ "ƒ°com", "m" ], [ "ic", "es" ], [ "ƒ°jo", "b" ], [ "ot", "her" ], [ "ro", "id" ], [ "ƒ°2", "0" ], [ "nt", "h" ], [ "ƒ°lm", "fa" ], [ "u", "ck" ], [ "ke", "t" ], [ "ƒ°tho", "se" ], [ "ƒ°prob", "le" ], [ "ƒ°supp", "ort" ], [ "ƒ°un", "i" ], [ "ƒ°k", "iss" ], [ "ƒ°in", "ter" ], [ "ƒ°cha", "n" ], [ "ƒ°jo", "ke" ], [ "i", "o" ], [ "l", "o" ], [ "nd", "om" ], [ "or", "y" ], [ "ƒ°h", "or" ], [ "nd", "s" ], [ "ƒ°bre", "ak" ], [ "ƒ°s", "te" ], [ "ƒ°ab", "le" ], [ "ƒ°h", "ur" ], [ "ƒ°with", "out" ], [ "ƒ°aw", "esome" ], [ "ce", "nt" ], [ "i", "a" ], [ "ƒ°man", "y" ], [ "ƒ°e", "ither" ], [ "ƒ°y", "ay" ], [ "n", "s" ], [ "ƒ°p", "ass" ], [ "ƒ°year", "s" ], [ "ar", "ds" ], [ "ƒ°con", "s" ], [ "ƒ°f", "ree" ], [ "ƒ°st", "ress" ], [ "ƒ°te", "ch" ], [ "ƒ°annoy", "ing" ], [ "us", "ed" ], [ "ƒ°sh", "op" ], [ "ƒ°ide", "as" ], [ "ƒ°p", "ain" ], [ "ƒ°s", "ch" ], [ "ƒ°got", "ta" ], [ "a", "c" ], [ "ƒ°k", "id" ], [ "ha", "ha" ], [ "ƒ°p", "ay" ], [ "ƒ°s", "ing" ], [ "he", "d" ], [ "ƒ°com", "ing" ], [ "ƒ°he", "ll" ], [ "f", "g" ], [ "ƒ°b", "oy" ], [ "ƒ°who", "le" ], [ "ƒ°", "7" ], [ "ƒ°o", "o" ], [ "ƒ°expl", "ain" ], [ "u", "b" ], [ "ƒ°honest", "ly" ], [ "ƒ°f", "an" ], [ "get", "her" ], [ "e", "l" ], [ "ƒ°wr", "ite" ], [ "k", "r" ], [ "t", "w" ], [ "ƒ°w", "ish" ], [ "ƒ°m", "ar" ], [ "ke", "r" ], [ "ƒ°wor", "king" ], [ "ƒ°et", "c" ], [ "t", "ra" ], [ "ƒ°s", "na" ], [ "it", "ar" ], [ "ƒ°diffe", "rent" ], [ "ƒ°pro", "per" ], [ "ƒ°ga", "me" ], [ "d", "er" ], [ "es", "e" ], [ "p", "ress" ], [ "ƒ°to", "gether" ], [ "pe", "ct" ], [ "is", "ed" ], [ "f", "u" ], [ "ƒ°sy", "nth" ], [ "ƒ°d", "j" ], [ "ƒ°su", "ch" ], [ "ation", "s" ], [ "it", "ch" ], [ "u", "u" ], [ "2", "0" ], [ "ƒ°gu", "itar" ], [ "ƒ°for", "get" ], [ "l", "ing" ], [ "a", "p" ], [ "ƒ°:", "/" ], [ "ƒ°somet", "imes" ], [ "e", "w" ], [ "ƒ°what", "s" ], [ "a", "ck" ], [ "ƒ°st", "ay" ], [ "ks", "ks" ], [ "ƒ°e", "s" ], [ "ƒ°p", "ast" ], [ "ƒ°be", "l" ], [ "u", "es" ], [ "ƒ°m", "ood" ], [ "ie", "w" ], [ "ƒ°a", "aa" ], [ "ƒ°all", "ow" ], [ "us", "s" ], [ "al", "s" ], [ "√≥", "¬æ" ], [ "a", "re" ], [ "ƒ°sch", "ool" ], [ "ƒ°bo", "red" ], [ "ƒ°is", "s" ], [ "ious", "ly" ], [ "ƒ°r", "ing" ], [ "c", "ted" ], [ "ƒ°lmfa", "o" ], [ "ou", "d" ], [ "ƒ°y", "et" ], [ "√§", "¬°" ], [ "ƒ°call", "ed" ], [ "ƒ°d", "ick" ], [ "ƒ°e", "as" ], [ "m", "it" ], [ "ƒ°wor", "r" ], [ "ƒ°i", "kr" ], [ "te", "nt" ], [ "ide", "nt" ], [ "y", "ing" ], [ "ƒ°k", "it" ], [ "te", "ad" ], [ "ƒ°v", "ide" ], [ "ol", "og" ], [ "n", "ight" ], [ "ƒ°ra", "ndom" ], [ "ƒ°op", "en" ], [ "ƒ°have", "nt" ], [ "ƒ°", "rain" ], [ "ƒ°se", "nt" ], [ "ƒ°e", "ach" ], [ "ƒ°disc", "ord" ], [ "ƒ°c", "re" ], [ "ƒ°e", "ar" ], [ "is", "m" ], [ "ƒ°smo", "ke" ], [ "ƒ°cha", "ir" ], [ "ƒ°ins", "tead" ], [ "i", "i" ], [ "ƒ°t", "ea" ], [ "ƒ°h", "igh" ], [ "ƒ°w", "ow" ], [ "ƒ°res", "p" ], [ "ƒ°bl", "an" ], [ "st", "ing" ], [ "ƒ°su", "n" ], [ "ƒ°gu", "y" ], [ "ƒ°dr", "inking" ], [ "ƒ°de", "cks" ], [ "ƒ°", ">" ], [ "eep", "y" ], [ "ƒ°min", "s" ], [ "ƒ°gu", "ys" ], [ "ƒ°sm", "inking" ], [ "ƒ°br", "ing" ], [ "j", "e" ], [ "n", "ed" ], [ "ƒ°u", "nt" ], [ "ƒ°t", "en" ], [ "ci", "al" ], [ "p", "ing" ], [ "ƒ°g", "ay" ], [ "ƒ°po", "ss" ], [ "c", "ked" ], [ "1", "00" ], [ "ƒ°in", "f" ], [ "am", "es" ], [ "ƒ°feel", "s" ], [ "s", "w" ], [ "ƒ°is", "nt" ], [ "ƒ°bas", "ically" ], [ "g", "n" ], [ "ƒ°lma", "ooo" ], [ "ƒ°kn", "ew" ], [ "r", "ight" ], [ "ƒ°s", "igh" ], [ "m", "ore" ], [ "ƒ°m", "od" ], [ "ƒ°b", "us" ], [ "ƒ°im", "a" ], [ "re", "ct" ], [ "u", "be" ], [ "ll", "e" ], [ "r", "r" ], [ "n", "c" ], [ "ve", "l" ], [ "nt", "s" ], [ "ƒ°there", "s" ], [ "it", "s" ], [ "hh", "h" ], [ "ƒ°spea", "k" ], [ "ƒ°d", "ark" ], [ "ƒ°h", "mm" ], [ "b", "y" ], [ "ie", "ty" ], [ "ƒ°p", "ur" ], [ "s", "et" ], [ "ƒ°me", "d" ], [ "ƒ°r", "ly" ], [ "a", "pp" ], [ "as", "h" ], [ "ƒ°r", "ude" ], [ "ƒ°eng", "l" ], [ "ic", "s" ], [ "whe", "re" ], [ "ƒ°g", "eepy" ], [ "o", "x" ], [ "ƒ°to", "ile" ], [ "ƒ°as", "ked" ], [ "ll", "ing" ], [ "al", "th" ], [ "ƒ°sa", "w" ], [ "ƒ°us", "ing" ], [ "ƒ°sen", "se" ], [ "p", "y" ], [ "l", "ine" ], [ "3", "0" ], [ "ƒ°ca", "me" ], [ "ƒ°o", "ven" ], [ "ro", "ss" ], [ "le", "d" ], [ "os", "ed" ], [ "ƒ°n", "on" ], [ "ƒ°se", "co" ], [ "ƒ°toile", "t" ], [ "ƒ°ar", "g" ], [ "ƒ°do", "or" ], [ "ƒ°g", "ra" ], [ "ƒ°no", "oo" ], [ "ƒ°an", "other" ], [ "ƒ°t", "rain" ], [ "ƒ°was", "nt" ], [ "ƒ°ex", "per" ], [ "ƒ°unt", "il" ], [ "ll", "ow" ], [ "ƒ°any", "more" ], [ "ƒ°k", "ill" ], [ "ƒ°t", "a" ], [ "ƒ°b", "or" ], [ "ƒ°ha", "lf" ], [ "id", "er" ], [ "ment", "s" ], [ "if", "ic" ], [ "ƒ°wor", "se" ], [ "j", "oy" ], [ "ƒ°h", "um" ], [ "ƒ°al", "one" ], [ "i", "an" ], [ "ƒ°s", "in" ], [ "at", "her" ], [ "ƒ°qu", "est" ], [ "g", "ry" ], [ "ƒ°mess", "age" ], [ "ƒ°d", "a" ], [ "ƒ°c", "ar" ], [ "ƒ°ag", "o" ], [ "ƒ°chan", "ge" ], [ "ƒ°", "√¢ƒ£" ], [ "ƒ°was", "n" ], [ "ƒ°tur", "nt" ], [ "ƒ°c", "ol" ], [ "ind", "ow" ], [ "ƒ°star", "ted" ], [ "le", "te" ], [ "ƒ°h", "ot" ], [ "ƒ°x", "x" ], [ "ur", "ing" ], [ "ƒ°play", "ed" ], [ "re", "en" ], [ "ƒ°ha", "ir" ], [ "ƒ°en", "joy" ], [ "ƒ°see", "m" ], [ "ve", "s" ], [ "ƒ°an", "sw" ], [ "ƒ°turnt", "able" ], [ "c", "ome" ], [ "ƒ°f", "o" ], [ "n", "ot" ], [ "ƒ°d", "umb" ], [ "ƒ°r", "n" ], [ "ƒ°", "0" ], [ "ƒ°m", "ark" ], [ "ƒ°cu", "dd" ], [ "ƒ°t", "ired" ], [ "ƒ°cou", "ple" ], [ "ƒ°comm", "un" ], [ "ƒ°", ";" ], [ "w", "ards" ], [ "ƒ°l", "ate" ], [ "ƒ°p", "an" ], [ "ƒ°c", "al" ], [ "ƒ°bel", "ie" ], [ "ƒ°know", "n" ], [ "ƒ°gl", "ad" ], [ "s", "c" ], [ "√£", "¬©" ], [ "ƒ°n", "orm" ], [ "ƒ°vide", "o" ], [ "ƒ°e", "le" ], [ "ou", "p" ], [ "ƒ°1", "2" ], [ "ƒ°aw", "ww" ], [ "che", "d" ], [ "l", "ic" ], [ "ƒ°imp", "ort" ], [ "ƒ°al", "right" ], [ "ƒ°d", "ance" ], [ "ƒ°re", "g" ], [ "ƒ°mix", "er" ], [ "ƒ°sin", "ce" ], [ "v", "o" ], [ "ƒ°aw", "k" ], [ "u", "te" ], [ "a", "h" ], [ "on", "es" ], [ "i", "er" ], [ "ƒ°sna", "cks" ], [ "ƒ°se", "c" ], [ "f", "s" ], [ "ro", "p" ], [ "ƒ°he", "alth" ], [ "ƒ°c", "reat" ], [ "ƒ°th", "ree" ], [ "ƒ°s", "ick" ], [ "ƒ°ch", "ild" ], [ "ƒ°worr", "ied" ], [ "ƒ°sp", "ace" ], [ "ƒ°app", "re" ], [ "ƒ°s", "een" ], [ "g", "an" ], [ "ff", "ic" ], [ "ƒ°month", "s" ], [ "l", "ight" ], [ "ƒ°let", "s" ], [ "ƒ°m", "or" ], [ "ƒ°app", "a" ], [ "'", "." ], [ "ƒ°e", "nt" ], [ "ƒ°f", "ig" ], [ "ƒ°am", "az" ], [ "u", "p" ], [ "ƒ°qu", "ite" ], [ "ƒ°sa", "fe" ], [ "anc", "ing" ], [ "r", "on" ], [ "ag", "es" ], [ "ƒ°;", ")" ], [ "at", "s" ], [ "ƒ°j", "on" ], [ "ƒ°c", "or" ], [ "ƒ°ob", "v" ], [ "u", "gg" ], [ "ƒ°", "#" ], [ "ƒ°m", "id" ], [ "fe", "ct" ], [ "ƒ°ex", "c" ], [ "ƒ°awk", "ward" ], [ "ist", "ic" ], [ "it", "ion" ], [ "ƒ°no", "body" ], [ "ƒ°or", "der" ], [ "ƒ°ha", "nd" ], [ "ƒ°is", "n" ], [ "ƒ°appa", "rently" ], [ "ƒ°fe", "w" ], [ "ƒ°s", "ays" ], [ "ƒ°b", "tw" ], [ "re", "s" ], [ "ƒ°happen", "ed" ], [ "mo", "st" ], [ "ƒ°def", "in" ], [ "s", "ide" ], [ "ƒ°t", "er" ], [ "ƒ°for", "g" ], [ "un", "k" ], [ "ƒ°appre", "ci" ], [ "\"", "," ], [ "ƒ°hor", "r" ], [ "je", "ct" ], [ "ƒ°me", "h" ], [ "en", "s" ], [ "ter", "day" ], [ "il", "st" ], [ "ƒ°m", "ic" ], [ "ƒ°ment", "al" ], [ "n", "ch" ], [ "ƒ°o", "oo" ], [ "ƒ°to", "p" ], [ "or", "s" ], [ "ƒ°che", "ese" ], [ "ƒ°r", "un" ], [ "ƒ°any", "one" ], [ "ƒ°joke", "s" ], [ "p", "ed" ], [ "ƒ°d", "un" ], [ "ve", "d" ], [ "ƒ°p", "i" ], [ "ƒ°yes", "terday" ], [ "ƒ°intere", "sting" ], [ "ƒ°wh", "ilst" ], [ "ƒ°f", "ar" ], [ "w", "e" ], [ "ƒ°em", "ot" ], [ "ƒ°e", "y" ], [ "ƒ°anx", "iety" ], [ "ug", "s" ], [ "us", "ing" ], [ "ƒ°e", "d" ], [ "b", "a" ], [ "f", "r" ], [ "ƒ°to", "night" ], [ "ƒ°as", "king" ], [ "√£", "ƒ£" ], [ "ƒ°cle", "an" ], [ "v", "iew" ], [ "?", "!" ], [ "ƒ°om", "fg" ], [ "ƒ°d", "ancing" ], [ "ƒ°wa", "ter" ], [ "ƒ°proble", "m" ], [ "√£ƒ£", "¬£" ], [ "ƒ°your", "self" ], [ "i", "gg" ], [ "ƒ°su", "r" ], [ "ƒ°w", "indow" ], [ "ƒ°li", "ght" ], [ "h", "ow" ], [ "ƒ°1", "1" ], [ "ƒ°", "nd" ], [ "ƒ°a", "p" ], [ "ƒ°c", "ut" ], [ "ƒ°import", "ant" ], [ "ƒ°f", "ore" ], [ "ƒ°e", "w" ], [ "p", "m" ], [ "ƒ°a", "h" ], [ "ƒ°p", "iss" ], [ "t", "he" ], [ "ƒ°tr", "ied" ], [ "re", "t" ], [ "ƒ°up", "set" ], [ "ƒ°sit", "u" ], [ "f", "ort" ], [ "ƒ°p", "oo" ], [ "ƒ°d", "w" ], [ "ƒ°gen", "er" ], [ "ƒ°blan", "ket" ], [ "w", "een" ], [ "ƒ°tr", "ans" ], [ "ƒ°boo", "f" ], [ "ƒ°main", "ly" ], [ "ƒ°ch", "ris" ], [ "che", "s" ], [ "ƒ°wh", "ile" ], [ "ƒ°wal", "k" ], [ "ou", "nt" ], [ "ƒ°seco", "nd" ], [ "v", "ice" ], [ "ƒ°f", "am" ], [ "ƒ°ph", "ot" ], [ "ƒ°in", "v" ], [ "ƒ°on", "es" ], [ "ƒ°tech", "n" ], [ "te", "m" ], [ "er", "t" ], [ "ƒ°v", "is" ], [ "ƒ°re", "st" ], [ "ƒ°cl", "ass" ], [ "ƒ°spe", "c" ], [ "ƒ°song", "s" ], [ "ƒ°belie", "ve" ], [ "he", "s" ], [ "ƒ°watch", "ing" ], [ "ƒ°amaz", "ing" ], [ "ƒ°forg", "ot" ], [ "ƒ°b", "an" ], [ "ƒ°lo", "ving" ], [ "ƒ°you", "re" ], [ "ƒ°per", "fect" ], [ "e", "ar" ], [ "ƒ°at", "ta" ], [ "ƒ°w", "ar" ], [ "ƒ°b", "ir" ], [ "ƒ°su", "gg" ], [ "at", "ive" ], [ "ƒ°fu", "ll" ], [ "ƒ°cre", "ate" ], [ "ƒ°g", "oo" ], [ "ƒ°s", "ksks" ], [ "us", "h" ], [ "ƒ°che", "ck" ], [ "ƒ°pl", "z" ], [ "ƒ°v", "al" ], [ "ƒ°need", "ed" ], [ "ƒ°l", "uck" ], [ "ƒ°col", "our" ], [ "ƒ°s", "ign" ], [ "m", "y" ], [ "ƒ°di", "ffic" ], [ "ƒ°pro", "c" ], [ "ƒ°pa", "rent" ], [ "ate", "ly" ], [ "ƒ°op", "in" ], [ "e", "nt" ], [ "in", "ner" ], [ "ƒ°hur", "t" ], [ "iz", "z" ], [ "ƒ°b", "ur" ], [ "b", "le" ], [ "'", "(" ], [ "fu", "lly" ], [ "in", "s" ], [ "ƒ°w", "ha" ], [ "ƒ°re", "c" ], [ "ƒ°fre", "nch" ], [ "s", "y" ], [ "ill", "y" ], [ "out", "h" ], [ "ƒ°al", "most" ], [ "ƒ°m", "eet" ], [ "ƒ°√¢ƒ£", "ƒ∫" ], [ "ƒ°to", "y" ], [ "ƒ°ag", "ree" ], [ "ƒ°e", "mb" ], [ "ƒ°bet", "ween" ], [ "ƒ°commun", "ic" ], [ "ite", "ly" ], [ "e", "ver" ], [ "ƒ°sc", "ream" ], [ "ƒ°", "√∞≈Ç" ], [ "ƒ°c", "our" ], [ "ƒ°v", "a" ], [ "ƒ°re", "f" ], [ "ƒ°acc", "ident" ], [ "ƒ°do", "g" ], [ "ƒ°conf", "used" ], [ "ƒ°fun", "ny" ], [ "i", "al" ], [ "ƒ°bu", "y" ], [ "ƒ°t", "as" ], [ "ma", "il" ], [ "i", "ps" ], [ "ƒ°us", "ually" ], [ "n", "et" ], [ "ƒ°dun", "no" ], [ "ƒ°c", "oo" ], [ "ro", "om" ], [ ")", "," ], [ "ƒ°in", "st" ], [ "il", "ity" ], [ "if", "u" ], [ "ie", "nce" ], [ "ƒ°listen", "ed" ], [ "ƒ°h", "un" ], [ "ƒ°e", "gg" ], [ "ƒ°st", "ra" ], [ "od", "u" ], [ "ƒ°mor", "ning" ], [ "ƒ°ye", "p" ], [ "ƒ°bab", "e" ], [ "ƒ°dark", "ness" ], [ "ƒ°re", "m" ], [ "i", "ving" ], [ "ƒ°:", "'(" ], [ "ion", "al" ], [ "ƒ°de", "al" ], [ "ƒ°iss", "ues" ], [ "ƒ°vo", "ices" ], [ "ƒ°su", "b" ], [ "ƒ°g", "un" ], [ "m", "ing" ], [ "ƒ°ex", "cept" ], [ "1", "2" ], [ "il", "ar" ], [ "t", "ure" ], [ "ƒ°comp", "lete" ], [ "ow", "er" ], [ "ƒ°rep", "ly" ], [ "ƒ°like", "s" ], [ "ƒ°fa", "ct" ], [ "ƒ°answ", "er" ], [ "ƒ°pan", "ic" ], [ "ƒ°p", "op" ], [ "ƒ°wor", "ks" ], [ "ƒ°sm", "all" ], [ "ƒ°diffic", "ult" ], [ "p", "id" ], [ "ƒ°m", "at" ], [ "ƒ°some", "how" ], [ "ƒ°defin", "itely" ], [ "u", "tes" ], [ "ƒ°ide", "k" ], [ "ƒ°out", "side" ], [ "ƒ°too", "k" ], [ "ter", "s" ], [ "ƒ°supp", "osed" ], [ "ƒ°of", "ten" ], [ "ƒ°po", "st" ], [ "h", "d" ], [ "ƒ°wor", "ld" ], [ "ƒ°parent", "s" ], [ "i", "pp" ], [ "ƒ°take", "s" ], [ "pt", "ion" ], [ "ƒ°", "z" ], [ "ƒ°have", "n" ], [ "b", "ook" ], [ "ƒ°t", "ill" ], [ "wa", "ifu" ], [ "ƒ°bir", "th" ], [ "ƒ°d", "ie" ], [ "ƒ°tell", "ing" ], [ "ƒ°day", "light" ], [ "ad", "s" ], [ "rate", "waifu" ], [ "ƒ°allow", "ed" ], [ "ƒ°c", "op" ], [ "ic", "ul" ], [ "id", "ed" ], [ "ƒ°g", "ames" ], [ "se", "nt" ], [ "ƒ°ag", "es" ], [ "ƒ°they", "re" ], [ "00", "0" ], [ "ƒ°appreci", "ate" ], [ "ƒ°fo", "llow" ], [ "r", "u" ], [ "ƒ°e", "ff" ], [ "ƒ°de", "press" ], [ "od", "e" ], [ "ƒ°d", "uring" ], [ "z", "y" ], [ "y", "le" ], [ "ƒ°aut", "istic" ], [ "o", "b" ], [ "ƒ°wor", "ry" ], [ "g", "er" ], [ "ing", "s" ], [ "ƒ°to", "wards" ], [ "ƒ°sex", "y" ], [ "ƒ°wo", "nder" ], [ "ƒ°poss", "ible" ], [ "ƒ°cont", "ro" ], [ "ƒ°cha", "ng" ], [ "ƒ°cry", "ing" ], [ "ƒ°s", "at" ], [ "ƒ°co", "lle" ], [ "ƒ°1", "8" ], [ "sh", "ip" ], [ "ƒ°re", "qu" ], [ "t", "m" ], [ "u", "k" ], [ "y", "out" ], [ "ƒ°lo", "st" ], [ "ƒ°love", "s" ], [ "ƒ°cha", "r" ], [ "ƒ°c", "ree" ], [ "ƒ°r", "ather" ], [ "p", "ut" ], [ "ƒ°ele", "ct" ], [ "ƒ°n", "umb" ], [ "ƒ°e", "mail" ], [ "ƒ°eff", "ort" ], [ "ƒ°by", "e" ], [ "u", "gh" ], [ "ƒ°to", "t" ], [ "in", "y" ], [ "ƒ°ty", "pe" ], [ "ƒ°ex", "p" ], [ "ƒ°add", "ress" ], [ "g", "r" ], [ "ƒ°inc", "l" ], [ "o", "on" ], [ "ƒ°w", "ood" ], [ "ic", "t" ], [ "w", "or" ], [ "ƒ°min", "utes" ], [ "ƒ°bor", "ing" ], [ "nd", "ed" ], [ "ƒ°ent", "ire" ], [ "g", "y" ], [ "ƒ°e", "qu" ], [ "ƒ°ha", "ting" ], [ "ƒ°opin", "ion" ], [ "ci", "ted" ], [ "ƒ°sugg", "est" ], [ "ƒ°sor", "t" ], [ "ƒ°anx", "ious" ], [ "ƒ°b", "ody" ], [ "ƒ°stu", "pid" ], [ "ƒ°d", "inner" ], [ "a", "ir" ], [ "om", "an" ], [ "ib", "ly" ], [ "ƒ°would", "nt" ], [ "m", "p" ], [ "ƒ°ha", "ng" ], [ "ƒ°fa", "v" ], [ "ate", "s" ], [ "w", "atch" ], [ "ar", "ch" ], [ "ƒ°de", "ad" ], [ "ƒ°me", "me" ], [ "ƒ°rel", "ation" ], [ "ƒ°un", "less" ], [ "3", "3" ], [ "ƒ°me", "t" ], [ "ƒ°we", "ar" ], [ "ƒ°pic", "ture" ], [ "ƒ°h", "id" ], [ "ƒ°read", "ing" ], [ "l", "eep" ], [ "ƒ°dam", "n" ], [ "ƒ°d", "roid" ], [ "ƒ°b", "ou" ], [ "ƒ°1", "5" ], [ "ƒ°cour", "se" ], [ "ag", "ed" ], [ "ƒ°he", "he" ], [ "ƒ°fuck", "in" ], [ "ƒ°he", "ating" ], [ "ƒ°hear", "d" ], [ "ƒ°ooo", "h" ], [ "m", "me" ], [ "ƒ°st", "and" ], [ "ƒ°love", "ly" ], [ "ƒ°fl", "o" ], [ "ƒ°loo", "ked" ], [ "ƒ°te", "xt" ], [ "ci", "ally" ], [ "ƒ°cl", "ose" ], [ "d", "y" ], [ "ƒ°con", "vers" ], [ "ƒ°hum", "an" ], [ "ƒ°im", "ag" ], [ "ƒ°how", "s" ], [ "ƒ°com", "es" ], [ "ƒ°p", "izz" ], [ "ƒ°po", "s" ], [ "ƒ°a", "ge" ], [ "ƒ°ho", "ld" ], [ "ƒ°d", "u" ], [ "ƒ°tr", "u" ], [ "ƒ°sl", "ight" ], [ "ƒ°real", "ised" ], [ "ƒ°fig", "ure" ], [ "ƒ°x", "ox" ], [ "√¢ƒ£", "¬¢" ], [ "m", "on" ], [ "ƒ°ex", "cited" ], [ "ƒ°even", "ing" ], [ "ƒ°fin", "ally" ], [ "ƒ°cons", "ider" ], [ "!", "'" ], [ "er", "ic" ], [ "ƒ°disc", "uss" ], [ "ƒ°supp", "ose" ], [ "a", "ut" ], [ "ƒ°on", "line" ], [ "r", "ing" ], [ "ffe", "ct" ], [ "ƒ°aut", "ism" ], [ "ƒ°situ", "ation" ], [ "ƒ°get", "s" ], [ "ƒ°boo", "k" ], [ "ƒ°cou", "nt" ], [ "ƒ°oo", "f" ], [ "ƒ°f", "ix" ], [ "ra", "ct" ], [ "ƒ°norm", "al" ], [ "ƒ°m", "ome" ], [ "an", "gle" ], [ "ƒ°w", "at" ], [ "ƒ°ins", "ide" ], [ "1", "5" ], [ "ƒ°pass", "age" ], [ "ƒ°gun", "na" ], [ "k", "k" ], [ "li", "er" ], [ "ƒ°d", "at" ], [ "m", "er" ], [ "id", "ay" ], [ "ƒ°do", "ct" ], [ "ƒ°p", "en" ], [ "ƒ°p", "u" ], [ "g", "ra" ], [ "√™", "ƒ∑" ], [ "ƒ°sw", "eet" ], [ "ƒ°f", "ur" ], [ "ƒ°loo", "ol" ], [ "ƒ°de", "l" ], [ "ƒ°fl", "at" ], [ "ƒ°sc", "ary" ], [ "ƒ°spea", "king" ], [ "ƒ°engl", "ish" ], [ "ƒ°h", "y" ], [ "ƒ°as", "leep" ], [ "gg", "ing" ], [ "ƒ°would", "n" ], [ "ƒ°creat", "ing" ], [ "ƒ°u", "k" ], [ "ƒ°am", "eric" ], [ "ƒ°pizz", "a" ], [ "c", "l" ], [ "ƒ°c", "ase" ], [ "ƒ°lo", "ts" ], [ "ƒ°fa", "il" ], [ "nd", "ay" ], [ "m", "an" ], [ "ƒ°i", "good night" ], [ "ƒ°happen", "ing" ], [ "ƒ°1", "4" ], [ "c", "u" ], [ "ic", "ed" ], [ "ƒ°3", "0" ], [ "g", "a" ], [ "√™", "ƒ∑" ], [ "ƒ°tur", "n" ], [ "te", "ver" ], [ "c", "es" ], [ "h", "y" ], [ "ƒ°r", "ant" ], [ "ƒ°pl", "us" ], [ "ƒ°fa", "ult" ], [ "ƒ°tw", "itch" ], [ "ƒ°ch", "o" ], [ "ƒ°past", "a" ], [ "ƒ°w", "oman" ], [ "ƒ°eve", "nt" ], [ "ƒ°bl", "ack" ], [ "ƒ°co", "ld" ], [ "ƒ°e", "v" ], [ "a", "il" ], [ "ƒ°t", "u" ], [ "ƒ°p", "c" ], [ "ƒ°su", "cks" ], [ "ƒ°htt", "p" ], [ "ƒ°ear", "lier" ], [ "ƒ°horr", "ible" ], [ "ƒ°s", "n" ], [ "e", "x" ], [ "ƒ°goo", "gle" ], [ "ir", "t" ], [ "ƒ°xx", "x" ], [ "ƒ°f", "all" ], [ "ƒ°ga", "ve" ], [ "ƒ°fore", "ver" ], [ "n", "n" ], [ "ƒ°m", "il" ], [ "ƒ°hope", "fully" ], [ "yout", "ube" ], [ "ƒ°s", "ooo" ], [ "o", "ck" ], [ "ƒ°me", "ow" ], [ "ƒ°so", "cial" ], [ "ƒ°aw", "w" ], [ "dd", "en" ], [ "ƒ°dr", "unk" ], [ "ƒ°birth", "day" ], [ "ƒ°wor", "st" ], [ "ƒ°c", "ba" ], [ "ange", "l" ], [ "ens", "ive" ], [ "ƒ°slight", "ly" ], [ "ƒ°l", "ow" ], [ "ƒ°m", "ouse" ], [ "ƒ°v", "ib" ], [ "ƒ°m", "ass" ], [ "nd", "ing" ], [ "ƒ°p", "at" ], [ "ƒ°g", "one" ], [ "ƒ°su", "ck" ], [ "f", "er" ], [ "ƒ°w", "on" ], [ "ƒ°f", "our" ], [ "ƒ°t", "iny" ], [ "ƒ°bro", "ke" ], [ "ƒ°obv", "iously" ], [ "ƒ°su", "dden" ], [ "ƒ°love", "d" ], [ "is", "ing" ], [ "ƒ°we", "bs" ], [ "ƒ°ba", "g" ], [ "a", "v" ], [ "ƒ°g", "iving" ], [ "ƒ°so", "l" ], [ "l", "s" ], [ "ƒ°u", "mm" ], [ "ƒ°sp", "am" ], [ "p", "r" ], [ "so", "l" ], [ "ƒ°co", "ck" ], [ "ƒ°pr", "om" ], [ "ƒ°p", "as" ], [ "ra", "c" ], [ "ƒ°stu", "ck" ], [ "c", "on" ], [ "ƒ°e", "ss" ], [ "ƒ°ex", "pect" ], [ "ƒ°oo", "h" ], [ "ƒ°f", "fs" ], [ "olog", "y" ], [ "ƒ°ke", "y" ], [ "ƒ°va", "pe" ], [ "ƒ°webs", "ite" ], [ "ll", "ed" ], [ "ƒ°b", "ath" ], [ "ƒ°act", "ual" ], [ "ƒ°li", "ving" ], [ "1", "7" ], [ "ly", "m" ], [ "ƒ°wha", "tever" ], [ "ƒ°hmm", "m" ], [ "ƒ°proc", "ess" ], [ "ƒ°fav", "our" ], [ "ƒ°wor", "th" ], [ "ƒ°l", "ie" ], [ "ƒ°cur", "rently" ], [ "u", "le" ], [ "ƒ°an", "gry" ], [ "ƒ°per", "f" ], [ "c", "ed" ], [ "ƒ°p", "sy" ], [ "ƒ°fe", "ll" ], [ "is", "ion" ], [ "ƒ°la", "ugh" ], [ "ƒ°cal", "m" ], [ "l", "u" ], [ "ƒ°w", "in" ], [ "ƒ°f", "oc" ], [ "ke", "y" ], [ "ƒ°re", "d" ], [ "ƒ°re", "t" ], [ "ƒ°and", "roid" ], [ "ƒ°your", "s" ], [ "ƒ°bit", "ch" ], [ "ƒ°ph", "ys" ], [ "h", "t" ], [ "fr", "iend" ], [ "c", "er" ], [ "ƒ°m", "ee" ], [ "ƒ°tr", "ust" ], [ "ƒ°inter", "net" ], [ "ƒ°re", "ady" ], [ "ve", "nt" ], [ "c", "ha" ], [ "f", "k" ], [ "ƒ°m", "ult" ], [ "se", "l" ], [ "2", "4" ], [ "u", "red" ], [ "on", "se" ], [ "it", "ies" ], [ "ƒ°cont", "in" ], [ "w", "u" ], [ "ƒ°ex", "tra" ], [ "ƒ°sim", "ilar" ], [ "p", "o" ], [ "u", "ine" ], [ "ƒ°re", "cent" ], [ "ƒ°bl", "ue" ], [ "sc", "ri" ], [ "ƒ°f", "il" ], [ "ƒ°str", "ugg" ], [ "b", "er" ], [ "ƒ°pre", "ss" ], [ "ƒ°h", "it" ], [ "ƒ°are", "nt" ], [ "1", "6" ], [ "ƒ°w", "oo" ], [ "ƒ°ra", "p" ], [ "ƒ°ey", "es" ], [ "ƒ°stra", "ight" ], [ "2", "5" ], [ "ie", "nt" ], [ "ƒ°love", "angel" ], [ "ƒ°emb", "ar" ], [ "ƒ°mome", "nt" ], [ "ƒ°n", "er" ], [ "ƒ°quest", "ion" ], [ "ant", "ly" ], [ "b", "b" ], [ "ƒ°", "/" ], [ "ƒ°gen", "uine" ], [ "bo", "ard" ], [ "ƒ°val", "id" ], [ "ƒ°y", "a" ], [ "t", "r" ], [ "ƒ°wh", "ite" ], [ "ƒ°aw", "ake" ], [ "pt", "op" ], [ "ƒ°fin", "is" ], [ "ƒ°eas", "y" ], [ "ƒ°m", "is" ], [ "ƒ°def", "o" ], [ "ƒ°e", "ffect" ], [ "ƒ°fr", "ance" ], [ "ƒ°know", "s" ], [ "ƒ°ch", "ill" ], [ "i", "fe" ], [ "ƒ°convers", "ation" ], [ "ƒ°s", "ide" ], [ "ƒ°brain", "s" ], [ "ƒ°ment", "ion" ], [ "ƒ°proper", "ly" ], [ "ƒ°be", "come" ], [ "h", "o" ], [ "ƒ°c", "ook" ], [ "ƒ°qu", "ick" ], [ "ƒ°sc", "reen" ], [ "in", "es" ], [ "en", "ce" ], [ "ƒ°ad", "hd" ], [ "ƒ°dec", "ided" ], [ "ƒ°favour", "ite" ], [ "ƒ°bou", "ght" ], [ "ƒ°me", "n" ], [ "ƒ°e", "h" ], [ "ƒ°resp", "onse" ], [ "ƒ°sex", "ual" ], [ "ƒ°1", "9" ], [ "ƒ°sl", "ow" ], [ "ƒ°ser", "iously" ], [ "g", "if" ], [ "ƒ°could", "nt" ], [ "ƒ°", "+" ], [ "ƒ°d", "ream" ], [ "ƒ°for", "m" ], [ "ƒ°hu", "ge" ], [ "ƒ°complete", "ly" ], [ "sel", "ves" ], [ "ƒ°ex", "am" ], [ "ƒ°care", "s" ], [ "ƒ°d", "et" ], [ "ƒ°rec", "ord" ], [ "ƒ°pr", "oud" ], [ "ƒ°proble", "ms" ], [ "u", "x" ], [ "ƒ°", "ver" ], [ "ƒ°le", "vel" ], [ "ƒ°la", "ptop" ], [ "air", "s" ], [ "ƒ°p", "lym" ], [ "ra", "l" ], [ "ƒ°tra", "ck" ], [ "ƒ°cor", "rect" ], [ "ƒ°tw", "ice" ], [ "in", "ess" ], [ "ƒ°sh", "ort" ], [ "ƒ°at", "m" ], [ "ƒ°can", "not" ], [ "ƒ°ke", "pt" ], [ "ƒ°hate", "s" ], [ "a", "ly" ], [ "j", "k" ], [ "ƒ°f", "ive" ], [ "ƒ°hu", "h" ], [ "ƒ°re", "pl" ], [ "ƒ°hell", "o" ], [ "ƒ°le", "mme" ], [ "ƒ°cudd", "les" ], [ "c", "y" ], [ "ƒ°mess", "ages" ], [ "ƒ°l", "oud" ], [ "ƒ°be", "aut" ], [ "ƒ°not", "iced" ], [ "ip", "le" ], [ "ƒ°psy", "ch" ], [ "ƒ°c", "ust" ], [ "ƒ°wait", "ing" ], [ "ƒ°miss", "ed" ], [ "ƒ°numb", "er" ], [ "√™ƒ∑", "√£ƒ£¬£" ], [ "ƒ°lo", "g" ], [ "u", "in" ], [ "ƒ°no", "pe" ], [ "ƒ°pr", "odu" ], [ "ƒ°relation", "ship" ], [ "ƒ°br", "u" ], [ "ƒ°ess", "ay" ], [ "ƒ°s", "ys" ], [ "w", "ise" ], [ "ƒ°ind", "eed" ], [ "re", "w" ], [ "re", "nce" ], [ "ƒ°give", "s" ], [ "ƒ°bas", "ed" ], [ "if", "y" ], [ "iz", "e" ], [ "ƒ°con", "ta" ], [ "ƒ°wal", "ked" ], [ "ƒ°g", "ross" ], [ "ƒ°some", "where" ], [ "ƒ°f", "it" ], [ "ƒ°ne", "ar" ], [ "ƒ°1", "3" ], [ "ƒ°oh", "hh" ], [ "h", "s" ], [ "ƒ°d", "eep" ], [ "ƒ°con", "st" ], [ "lo", "ad" ], [ "ƒ°ste", "p" ], [ "ƒ°ser", "ver" ], [ "ƒ°tot", "ally" ], [ "ƒ°e", "arly" ], [ "ee", "r" ], [ "or", "n" ], [ "ƒ°not", "ice" ], [ "ƒ°sh", "ut" ], [ "ƒ°dis", "t" ], [ "ing", "e" ], [ "tm", "as" ], [ "ƒ°sudden", "ly" ], [ "un", "e" ], [ "ƒ°fam", "ily" ], [ "ƒ°star", "ting" ], [ "ƒ°inc", "red" ], [ "ƒ°fr", "ont" ], [ "ƒ°b", "i" ], [ "ive", "ly" ], [ "ƒ°accident", "ally" ], [ "ƒ°colle", "ge" ], [ "√¢ƒ£¬¢", "√¨" ], [ "ƒ°cl", "os" ], [ "ƒ°par", "ty" ], [ "ƒ°v", "i" ], [ "it", "al" ], [ "ƒ°go", "es" ], [ "p", "a" ], [ "ess", "s" ], [ "ƒ°chris", "tmas" ], [ "i", "ted" ], [ "ƒ°st", "ory" ], [ "ƒ°girl", "s" ], [ "ƒ°hun", "gry" ], [ "ƒ°cree", "py" ], [ "ƒ°r", "ound" ], [ "1", "4" ], [ "ƒ°p", "ol" ], [ "ƒ°hug", "s" ], [ "ake", "d" ], [ "ur", "s" ], [ "ƒ°flo", "or" ], [ "ƒ°s", "qu" ], [ "ƒ°r", "an" ], [ "ƒ°hear", "t" ], [ "ƒ°jo", "in" ], [ "ƒ°>", "." ], [ "ƒ°bus", "y" ], [ "ƒ°1", "6" ], [ "ƒ°engl", "and" ], [ "f", "ast" ], [ "ƒ°s", "illy" ], [ "if", "ul" ], [ "anc", "y" ], [ "ƒ°techn", "ically" ], [ "ƒ°part", "ner" ], [ "ƒ°gener", "al" ], [ "e", "m" ], [ "oo", "t" ], [ "fort", "able" ], [ "pe", "cially" ], [ "ƒ°inf", "orm" ], [ "ƒ°a", "ir" ], [ "ƒ°happen", "s" ], [ "d", "ing" ], [ "i", "ot" ], [ "ƒ°exper", "ience" ], [ "ƒ°quest", "ions" ], [ "ƒ°xox", "o" ], [ "1", "9" ], [ "ƒ°dec", "ide" ], [ "1", "3" ], [ "le", "x" ], [ "ert", "ain" ], [ "ƒ°long", "er" ], [ "ƒ°bro", "ther" ], [ "ron", "ic" ], [ "ƒ°t", "est" ], [ "ƒ°th", "row" ], [ "ƒ°st", "yle" ], [ "ƒ°wo", "ke" ], [ "ƒ°incl", "ud" ], [ "he", "ad" ], [ "ke", "ts" ], [ "i", "k" ], [ "ƒ°we", "l" ], [ "ƒ°imag", "ine" ], [ "ƒ°bu", "ild" ], [ "tent", "ion" ], [ "ƒ°s", "me" ], [ "ƒ°b", "ass" ], [ "ƒ°lis", "t" ], [ "ƒ°sing", "ing" ], [ "ƒ°iss", "ue" ], [ "d", "le" ], [ "m", "in" ], [ "ƒ°n", "ic" ], [ "ƒ°ne", "ither" ], [ "che", "n" ], [ "a", "x" ], [ "ƒ°l", "ar" ], [ "ƒ°y", "h" ], [ "nd", "on" ], [ "ƒ°pro", "f" ], [ "is", "ts" ], [ "f", "l" ], [ "j", "s" ], [ "ƒ°", "?" ], [ "ƒ°like", "ly" ], [ "ƒ°face", "book" ], [ "ƒ°spec", "ific" ], [ "ƒ°you", "t" ], [ "ƒ°sp", "o" ], [ "ƒ°mum", "s" ], [ "ƒ°part", "icul" ], [ "ƒ°conta", "ct" ], [ "ƒ°pro", "v" ], [ "cc", "oon" ], [ "ƒ°sleep", "ing" ], [ "ƒ°war", "m" ], [ "ƒ°genuine", "ly" ], [ "ƒ°ne", "arly" ], [ "ƒ°call", "ing" ], [ "ƒ°b", "ra" ], [ "5", "0" ], [ "ter", "n" ], [ "ƒ°fu", "cked" ], [ "ƒ°sen", "s" ], [ "1", "8" ], [ "ƒ°s", "oup" ], [ "ƒ°j", "u" ], [ "ƒ°mee", "ting" ], [ "y", "ou" ], [ "ƒ°boy", "friend" ], [ "ƒ°i", "ly" ], [ "ile", "d" ], [ "ƒ°mil", "k" ], [ "ƒ°l", "ine" ], [ "ƒ°annoy", "ed" ], [ "ƒ°o", "dd" ], [ "ƒ°bre", "ad" ], [ "ƒ°could", "n" ], [ "ƒ°kit", "chen" ], [ "ƒ°arg", "o" ], [ "ƒ°mult", "iple" ], [ "ƒ°scream", "ing" ], [ "p", "ris" ], [ "ƒ°cl", "ot" ], [ "ƒ°tur", "ned" ], [ "ƒ°b", "ar" ], [ "ƒ°br", "ight" ], [ "ƒ°spe", "nt" ], [ "ƒ°dam", "mit" ], [ "ƒ°es", "pecially" ], [ "√™", "ƒ∫" ], [ "ƒ°s", "ile" ], [ "ƒ°com", "ment" ], [ "ƒ°dis", "app" ], [ "ƒ°mass", "ive" ], [ "ill", "s" ], [ "ƒ°str", "ong" ], [ "ƒ°w", "ake" ], [ "ie", "t" ], [ "ƒ°whe", "ther" ], [ "ƒ°comp", "l" ], [ "ƒ°gr", "oup" ], [ "ƒ°t", "v" ], [ "re", "ad" ], [ "q", "ue" ], [ "u", "rate" ], [ "ƒ°stu", "de" ], [ "ƒ°show", "er" ], [ "ƒ°o", "pp" ], [ "ƒ°break", "fast" ], [ "ƒ°sksks", "k" ], [ "\"", ")" ], [ "ƒ°other", "wise" ], [ "ƒ°tra", "um" ], [ "ƒ°plym", "outh" ], [ "ƒ°piss", "ed" ], [ "a", "w" ], [ "d", "o" ], [ "te", "nd" ], [ "ƒ°", "vers" ], [ "ƒ°cha", "os" ], [ "ƒ°other", "s" ], [ "ƒ°eas", "ier" ], [ "ƒ°poo", "p" ], [ "ƒ°lo", "ndon" ], [ "en", "g" ], [ "ƒ°g", "reen" ], [ "ƒ°1", "00" ], [ "ƒ°pa", "id" ], [ "ƒ°ap", "olog" ], [ "ƒ°f", "ire" ], [ "ƒ°u", "wu" ], [ "ƒ°h", "m" ], [ "ker", "s" ], [ "d", "own" ], [ "es", "us" ], [ "ƒ°re", "mo" ], [ "ƒ°a", "nt" ], [ "ƒ°s", "ix" ], [ "ƒ°end", "ed" ], [ "ƒ°d", "ue" ], [ "ƒ°d", "ans" ], [ "ƒ°sha", "re" ], [ "ƒ°you", "ng" ], [ "ƒ°>", ":" ], [ "ƒ°di", "rect" ], [ "med", "i" ], [ "ƒ°pur", "ple" ], [ "2", "3" ], [ "he", "t" ], [ "ƒ°ca", "mer" ], [ "ƒ°des", "er" ], [ "ƒ°contro", "l" ], [ "ƒ°recent", "ly" ], [ "ƒ°kiss", "ing" ], [ "d", "f" ], [ "s", "ted" ], [ "ƒ°spe", "nd" ], [ "ƒ°beaut", "iful" ], [ "z", "z" ], [ "ƒ°leave", "s" ], [ "ƒ°u", "h" ], [ "ƒ°p", "our" ], [ "ƒ°love", "angle" ], [ "e", "g" ], [ "ƒ°i", "g" ], [ "ƒ°like", "d" ], [ "ƒ°sur", "pris" ], [ "w", "hat" ], [ "ƒ°lo", "s" ], [ "ƒ°person", "al" ], [ "'", "," ], [ "h", "ugs" ], [ "ƒ°how", "ever" ], [ "ƒ°move", "s" ], [ "ƒ°cle", "arly" ], [ "ƒ°stress", "ed" ], [ "ƒ°emot", "ional" ], [ "ƒ°help", "ing" ], [ "ƒ°acc", "urate" ], [ "b", "r" ], [ "m", "at" ], [ "et", "s" ], [ "ƒ°again", "st" ], [ "ome", "n" ], [ "um", "p" ], [ "pe", "cted" ], [ "ƒ°f", "ight" ], [ "ƒ°w", "ife" ], [ "ƒ°de", "ath" ], [ "ƒ°cra", "p" ], [ "ƒ°c", "e" ], [ "ƒ°arg", "u" ], [ "ƒ°sh", "out" ], [ "ƒ°am", "ount" ], [ "ƒ°take", "n" ], [ "ƒ°y", "o" ], [ "an", "y" ], [ "ƒ°so", "fa" ], [ "ƒ°intere", "sted" ], [ "ƒ°pro", "ject" ], [ "n", "se" ], [ "ƒ°sit", "ting" ], [ "(", ")" ], [ "4", "0" ], [ "p", "op" ], [ "ƒ°d", "ou" ], [ "ƒ°p", "ee" ], [ "ƒ°be", "n" ], [ "ƒ°hu", "gging" ], [ "p", "ar" ], [ "ƒ°c", "ertain" ], [ "ƒ°send", "ing" ], [ "ƒ°atta", "ck" ], [ "ƒ°coo", "king" ], [ "ƒ°o", "bs" ], [ "ƒ°mid", "dle" ], [ "ƒ°yout", "ube" ], [ "g", "o" ], [ "ƒ°f", "ake" ], [ "ƒ°fr", "iday" ], [ "ƒ°sing", "le" ], [ "ƒ°y", "esss" ], [ "v", "i" ], [ "ƒ°j", "esus" ], [ "ten", "or" ], [ "ƒ°pre", "fer" ], [ "k", "a" ], [ "ant", "s" ], [ "ƒ°or", "gan" ], [ "d", "n" ], [ "re", "n" ], [ "ƒ°d", "ude" ], [ "ƒ°o", "w" ], [ "ll", "s" ], [ "pp", "ed" ], [ "il", "t" ], [ "ƒ°them", "selves" ], [ "ƒ°cho", "ice" ], [ "o", "nd" ], [ "a", "me" ], [ "ƒ°at", "tention" ], [ "ƒ°smo", "ked" ], [ "ƒ°id", "iot" ], [ "g", "u" ], [ "l", "og" ], [ "ƒ°st", "ate" ], [ "ƒ°ba", "re" ], [ "ƒ°consider", "ing" ], [ "ƒ°a", "i" ], [ "ƒ°li", "l" ], [ "ƒ°q", "ue" ], [ "g", "s" ], [ "ƒ°a", "a" ], [ "ƒ°lo", "ads" ], [ "ƒ°d", "ate" ], [ "ƒ°exp", "ensive" ], [ "ƒ°bro", "ken" ], [ "ƒ°as", "her" ], [ "b", "ot" ], [ "ul", "ar" ], [ "ƒ°mean", "ing" ], [ "ƒ°m", "ag" ], [ "ƒ°c", "ard" ], [ "ƒ°con", "ne" ], [ "h", "ol" ], [ "ƒ°o", "nt" ], [ "ƒ°app", "ro" ], [ "ƒ°cra", "zy" ], [ "s", "or" ], [ "ƒ°re", "nt" ], [ "ƒ°g", "row" ], [ "100", "0" ], [ "a", "i" ], [ "r", "ic" ], [ "ƒ°ac", "cept" ], [ "ƒ°foc", "us" ], [ "ƒ°finis", "hed" ], [ "e", "red" ], [ "te", "xt" ], [ "ƒ°am", "b" ], [ "ƒ°sw","ear" ], [ "f", "y" ], [ "ƒ°a", "vo" ], [ "ƒ°de", "pe" ], [ "o", "ve" ], [ "ƒ°m", "as" ], [ "n", "orm" ], [ "is", "es" ], [ "ƒ°ad", "vice" ], [ "ƒ°>:", "(" ], [ "it", "ive" ], [ "er", "gy" ], [ "haha", "ha" ], [ "li", "ke" ], [ "igg", "er" ], [ "√™ƒ∑", "√£ƒ£¬£" ], [ "ƒ°t", "ic" ], [ "ƒ°w", "all" ], [ "ƒ°un", "s" ], [ "c", "le" ], [ "ƒ°d", "ress" ], [ "ƒ°1", "7" ], [ "ƒ°boo", "bs" ], [ "ƒ°ty", "ping" ], [ "ƒ°kid", "s" ], [ "2", "1" ], [ "ƒ°re", "act" ], [ "4", "5" ], [ "ƒ°dr", "ugs" ], [ "ƒ°:(", "(" ], [ "te", "l" ], [ "2", "6" ], [ "p", "ort" ], [ "in", "a" ], [ "ƒ°p", "ot" ], [ "ƒ°acc", "ess" ], [ "ƒ°em", "o" ], [ "ƒ°sys", "tem" ], [ "ƒ°re", "pe" ], [ "ƒ°ex", "press" ], [ "ƒ°em", "p" ], [ "ƒ°camer", "a" ], [ "ƒ°sat", "ur" ], [ "ag", "er" ], [ "m", "i" ], [ "ƒ°w", "omen" ], [ "ƒ°real", "ise" ], [ "ƒ°man", "age" ], [ "ƒ°see", "s" ], [ "ul", "es" ], [ "ime", "nt" ], [ "rop", "ri" ], [ "ƒ°bo", "i" ], [ "ƒ°so", "ft" ], [ "ƒ°bl", "ood" ], [ "ƒ°ont", "o" ], [ "ƒ°d", "est" ], [ "se", "arch" ], [ "ƒ°sm", "ile" ], [ "ƒ°sec", "ret" ], [ "ƒ°of", "fe" ], [ "ƒ°kit", "ty" ], [ "]", ":" ], [ "ƒ°c", "ode" ], [ "ƒ°v", "iew" ], [ "ƒ°pro", "te" ], [ "i", "ally" ], [ "ce", "l" ], [ "ƒ°t", "een" ], [ "ƒ°cha", "rac" ], [ "st", "airs" ], [ "ut", "h" ], [ "ƒ°r", "id" ], [ "ƒ°wal", "king" ], [ "ƒ°me", "mor" ], [ "oo", "f" ], [ "ƒ°off", "er" ], [ "ƒ°qu", "iet" ], [ "ƒ°let", "ter" ], [ "ƒ°him", "self" ], [ "wor", "k" ], [ "ƒ°be", "g" ], [ "im", "in" ], [ "ƒ°vib", "es" ], [ "ƒ°cust", "om" ], [ "f", "or" ], [ "ƒ°h", "ol" ], [ "ƒ°ab", "sol" ], [ "whe", "l" ], [ "ƒ°o", "pt" ], [ "at", "ure" ], [ "ƒ°resp", "ons" ], [ "ƒ°communic", "ation" ], [ "b", "u" ], [ "ƒ°o", "op" ], [ "ƒ°p", "ub" ], [ "an", "e" ], [ "a", "ten" ], [ "i", "od" ], [ "r", "ib" ], [ "ƒ°bo", "x" ], [ "ƒ°cle", "ar" ], [ "ƒ°clot", "hes" ], [ "ƒ°en", "ergy" ], [ "ƒ°give", "n" ], [ "ƒ°r", "ip" ], [ "if", "t" ], [ "ƒ°m", "ot" ], [ "ƒ°over", "whel" ], [ "ƒ°br", "it" ], [ "ƒ°yeah", "hh" ], [ "ƒ°fair", "ly" ], [ "ƒ°rem", "ind" ], [ "ƒ°absol", "ute" ], [ "ƒ°res", "pect" ], [ "r", "s" ], [ "ƒ°b", "in" ], [ "ƒ°p", "ie" ], [ "ƒ°wa", "ht" ], [ "ƒ°feeling", "s" ], [ "oc", "ked" ], [ "ab", "ility" ], [ "ƒ°ha", "nds" ], [ "ƒ°aw", "h" ], [ "ƒ°who", "s" ], [ "ƒ°re", "co" ], [ "ƒ°fin", "ish" ], [ "ƒ°wel", "come" ], [ "c", "al" ], [ "ƒ°wor", "ked" ], [ "our", "s" ], [ "o", "int" ], [ "p", "ro" ], [ "ƒ°put", "ting" ], [ "ƒ°e", "st" ], [ "ir", "ty" ], [ "ƒ°ret", "ur" ], [ "ƒ°a", "v" ], [ "ƒ°b", "ts" ], [ "ƒ°re", "le" ], [ "ƒ°should", "nt" ], [ "ƒ°po", "ke" ], [ "ƒ°ad", "mit" ], [ "ƒ°med", "ic" ], [ "ƒ°tru", "ly" ], [ "ƒ°be", "h" ], [ "ƒ°fu", "lly" ], [ "ƒ°chang", "ed" ], [ "b", "ut" ], [ "yy", "y" ], [ "ƒ°s", "am" ], [ "ast", "ic" ], [ "ƒ°2", "4" ], [ "ƒ°phot", "o" ], [ "ƒ°", "√¢" ], [ "ƒ°plan", "ning" ], [ "ƒ°vi", "be" ], [ "o", "k" ], [ "ƒ°g", "re" ], [ "ƒ°elod", "ies" ], [ "ƒ°ro", "ck" ], [ "ƒ°lar", "ge" ], [ "ƒ°compl", "ain" ], [ "m", "iss" ], [ "ig", "in" ], [ "ƒ°per", "iod" ], [ "ƒ°forget", "ting" ], [ "ƒ°t", "re" ], [ "ƒ°ter", "r" ], [ "ƒ°inform", "ation" ], [ "ƒ°co", "ver" ], [ "2", "2" ], [ "ƒ°pr", "iv" ], [ "ƒ°satur", "day" ], [ "u", "c" ], [ "ic", "ken" ], [ "ct", "ive" ], [ "ƒ°ign", "ore" ], [ "ƒ°particul", "arly" ], [ "ropri", "ate" ], [ "w", "s" ], [ "ƒ°t", "im" ], [ "ur", "l" ], [ "ƒ°pe", "tes" ], [ "ac", "es" ], [ "ƒ°pi", "pe" ], [ "ƒ°a", "ud" ], [ "ƒ°mo", "nday" ], [ "ƒ°5", "0" ], [ "ing", "er" ], [ "ƒ°be", "ha" ], [ "o", "le" ], [ "ƒ°d", "rop" ], [ "ƒ°p", "ort" ], [ "ƒ°think", "s" ], [ "ƒ°ar", "r" ], [ "ang", "er" ], [ "ƒ°doct", "ors" ], [ "ƒ°contin", "ue" ], [ "z", "e" ], [ "ƒ°b", "at" ], [ "ƒ°aw", "ful" ], [ "ƒ°hel", "ps" ], [ "b", "i" ], [ "d", "am" ], [ "ƒ°det", "ail" ], [ "ƒ°t", "reat" ], [ "ra", "ph" ], [ "ƒ°im", "medi" ], [ "ƒ°find", "ing" ], [ "ƒ°o", "ption" ], [ "?", "'" ], [ "k", "y" ], [ "ƒ°i", "r" ], [ "at", "or" ], [ "ƒ°remember", "ing" ], [ "ƒ°conf", "ir" ], [ "(", "\"" ], [ "ƒ°l", "ady" ], [ "ƒ°p", "ower" ], [ "ƒ°un", "com" ], [ "ƒ°p", "ract" ], [ "ƒ°te", "mp" ], [ "ƒ°kid", "ding" ], [ "ne", "l" ], [ "ƒ°move", "d" ], [ "ƒ°b", "by" ], [ "ƒ°str", "ange" ], [ "gr", "ound" ], [ "ƒ°no", "ise" ], [ "ƒ°pa", "ck" ], [ "ƒ°inv", "ol" ], [ "l", "r" ], [ "m", "g" ], [ "ƒ°s", "al" ], [ "ƒ°l", "y" ], [ "ƒ°v", "ag" ], [ "ƒ°or", "ange" ], [ "ƒ°some", "body" ], [ "ƒ°", "_" ], [ "ƒ°understand", "ing" ], [ "ƒ°clos", "er" ], [ "ƒ°avo", "id" ], [ "ƒ°l", "es" ], [ "ƒ°k", "pop" ], [ "ƒ°a", "int" ], [ "ƒ°w", "arn" ], [ "or", "ing" ], [ "c", "ou" ], [ "ƒ°un", "f" ], [ "ƒ°ass", "a" ], [ "ƒ°ju", "ice" ], [ "ct", "ure" ], [ "ƒ°tr", "uth" ], [ "ƒ°proper", "ty" ], [ "scri", "be" ], [ "ƒ°s", "le" ], [ "ƒ°m", "ice" ], [ "ƒ°u", "m" ], [ "ƒ°in", "t" ], [ "ƒ°st", "re" ], [ "ad", "e" ], [ "ƒ°communic", "ate" ], [ "ƒ°beh", "ind" ], [ "b", "al" ], [ "ƒ°t", "it" ], [ "ƒ°w", "ooo" ], [ "ƒ°f", "eed" ], [ "ƒ°al", "ive" ], [ "ƒ°dr", "ug" ], [ "f", "d" ], [ "l", "and" ], [ "am", "p" ], [ "ƒ°wa", "king" ], [ "ƒ°keep", "s" ], [ "ƒ°sk", "y" ], [ "2", "8" ], [ "ƒ°l", "ink" ], [ "ƒ°fuck", "s" ], [ "ar", "c" ], [ "ƒ°incred", "ibly" ], [ "ƒ°2", "3" ], [ "ƒ°norm", "ally" ], [ "o", "h" ], [ "ƒ°me", "gan" ], [ "ƒ°f", "ast" ], [ "ƒ°ex", "ist" ], [ "ƒ°event", "ually" ], [ "r", "d" ], [ "ƒ°stop", "ped" ], [ "ƒ°americ", "a" ], [ "ie", "nc" ], [ "se", "x" ], [ "ƒ°fe", "ar" ], [ "p", "ose" ], [ "ƒ°o", "cc" ], [ "lf", "r" ], [ "ƒ°ba", "nd" ], [ "ƒ°bot", "t" ], [ "ƒ°cr", "inge" ], [ "1", "1" ], [ "it", "ter" ], [ "ƒ°her", "self" ], [ "ƒ°press", "ure" ], [ "c", "ing" ], [ "ƒ°or", "igin" ], [ "ƒ°bo", "om" ], [ "ƒ°elect", "ronic" ], [ "ƒ°f", "at" ], [ "ƒ°th", "urs" ], [ "gg", "g" ], [ "ag", "ing" ], [ "ƒ°conf", "using" ], [ "ƒ°spec", "if" ], [ "o", "ss" ], [ "ƒ°b", "all" ], [ "ƒ°for", "ce" ], [ "ver", "se" ], [ "c", "ra" ], [ "ign", "ed" ], [ "ƒ°computer", "s" ], [ "ƒ°ho", "tel" ], [ "ƒ°vers", "ion" ], [ "d", "io" ], [ "ƒ°l", "ang" ], [ "ƒ°be", "ds" ], [ "ƒ°al", "co" ], [ "ƒ°ad", "v" ], [ "ƒ°ma", "ch" ], [ "f", "ace" ], [ "ƒ°", "&" ], [ "ne", "y" ], [ "lfr", "iend" ], [ "ƒ°w", "ot" ], [ "ve", "ra" ], [ "ƒ°k", "ore" ], [ "aa", "ay" ], [ "ƒ°rel", "ax" ], [ "us", "ive" ], [ "ƒ°co", "st" ], [ "ƒ°ma", "c" ], [ "ipp", "ed" ], [ "ƒ°for", "ward" ], [ "ƒ°dis", "tra" ], [ "√¢ƒ£", "¬ø" ], [ "ƒ°star", "s" ], [ "ƒ°ter", "m" ], [ "m", "es" ], [ "ƒ°c", "ir" ], [ "ƒ°pro", "gra" ], [ "ƒ°wee", "ke" ], [ "ƒ°n", "or" ], [ "le", "ted" ], [ "ƒ°ad", "or" ], [ "ƒ°miss", "ing" ], [ "ƒ°re", "v" ], [ "ƒ°mat", "ter" ], [ "ƒ°see", "med" ], [ "ƒ°requ", "est" ], [ "ƒ°f", "ut" ], [ "ƒ°be", "ne" ], [ "ow", "s" ], [ "ƒ°de", "cent" ], [ "ƒ°des", "k" ], [ "ƒ°pi", "per" ], [ "ƒ°ey", "e" ], [ "ƒ°c", "y" ], [ "ƒ°fr", "ust" ], [ "ƒ°ra", "ccoon" ], [ "ƒ°pr", "ice" ], [ "ƒ°ma", "le" ], [ "ƒ°deser", "ve" ], [ "9", "9" ], [ "ƒ°p", "orn" ], [ "ƒ°h", "ide" ], [ "ic", "ked" ], [ "ƒ°cur", "rent" ], [ "i", "or" ], [ "ƒ°ur", "self" ], [ "ƒ°ide", "nt" ], [ "c", "at" ], [ "ƒ°listen", "s" ], [ "ƒ°spe", "cial" ], [ "ƒ°stude", "nt" ], [ "ƒ°f", "ill" ], [ "ƒ°tou", "ches" ], [ "ƒ°sleep", "y" ], [ "us", "ted" ], [ "ƒ°har", "sh" ], [ "ƒ°boy", "s" ], [ "ra", "p" ], [ "ive", "d" ], [ "ƒ°lang", "u" ], [ "ƒ°alco", "hol" ], [ "s", "p" ], [ "ƒ°m", "ate" ], [ "ƒ°st", "an" ], [ "ƒ°ex", "pected" ], [ "ƒ°were", "nt" ], [ "ƒ°ac", "ross" ], [ "al", "ing" ], [ "ƒ°cu", "p" ], [ "ƒ°bad", "ly" ], [ "ƒ°dec", "ision" ], [ "ƒ°includ", "ing" ], [ "l", "er" ], [ "v", "ous" ], [ "ƒ°", "ou" ], [ "ƒ°depress", "ed" ], [ "2", "7" ], [ "l", "a" ], [ "ƒ°s", "ake" ], [ "to", "ken" ], [ "ath", "s" ], [ "s", "es" ], [ "ƒ°", "ris" ], [ "ƒ°to", "wn" ], [ "ƒ°kevin", "s" ], [ "ƒ°when", "ever" ], [ "ƒ°stream", "ing" ], [ "ar", "s" ], [ "n", "ted" ], [ "ƒ°bur", "n" ], [ "d", "j" ], [ "ƒ°e", "p" ], [ "ƒ°ro", "ll" ], [ "i", "ant" ], [ "ƒ°comp", "any" ], [ "ƒ°shit", "ty" ], [ "ƒ°app", "oint" ], [ "ƒ°gir", "lfriend" ], [ "ƒ°tas", "te" ], [ "s", "ha" ], [ "u", "me" ], [ "ƒ°e", "ight" ], [ "ƒ°cudd", "le" ], [ "ƒ°with", "in" ], [ "ƒ°man", "aged" ], [ "ƒ°no", "te" ], [ "ƒ°an", "aly" ], [ "ƒ°l", "ay" ], [ "ƒ°reason", "s" ], [ "ƒ°bright", "on" ], [ "b", "l" ], [ "ƒ°s", "on" ], [ "ten", "ce" ], [ "ƒ°dr", "um" ], [ "ƒ°doct", "or" ], [ "ƒ°j", "ud" ], [ "ƒ°sa", "ve" ], [ "ƒ°w", "ays" ], [ "ƒ°re", "dd" ], [ "ƒ°sk", "ills" ], [ "ƒ°fur", "ther" ], [ "ƒ°a", "hhh" ], [ "ƒ°r", "ules" ], [ "p", "ital" ], [ "ha", "el" ], [ "ƒ°b", "ill" ], [ "ƒ°n", "one" ], [ "ƒ°di", "ag" ], [ "ƒ°wat", "ched" ], [ "ƒ°repe", "at" ], [ "ƒ°", "{" ], [ "ƒ°to", "r" ], [ "ƒ°st", "at" ], [ "ƒ°al", "ong" ], [ "ƒ°bu", "ll" ], [ "ƒ°gr", "ind" ], [ "ƒ°phot", "os" ], [ "ƒ°oop", "s" ], [ "ƒ°beha", "vi" ], [ "ƒ°c", "ris" ], [ "ƒ°d", "en" ], [ "ƒ°there", "fore" ], [ "f", "c" ], [ "l", "ist" ], [ "ƒ°lol", "ol" ], [ "ƒ°help", "ful" ], [ "ƒ°d", "ro" ], [ "ƒ°p", "et" ], [ "ƒ°p", "ark" ], [ ")", ")" ], [ "it", "ten" ], [ "ƒ°oh", "h" ], [ "ƒ°brit", "ish" ], [ "b", "in" ], [ "ƒ°ch", "icken" ], [ "ƒ°t", "ty" ], [ "ƒ°de", "p" ], [ "ƒ°dra", "g" ], [ "ƒ°gen", "re" ], [ "ƒ°j", "k" ], [ "ƒ°of", "c" ], [ "ƒ°pl", "ans" ], [ "ƒ°frogg", "ys" ], [ "ƒ°diffe", "rence" ], [ "w", "h" ], [ "al", "ity" ], [ "ƒ°ex", "is" ], [ "ƒ°ur", "s" ], [ "ƒ°exper", "ienc" ], [ "ƒ°exc", "iting" ], [ "ƒ°ill", "eg" ], [ "ƒ°met", "al" ], [ "ƒ°embar", "r" ], [ "ƒ°absolute", "ly" ], [ "ƒ°fut", "ure" ], [ "b", "ab" ], [ "ƒ°you", "uu" ], [ "ƒ°po", "or" ], [ "ƒ°mic", "hael" ], [ "ƒ°specif", "ically" ], [ "s", "ter" ], [ "u", "ce" ], [ "ƒ°m", "ild" ], [ "re", "f" ], [ "ƒ°no", "od" ], [ "ƒ°at", "tem" ], [ "ƒ°sor", "ted" ], [ "ƒ°sw", "itch" ], [ "c", "ri" ], [ "ƒ°pl", "aces" ], [ "ƒ°hear", "s" ], [ "ƒ°la", "ck" ], [ "ƒ°learn", "s" ], [ "ƒ°run", "ning" ], [ "d", "en" ], [ "t", "ain" ], [ "u", "ght" ], [ "ƒ°imp", "ress" ], [ "ƒ°pur", "pose" ], [ "ƒ°hy", "per" ], [ "ƒ°opt", "ions" ], [ "ƒ°vo", "c" ], [ "ƒ°m", "c" ], [ "ƒ°f", "oot" ], [ "id", "ence" ], [ "ted", "ly" ], [ "ƒ°try", "na" ], [ "ƒ°chan", "nel" ], [ "ƒ°inf", "o" ], [ "ƒ°train", "ing" ], [ "ƒ°lo", "ad" ], [ "pp", "ing" ], [ "ƒ°pr", "int" ], [ "ƒ°del", "ive" ], [ "a", "ks" ], [ "o", "in" ], [ "in", "ts" ], [ "ƒ°b", "illy" ], [ "ƒ°sc", "hed" ], [ "ƒ°reg", "ard" ], [ "ƒ°w", "ro" ], [ "ƒ°b", "le" ], [ "ƒ°up", "d" ], [ "a", "ve" ], [ "t", "on" ], [ "y", "e" ], [ "ƒ°d", "ied" ], [ "ƒ°d", "irty" ], [ "ƒ°re", "ce" ], [ "ƒ°know", "ing" ], [ "ƒ°un", "able" ], [ "ƒ°equ", "als" ], [ "ƒ°ner", "vous" ], [ "ƒ°bare", "ly" ], [ "ƒ°2", "5" ], [ "ƒ°hur", "ts" ], [ "ƒ°shop", "s" ], [ "ƒ°weeke", "nd" ], [ "ƒ°redd", "it" ], [ "l", "p" ], [ "u", "nder" ], [ "ƒ°su", "nday" ], [ "pp", "y" ], [ "ƒ°aw", "are" ], [ "ƒ°sk", "in" ], [ "na", "me" ], [ "ƒ°ge", "nder" ], [ "ƒ°hehe", "he" ], [ "ƒ°ador", "able" ], [ "s", "ed" ], [ "ƒ°t", "une" ], [ "ha", "r" ], [ "id", "ge" ], [ "ƒ°bott", "om" ], [ "ƒ°t", "ro" ], [ "ƒ°b", "um" ], [ "ƒ°r", "uin" ], [ "ƒ°mention", "ed" ], [ "ƒ°surpris", "ed" ], [ "ƒ°tty", "l" ], [ "b", "ing" ], [ "ƒ°be", "at" ], [ "ƒ°ab", "ove" ], [ "ƒ°min", "ute" ], [ "ƒ°const", "antly" ], [ "ƒ°b", "b" ], [ "ƒ°dis", "g" ], [ "ƒ°bed", "room" ], [ "vel", "op" ], [ "ƒ°mach", "ine" ], [ "3", "8" ], [ "on", "ed" ], [ "at", "t" ], [ "ƒ°j", "ess" ], [ "ƒ°poss", "ibly" ], [ "ƒ°>.", ">" ], [ "ƒ°re", "fe" ], [ "ion", "ally" ], [ "ƒ°eat", "s" ], [ "ƒ°dad", "s" ], [ "ƒ°ass", "um" ], [ "ƒ°m", "other" ], [ "ƒ°l", "ying" ], [ "ƒ°k", "k" ], [ "ƒ°mo", "on" ], [ "ver", "t" ], [ "ƒ°pic", "ks" ], [ "ƒ°acc", "ount" ], [ "ƒ°wro", "te" ], [ "ƒ°was", "hing" ], [ "pl", "ay" ], [ "ƒ°strugg", "ling" ], [ "ƒ°m", "ur" ], [ "ƒ°c", "ross" ], [ "ƒ°ta", "x" ], [ "es", "ted" ], [ "ƒ°te", "nd" ], [ "ƒ°pen", "is" ], [ "c", "han" ], [ "ul", "ation" ], [ "ƒ°jo", "bs" ], [ "ƒ°4", "0" ], [ "ess", "ed" ], [ "gg", "ed" ], [ "ƒ°√∞≈Ç", "ƒ∫" ], [ "ƒ°bru", "h" ], [ "bo", "x" ], [ "√≥¬æ", "ƒ©" ], [ "ut", "ion" ], [ "ƒ°direct", "ly" ], [ "ƒ°c", "ake" ], [ "ƒ°con", "cer" ], [ "umb", "lr" ], [ "2", "9" ], [ "p", "re" ], [ "ƒ°t", "able" ], [ "ƒ°b", "ow" ], [ "ƒ°qu", "it" ], [ "ƒ°t", "ues" ], [ "ƒ°de", "lete" ], [ "ƒ°co", "nd" ], [ "ƒ°pay", "ing" ], [ "ƒ°worr", "ies" ], [ "ƒ°cop", "y" ], [ "ƒ°prof", "ess" ], [ "6", "6" ], [ "ƒ°h", "om" ], [ "ƒ°sad", "ly" ], [ "ƒ°dou", "ble" ], [ "ƒ°langu", "age" ], [ "p", "g" ], [ "ƒ°m", "aths" ], [ "ƒ°d", "ying" ], [ "ƒ°h", "oney" ], [ "id", "es" ], [ "al", "ous" ], [ "ƒ°ne", "g" ], [ "ƒ°inter", "view" ], [ "ƒ°mod", "e" ], [ "ƒ°ban", "ned" ], [ "b", "t" ], [ "ƒ°m", "ist" ], [ "nd", "ent" ], [ "ƒ°u", "uu" ], [ "ƒ°he", "av" ], [ "ƒ°st", "ick" ], [ "ƒ°se", "ll" ], [ "ƒ°sp", "ot" ], [ "ƒ°every", "where" ], [ "ƒ°dr", "ive" ], [ "ƒ°ho", "ly" ], [ "ƒ°apolog", "ise" ], [ "ƒ°poke", "mon" ], [ "ƒ°sle", "pt" ], [ "ƒ°sa", "nd" ], [ "ƒ°ser", "ious" ], [ "g", "es" ], [ "r", "ied" ], [ "u", "ff" ], [ "ƒ°y", "ee" ], [ "ƒ°sha", "d" ], [ "nc", "y" ], [ "ƒ°fix", "ed" ], [ "ƒ°fil", "m" ], [ "ƒ°r", "om" ], [ "ƒ°c", "ent" ], [ "ƒ°p", "un" ], [ "ƒ°g", "url" ], [ "ie", "n" ], [ "ƒ°wr", "itten" ], [ "ƒ°ph", "ones" ], [ "ƒ°embar", "ass" ], [ "ƒ°medic", "ation" ], [ "0", "8" ], [ "ƒ°b", "reat" ], [ "us", "es" ], [ "ƒ°lo", "se" ], [ "ƒ°su", "mm" ], [ "ƒ°se", "ven" ], [ "ƒ°2", "2" ], [ "led", "ge" ], [ "ƒ°illeg", "al" ], [ "4", "8" ], [ "le", "nt" ], [ "ƒ°cha", "ll" ], [ "ƒ°tr", "ip" ], [ "ƒ°ir", "l" ], [ "t", "ime" ], [ "ƒ°j", "oy" ], [ "ƒ°an", "im" ], [ "am", "ine" ], [ "ƒ°fin", "al" ], [ "ƒ°random", "ly" ], [ "ƒ°emot", "ions" ], [ "5", "7" ], [ "m", "pt" ], [ "ƒ°be", "com" ], [ "ƒ°let", "ting" ], [ "a", "ime" ], [ "ƒ°j", "um" ], [ "ƒ°use", "ful" ], [ "ƒ°imp", "oss" ], [ "ƒ°je", "alous" ], [ "ƒ°entire", "ly" ], [ "√¢ƒ£¬¢√¨", "ƒ£" ], [ "ƒ°traum", "a" ], [ "ƒ°immedi", "ately" ], [ "ƒ°uncom", "fortable" ], [ "n", "it" ], [ "ƒ°a", "te" ], [ "ƒ°bene", "f" ], [ "ƒ°s", "kevin" ], [ "ƒ°pic", "s" ], [ "ƒ°americ", "an" ], [ "ƒ°s", "ize" ], [ "ƒ°l", "u" ], [ "ƒ°any", "where" ], [ "ƒ°ba", "e" ], [ "ƒ°wear", "ing" ], [ "g", "pt" ], [ "ƒ°s", "ce" ], [ "ure", "s" ], [ "ƒ°beg", "in" ], [ ")", ":" ], [ "ƒ°su", "cc" ], [ "ƒ°app", "ly" ], [ "ƒ°sha", "me" ], [ "ƒ°rem", "ix" ], [ "ƒ°low", "key" ], [ "ƒ°imposs", "ible" ], [ "ƒ°st", "al" ], [ "ƒ°why", "yy" ], [ "ƒ°imp", "ro" ], [ "ƒ°gl", "ass" ], [ "ƒ°depress", "ion" ], [ "ƒ°los", "er" ], [ "ƒ°priv", "ate" ], [ "cra", "ft" ], [ "4", "7" ], [ "te", "p" ], [ "ƒ°are", "n" ], [ "if", "ying" ], [ "ƒ°help", "ed" ], [ "ƒ°bab", "ies" ], [ "og", "raph" ], [ "ƒ°ind", "e" ], [ "wa", "re" ], [ "ƒ°exc", "use" ], [ "ƒ°phys", "ical" ], [ "√¢ƒ£¬¢√¨", "ƒ£" ], [ "g", "f" ], [ "ƒ°l", "ed" ], [ "ƒ°j", "our" ], [ "ust", "ing" ], [ "ƒ°uni", "vers" ], [ "y", "a" ], [ "ƒ°no", "ice" ], [ "ct", "ions" ], [ "ƒ°cut", "ie" ], [ "o", "f" ], [ "ƒ°f", "ish" ], [ "ƒ°cop", "e" ], [ "ma", "le" ], [ "ƒ°ob", "ject" ], [ "ƒ°retur", "n" ], [ "ight", "s" ], [ "ƒ°bu", "ilt" ], [ "ƒ°je", "lly" ], [ "n", "b" ], [ "ƒ°i", "rr" ], [ "ƒ°y", "eee" ], [ "ƒ°n", "ine" ], [ "ƒ°u", "t" ], [ "ƒ°im", "age" ], [ "ƒ°con", "vo" ], [ "ƒ°sign", "ific" ], [ "ƒ°thurs", "day" ], [ "a", "ff" ], [ "ƒ°b", "ound" ], [ "ƒ°had", "n" ], [ "ƒ°app", "ear" ], [ "ph", "one" ], [ "ƒ°t", "f" ], [ "or", "ted" ], [ "ƒ°re", "ach" ], [ "ƒ°an", "ime" ], [ "ha", "ps" ], [ "te", "ms" ], [ "ƒ°sh", "o" ], [ "ƒ°ch", "ips" ], [ "ƒ°sc", "r" ], [ "ire", "s" ], [ "ƒ°tra", "cks" ], [ "ƒ°ac", "es" ], [ "ƒ°hum", "ans" ], [ "te", "red" ], [ "ra", "m" ], [ "ƒ°ma", "x" ], [ "ƒ°gr", "an" ], [ "ƒ°clean", "ing" ], [ "icul", "ous" ], [ "ul", "ts" ], [ "ƒ°ass", "ist" ], [ "ƒ°stress", "ful" ], [ "ƒ°fig", "ured" ], [ "ƒ°vis", "it" ], [ "ƒ°s", "ou" ], [ "ƒ°se", "arch" ], [ "ƒ°key", "board" ], [ "ƒ°quick", "ly" ], [ "b", "ack" ], [ "ƒ°g", "ar" ], [ "ƒ°end", "s" ], [ "ƒ°perf", "orm" ], [ "ƒ°p", "ill" ], [ "ƒ°con", "text" ], [ "ƒ°child", "ren" ], [ "ƒ°depe", "nds" ], [ "ƒ°r", "ice" ], [ "ƒ°de", "leted" ], [ "ul", "ance" ], [ "ƒ°tea", "m" ], [ "ƒ°embarr", "ass" ], [ "ƒ°d", "ed" ], [ "us", "ion" ], [ "ƒ°th", "re" ], [ "ƒ°st", "ab" ], [ "..", "?" ], [ "ƒ°en", "s" ], [ "d", "is" ], [ "ƒ°b", "ang" ], [ "ƒ°in", "it" ], [ "ƒ°k", "ay" ], [ "ƒ°shout", "ing" ], [ "c", "her" ], [ "r", "ation" ], [ "v", "ant" ], [ "v", "ices" ], [ "ƒ°a", "hh" ], [ "ƒ°d", "anger" ], [ "ur", "t" ], [ "ƒ°ye", "h" ], [ "ƒ°dra", "ma" ], [ "ƒ°ra", "c" ], [ "ƒ°hid", "ing" ], [ "ƒ°laugh", "ing" ], [ "ƒ°b", "on" ], [ "id", "ing" ], [ "3", "5" ], [ "ƒ°p", "age" ], [ "ƒ°p", "ush" ], [ "as", "ed" ], [ "ƒ°ab", "use" ], [ "ƒ°ab", "usive" ], [ "g", "ed" ], [ "ƒ°su", "is" ], [ "ƒ°ex", "ha" ], [ "ƒ°com", "fortable" ], [ "ƒ°te", "x" ], [ "ƒ°ch", "in" ], [ "ƒ°che", "ap" ], [ "ƒ°hor", "ny" ], [ "n", "ow" ], [ "ƒ°b", "ee" ], [ "ide", "red" ], [ "dd", "ing" ], [ "ƒ°jo", "king" ], [ "ƒ°tw", "itter" ], [ "sh", "it" ], [ "ƒ°stay", "ing" ], [ "ƒ°inst", "ru" ], [ "d", "u" ], [ "ƒ°p", "us" ], [ "ƒ°h", "os" ], [ "ƒ°cha", "nce" ], [ "ƒ°us", "ual" ], [ "ƒ°wa", "aa" ], [ "ƒ°allow", "s" ], [ "ƒ°ow", "o" ], [ "miss", "ion" ], [ "ƒ°fun", "ction" ], [ "ƒ°live", "s" ], [ "ƒ°egg", "s" ], [ "ƒ°wood", "s" ], [ "ƒ°count", "ry" ], [ "ƒ°sme", "ll" ], [ "ƒ°rid", "iculous" ], [ "ƒ°reco", "good night" ], [ "ƒ°ag", "re" ], [ "qu", "en" ], [ "ƒ°cou", "nse" ], [ "ƒ°seco", "nds" ], [ "m", "l" ], [ "ƒ°i", "con" ], [ "ƒ°b", "igg" ], [ "ƒ°f", "ra" ], [ "om", "s" ], [ "ƒ°im", "o" ], [ "ƒ°r", "hy" ], [ "ƒ°fu", "c" ], [ "ƒ°act", "iv" ], [ "ƒ°per", "haps" ], [ "ƒ°bre", "aks" ], [ "ƒ°mist", "ake" ], [ "j", "ust" ], [ "ƒ°we", "dn" ], [ "ƒ°bot", "her" ], [ "ƒ°bath", "room" ], [ "ber", "ry" ], [ "ƒ°charac", "ter" ], [ "ƒ°appoint", "ment" ], [ "ƒ°wedn", "es" ], [ "3", "6" ], [ "4", "6" ], [ "he", "re" ], [ "ƒ°m", "ut" ], [ "ƒ°me", "r" ], [ "ol", "ate" ], [ "be", "red" ], [ "ƒ°ap", "art" ], [ "ƒ°sile", "nt" ], [ "ƒ°irr", "it" ], [ "t", "re" ], [ "ƒ°mo", "v" ], [ "ƒ°le", "ad" ], [ "ƒ°art", "ic" ], [ "ƒ°intere", "st" ], [ "ƒ°vide", "os" ], [ "!'", "," ], [ "ƒ°opp", "os" ], [ "3", "9" ], [ "ƒ°m", "eee" ], [ "ƒ°f", "ancy" ], [ "ƒ°n", "aked" ], [ "at", "o" ], [ "ƒ°br", "b" ], [ "ƒ°9", "0" ], [ "ipp", "ing" ], [ "ƒ°suggest", "ing" ], [ "f", "orm" ], [ "our", "ce" ], [ "de", "red" ], [ "f", "et" ], [ "ƒ°it", "self" ], [ "ƒ°chat", "gpt" ], [ "ƒ°obv", "ious" ], [ "ƒ°wonder", "ing" ], [ "ƒ°sile", "nce" ], [ "ƒ°sched", "ule" ], [ "ƒ°y", "aaay" ], [ "ƒ°en", "ter" ], [ "ƒ°tou", "gh" ], [ "ƒ°app", "ropriate" ], [ "fa", "ir" ], [ "ƒ°show", "ing" ], [ "ƒ°exper", "iment" ], [ "ƒ°dat", "a" ], [ "ƒ°vag", "ina" ], [ "h", "ha" ], [ "is", "ter" ], [ "ƒ°st", "ic" ], [ "ƒ°ch", "oo" ], [ "ƒ°sim", "ple" ], [ "fort", "un" ], [ "ƒ°pub", "lic" ], [ "c", "ess" ], [ "f", "w" ], [ "ƒ°to", "i" ], [ "ƒ°p", "een" ], [ "ƒ°can", "ce" ], [ "ƒ°pre", "tend" ], [ "igh", "ten" ], [ "anc", "es" ], [ "ƒ°mix", "ed" ], [ "ƒ°ear", "s" ], [ "ƒ°ban", "k" ], [ "f", "un" ], [ "p", "es" ], [ "ƒ°an", "ge" ], [ "ƒ°wan", "k" ], [ "ƒ°pa", "per" ], [ "ƒ°mar", "ry" ], [ "ƒ°gra", "nd" ], [ "ƒ°ver", "bal" ], [ "5", "8" ], [ "r", "ics" ], [ "ƒ°n", "ames" ], [ "ƒ°le", "ge" ], [ "ƒ°should", "n" ], [ "ƒ°tea", "ch" ], [ "ƒ°s", "us" ], [ "ƒ°sh", "y" ], [ "ƒ°sh", "irt" ], [ "ƒ°butt", "s" ], [ "il", "ing" ], [ "ƒ°ad", "am" ], [ "ƒ°offe", "nded" ], [ "ƒ°origin", "al" ], [ "v", "id" ], [ "|", "|" ], [ "st", "ats" ], [ "ƒ°de", "g" ], [ "ƒ°com", "fy" ], [ "ƒ°pre", "scri" ], [ "ƒ°gr", "im" ], [ "ƒ°add", "ed" ], [ "ƒ°mine", "craft" ], [ "ƒ°es", "sent" ], [ "g", "ing" ], [ "k", "j" ], [ "s", "u" ], [ "ƒ°tr", "igger" ], [ "ƒ°call", "s" ], [ "ƒ°tra", "vel" ], [ "a", "o" ], [ "g", "l" ], [ "ƒ°", "√≥¬æ" ], [ "if", "ied" ], [ "ƒ°cons", "idered" ], [ "ƒ°tic", "kets" ], [ "ƒ°hol", "iday" ], [ "ƒ°a", "gg" ], [ "ƒ°in", "ten" ], [ "ick", "y" ], [ "ƒ°co", "ll" ], [ "ƒ°jo", "h" ], [ "ƒ°reme", "m" ], [ "ƒ°situ", "ations" ], [ "t", "hat" ], [ "y", "es" ], [ "ƒ°un", "fortun" ], [ "ƒ°draw", "s" ], [ "ƒ°bre", "aking" ], [ "ƒ°inde", "pe" ], [ "ct", "ing" ], [ "ƒ°app", "lic" ], [ "ƒ°reg", "ret" ], [ "ƒ°tas", "ks" ], [ "ƒ°a", "u" ], [ "ve", "re" ], [ "ƒ°p", "ure" ], [ "ƒ°v", "io" ], [ "ƒ°tr", "ash" ], [ "ƒ°litera", "l" ], [ "ƒ°ins", "ane" ], [ "ƒ°ass", "ume" ], [ "lo", "ve" ], [ "we", "ll" ], [ "ƒ°ed", "it" ], [ "ƒ°sksks", "ks" ], [ "uin", "o" ], [ "4", "9" ], [ "u", "ll" ], [ "ƒ°d", "ry" ], [ "er", "y" ], [ "ƒ°his", "t" ], [ "ƒ°>.", "<" ], [ "0", "1" ], [ "v", "a" ], [ "ƒ°om", "ggg" ], [ "ƒ°tal", "ked" ], [ "pl", "oy" ], [ "ƒ°old", "er" ], [ "ƒ°chris", "t" ], [ "ƒ°neg", "ative" ], [ "ƒ°oppos", "ite" ], [ "ƒ°t", "att" ], [ "ƒ°c", "um" ], [ "le", "t" ], [ "le", "y" ], [ "ƒ°j", "imin" ], [ "ƒ°ad", "vent" ], [ "ƒ°hard", "er" ], [ "ƒ°set", "ting" ], [ "ƒ°techn", "ology" ], [ "ƒ°overwhel", "med" ], [ "ƒ°assa", "ult" ], [ "ƒ°diag", "n" ], [ "i", "um" ], [ "i", "est" ], [ "√¢", "ƒæ" ], [ "ƒ°pro", "g" ], [ "ƒ°ve", "ux" ], [ "ƒ°cl", "a" ], [ "ƒ°ep", "is" ], [ "ar", "ies" ], [ "ƒ°lo", "ck" ], [ "ƒ°fro", "g" ], [ "ƒ°remem", "bered" ], [ "3", "7" ], [ "a", "cc" ], [ "ƒ°d", "are" ], [ "ƒ°n", "et" ], [ "te", "ry" ], [ "ƒ°su", "it" ], [ "ƒ°ca", "used" ], [ "ƒ°fre", "aking" ], [ "ƒ°prom", "ise" ], [ "c", "r" ], [ "ƒ°st", "orm" ], [ "ƒ°con", "vin" ], [ "ƒ°sha", "ll" ], [ "ƒ°chin", "ese" ], [ "√†", "" ], [ "ƒ°t", "ha" ], [ "at", "ic" ], [ "ƒ°j", "am" ], [ "ƒ°off", "ic" ], [ "aa", "a" ], [ "ƒ°sol", "id" ], [ "d", "i" ], [ "v", "al" ], [ "ƒ°a", "lex" ], [ "on", "a" ], [ "ut", "m" ], [ "ƒ°st", "or" ], [ "ƒ°man", "ip" ], [ "ƒ°sha", "ring" ], [ "ƒ°slow", "ly" ], [ "ƒ°me", "at" ], [ "ƒ°ab", "ility" ], [ "th", "ough" ], [ "ci", "ous" ], [ "ƒ°gr", "ound" ], [ "ƒ°incl", "ude" ], [ "ƒ°unfortun", "ately" ], [ "ƒ°h", "ous" ], [ "ƒ°rant", "ing" ], [ "ƒ°tatt", "oo" ], [ "b", "by" ], [ "d", "it" ], [ "ƒ°", "√™ƒ∑√£ƒ£¬£" ], [ "ƒ°id", "fk" ], [ "ƒ°ch", "oc" ], [ "ƒ°char", "ge" ], [ "ƒ°prov", "ide" ], [ "ƒ°delive", "ry" ], [ "ll", "m" ], [ "an", "k" ], [ "am", "ing" ], [ "ƒ°sa", "uce" ], [ "ƒ°de", "velop" ], [ "ph", "ones" ], [ "ƒ°ign", "oring" ], [ "ƒ°prog", "ress" ], [ "b", "ed" ], [ "ke", "s" ], [ "ƒ°show", "ed" ], [ "ƒ°8", "0" ], [ "ƒ°comm", "on" ], [ "ict", "ion" ], [ "ƒ°cu", "nt" ], [ "ƒ°phys", "ically" ], [ "ƒ°memor", "y" ], [ "ƒ°attem", "pt" ], [ "ƒ°hos", "pital" ], [ "ƒ°t", "id" ], [ "ƒ°p", "in" ], [ "as", "m" ], [ "ƒ°j", "s" ], [ "ƒ°2", "1" ], [ "ƒ°cat", "ch" ], [ "ƒ°light", "s" ], [ "ƒ°build", "ing" ], [ "ƒ°jud", "ge" ], [ "ƒ°prescri", "ption" ], [ "ing", "ly" ], [ "ƒ°sh", "ou" ], [ "ƒ°v", "ar" ], [ "ƒ°bl", "ame" ], [ "ƒ°cl", "ick" ], [ "ƒ°ma", "is" ], [ "ƒ°ste", "al" ], [ "ƒ°top", "ic" ], [ "ƒ°reply", "ing" ], [ "ƒ°follow", "ing" ], [ "ƒ°chang", "ing" ], [ "ƒ°repl", "ied" ], [ "ƒ°amb", "ulance" ], [ "fet", "y" ], [ "u", "ral" ], [ "ƒ°u", "hhh" ], [ "ƒ°sound", "ed" ], [ "ƒ°ad", "ult" ], [ "ƒ°bo", "at" ], [ "ƒ°new", "s" ], [ "ƒ°me", "lt" ], [ "ƒ°ca", "p" ], [ "ab", "les" ], [ "ƒ°ex", "tre" ], [ "ƒ°hop", "ing" ], [ "ƒ°down", "stairs" ], [ "ƒ°underst", "ood" ], [ "ƒ°aaa", "gh" ], [ "ƒ°enjoy", "ing" ], [ "ƒ°ly", "rics" ], [ "ƒ°tues", "day" ], [ "ƒ°agg", "ress" ], [ "5", "9" ], [ "h", "op" ], [ "ƒ°n", "hs" ], [ "ƒ°me", "th" ], [ "ƒ°j", "fc" ], [ "ƒ°re", "search" ], [ "ƒ°univers", "ity" ], [ "ss", "s" ], [ "ƒ°en", "v" ], [ "ƒ°med", "ical" ], [ "ƒ°window", "s" ], [ "under", "st" ], [ "ƒ°sand", "w" ], [ "ƒ°wednes", "day" ], [ "u", "it" ], [ "ƒ°c", "ult" ], [ "ƒ°l", "ou" ], [ "ƒ°be", "ar" ], [ "ƒ°do", "se" ], [ "ge", "nt" ], [ "ƒ°vo", "id" ], [ "ƒ°pr", "ior" ], [ "act", "iv" ], [ "ƒ°sy", "mpt" ], [ "ƒ°aud", "io" ], [ "l", "ol" ], [ "ƒ°m", "r" ], [ "ƒ°d", "om" ], [ "an", "o" ], [ "ƒ°k", "icked" ], [ "ƒ°act", "ing" ], [ "ƒ°voc", "als" ], [ "om", "g" ], [ "ƒ°we", "t" ], [ "ra", "ted" ], [ "ƒ°lma", "oo" ], [ "ƒ°pre", "sent" ], [ "ƒ°ser", "vice" ], [ "ƒ°mar", "ried" ], [ "ƒ°chall", "eng" ], [ "c", "ore" ], [ "g", "b" ], [ "ic", "y" ], [ "st", "r" ], [ "ation", "al" ], [ "ib", "ility" ], [ "ƒ°cl", "ar" ], [ "ƒ°cl", "ub" ], [ "ƒ°boo", "b" ], [ "ƒ°reason", "able" ], [ "√™ƒ∫", "√™ƒ∑√£ƒ£¬£" ], [ "u", "ro" ], [ "re", "nc" ], [ "ƒ°n", "at" ], [ "ƒ°p", "unch" ], [ "ƒ°know", "ledge" ], [ "ƒ°2", "8" ], [ "ƒ°ser", "vices" ], [ "ƒ°pos", "ted" ], [ "cha", "ris" ], [ "ƒ°effect", "s" ], [ "ƒ°i", "ce" ], [ "ƒ°to", "ys" ], [ "ie", "ld" ], [ "ƒ°k", "ing" ], [ "ƒ°sor", "ta" ], [ "ƒ°pre", "v" ], [ "ƒ°sim", "p" ], [ "ƒ°mod", "ule" ], [ "ƒ°ref", "er" ], [ "ƒ°ev", "il" ], [ "ƒ°m", "outh" ], [ "ic", "ks" ], [ "ƒ°at", "tra" ], [ "ot", "s" ], [ "ƒ°en", "cou" ], [ "ƒ°fe", "male" ], [ "os", "is" ], [ "sh", "ips" ], [ "el", "s" ], [ "ƒ°vis", "ual" ], [ "ƒ°argu", "ment" ], [ "ƒ°r", "out" ], [ "ƒ°sa", "fety" ], [ "il", "ities" ], [ "ƒ°per", "mission" ], [ "ƒ°rel", "ated" ], [ "bab", "y" ], [ "j", "pg" ], [ "ƒ°s", "an" ], [ "ƒ°u", "hh" ], [ "ƒ°tr", "ou" ], [ "ƒ°ve", "get" ], [ "ƒ°show", "s" ], [ "ƒ°ten", "ant" ], [ "ƒ°bor", "n" ], [ "ƒ°hang", "ing" ], [ "ƒ°mild", "ly" ], [ "i", "ry" ], [ "ƒ°b", "ase" ], [ "is", "ting" ], [ "ƒ°p", "ink" ], [ "ƒ°h", "ilar" ], [ "ƒ°ca", "using" ], [ "ƒ°un", "fair" ], [ "ƒ°eas", "ily" ], [ "0", "7" ], [ "on", "y" ], [ "ƒ°we", "dding" ], [ "ƒ°bus", "iness" ], [ ":", ":" ], [ "ƒ°in", "tera" ], [ "id", "s" ], [ "ra", "id" ], [ "ƒ°an", "g" ], [ "ƒ°ne", "igh" ], [ "il", "s" ], [ "gg", "y" ], [ "ƒ°un", "like" ], [ "ƒ°custom", "er" ], [ "ƒ°rev", "ision" ], [ "c", "ord" ], [ "d", "r" ], [ "i", "ans" ], [ "ll", "o" ], [ "ƒ°no", "tes" ], [ "ƒ°play", "list" ], [ "ƒ°comp", "lic" ], [ "ƒ°leg", "s" ], [ "ƒ°god", "dam" ], [ "ƒ°explain", "ing" ], [ "ƒ°shou", "ted" ], [ "ƒ°id", "c" ], [ "ƒ°vo", "lu" ], [ "ƒ°ter", "ms" ], [ "ƒ°cook", "ies" ], [ "ƒ°exam", "ple" ], [ "ƒ°detail", "s" ], [ "ƒ°mur", "der" ], [ "a", "f" ], [ "ƒ°i", "l" ], [ "te", "st" ], [ "ƒ°in", "put" ], [ "ƒ°le", "cture" ], [ "ff", "f" ], [ "ƒ°bo", "ss" ], [ "ƒ°part", "s" ], [ "ƒ°resp", "ond" ], [ "ƒ°hold", "ing" ], [ "ƒ°disg", "usting" ], [ "6", "7" ], [ "ƒ°s", "i" ], [ "ve", "ry" ], [ "ƒ°g", "inger" ], [ "ƒ°j", "us" ], [ "ƒ°st", "ation" ], [ "ƒ°fee", "t" ], [ "ƒ°real", "ity" ], [ "ƒ°react", "ion" ], [ "l", "it" ], [ "ƒ°ha", "t" ], [ "ƒ°me", "al" ], [ "et", "te" ], [ "me", "n" ], [ "est", "yle" ], [ "ƒ°mar", "ks" ], [ "het", "ic" ], [ "ƒ°emp", "ty" ], [ "ƒ°terr", "ible" ], [ "ƒ°hist", "ory" ], [ "ƒ°neigh", "b" ], [ "ƒ°", "@" ], [ "is", "ions" ], [ "ƒ°or", "dered" ], [ "th", "is" ], [ "ƒ°po", "tent" ], [ "ƒ°rep", "ea" ], [ "bo", "om" ], [ "ƒ°explain", "ed" ], [ "ƒ°indepe", "ndent" ], [ "ƒ°√™ƒ∑√£ƒ£¬£", "√™ƒ∫" ], [ "0", "5" ], [ "ƒ°n", "om" ], [ "ƒ°h", "ig" ], [ "ƒ°ph", "r" ], [ "ƒ°pr", "in" ], [ "ƒ°ro", "le" ], [ "haha", "h" ], [ "ƒ°c", "od" ], [ "ve", "red" ], [ "ƒ°n", "ick" ], [ "ƒ°wh", "is" ], [ "ƒ°mo", "i" ], [ "ib", "er" ], [ "ƒ°start", "s" ], [ "ƒ°mis", "underst" ], [ "ƒ°record", "ing" ], [ "ƒ°mag", "ic" ], [ "ƒ°uns", "ure" ], [ "as", "es" ], [ "ƒ°r", "ush" ], [ "ƒ°se", "par" ], [ "ive", "s" ], [ "um", "my" ], [ "ƒ°om", "l" ], [ "ƒ°bu", "ying" ], [ "ƒ°add", "ing" ], [ "ƒ°imag", "in" ], [ "sor", "ry" ], [ "ƒ°behavi", "our" ], [ "ƒ°d", "ig" ], [ "ƒ°d", "ating" ], [ "ƒ°h", "oo" ], [ "ƒ°stu", "dy" ], [ "ƒ°bl", "ind" ], [ "iz", "ed" ], [ "o", "om" ], [ "s", "ing" ], [ "ƒ°s", "ea" ], [ "ƒ°every", "ones" ], [ "ƒ°har", "row" ], [ "uss", "y" ], [ "ƒ°choc", "olate" ], [ "8", "8" ], [ "ƒ°e", "r" ], [ "ƒ°it", "ll" ], [ "ill", "ed" ], [ "ƒ°st", "ore" ], [ "ƒ°so", "c" ], [ "ƒ°an", "ger" ], [ "ƒ°mu", "a" ], [ "ƒ°na", "p" ], [ "cc", "o" ], [ "ƒ°shop", "ping" ], [ "ƒ°chair", "s" ], [ "ƒ°certain", "ly" ], [ "ƒ°invol", "ved" ], [ "ƒ°deg", "ree" ], [ "c", "an" ], [ "l", "ines" ], [ "ƒ°a", "ve" ], [ "ƒ°b", "es" ], [ "ƒ°to", "oo" ], [ "ƒ°are", "a" ], [ "ne", "ur" ], [ "ƒ°pos", "itive" ], [ "ƒ°", "." ], [ "re", "st" ], [ "ƒ°c", "ream" ], [ "ƒ°for", "ced" ], [ "ir", "on" ], [ "ca", "pe" ], [ "ƒ°ra", "dio" ], [ "ƒ°ad", "vert" ], [ "ƒ°sk", "ull" ], [ "ƒ°pot", "ato" ], [ "ƒ°sce", "ne" ], [ "ƒ°hilar", "ious" ], [ "4", "4" ], [ "ƒ°re", "ee" ], [ "ƒ°k", "ink" ], [ "ƒ°ke", "ys" ], [ "ƒ°pre", "good night" ], [ "ƒ°na", "hhh" ], [ "ƒ°af", "raid" ], [ "ƒ°sc", "am" ], [ "ƒ°ho", "le" ], [ "ƒ°sen", "tence" ], [ "ƒ°mid", "night" ], [ "ƒ°distra", "cted" ], [ "b", "um" ], [ "ƒ°u", "g" ], [ "ƒ°j", "ul" ], [ "ƒ°di", "re" ], [ "ƒ°home", "less" ], [ "ƒ°stat", "us" ], [ "ƒ°drag", "on" ], [ "ƒ°lege", "nd" ], [ "ƒ°extre", "me" ], [ "m", "od" ], [ "ƒ°t", "un" ], [ "an", "a" ], [ "out", "put" ], [ "ƒ°fa", "iled" ], [ "ƒ°colour", "s" ], [ "ƒ°occ", "as" ], [ "ƒ°artic", "le" ], [ "d", "k" ], [ "ƒ°l", "ab" ], [ "ic", "ted" ], [ "ƒ°k", "s" ], [ "ant", "ic" ], [ "ƒ°pr", "oof" ], [ "ƒ°bring", "ing" ], [ "ƒ°spam", "ming" ], [ "ƒ°accept", "able" ], [ "b", "and" ], [ "ƒ°he", "at" ], [ "ƒ°me", "ga" ], [ "ro", "se" ], [ "ƒ°back", "ground" ], [ "ƒ°che", "cking" ], [ "ƒ°fl", "ow" ], [ "ƒ°bas", "ic" ], [ "ƒ°point", "less" ], [ "ƒ°strugg", "le" ], [ "ƒ°stre", "et" ], [ "ƒ°bull", "shit" ], [ "s", "r" ], [ "ƒ°m", "al" ], [ "ƒ°to", "n" ], [ "ƒ°pro", "p" ], [ "ƒ°person", "ally" ], [ "ƒ°t", "es" ], [ "ƒ°d", "y" ], [ "ƒ°e", "aten" ], [ "ƒ°st", "aff" ], [ "ƒ°up", "stairs" ], [ "ƒ°sa", "v" ], [ "ƒ°de", "vice" ], [ "ƒ°happ", "iness" ], [ "ƒ°co", "ff" ], [ "ƒ°dec", "isions" ], [ "ƒ°level", "s" ], [ "3", "2" ], [ "8", "7" ] ] } }# charis cat 2025 # -  ï„Å£ ò‚Äø ò î‚äÉ -*- babyllm -*- ‚äÇ ï ò‚Äø ò‡´Æ î - # output layer for logit prediction # brain/layers/logits.py import torch import torch.nn as nn from config import * """final layer, maps neuron activations to logits for each token in the vocab""" class logits(nn.module): def __init__(self, _counsellor, _device): super().__init__() self.device = _device self.counsellor = _counsellor self.lastsavedweights = 0 # for stats self.l_weights = nn.parameter(torch.randn(numneurons, vocabsize, device = self.device)) # this is set to move the neuron activations (1000) onto vocab size (2000) self.l_bias = nn.parameter(torch.zeros(vocabsize, device = self.device)) self.activationnorm = nn.layernorm(numneurons, device = self.device) self.rawactivationsscale = nn.parameter(torch.tensor(0.5)) self.normedactivationsscale = nn.parameter(torch.tensor(0.5)) self.logitnorm = nn.layernorm(vocabsize, device = self.device) self.outputscale = nn.parameter(torch.tensor(0.5)) self.normoutputscale = nn.parameter(torch.tensor(0.5)) self.stats = {} self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] @whocalled def forward(self, _meanactivationstensor): with self.counsellor.infodump("forward") as  ï„Å£ ò‚Äø ò î„Å£: # <- = from # inn? -> l1 -> l2 -> l3 -> l4 -> l5 -> l6 -> * """imports the activations from interneuronnetwork, assuming that is is a tensor"""  ï„Å£ ò‚Äø ò î„Å£("l1: activationstensor") # <- inn? no? seems to come from babyllm? maybe through babyllm? self.activationstensor = _meanactivationstensor # _1 #self.testat = _meanactivationstensor  ï„Å£ ò‚Äø ò î„Å£("l2: normedactivationstensor") # <- l1 self.normedactivationstensor = self.activationnorm(self.activationstensor) # _2  ï„Å£ ò‚Äø ò î„Å£("l3: scaledactivations") # <- l1 + l2 self.scaledactivations = (self.activationstensor * self.rawactivationsscale) + (self.normedactivationstensor * self.normedactivationsscale) # _3 if debugprints: print(f"debug logits: activations shape before @ weights: {self.scaledactivations.shape}") if debugprints: print(f"debug logits: weights shape: {self.l_weights.shape}")  ï„Å£ ò‚Äø ò î„Å£("l4: logitoutput") # <- l3 (with weights and bias) logitoutputnormalized = (self.scaledactivations @ self.l_weights) / (numneurons ** 0.5) + self.l_bias logitoutputoriginal = self.scaledactivations @ self.l_weights + self.l_bias self.logitoutput = (logitoutputoriginal + logitoutputnormalized)/2 # _4  ï„Å£ ò‚Äø ò î„Å£("l5: logitnormed") # <- l4 self.logitnormed = self.logitnorm(self.logitoutput) # _5  ï„Å£ ò‚Äø ò î„Å£("l6: finallogit") # <- l4 + l5 self.finallogit = (self.logitoutput * self.outputscale) + (self.logitnormed * self.normoutputscale) # _6 if debugprints: print(f"debug logits: logitoutput shape after @ weights: {self.logitoutput.shape}")  ï„Å£ ò‚Äø ò î„Å£("append rolling self.stats") self.tensorhist.append(self.activationstensor.norm().item()) self.normedhist.append(self.normedactivationstensor.norm().item()) self.activhist.append(self.scaledactivations.norm().item()) self.logithist.append(self.logitoutput.norm().item()) self.logitnormhist.append(self.logitnormed.norm().item()) self.finallogithist.append(self.finallogit.norm().item()) if len(self.tensorhist) >= windowmax:  ï„Å£ ò‚Äø ò î„Å£("clear rolling self.stats at end of window") self.stats = { "6l_0_activationstensor_norm": sum(self.tensorhist) / len(self.tensorhist), "6l_1_normedactivationstensor_norm": sum(self.normedhist) / len(self.normedhist), "6l_2_scaledactivations_norm": sum(self.activhist) / len(self.activhist), "6l_3_logitoutput_norm": sum(self.logithist) / len(self.logithist), "6l_4_logitnormed_norm": sum(self.logitnormhist) / len(self.logitnormhist), "6l_x_finallogit_norm": sum(self.finallogithist) / len(self.finallogithist), } self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] # return logits (not softmax) for better gradient computation in cross-entropy loss return self.finallogit # l6 -> def getlogitstats(self): with self.counsellor.infodump("getlogitstats") as  ï„Å£ ò‚Äø ò î„Å£: with torch.no_grad():  ï„Å£ ò‚Äø ò î„Å£("weightnormstats") weightnorms = torch.norm(self.l_weights, dim = 0) self.stats["logitweightnormmean"] = weightnorms.mean() self.stats["logitweightnormstd"] = weightnorms.std() self.stats["logitweightnormmax"] = weightnorms.max() # scales (dont need on per token history as only updated in backward) self.stats["6l_0_activationstensor_scale"] = self.rawactivationsscale.norm().item() self.stats["6l_1_normedactivationstensor_scale"] = self.normedactivationsscale.norm().item() self.stats["6l_3_logitoutput_scale"] = self.outputscale.norm().item() self.stats["6l_4_logitnormed_scale"] = self.normoutputscale.norm().item()  ï„Å£ ò‚Äø ò î„Å£("sparsitystat") sparsity = (self.l_weights.abs() < 1e-5).float().mean() self.stats["logitweightsparsity"] = sparsity  ï„Å£ ò‚Äø ò î„Å£("weightdriftstat") drift = torch.norm(self.l_weights - self.lastsavedweights) self.stats["logitweightdrift"] = drift self.lastsavedweights = self.l_weights.clone().detach()  ï„Å£ ò‚Äø ò î„Å£("biasstats") self.stats["logitbiasmean"] = self.l_bias.mean() self.stats["logitbiasstd"] = self.l_bias.std() self.stats["logitbiasmax"] = self.l_bias.max() if hasattr(self, 'latestactivations'):  ï„Å£ ò‚Äø ò î„Å£("activationstats") act = self.latestactivations self.stats["activationstd"] = act.std() self.stats["activationmean"] = act.mean() self.stats["activationmax"] = act.max() self.stats["activationmin"] = act.min() self.stats["activationsparsity"] = (act.abs() < 1e-6).float().mean() return self.stats if __name__ == "__main__": testlayeractivations = torch.randn(numneurons) logits = logits(numneurons = numneurons, vocabsize = vocabsize) logitoutput = logits.forward(testlayeractivations) print("- logits testing start -") print(f"output layer created with {logits.vocabsize} vocabulary tokens.") print(f"weight matrix shape: {logits.weights.shape}") print(f"bias vector shape: {logits.bias.shape}") print(f"logits (first 100):") print(logitoutput[:10]) print(f"logits shape: {logitoutput.shape}") print("- logits testing complete -")