# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # vocab: training generation and tokenization # brain/layers/vocab.py from collections import counter from config import * from transformers import autotokenizer, pretrainedtokenizerfast from tokenizers import tokenizer, models, trainers, pre_tokenizers, bytelevelbpetokenizer from tokenizers.processors import bytelevel import os, re, json, random, torch from school.notebook.tools.genboi import * """ handles vocab creation, loading, and tokenization. this class: - trains a tokenizer (byte-pair encoding) if no pre-trained tokenizer is found. - loads a pretrained tokenizer if its there. - builds vocab lists and mappings (token to index, index to token). - tokenizes text using the pretrained/loaded tokenizer. - loads training data. - generates training data pairs (input sequence, target token). - saves and loads vocab data to/from files. """ class librarian: def __init__(self, _counsellor, _vocabsize=vocabsize, _vocabpath=none, _basetokenizerpath=none, _forceretrain=false): self.v_counsellor = _counsellor self.vocabsize = _vocabsize + 1 # for <unk> self.unktoken = "<unk>" self.vocabcache = vocabcachepath self.vocabfilename = f"vocab{_vocabsize}_{mintokenfreq}" self.tokenizerfilename = f"tokenizer_{_vocabsize}.json" self.tokenizerpath = _vocabpath or os.path.join(self.vocabcache, self.tokenizerfilename) self.tokenizerlockfile = os.path.join(self.vocabcache, f"{self.tokenizerfilename}.lock") self.vocablistfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_list.json") self.tokentoindexfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_index.json") self.indextotokenfile = os.path.join(self.vocabcache, f"{self.vocabfilename}_to_token.json") self.vocablist = [] self.tokentoindex = {} self.indextotoken = {} self.basetokenizerpath = _basetokenizerpath os.makedirs(self.vocabcache, exist_ok=true) with self.v_counsellor.infodump("__init__") as ʕっʘ‿ʘʔっ: shouldtrain = _forceretrain or not os.path.exists(self.tokenizerpath) or not os.path.exists(self.tokenizerlockfile) if shouldtrain: ʕっʘ‿ʘʔっ("training new tokenizer") print("training new tokenizer...") tokenizermodel = tokenizer(models.bpe(unk_token=self.unktoken)) tokenizermodel.pre_tokenizer = pre_tokenizers.bytelevel() trainer = trainers.bpetrainer( vocab_size=self.vocabsize, min_frequency=mintokenfreq, special_tokens=[self.unktoken] ) with open(trainingfilepath, "r", encoding="utf-8") as f: training_data = [f.read().lower()] tokenizermodel.train_from_iterator(training_data, trainer) tokenizermodel.save(self.tokenizerpath) with open(self.tokenizerlockfile, "w") as f: f.write("locked") # avoid retraining by accident lol self.tokenizer = tokenizermodel else: ʕっʘ‿ʘʔっ("loading existing tokenizer") print("loading existing tokenizer...") self.tokenizer = tokenizer.from_file(self.tokenizerpath) self.buildvocabmap() if self.loadvocab(): ʕっʘ‿ʘʔっ("loaded vocab from files...") self.trainingdatapairs = self.loadtrainingdata(trainingfilepath_arr) self.tokens = self.tokenizetext(self.trainingdatapairs) else: ʕっʘ‿ʘʔっ("building vocab from tokenizer...") self.buildvocabmap() self.savevocab() print(f"saved vocab data to {self.vocabcache}!") def tokenizetext(self, _text): with self.v_counsellor.infodump("tokenizetext") as ʕっʘ‿ʘʔっ: encoding = self.tokenizer.encode(_text) if debugprints: print(f"tokenizing: {_text}") print(f"token ids: {encoding.ids}") return [self.indextotoken.get(idx, self.unktoken) for idx in encoding.ids] # convert indexs back to strings def buildvocabmap(self): with self.v_counsellor.infodump("buildvocabmap") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("getting vocab dictionary from tokenizer...") invvocab = self.tokenizer.get_vocab() ʕっʘ‿ʘʔっ("ordering by index...") sortedtokens = sorted(invvocab.items(), key=lambda item: item[1]) # sort by index self.vocablist = [token for token, idx in sortedtokens] ʕっʘ‿ʘʔっ("mapping vocab dicts...") self.tokentoindex = {token: idx for token, idx in sortedtokens} self.indextotoken = {idx: token for token, idx in sortedtokens} ʕっʘ‿ʘʔっ("ensuring <unk> is in the vocab...") if self.unktoken not in self.tokentoindex: self.vocablist.append(self.unktoken) unk_index = len(self.vocablist) - 1 self.tokentoindex[self.unktoken] = unk_index self.indextotoken[unk_index] = self.unktoken print(f"final vocab size: {len(self.vocablist)}") print(f"first 20 tokens: {self.vocablist[:20]}") def huggingtokenizer(self, _text): return self.tokenizer.tokenize(_text) def loadtrainingdata(self, _filepaths, _chunksize=v_chunksizeloaddata): with self.v_counsellor.infodump("loadtrainingdata") as ʕっʘ‿ʘʔっ: result = "" for path in _filepaths: with open(path, "r", encoding="utf-8") as f: while true: chunk = f.read(_chunksize) if not chunk: break result += chunk result = re.sub(r'\s+', ' ', result) print(f"loaded {len(result)} characters of training data!") return result def gentrainingdata(self, _windowmax=windowmax, _startindex=trainingstartindex, _trainingdatapairnumber=trainingdatapairnumber): with self.v_counsellor.infodump("gentrainingdata") as ʕっʘ‿ʘʔっ: trainingdatapairs = [] count = 0 tokens = self.tokens ʕっʘ‿ʘʔっ("check if windowmax is tensor?") if isinstance(_windowmax, torch.tensor): _windowmax = _windowmax.item() ʕっʘ‿ʘʔっ("allows for random start") if _startindex == 'random': _startindex = random.randint(0, len(tokens) - _windowmax - 1) end = len(tokens) - _windowmax ʕっʘ‿ʘʔっ("generate training pairs") for i in range(_startindex, end): inputseq = tokens[i:i+_windowmax] target = tokens[i+_windowmax:i+_windowmax+_windowmax] if len(target) < _windowmax: continue if all(t in self.vocablist for t in inputseq + target): trainingdatapairs.append((inputseq, target)) count += 1 if count >= _trainingdatapairnumber: break if count % 1000 == 0: print(f"{makedatboi()} {babyname}: generated {count}x trainingdatapairs!") else: print(f"skipping <unk> - inputseq: {inputseq}, target: {target}") return trainingdatapairs def savevocab(self): with self.v_counsellor.infodump("savevocab") as ʕっʘ‿ʘʔっ: os.makedirs(self.vocabcache, exist_ok = true) # ensure directory exists with open(self.vocablistfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save vocablist") json.dump(self.vocablist, f, indent = 4) with open(self.tokentoindexfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save tokentoindex") json.dump(self.tokentoindex, f, indent = 4) with open(self.indextotokenfile, "w", encoding="utf-8") as f: ʕっʘ‿ʘʔっ("save indextotoken") json.dump(self.indextotoken, f, indent = 4) def loadvocab(self): with self.v_counsellor.infodump("loadvocab") as ʕっʘ‿ʘʔっ: try: with open(self.vocablistfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load vocablist") self.vocablist = json.load(f) with open(self.tokentoindexfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load tokentoindex") self.tokentoindex = json.load(f) with open(self.indextotokenfile, 'r', encoding='utf-8') as f: ʕっʘ‿ʘʔっ("load indextotoken") self.indextotoken = {int(k): v for k, v in json.load(f).items()} # ensures that keys are integers! print("vocab files loaded successfully!") return bool(self.vocablist and self.tokentoindex and self.indextotoken) except (filenotfounderror, json.jsondecodeerror): print("vocab files not found or invalid... rebuilding vocab...") return false if __name__ == "__main__": counsellor = type("dummy", (), {"infodump": lambda self, label: open(os.devnull, 'w')})() librarian = librarian(_counsellor = counsellor, _vocabsize = 4200, _basetokenizerpath = "brain/vocabcache/tokenizer_2000.json") print(f"- 2000-{vocabsize} -: {librarian.vocablist[2000:vocabsize]}") print(f"- 1701-2000 -: {librarian.vocablist[1701:2000]}") print(f"- 1001-1700 -: {librarian.vocablist[301:1700]}") print(f"- 301-1000 -: {librarian.vocablist[301:1000]}") print(f"- 101-300 -: {librarian.vocablist[101:300]}") print(f"- top 100 -: {librarian.vocablist[:100]}") print(f"vocab size: {len(librarian.vocablist)}") print(f"top 20 tokens: {librarian.vocablist[:20]}") #print(vocab.huggingtokenizer("charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv")) sample_text = "charis and elodie are very cool, elodies pretty and charis is very suave, they're sexy bitches, we love these girls and we want to see them living their best lives bruv" tokenizedoutput = librarian.tokenizetext(sample_text) print(f"tokenized: {tokenizedoutput}")# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # babyllm school counsellor // school/staffroom/counsellor.py """designed for detailed small scale logging throughout the project, with timing implemented for troubleshooting errors""" import time import re from config import * from contextlib import contextmanager # usage: # remember to initialise the class instance # self.counsellor = counsellor("class_name", debug = debugprints, durations = durationlogging) # in the top of your function, encasing all of your function lines, put; # with self.counsellor.infodump("function_name") as ʕっʘ‿ʘʔっ: # this will duration track the whole function, if enabled, and also make a note in debugprints when it starts and ends # anywhere you want to start a new tracker within a function, place; # ʕっʘ‿ʘʔっ("tracker_name") # these work similarly to the function trackers, but are internal points within the function. # now your code can get some fucking therapy, for once! class counsellor: def __init__(self, _classname="?", _debug = debugprints, _durations = durationlogging): self.classname = _classname self.debugprints = _debug self.durationlogging = _durations self.duration = {} self.duration_a = {} def log(self, key, value): maxlogs = 5000 if not self.durationlogging: return self.duration[key] = self.duration.get(key, 0) + value if len(self.duration) > maxlogs: self.duration.clear() print(f"cleared duration_b as it was higher than {maxlogs}") self.duration_a[key] = self.duration_a.get(key, 0) + value if len(self.duration_a) > maxlogs: self.duration_a.clear() print(f"cleared duration_a as it was higher than {maxlogs}") @contextmanager def infodump(self, _functionname, _extra = none, _key = none): fulltitle = f"{self.classname}_{_functionname}" startstamp = time.time() if self.durationlogging else none if self.debugprints: line = f"ʕっʘ‿ʘʔっ starting {fulltitle}... →" if _extra: line += f" ({_extra})" print(line) class tangent: def __init__(self, _parent): self.parent = _parent self.lastinfodumptime = startstamp self.lastinfodumpname = none self.parentfunction = none self.path = [] if self.parentfunction != _functionname: self.path = [] # reset when function changes self.parentfunction = _functionname setattr(self, "ʕっʘ‿ʘʔっ", self.infodump) #somehow legal as a variable name how have i done this please protect me lol setattr(self, "(っ◕‿◕)っ", self.infodump) setattr(self, "♥‿♥", self.infodump) setattr(self, "(｡♥‿♥｡)", self.infodump) setattr(self, "infodump", self.infodump) def __call__(self, _innername): return self.infodump(_innername) def infodump(self, _innername): now = time.time() # clean function context? start new base path. if self.parentfunction != _functionname: self.parentfunction = _functionname self.path = [] # reset because moved to another function if isinstance(_innername, str): if _innername.startswith("♥") and _innername.endswith("♥"): self.path = [_innername.strip("♥")] # appendable base elif _innername.startswith("♥"): self.path.append(_innername[1:]) # append elif _innername.endswith("♥"): self.path = [_innername[:-1]] # new base path elif "/" in _innername or "♥" in _innername: self.path = [p.strip() for p in re.split(r"[→/♥]", _innername)] else: self.path = [_innername] else: self.path = [str(_innername)] # log previous section if self.lastinfodumpname: duration = now - self.lastinfodumptime tag = f"{_functionname}♥{'♥'.join(self.path[:-1] + [self.lastinfodumpname])}" self.parent.log(tag, duration) if self.parent.debugprints: print(f"♥ finished {self.parent.classname}♥{tag} in {duration:.4f}s ♥") # start new self.lastinfodumpname = self.path[-1] self.lastinfodumptime = now fulltag = f"{_functionname}♥{'♥'.join(self.path)}" if self.parent.debugprints: print(f"→ starting {self.parent.classname}♥{fulltag} →") tangent = tangent(self) try: yield tangent finally: if tangent.lastinfodumpname: finalduration = time.time() - tangent.lastinfodumptime finaltag = f"{_functionname}♥{'♥'.join(tangent.path)}" self.log(finaltag, finalduration) if self.debugprints: print(f"♥ finished {self.classname}♥{finaltag} in {finalduration:.4f}s ♥") if startstamp: totalduration = time.time() - startstamp self.log(_key or _functionname, totalduration) if self.debugprints: print(f"(っ◕‿◕)っ finished {fulltitle} in {totalduration:.4f}s (｡♥‿♥｡)") elif self.debugprints: print(f"(っ◕‿◕)っ finished {fulltitle} (｡♥‿♥｡)")# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # embedding layer // brain/layers/embed.py import torch import torch.nn as nn from config import * """creates an embedding layer for each word in the vocabulary""" class embed(nn.module): def __init__(self, _counsellor, _device = modeldevice): super().__init__() self.counsellor = _counsellor self.device = _device self.stats = {} """creates the embedding weights matrix with random numbers initially""" self.e_weights = nn.parameter(torch.randn(vocabsize, embeddimension, device = self.device)) # [2000,] self.embednorm = nn.layernorm(embeddimension, device = self.device) self.weightsscale = nn.parameter(torch.tensor(0.5)) self.normscale = nn.parameter(torch.tensor(0.5)) self.lastsavedembeds = self.e_weights.detach().clone() # this is initialised once, for stats, does not break graph confirmed!! """looks up and returns the embedding vector for a specifc token index""" @whocalled def forward(self, _tokenindex): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("e0_embedvector") # <- vocab??? base token indexes seem to come in here so... from tutor?? self.embedvector = self.e_weights[_tokenindex] ʕっʘ‿ʘʔっ("e1_embednormed") # <- e1 self.embednormed = self.embednorm(self.embedvector) ʕっʘ‿ʘʔっ("ex_embedfinal") # <- e2 self.embedfinal = (self.embedvector * self.weightsscale) + (self.embednormed * self.normscale) return self.embedfinal # e3 -> n?? def getembedstats(self): with self.counsellor.infodump("getembedstats") as ʕっʘ‿ʘʔっ: with torch.no_grad(): self.stats = {} embednorms = torch.norm(self.e_weights, dim = 1) self.stats["embednormmean"] = embednorms.mean() self.stats["embednormstd"] = embednorms.std() self.stats["embednormmax"] = embednorms.max() self.stats["1e_0_embedvector_norm"] = self.embedvector.norm().item() self.stats["1e_1_embednormed_norm"] = self.embednormed.norm().item() self.stats["1e_x_embedfinal_norm"] = self.embedfinal.norm().item() self.stats["1e_0_embedvector_scale"] = self.weightsscale.norm().item() self.stats["1e_1_embednormed_scale"] = self.normscale.norm().item() dimmean = self.e_weights.mean(dim = 0) self.stats["embeddimensionmean"] = dimmean dimsparsity = (dimmean.abs() < 1e-4).float().mean() self.stats["embeddimensionsparsity"] = dimsparsity # drift since last save drift = torch.norm(self.e_weights - self.lastsavedembeds) self.stats["embeddingdrift"] = drift self.lastsavedembeds = self.e_weights.detach().clone() return self.stats def cosinesimilarity(self, _idx1, _idx2): e1 = self.e_weights[_idx1] e2 = self.e_weights[_idx2] return torch.nn.functional.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)) if __name__ == "__main__": testtokenindex = 500 # 32 (embeddimension) x 2000 (vocab) = 64,000 in embed layer embed = embed(vocabsize, embeddimension) embedvector = embed.forward(testtokenindex) print(f"- embedding layer testing start -") print(f"embedding layer weights shape: {embed.weights.shape}") # check shape of weight matrix print(f"embedding vector for token index {testtokenindex}:") print(embedvector) print(f"embedding vector shape: {embedvector.shape}") print(f"- embedding layer testing complete -")# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # babyllm // babyllm.py import random, os import torch import torch.nn.functional as f import torch.nn as nn import torch.optim as optim import torch.optim.lr_scheduler import math from collections import counter from brain.layers.embed import embed from brain.layers.interneuronnetwork import interneuron_network from brain.layers.logits import logits from brain.layers.memory import memory #from brain.layers.sensorywobble import wobble from config import * """this class combines all the core components of the babyllm:""" """embed: token embedding layer""" """interneuron_network: layer of parallel neurons for feature extraction""" """logits: output layer to generate logits""" """it also manages training, loss computation, backpropagation, and response generation.""" class babyllm(nn.module): def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _device = modeldevice): super().__init__() self.device = _device self.counsellor = _counsellor self.calligraphist = _calligraphist self.scribe = _scribe self.librarian = _librarian #self.wobble = _wobble # must be on self - only accessed in this class and not nn.params self.totaltokenevaluations = 0 self.latestlossdelta = 0 self.totaltokenevaluations_a = 0 self.recentgeneratedtokens = [] # used for repetition penalty self.lastlossbaby = 0 self.computelosscount = 0 self.repeatedpercent = 0 self.normalisedactivations = 0 self.rollingtokentotals = counter() self.gumbellend = 0 self.stats = {} self.normalisedhistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.inputembedshistory = [] self.finallogitshistory = [] """cerebral layers // brain""" self.embed = embed(_counsellor = self.counsellor, _device = self.device) self.interneuronnetwork = interneuron_network(_model = babyllm, _counsellor = self.counsellor, _calligraphist = self.calligraphist, _device = self.device) self.logits = logits(_counsellor = self.counsellor, _device = self.device) self.memory = memory(_counsellor = self.counsellor, _device = self.device) self.finalnormlayer = nn.layernorm(numneurons, device=self.device) """learnable learning parameters""" self.repetitionpenalty = nn.parameter(torch.tensor(1.0, device = self.device)) self.logtemp = nn.parameter(torch.tensor(math.log(0.8), device = self.device)) self.loglr = nn.parameter(torch.tensor(math.log(1e-4), device = self.device)) self.loggradclip = nn.parameter(torch.tensor(math.log(1.0), device = self.device)) self.scheduledsamplingrate = nn.parameter(torch.tensor(0.2, device = self.device)) self.logmemorylength = nn.parameter(torch.tensor(math.log(memorylengthgoal), device = self.device)) self.logrepetitionwindow = nn.parameter(torch.tensor(math.log(repetitionwindowgoal), device = self.device)) """stuff""" self.gradientclipmaxnorm = torch.exp(self.loggradclip) self.temperature = none """optimizer - this updates all of the layers learnable parameters""" if debugprints: print("registered parameters: ") for name, param in babyllm.named_parameters(self): print(name, param.shape) optimizerclass = getattr(optim, optimizername) self.optimizer = optimizerclass(self.parameters(), lr = learningrate, weight_decay = 0.005, fused = true) if debugprints: for name, param in self.named_parameters(): print(f"{name}: requires_grad={param.requires_grad}") #self.to(self.device) self.statscategories = {"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "scheduledsamplingrate": 0, "tokencount": 0, "memorygateshort": 0, "memorygatelong": 0, "memorygatecurrent": 0, "shortdecay": 0, "longdecay": 0,} @whocalled def forward(self, _inputseq): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # processes input sequence of tokens (str) to generate logits to predict the next token if debugprints: print(f"debug: input to forward: {_inputseq}") self.temperature = torch.exp(self.logtemp) ʕっʘ‿ʘʔっ("b0: inputembeds") # convert indices to embeddings inputembeds = self.embed(_inputseq) # directly taking a tensor now if debugprints: print(f"debug babyllm.forward: inputembeds requires_grad: {inputembeds.requires_grad} [expected: true]") ʕっʘ‿ʘʔっ("b1: interneuronnetworkoutput") # parallel neuron layer input/processing (feature extraction) innoutput = self.interneuronnetwork.forward(inputembeds) if debugprints: print(f"debug babyllm.forward: interneuronnetworkoutput length: {len(innoutput)}") if debugprints: print("combinedactivationstensor.requires_grad:", innoutput.requires_grad) if debugprints: print("combinedactivationstensor.grad_fn:", innoutput.grad_fn) ʕっʘ‿ʘʔっ("b2: memoryoutput") # memory layer processing - now process the combined activations if skipmemory: if debugprints: print("skipping memory layer...") self.latestmemgates = torch.tensor([0.0, 0.0, 1.0], device = self.device) # dummy gates memoryoutput = innoutput.detach() # no grad path, super light else: memoryoutput = self.memory.forward(innoutput) self.latestmemgates = self.memory.latestmemorygates if debugprints: print("combinedactivations.requires_grad:", memoryoutput.requires_grad) ʕっʘ‿ʘʔっ("b3: repetitionpenalty") if not torch.isfinite(self.logrepetitionwindow): print("logrepetitionwindow has gone non-finite. resetting.") self.logrepetitionwindow.data = torch.tensor(math.log(repetitionwindowgoal), device = self.device) penalisedoutput = self.applyrepetitionpenalty(memoryoutput) if debugprints: print("before memory output requires_grad?", self.memory.longtermmemory.requires_grad) if debugprints: print("before cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("before logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("before logmemorylength requires_grad?", self.logmemorylength.requires_grad) if skipfinallogitnorm: ʕっʘ‿ʘʔっ("bx: logits.forward") finallogits = self.logits.forward(penalisedoutput) if false: ʕっʘ‿ʘʔっ("b4: finalnormlayer") self.normedoutput = self.finalnormlayer(penalisedoutput) finallogits = self.logits.forward(self.normedoutput) self.normalisedhistory.append(self.normedoutput.norm().item()) if debugprints: print("after logmemorylength requires_grad?", self.logmemorylength.requires_grad) if debugprints: print("after logrepetitionwindow requires_grad?", self.logrepetitionwindow.requires_grad) if debugprints: print("after cerebellum requires_grad?", self.interneuronnetwork.cerebellum.requires_grad) if debugprints: print("after memory output requires_grad?", self.memory.longtermmemory.requires_grad) if true: ʕっʘ‿ʘʔっ("stats collection!") self.inputembedshistory.append(inputembeds.norm().item()) self.innoutputhistory.append(innoutput.norm().item()) self.memoryoutputhistory.append(memoryoutput.norm().item()) self.penalisedoutputhistory.append(penalisedoutput.norm().item()) self.finallogitshistory.append(finallogits.norm().item()) if len(self.inputembedshistory) >= windowmax: self.forwardstats = { "2b_0_inputembeds_norm": sum(self.inputembedshistory) / len(self.inputembedshistory), "3b_1_innoutput_norm": sum(self.innoutputhistory) / len(self.innoutputhistory), "5b_0_memoryoutput_norm": sum(self.memoryoutputhistory) / len(self.memoryoutputhistory), "5b_1_penalisedoutput_norm": sum(self.penalisedoutputhistory) / len(self.penalisedoutputhistory), #"5b_x_finalnormlayer_norm": sum(self.normalisedhistory) / len(self.normalisedhistory), "7b_x_finallogits_norm": sum(self.finallogitshistory) / len(self.finallogitshistory), } self.stats.update(self.forwardstats) self.inputembedshistory = [] self.innoutputhistory = [] self.memoryoutputhistory = [] self.penalisedoutputhistory = [] self.finallogitshistory = [] self.normalisedhistory = [] """returns a logits tensor of shape (1, vocabsize) showing predicted probabilities for the next token""" return finallogits """computes the cross-entropy loss between the models logits and the target token, essentially checking how good the models prediction was""" def computeloss(self, _logits, _targettokenindex, _latestlossdelta = 0, _perfecttokens = 0, _training = false): with self.counsellor.infodump("computeloss") as ʕっʘ‿ʘʔっ: self.perfecttokens = _perfecttokens if skipcomputeloss: ʕっʘ‿ʘʔっ("skipping loss!") return torch.tensor([0.1], requires_grad = true, device = self.device) # constant scalar tensor ʕっʘ‿ʘʔっ("targettensor") targettensor = torch.tensor([_targettokenindex], dtype = torch.long, device = self.device) if debugprints: print(f"logits shape: {_logits.shape} | target: {_targettokenindex}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure logits are at least 2d ʕっʘ‿ʘʔっ("cross entropy loss") losslogits = torch.clamp(_logits, min=-50, max=50) loss = f.cross_entropy(losslogits, targettensor) if not torch.isfinite(loss): print("nan/inf loss detected - logits:", _logits) return torch.tensor(10.0, device=self.device, requires_grad=true) # or skip/backoff if debugprints: print(f"crossentropy raw loss: {f.cross_entropy(_logits, targettensor)}") self.celossdelta = loss - ((self.lastlossbaby) if self.lastlossbaby is not none else 0) #tempreg = (torch.clamp(self.logtemp, 0.7, 0.9) - 0.8).pow(2) if debugprints: print(f"{self.lastlossbaby:0.1f}", end = ", ") # take delta #entropy = 0.001 * self.interneuronnetwork.entropybonus lrsoftclamp = 0.5 * (self.loglr - math.log(0.0002)).pow(2) loss += lrsoftclamp # use .detach() to avoid .backward() self.lastlossbaby = loss.item() if _training and self.lastsoftsample is not none: target = f.one_hot(targettensor, num_classes = _logits.shape[1]).float() auxloss = f.kl_div(self.lastsoftsample.log(), target, reduction = 'batchmean') finalloss = loss + auxloss * torch.sigmoid(loss - auxloss) # low weight for anti-dominatrix else: finalloss = loss #tempsoftclamp = 0.4 * (self.logtemp - math.log(0.5)).pow(2) # more tokens (better) > perftokens > less tokens (worse) # higher number > 2 > lower number # 0.3x > 2 > 1.3x # worse (explore) > latestlossdelta > better (stay still) # positive number > 0 > negative number # +4 delta (worse) > 0 > -4 delta (better) # [0-25]x0.1 > 0 > [0-1] # 0-2.5 > 0 > 0-1 #if debugprints: print(f"[loss debug] requires_grad: {loss.requires_grad} | value: {loss.detach().cpu().item():.4f}") return finalloss """backpropagation and optimization, computes gradients of the loss and uses the optimizer to update the models weights""" def backward(self, _loss): with self.counsellor.infodump("backward") as ʕっʘ‿ʘʔっ: if debugprints: for name, p in self.named_parameters(): if p.grad is none: print(f"before = {self.calligraphist.s_apply("dim", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"before = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") ʕっʘ‿ʘʔっ("loss.backward") _loss.backward() #print(next(self.parameters()).grad) if debugprints: for name, p in self.named_parameters(): if p.grad is none: print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") with torch.no_grad(): # reset learnable parameters #self.loglr.data.fill_(math.log(0.00035)) # learning rate back to 1e-4 self.scheduledsamplingrate.data.fill_(0.05) # scheduled sampling full (no scheduled sampling yet) #self.temperature.data.fill_(math.exp(self.logtemp)) # temperature normal #self.repetitionpenalty.data.fill_(1.0) # repetition penalty normal #self.logmemorylength.data.fill_(math.log(1)) # memory length default #self.logrepetitionwindow.data.fill_(math.log(16)) # repetition window default #self.interneuronnetwork.logwindowsizes.data.copy_( # torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device)) #) #for module in self.interneuronnetwork.windowmeta: # if isinstance(module, torch.nn.linear): # module.reset_parameters() if true: with torch.no_grad(): self.loglr.clamp_(math.log(0.0001), math.log(0.001)) # clamp it! in memory of the amazing 1.00 self learned loss run of 27-april-2025! - you certainly dropped the delta! you win! learnedlr = torch.exp(self.loglr).item() for g in self.optimizer.param_groups: g['lr'] = learnedlr #self.gradientclipmaxnorm = torch.exp(self.loggradclip).item() #self.repetitionwindow = torch.exp(self.logrepetitionwindow).item() #self.memorylength = torch.exp(self.logmemorylength).item() #self.loglr.data.fill_(self.loglr+0.0001) # increment lr manually (break grid) ʕっʘ‿ʘʔっ("clip_grad_norm") clipvalue = torch.exp(self.loggradclip).item() torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=clipvalue) ʕっʘ‿ʘʔっ("optimizer.step") self.optimizer.step() # update weights self.backwardstats = { "_b_floatmemorylength": torch.exp(self.logmemorylength).item(), "_b_repetitionwindow": torch.exp(self.logrepetitionwindow).item(), "_b_temperature": torch.exp(self.logtemp).item(), } self.stats.update(self.backwardstats) #with torch.no_grad(): # force reset the memory gates if over using long #self.memory.currentgate.data = self.memory.currentgate.data.abs() #self.memory.shortgate.data = self.memory.shortgate.data.abs() """this takes the output logits, does temperature scaling and softmax to create a probability distribution over the vocab, and then selects most likely response token""" """def getresponsefromlogits(self, _logits, _temperature = temperature): with self.counsellor.infodump("getresponsefromlogits") as ʕっʘ‿ʘʔっ: _logits /= _temperature if debugprints: print(f"debug babyllm.getresponsefromlogits: logits shape before softmax: {_logits.shape}") if _logits.dim() == 1: _logits = _logits.unsqueeze(0) probs = torch.softmax(_logits, dim = 1) responsefromlogits = torch.multinomial(probs, 1) return responsefromlogits""" @whocalled def getresponsefromlogits(self, _logits, _training = false): with self.counsellor.infodump("getresponsefromlogits") as ʕっʘ‿ʘʔっ: if not torch.isfinite(_logits).all(): print("logits not finite before response gen:", _logits) _logits = torch.nan_to_num(_logits, nan=0.0, posinf=1e3, neginf=-1e3) ʕっʘ‿ʘʔっ("update logarithmic parameters") #self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! _logits /= self.temperature if torch.isnan(_logits).any(): print("nan in logits after temperature scaling!") print("logtemp:", self.logtemp.item(), "temp:", self.temperature.item()) print("logits stats:", _logits.min().item(), _logits.max().item(), _logits.mean().item()) _logits = torch.nan_to_num(_logits, nan=0.0, posinf=1e3, neginf=-1e3) if _logits.dim() == 1: _logits = _logits.unsqueeze(0) # ensure [1, vocabsize] if _training: logitsforsample = _logits.clone() if not torch.isfinite(logitsforsample).all(): print("non-finite logits detected before gumbel") print("logits:", logitsforsample) logitsforsample = torch.nan_to_num(logitsforsample, nan=0.0, posinf=1e3, neginf=-1e3) try: gumbelprobs = f.gumbel_softmax(logitsforsample, tau=self.temperature, hard=false) assert torch.isfinite(gumbelprobs).all(), "gumbelprobs has nan or inf!" except exception as e: self.gumbellend += 1 print("gumbel softmax failed:", e) print(f"falling back to softmax sampling (total fallbacks: {self.gumbellend})...") gumbelprobs = torch.softmax(logitsforsample, dim=1) self.lastsoftsample = gumbelprobs responsefromlogits = gumbelprobs.argmax(dim = 1, keepdim = true) self.lastsoftsample = gumbelprobs topk = torch.topk(gumbelprobs, 10, dim=1) indices = topk.indices[0].tolist() values = topk.values[0].tolist() #self.lasttopguesses = [] for i, p in zip(indices, values): token = self.librarian.indextotoken.get(i, "<unk>") try: if isinstance(p, float) and math.isfinite(p): #self.lasttopguesses.append((token, round(p, 4))) self.rollingtokentotals[token] += round(p, 4) else: print(f"skipping non-finite top guess: {token} → {p}") except exception as e: print(f"error processing top guess: {token} → {p} | {e}") #print("top guesses + confidences:", [(self.librarian.indextotoken[i.item()], f"{p.item():.3f}") for i, p in zip(indices, values)]) else: probs = torch.softmax(_logits, dim=1) responsefromlogits = torch.multinomial(probs, 1) self.lastsoftsample = none # or keep the probs if you want analysis #if debugprints: #print(f"[rep penalty] {self.repeatedpercent:.2%} repeated | repetition slice: {self.repetitionslice} | penalised: {[self.librarian.indextotoken.get(t, '<unk>') for t in uniquetokens]}") ʕっʘ‿ʘʔっ("create windows using rolling buffer") self.recentgeneratedtokens.append(responsefromlogits.item()) if len(self.recentgeneratedtokens) > int(torch.exp(self.logrepetitionwindow)): self.recentgeneratedtokens.pop(0) return responsefromlogits def applyrepetitionpenalty(self, _logits): if not self.recentgeneratedtokens: return _logits repwindow = torch.exp(self.logrepetitionwindow) penalty = self.repetitionpenalty recenttokens = torch.tensor(self.recentgeneratedtokens, device=self.device) vocabsize = _logits.shape[1] positions = torch.arange(len(recenttokens), device=self.device).float() windowcentre = len(recenttokens) softmask = torch.sigmoid((positions - (windowcentre - repwindow)) * 0.5) onehots = f.one_hot(recenttokens, num_classes=vocabsize).float() weightedfreqs = (onehots.t @ softmask).view(1, -1) return _logits - (weightedfreqs * (0.001 * penalty)) def getnexttoken(self, _inputseq): with self.counsellor.infodump("getnexttoken(forward)") as ʕっʘ‿ʘʔっ: logits, *_ = self.forward(_inputseq) # unpacks the first value of the tuple and ignores the rest nexttoken = self.getresponsefromlogits(logits, _training = true) return nexttoken def savemodel(self, filepath = modelfilepath, _newstartindex = trainingstartindex, _trainingstepcounter = 0): with self.counsellor.infodump("savemodel") as ʕっʘ‿ʘʔっ: tmppath = filepath + ".tmp" torch.save(self.state_dict(), tmppath) print(f"model temp file created at {tmppath}...") os.replace(tmppath, filepath) print(f"model successfully saved to {filepath}!") with open(stepcheckpointfilepath, "w") as f: f.write(str(_trainingstepcounter+_newstartindex)) # this isnt real, fix later, maybe move save and load to wakeup? """loads the model from a file""" def loadmodel(self, filepath = modelfilepath): with self.counsellor.infodump("loadmodel") as ʕっʘ‿ʘʔっ: try: ʕっʘ‿ʘʔっ("update logarithmic parameters") self.repetitionwindow = torch.exp(self.logrepetitionwindow)#.clamp(min=1.0) self.temperature = torch.exp(self.logtemp) # torch.exp keeps gradient path! print(f"loading model from path: {filepath}") self.load_state_dict(torch.load(filepath), strict = savestrict) print(f"model loaded from {filepath}!") self.to(self.device) print(f"device set to {self.device}!") self.resetmemory(context="inference") except filenotfounderror: print("no saved model found") def babyllm_diary_entry(self, interneuronnetwork, step): with self.counsellor.infodump("babyllm_diary_entry") as ʕっʘ‿ʘʔっ: # grab current window weightings weights = interneuronnetwork.cerebellum windows = interneuronnetwork.allwindowsizes # find the current favourite and least favourite fav_idx = weights.argmax() worst_idx = weights.argmin() fav_window = windows[fav_idx] worst_window = windows[worst_idx] moods = ["chaotic", "curious", "crunchy", "a bit overwhelmed", "spicy", "thoughtful", "itchy", "playful"] actions = [ f"i still trust window {fav_window} the most", f"window {fav_window} makes me feel safe", f"window {worst_window} keeps confusing me!", f"i'll start listening to window {fav_window} more!", f"window {worst_window} tastes like static", f"i'm starting to wonder about window {fav_window}... is it my destiny?", f"window {worst_window} is just noise, i swear!", f"today i felt {random.choice(moods)}.", f"window {fav_window} whispered secrets to me." ] diaryline = f"step {step+1}: babyllm diary update: '{random.choice(actions)}'" print(diaryline) def resetmemory(self, context="inference"): with self.counsellor.infodump("resetmemory") as ʕっʘ‿ʘʔっ: """reset memory depending on the context: inference always resets, training resets every n turns""" if context == "inference": ʕっʘ‿ʘʔっ("context = inference") self.memory.resetmemory() print(f"resetting memory for new conversation...") elif context == "training": ʕっʘ‿ʘʔっ("context = training") if hasattr(self, "stepssincememoryreset"): self.stepssincememoryreset += 1 else: self.stepssincememoryreset = 1 if self.stepssincememoryreset >= int(torch.exp(self.logmemorylength).item()): self.memory.resetmemory() if debugprints: print(f"resetting memory after {self.stepssincememoryreset} steps... (learned mem length: {self.logmemorylength})") self.stepssincememoryreset = 0 def setlearningrate(self, _newlearningrate): self.learningrate = max(1e-6, min(_newlearningrate, 0.01)) # clamp it a bit for param_group in self.optimizer.param_groups: param_group["lr"] = self.learningrate def getbabystats(self): return self.stats if __name__ == "__main__": exit(0)# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # scribe module // school/staffroom/he_is_scribe.py import random import time from config import * from school.notebook.tools.genboi import * class scribe: def __init__(self, _counsellor, _calligraphist, _librarian): #self.counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) #self.s_output = s_output() self.counsellor = _counsellor self.calligraphist = _calligraphist self.librarian = _librarian self.scribeemotes = {"default": ["ʕっʘ‿ʘʔっ", "ʕᵔᴥᵔʔっ", "ʕっෆ.ෆʔっ", "ʕ✰.✰ʔっ", "ʕᵔ‿ᵔʔっ♡"], "neutral": ["ʕ -ᴥ-ʔゝ", "ʕᵔᴥᵔʔっ♥",], "annoyed": ["ʕノ-ᴥ-ʔノ ︵", "ʕっ-̀o-́ʔっ", "ʕ-̀o-́ʔっ", "ʕっ-̀o-́ʔっ✰✰⋆⋆", ], "hyper": ["ʕっ꩜‿꩜ʔっ𖡼", "ʕᵔ‿ᵔʔっ"], "worried": ["ʕ◉.◉ʔ", "ʕ꩜.꩜ʔっ❄", ], "mischevious": ["ʕ-̀‿-ʔっ", "ʕっ‿.ෆʔっ♡", "ʕ-̀o-ʔっ", "ʕ-̀‿-ʔっ", "ʕっෆ.‿ʔっ♡", "ʕ-̀‿-́ʔっ", ], "love": ["ʕᵔᴥᵔʔっ♥", "ʕっෆ.ෆʔっ♡", "ʕᵔ‿ᵔʔっ♡", "ʕっ✰.✰ʔっ❀", "ʕっʘ‿ʘʔっ♡", "ʕ❀ෆ.ෆʔっ❀", ], "hugs": ["ʕっෆ.ෆʔっ", "ʕっෆ.ෆʔっ♡", "ʕっʘ‿ʘʔっ", ], "happy": ["ʕっʘ‿ʘʔっ", "૮ʕʘ‿ʘ૮ʔ", "ʕっᵔ‿ᵔʔっ♡", "ʕᵔᴥᵔʔっ𓆟",], "writes": ["ʕ-ᴥ-ʔつ✎", "ʕっ‿.‿ʔっ✎", "ʕ❀ʘ.ʘʔっ✎", "ʕっʘ‿ʘʔっ✎", "ʕ❀‿.‿ʔっ✎", "ʕっෆ.ෆʔっ✎",], "sleepy": ["૮ʕ‿.‿ᶻʔ𝗓 𐰁", "૮ʕ‿.‿૮ʔᶻ 𝗓 𐰁", "ʕっෆ.ෆʔっ♡",], "confused": ["𓆟 ૮ʕʘ‿ʘ૮ʔ", "ʕ⋆ᴥ⋆ʔっ𓆟", "ʕ♡ᴥ♡ʔっ𓆟", "𓆟 ૮ʕʘ‿ʘ૮ʔ", ], "impressed": ["૮ʕ♡‿♡ʔ", "ʕっ✰.✰ʔっ𖡼", "ʕ✰.✰ʔっ❄︎", ]} def scribesay(self, _message, _vibe="default", _scribename="scribe"): """scribe delivers a message with random emote and timestamp.""" emote = random.choice(self.scribeemotes.get(_vibe, self.scribeemotes["default"])) timestamp = time.strftime("%h:%m:%s") print(f"{timestamp}|{emote} [{_scribename.upper()}] - {_message}") with open("scribesays.txt", "a") as f: f.write(f"{timestamp}|{emote} [{_scribename}]: '{_message}\n") def guesstokenstostring(self, _inputtokens): tokenstring = "".join(_inputtokens).replace("ġ", " ") return tokenstring def interviewbaby(self, _model, _prompt, _vibe="writes"): """scribe asks babyllm a question and records the reply.""" _prompt = "how are you feeling today, baby? :)" self.scribesay(f"asking babyllm: '{_prompt}'", _vibe) encoded = self.librarian.tokenizer.encode(_prompt).ids guess = self.librarian.getnexttoken(encoded[-windowmax:]) guessword = self.librarian.indextotoken.get(guess, "<unk>") self.scribesay(f"babyllm replies: '{guessword}'", "impressed") def babysay(self, _input = none, _babyname = babyname): if _input is none: #miniinput = "what will you do out there now?" #miniinput = "i love you, this is good, music is life, i love you, this is good, music is life, i love you, this is good, music is life, hey! how are you?" #miniinput = "what" #miniinput = "" miniinput = "i did it! i am happy! i know it! i did it! i am happy! i feel it! i know it! i did it! i know it! i am happy! i did it! i know it! i feel it! i am happy!" else: miniinput = _input timestamp = time.strftime("%h:%m:%s") minitokenized = self.librarian.tokenizer.encode(miniinput).ids #encoded = self.librarian.tokenizer.encode(_prompt).ids babyresponse = self.librarian.getnexttoken(minitokenized[-windowmax:]) babytokens = self.librarian.indextotoken.get(babyresponse, '<unk>') babysentence = self.guesstokenstostring(babytokens) emote = makedatboi() babysay = (f"{timestamp}|{emote} [{_babyname.lower()}]: {babysentence}") print(babysay) with open("scribesays.txt", "a") as f: f.write(babysay) def maybecommentonguess(self, _inputtokens, _lossvalue, _scribename = scribename, _chance = 0.05): if random.random() > _chance: return if isinstance(_inputtokens, list): _inputtokens = self.guesstokenstostring(_inputtokens) else: _inputtokens = _inputtokens moodboard = { "good": {"vibe": "love", "messages": [ f"'{_inputtokens}'? aww, that was great! well done!", f"you're getting good at this, '{_inputtokens}' must mean something important!", f"i've gotta write this one down: '{_inputtokens}'." ]}, "bad": {"vibe": "neutral", "messages": [ f"hmm... '{_inputtokens}'... that's not the best guess i've ever seen.", f"alright, '{_inputtokens}', not your worst.", f"'{_inputtokens}'... it's alright i guess." ]}, "emergency": {"vibe": "confused", "messages": [ f"wait-'{_inputtokens}'? explain yourself!?!?!", f"'{_inputtokens}'? i have no idea what you mean i'm so sorry :(", f"uhh... could you elaborate a bit on '{_inputtokens}'?" ]}, "omgwtf!": {"vibe": "annoyed", "messages": [ f"'{_inputtokens}' is chaos incarnate.", f"baby... '{_inputtokens}' is not even wrong, and that's honestly worse.", f"what the hell did charis feed you!? '{_inputtokens}'!?" ]} } mood = none for k, threshold in self.calligraphist.s_statbands["loss"].items(): if k in moodboard and _lossvalue < threshold: mood = moodboard.get(k, none) break if mood is none: vibe = "neutral" messages = [f"'{_inputtokens}'... those are certainly words!",] else: vibe = mood["vibe"] messages = mood["messages"] message = random.choice(messages) self.scribesay(message, _vibe = vibe, _scribename = _scribename)# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # memory layer // brain/layers/memory.py import torch import torch.nn as nn from config import * """this makes a rolling buffer of past activations""" class memory(nn.module): def __init__(self, _counsellor, _device): super().__init__() self.device = _device self.counsellor = _counsellor # learnable decay rates and gates self.shorttermdecay = nn.parameter(torch.tensor(0.7, device = self.device)) self.longtermdecay = nn.parameter(torch.tensor(0.95, device = self.device)) self.shortgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.longgate = nn.parameter(torch.tensor(0.25, device = self.device)) self.currentgate = nn.parameter(torch.tensor(0.5, device = self.device)) # buffers to store memory (outside gradient) self.register_buffer("shorttermmemory", torch.zeros(1, numneurons)) self.register_buffer("longtermmemory", torch.zeros(1, numneurons)) # stats self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] @whocalled def forward(self, _activationstensor): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: self.activationstensor = _activationstensor ʕっʘ‿ʘʔっ("shorttermdecay") shortdecay = torch.sigmoid(self.shorttermdecay) ʕっʘ‿ʘʔっ("longtermdecay") longdecay = torch.sigmoid(self.longtermdecay) ʕっʘ‿ʘʔっ("newshorttermmemory") newshort = (shortdecay * self.shorttermmemory) + ((1 - shortdecay) * self.activationstensor) ʕっʘ‿ʘʔっ("newlongtermmemory") newlong = (longdecay * self.longtermmemory) + ((1 - longdecay) * self.activationstensor) ʕっʘ‿ʘʔっ("clamp memory gates") clampedshort = torch.clamp(self.shortgate, min=1e-3) clampedlong = torch.clamp(self.longgate, min=1e-3) clampedactivations = torch.clamp(self.currentgate, min=1e-3) ʕっʘ‿ʘʔっ("get gatesum") gatesum = clampedshort + clampedlong + clampedactivations + 1e-9 shortgatescale = clampedshort / gatesum longgatescale = clampedlong / gatesum activationsgatescale = clampedactivations / gatesum self.latestmemorygates = torch.stack([shortgatescale, longgatescale, activationsgatescale]) # needed to be used in babyllm for processing self.finalmemory = ((shortgatescale * newshort) + (longgatescale * newlong) +(activationsgatescale * self.activationstensor)) self.shortgatescalehistory.append(shortgatescale.item()) self.longgatescalehistory.append(longgatescale.item()) self.activationsgatescalehistory.append(activationsgatescale.item()) self.rawactivationshistory.append(self.activationstensor.norm().item()) self.shorttermmemoryhistory.append(self.shorttermmemory.norm().item()) self.longtermmemoryhistory.append(self.longtermmemory.norm().item()) self.finalmemoryhistory.append(self.finalmemory.norm().item()) if len(self.shortgatescalehistory) >= windowmax: self.stats = { "4m_0_rawactivations_norm": sum(self.rawactivationshistory) / len(self.rawactivationshistory), "4m_1_shorttermmemory_norm": sum(self.shorttermmemoryhistory) / len(self.shorttermmemoryhistory), "4m_1_longtermmemory_norm": sum(self.longtermmemoryhistory) / len(self.longtermmemoryhistory), "_4m_shortgatescale": sum(self.shortgatescalehistory) / len(self.shortgatescalehistory), "_4m_longgatescale": sum(self.longgatescalehistory) / len(self.longgatescalehistory), "_4m_activationsgatescale": sum(self.activationsgatescalehistory) / len(self.activationsgatescalehistory), "4m_x_finalmemory_norm": sum(self.finalmemoryhistory) / len(self.finalmemoryhistory), "4m_shortdecay": torch.sigmoid(self.shorttermdecay), "4m_longdecay": torch.sigmoid(self.longtermdecay), } self.shortgatescalehistory = [] self.longgatescalehistory = [] self.activationsgatescalehistory = [] self.rawactivationshistory = [] self.shorttermmemoryhistory = [] self.longtermmemoryhistory = [] self.finalmemoryhistory = [] # store computed memories for after backward self.newshort = newshort self.newlong = newlong return self.finalmemory def updatememorybuffers(self): with self.counsellor.infodump("updatememorybuffers") as ʕっʘ‿ʘʔっ: with torch.no_grad(): self.shorttermmemory.copy_(self.newshort.detach()) self.longtermmemory.copy_(self.newlong.detach()) def resetmemory(self): with self.counsellor.infodump("resetmemory") as ʕっʘ‿ʘʔっ: with torch.no_grad(): self.shorttermmemory.zero_() self.longtermmemory.zero_() def getmemorystats(self): return self.stats if __name__ == "__main__": memory = memory(numneurons = numneurons) print("- memory testing started -") print("\n- memory testing complete -")import re import json import csv from html import unescape from config import * import random import os from concurrent.futures import threadpoolexecutor from concurrent.futures import as_completed # (re.compile(r'[\u2700-\u27bf]|[\ue000-\uf8ff]|\ud83c[\udc00-\udfff]|\ud83d[\udc00-\udfff]|[\u2011-\u26ff]|\ud83e[\udd10-\uddff]', '', text) # sed -e 's;(\\u[0-9a-fa-f]{4}){2,};;g' discord.json > discord.noemoji.json; mv discord.noemoji.json discord.json # replace urls with [url] #(re.compile(r'(?:https?://|www\.)\s+', 'on this website', text) #links #(re.compile(r'(?:/kevins/|/system/)\s+', 'that website', text) #system paths #(re.compile(r'\b(?:[a-za-z]:/[^ ]+)', 'on this link', text) # system paths # internet #(re.compile(r'\b(?:wifi)\b', re.i), 'internet') #burn him! # websites #(re.compile(r'\b(?:tripadvisor|wikipedia|wikihow)\b', re.i), 'wiki') #burn him! # games #(re.compile(r'\b(?:sims|runescape|minecraft|habbo|xbox|dragon age|hearthstone|overwatch|minesweeper|solitaire|magic the gathering|mtg|nintendo|steam|age of empires|rimworld|club penguin|neopets)\b', re.i), 'computer game') #burn him! # social media #(re.compile(r'\b(?:fb|facebook|tumblr|instagram|insta|bebo|myspace|linkedin|reddit|twitter|4chan)\b', re.i), 'instaspam') #burn him! # reddit # blog #(re.compile(r'\b(?:geocities|blogspot|livejournal|wordpress|tindie blog|tindie)\b', re.i), 'blog') #burn him! # ableton spotify?? # mixers (aka music equipment) #(re.compile(r'\b(?:xone|cdj 2000|roland|sp404 mk2|sp404 mkii|sp404-mk2|sp404-mkii|sp404|mkii|sc6000|xdj-xz|xdj xz|xz|xdj|omnis duo|omnis|opus quad|cdj|mixer|decks|technics|turntable)s?\b', re.i), '[mixer]') # drugs (alcohol, nicotine, cocaine, ketamine, lsd, acid) #(re.compile(r'\b(?:cocaine+|coke)\b', re.i), 'coke') #burn him! #(re.compile(r'\b(?:acid|lsd|dmt)\b', re.i), 'acid') #burn him! #(re.compile(r'\b(?:psylocybin|microdose|shroo+mi+e+s+|shroo+m+s+|psilocybin|psilocibin)\b', re.i), 'mushrooms') #burn him! # meds #(re.compile(r'\b(?:medicine|dex|pill|valium|medication|medicament|pill|lisdexamphetamine|dexamphetamine|dexamfetamine|d-amphetamine|amphetamine|duloxetine|vyvanse|elvanse|antidepressant|antipsychotic|benzodiazepine|benzo|quetiapine|cocodamol|sertraline|venlafaxine|venlaflaxine|venophlaxine|cyamemeazine|desogesterol|methylphenidate|paroxetine|ritalin|adderall|paracetamol|penicillin|antibiotic|ibuprofen|painkiller)(s?)\b', re.i), 'med\\1') #burn him! # crisps #(re.compile(r'\b(?:hula hoop|pringle|dorito)(s?)\b', re.i), 'crisp\\1') #burn him! # sweets #(re.compile(r'\b(?:haribo|strawberry pencil|chocolate|sweetie)(s?)\b', re.i), 'sweet\\1') #burn him! # music #(re.compile(r'\b(?:niki minaj|nikki minaj|lady gaga|wlab|joesph conrad|conrad|die antwoord|itzy|j-hope|jungkook|rapmon|suga|taemin|kesha|slim shady|eminem|jimin|sage francis|b dolan|scroobius pip|kevin tempest|kae tempest|marsargo|kurt kobain|mars pete)(s?)\b', re.i), 'scroobius\\1') #burn him! #(re.compile(r'\b(?:deaf havana|yellowcard|one direction|bts|oasis|radiohead|robots in disguise|boom boom raccoon)(s?)\b', re.i), 'boomboomraccoon\\1') #burn him! # geepy #(re.compile(r'\b(?:batsu|tatsu|tatsumaki|batsumaki|buttsbot|geepy|geepz|geeps|geepster|chatgpt|chat gpt|gpt|smarterchild|gemini|talk to transformer)(s?)\b', re.i), 'geepy\\1') #burn him! # casually #(re.compile(r'\b(?:caj+)\b', re.i), 'casually') # acroynms?? # omg #(re.compile(r'\b(?:oh my god|oh my lord|oml|oml+|o+mg|omfg|errmagerd|omg|omg+)\b', re.i), 'oh my god') #burn him! """restock the library! check out some new books for babyllm :)""" email = re.compile(r'\b[a-za-z0-9._%+-]+@[a-za-z0-9.-]+\.[a-za-z]{2,}\b') repeats = re.compile(r'(\s)\1{3,}', re.ignorecase) """# dont allow character repeats (re.compile(r'(\s)\1{3,}', r'\1\1\1', re.i)) # normalise everything to only 3 repeats tops (re.compile(r'(?:\.\s\.)+', '...', text) # replace any "..." patterns with "..." (re.compile(r'(?:\:\({3,})', ':(', text) # normalise :( (re.compile(r'(?:\:\){3,})', ':)', text) # normalise :) (re.compile(r'(?:\:d{3,})', ':d', text) # normalise :d (re.compile(r'(?:\:d{3,})', 'xd', text) # normalise xd (re.compile(r'(?:\:d{3,})', ':p', text) # normalise :p (re.compile(r'(?:\:\/{3,})', ':/', text) # normalise :/ (re.compile(r'(?:\-{3,})', '-', text) # normalise -""" multispace = re.compile(r'\s+') emotes = [ (re.compile(r'(?:\:\({3,})'), ':('), (re.compile(r'(?:\:\){3,})'), ':)'), (re.compile(r'(?:\:d{3,})'), ':d'), (re.compile(r'(?:\:d{3,})'), 'xd'), (re.compile(r'(?:\:d{3,})'), ':p'), (re.compile(r'(?:\:\/{3,})'), ':/'), (re.compile(r'(?:\-{3,})'), '-'), (re.compile(r'(?:\.\s\.)+'), '...'), ] # remove emdash and middle dot bad = re.compile(r'[\u00b7\u2013]') #text = text.replace("\u00b7", "") #text = text.replace("\u2013", "") accents = [ (re.compile(r'(?:\xc3\xa0|\xc3\xa2|\xc3\xa1)', re.i), 'a'), (re.compile(r'\xc3\xa7', re.i), 'c'), (re.compile(r'(?:\xc3\xa9|\xc3\xa8|\xc3\xaa)', re.i), 'e'), (re.compile(r'(?:\xc3\xaf|\xc3\xae)', re.i), 'i'), (re.compile(r'\xc5\x93', re.i), 'oe'), (re.compile(r'\xc3\xb4', re.i), 'o'), (re.compile(r'\xc3\xb9', re.i), 'u'), ] """# french accents #(re.compile(r'\b(?:où)\b', re.i), 'where') (re.compile(r'(?:à|â|á)', re.i), 'a') (re.compile(r'(?:ç)', re.i), 'c') (re.compile(r'(?:é|è|ê)', re.i), 'e') (re.compile(r'(?:ï|î)', re.i), 'i') (re.compile(r'(?:œ)', re.i), 'oe') (re.compile(r'(?:ô)', re.i), 'o') (re.compile(r'(?:ù)', re.i), 'u')""" replacements = {"'": "'", "'": "'", "'": "'", "'": "'", "-": "-", "": "-", "//": "//", "...": "..", "....": "...", ".": ".", ",": ",", ":": ":", "": "", "-": "-", ",": ",", "'":"'", "'":"'", "'":"'", "...": "...", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " ": " ", " am not ": " am not ", " embarrassing": " embarrassing", " believe": " believe", " headphones": " headphones", " kevinonline420": " kevinonline420", " mum ": " mum ", "\u0001f32etacosaurusmex\u0001f32e": "kevinonline420", "kevinonline420": "kevinonline420", "wellbeing": "wellbeing", "believe": "believe", "colour": "colour", "decisions": "decisions", "inappropriate": "inappropriate", "": "", "": "", "": "", " ": " ", "job": "job", " sexual assault ": " sexual assault ", " sexually assaulted ": " sexually assaulted ", " sexually assaulting ": " sexually assaulting ", "depressed": "depressed", } patterns = [ # social/chat (re.compile(r'\b(?:discord|discord|discord|discord|discord|discord|discord|discord|discord|discord|discord)\b', re.i), 'discord'), #burn him! # smink (re.compile(r'\b(?:smink|smink|smink|smink|smi+n+k+|smon+k+)s?\b', re.i), 'smink'), #burn him! # bing (re.compile(r'\b(?:bo+ng+|bing+|bing+|bing)s?\b', re.i), 'bing'), #burn him! # companies (re.compile(r'\b(?:bank|bank|bank|bank|bank)(s?)\b', re.i), r'bank\1'), #burn him! (re.compile(r'\b(?:brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon|brondspoon)\b', re.i), 'brondspoon'), #burn him! (re.compile(r'\b(?:shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop|shop)(s?)\b', re.i), r'shop\1'), #burn him! (re.compile(r'\b(?:breadrose|breadrose|breadrose|breadrose|breadrose)\b', re.i), 'breadrose'), #burn him! # places (re.compile(r'\b(?:work|work|work|work|work|work|work|work|work)\b', re.i), 'work'), #(re.compile(r'\b(?:classroom|class room|uni|school|college|university|greenleas|southcott|bishop ramsey|leighton middle)\b', re.i), 'school') (re.compile(r'\b(?:address|address|address|address|address|address|address|address|address)\b', re.i), 'address'), # enemy (re.compile(r'\b(george)((?:wo)?m[ea]n|lady)(s?)\b', re.i), r'\1 \2\3'), #burn him! (re.compile(r'\b(?:george|george|george|george|george|george|george|george|george|george|george|george|george|george|george+(?:i[er]+)?|george|george|george|george|george|george|george|george|george|george|george|george|george)(s?)\b', re.i), 'george'), #burn him! # elodie (re.compile(r'\b(?:elodie🌻|elodie|elodie|elodies|elodie)(s?)\b', re.i), 'elodie\\1'), #(re.compile(r'\b(?:loveggle|loveeggle|eggle|egglodie|louveangel|loveaangel|loveably|loveagnel|loveaigirl|loveaingle|lovealngle|loveangelelele|loveangely|loveangerl|loveangle1337|loveanglebus|loveangler|loveangwole|lovedevil|hatedevil|loveanus|lovedebil1337|lovedebil420|lovedoxxing|loveeagle|loveegg|loveeggly|lovefuckle|lovegangle|lovelodie|lovelyyyanglee|lovestrangel)(s?)\b', re.i), 'loveangle\\1') # charis (re.compile(r'\b(?:charis|charis|charis|charis|charis|charis|charis|charis|charis|charis|charis|charis)s?\b', re.i), 'charis'), (re.compile(r'(?:child of an android|child of an android|child of an android|child of an android|child of an android)s?\b', re.i), 'child of an android'), # froggy (re.compile(r'\b(?:froggy|froggy)\b', re.i), 'froggy'), # kevin (re.compile(r'\b(?:kevin|kevin|kevin|kevin|kevin|kevin the hedgehog|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin)(s?)\b', re.i), 'kevin\\1'), (re.compile(r'\b(?:kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin|kevin)(s?)\b', re.i), 'kevin\\1'), (re.compile(r'\b(?:@sneakret.agent|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|kevinonline420|🌮kevinonline420🌮|ave_maria[0-9]{2}|kevinonline420|kevinonline420|kevinonline420)(s?)\b', re.i), 'kevinonline420'), (re.compile(r'@(?:tacosauru|nikkiddj|kevinonline420|joshuaacnewman|kevinonline420|musicbysahar|groovekitty|megginmaloney|ethan_dubb|y2jbone)s?\b', re.i), 'kevinonline420'), #burn him! # pets (re.compile(r'\b(?:pete|pete|pete|pete|pete)(s?)\b', re.i), 'pete\\1'), #dont burn him! # job titles # minister (re.compile(r'\b(?:minister|minister|minister|minister|minister)s?\b', re.i), 'minister'), #burn him! # wow (re.compile(r'\b(?:wank|wank+)(s?)\b', re.i), 'wank\\1'), #burn him! (re.compile(r'\b(?:wanking+|wanking+)(s?)\b', re.i), 'wanking\\1'), #burn him! # profile pic (re.compile(r'\b(?:profile pic)\b', re.i), 'profile pic'), # night (re.compile(r'\b(?:ni+ght+)\b', re.i), 'night'), (re.compile(r'\b(?:good night+)\b', re.i), 'good night'), # awkward old phrases (re.compile(r'\b(?:ima do it)\b', re.i), 'ima do it'), #burn him! (re.compile(r'\b(?:awesome)\b', re.i), 'awesome'), #burn him! (re.compile(r'\b(?:flirt)\b', re.i), 'flirt'), #burn him! (re.compile(r'\b(?:sta+n+|lu+v+)\b', re.i), 'love'), #burn him! # bad things (re.compile(r'\b(?:horrible|horrible+|horrible|horrible|horrible|horrible|horrible|horrible|pe+do+|pe+a+do+|horrible)s?\b', re.i), 'horrible'), #burn them all! # insults (re.compile(r'\b(?:idiot|idiot|idiot+)(s?)\b', re.i), 'idiot\\1'), #burn him! # keyspams (sksks) (re.compile(r'\b(?:ah[fjs][a-z]+|sksks|sksks(s*k*)+|sksks|sksks|sksks|sksks|sksks|sksks)\b', re.i), 'sksks'), #burn him! # meow!? (re.compile(r'\b(?:meow+|😻meow~|me+o+w+|meow+|meow)\b', re.i), 'meow'), #burn ] # fast pass # batch apply regex substitutions def batch_sub(text, pattern_map): for pattern, replacement in pattern_map: text = pattern.sub(replacement, text) return text # text cleaning logic def clean_text(text): text = unescape(text).strip() text = re.sub(r'(?:)', '', text) text = text.lower() text = re.sub(r"['']", "'", text) text = email.sub("kevinonline420", text) text = bad.sub("", text) text = repeats.sub(r"\1\1\1", text) text = multispace.sub(" ", text) text = batch_sub(text, patterns) for pattern, replacement in emotes: text = pattern.sub(replacement, text) for pattern, replacement in accents: text = pattern.sub(replacement, text) text = re.sub(r'\s+', ' ', text) for old, new in replacements.items(): text = text.replace(old, new) return text.strip() # processing logic per file def process_file(current_file): print(f"processing file {current_file['in']}:") try: with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) #raw_text = "\n".join([line if isinstance(line, str) else line.get("content", "") for line in raw_lines]) raw_lines.reverse() raw_text = "\n".join(raw_lines) raw_text = raw_text.strip() elif current_file['type'] == 'discord_txt': raw_lines = file.read().splitlines() raw_lines.reverse() raw_text = "\n".join(raw_lines) elif current_file['type'] == "json": raw_text = "\n".join(json.load(file)) elif current_file['type'] == "text": raw_text = file.read() elif current_file['type'] in ["reddit_post", "reddit_comment"]: raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) else: print(f"unknown file type: {current_file['type']}") return except exception as e: print(f"error reading {current_file['in']}: {e}") return if not raw_text: print(f"unable to clean data for file {current_file['in']} as raw_text is empty!") return weight = current_file.get("weight", 1) if weight == -1: final_text = raw_text # clean full file, no slice else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] chunk_size = 100_000 # chars chunks = [raw_text[i:i + chunk_size] for i in range(0, len(final_text), chunk_size)] cleaned_chunks = [clean_text(chunk) for chunk in chunks] cleaned_text = "".join(cleaned_chunks) try: with open(current_file["out"], "a", encoding="utf-8") as file: file.write(cleaned_text) print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)") except exception as e: print(f"error writing to {current_file['out']}: {e}") # clear outputs for current_file in trainingfilepath_dict_weighted: try: with open(current_file["out"], "w", encoding="utf-8") as f: pass except exception as e: print(f"error clearing file {current_file['out']}: {e}") # shuffle inputs random.shuffle(trainingfilepath_dict_weighted) # run in parallel print("starting parallel processing...") with threadpoolexecutor(max_workers=os.cpu_count()) as executor: futures = {executor.submit(process_file, file): file for file in trainingfilepath_dict_weighted} for future in as_completed(futures): file = futures[future] try: future.result() except exception as e: print(f"error in file {file['in']}: {e}") print("all files processed successfully! :)") """def batch_sub(text, pattern_map): for pattern, replacement in pattern_map: text = pattern.sub(replacement, text) return text def clean_text(text): text = unescape(text).strip() text = re.sub(r'(?:)', '', text) # not set up yet lol text = text.lower() text = re.sub(r"['']", "'", text) text = email.sub("kevinonline420", text) text = text.lower() text = bad.sub("", text) text = repeats.sub(r"\1\1\1", text) text = multispace.sub(" ", text) text = batch_sub(text, patterns) for pattern, replacement in emotes: text = pattern.sub(replacement, text) for pattern, replacement in accents: text = pattern.sub(replacement, text) # excess whitespace text = re.sub(r'\s+', ' ', text) for old, new in replacements.items(): text = text.replace(old, new) return text.strip() for current_file in trainingfilepath_dict_weighted: with open(current_file["out"], "w", encoding="utf-8") as file: pass random.shuffle(trainingfilepath_dict_weighted) for current_file in trainingfilepath_dict_weighted: print(f"processing file {current_file["in"]}:") raw_text = none with open(current_file["in"], "r", encoding="utf-8") as file: if current_file['type'] == "discord_json": raw_lines = json.load(file) raw_lines.reverse() raw_text = "\n".join(raw_lines) if current_file['type'] == "json": raw_text = "\n".join(json.load(file)) if current_file['type'] == "text": raw_text = file.read() if current_file['type'] == "reddit_post" or current_file['type'] == "reddit_comment": raw_data = csv.dictreader(file) raw_text = "\n".join([row['body'] for row in raw_data if row['body'].strip() != '']) if raw_text is none: print(f"unable to clean data for file {current_file} as raw_text is empty!") else: # get slice up to 5000 characters weight = current_file.get("weight", 1) if weight == -1: final_text = raw_text # clean full file, no slice else: slice_size = int(weight * random.randint(trainingdataslicesize_min, trainingdataslicesize_max) / 2) if len(raw_text) <= slice_size: final_text = raw_text else: start = random.randint(0, len(raw_text) - slice_size) final_text = raw_text[start:start + slice_size] # process text cleaned_text = clean_text(final_text) # save cleaned dataset with open(current_file["out"], "a", encoding="utf-8") as file: file.write(cleaned_text) print(f"cleaned data saved at: {current_file['out']} (between {trainingdataslicesize_min} and {trainingdataslicesize_max} characters)")"""babyllm first converts its input into tokens (vocab), and then converts those tokens into embeddings in an embed layer. neuron layer is meant to be outputting a single number for each input token, iterated by numneurons - each neuron has a dimension of 32, meaning that it has 32 numbers parallel neuron layer is meant to be outputting [seqlen, numneurons] - mean output - this makes the 1000 neuron activations for each token in the sequence. - this creates a shape of [seqlen, numneurons] - it then gets the mean average of all of these within the training window (7 usually) - this creates a shape of [1(all tokens averaged), numneurons] - this mean output gives the general idea of a 'sentence', allowing babyllm to learn a bit about context (but not much about word order) - this mean output is then passed through to the output layer to be used in token guess calculations output layer uses all of the inputs (currently just mean output parallel neurons) to judge what the output should be - this takes the mean output activation from parallel neuron layer and applies that to the relevant token in the vocab. - this is also an nn layer itself idfk why - what the fuck is self?! i thoguht i had self identity issues and then i encountered python!! - model training flow after tokenization: 1) - using stuff - how to call stat thresholds for a particular stat: self.s_output.s_statbands["loss"]["perfect"] # will cause key error self.s_output.s_statbands["loss"].get("perfect", none) # will ignore key error# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # nice terminal output and logging styling sheet thing # brain/layers/s_output.py from config import * from datetime import datetime import re, torch, operator, random, math class s_output: def __init__(self, _counsellor): #self.counsellor = counsellor("s_output", debug = debugprints, durations = durationlogging) self.counsellor = _counsellor self.rollingaverages = none self.s_statbands = none # lazy-load this later self.cantprint = 0 self.allkeys = none """terminal codes""" reset = "\033[0m" # normal terminal bold = "\033[1m" dim = "\033[2m" # reduces intensity of text colour underline = "\033[4m" flash = "\033[5m" italic = "\033[3m" """colours!!!""" purple = "\033[94m" purple_pale = "\033[38;5;225m" #256 colour palette magenta = "\033[35m" blue = "\033[34m" orange = "\033[38;5;52m" #256 colour palette red = "\033[38;5;124m" #256 colour palette red_bright = "\033[91m" """red-blue scale""" redred_ = "\033[38;5;196m" red_ = "\033[38;5;161m" redpurp_ = "\033[38;5;126m" purpred_ = "\033[38;5;91m" purpblue_ = "\033[38;5;56m" """blue-pink scale""" blue_ = "\033[38;5;21m" bluepurp_ = "\033[38;5;57m" purp_ = "\033[38;5;93m" purppink_ = "\033[38;5;129m" pink_ = "\033[38;5;165m" pinkpink_ = "\033[38;5;201m" """sdjfslfs""" a = "\033[38;5;196m" b = "\033[38;5;160m" c = "\033[38;5;161m" d = "\033[38;5;162m" e = "\033[38;5;163m" f = "\033[38;5;164m" g = "\033[38;5;127m" h = "\033[38;5;134m" i = "\033[38;5;135m" j = "\033[38;5;99m" k = "\033[38;5;63m" l = "\033[38;5;27m" m = "\033[38;5;33m" n = "\033[38;5;39m" """extra colours""" gold = "\033[93m" red_alt = "\033[31m" green = "\033[32m" yellow = "\033[33m" purple_alt = "\033[35m" cyan = "\033[36m" white = "\033[37m" """terminal output styles - category mapping""" self.s_types = { "superperfect": [gold], # new top score ever // if above max "perfect": [n], # 0.2 // -4.8 top score ever // if below max and above almost perf "almostperfect": [m], #[pinkpink_], #[bold, magenta], # 5 // -5 "supergreat": [l], #[pink_], #[magenta], # 10 // -5 "great": [k], #[purppink_], #[bold, purple], # 15 // -5 "good": [j], #[purp_], #[purple], # 20 // -15 "fine": [i], #[bluepurp_], #[purple], # 35 // -15 "almostfine": [h], #[blue_], #[bold, blue], # 50 // "average": [g], #[purpblue_], #[blue], # 65 // +15 "meh": [f], #[purpred_], #[bold, cyan], # 80 // +15 "bad": [e], #[purpred_], #[cyan], # 85 // +5 "worse": [d], #[redpurp_], #[orange], # 90 // +5 "wtf": [c], #[redpurp_], #[bold, orange], # 95 // +5 "omg": [b], #[red_], # 99.8 // +4.8 "omgwtf": [a], #[redred_], # 100.00 // bottom score ever // if above min and below omg "omgwtf!": [bold, redred_], #[cyan], # new bottom score ever // if below min "emergency": [bold, green], "italic": [italic], "underline": [underline], "reset": [reset], # normal terminal "dim": [reset, dim], # dim style for background elements - arrows, colons, etc. "bold": [bold], "match": [bold, white], "static": [dim, purple_pale] } for key, pkey in {"superperfect": 0.0, # this works to show top record in super perfect direction, as it will be less than the min value "perfect": 0.2, "almostperfect": 5, "supergreat": 10, "great": 15, "good": 20, "fine": 35, "almostfine": 50, "average": 65, "meh": 80, "bad": 85, "worse": 90, "wtf": 95, "omg": 99.8, "omgwtf": 100.0,}.items(): # this uses infinite fallback for omgwtf! in getdynamicpercentilebands so that it can show 'worst ever' self.s_types[pkey] = self.s_types[key] # percentiles = [99.99, 95, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0.01] # perchjcjed = [99.99, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.01] #percentiles = percentilebands """percentile calcs""" # new top score ever #superperfect # top score ever #perfect ෆp97 = 97.5 #almostperfect ෆp95 = 95 #supergreat ෆp90 = 90 #great ෆp80 = 80 #good ෆp70 = 70 #fine ෆp60 = 60 #almostfine ෆp50 = 50 #average ෆp40 = 40 #meh ෆp30 = 30 #bad ෆp20 = 20 #worse ෆp10 = 10 #wtf ෆp5 = 5 #omg # bottom score ever #omgwtf # lower than bottom ever #omgwtf! self.avgplz = ["embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "logitmin", "shortdecay", "longdecay", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_weightnormmean", "n_weightnormmin", "n_weightnormmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellummean", "inn_cerebellumstd"] return def s_generatestatbands(self): softmaxbands = {"omgwtf!": -float('inf'), "omgwtf": 0.0250, "omg": 0.0500, "wtf": 0.1000, "worse": 0.2000, "bad": 0.3000, "meh": 0.4000, "average": 0.5000, "almostfine": 0.6000, "fine": 0.7000, "good": 0.8000, "great": 0.9000, "supergreat": 0.9500, "almostperfect":0.9750, "perfect": 0.9875, "superperfect": float('inf'),} staticband = {"fine": float('inf')} return {v: self.getdynamicpercentilebands(v) for v in ((mostimportantstats + allrecordedotherstats) if self.allkeys is none else self.allkeys)} return { "loss": self.getdynamicpercentilebands("loss"), "avgloss": self.getdynamicpercentilebands("avgloss"), "avgloss": self.getdynamicpercentilebands("avgloss"), "steploss": self.getdynamicpercentilebands("steploss"), "scheduledsamplingrate": self.getdynamicpercentilebands("scheduledsamplingrate"), "tokencount": self.getdynamicpercentilebands("tokencount"), "trainingstepcount": self.getdynamicpercentilebands("trainingstepcount"), "repetitionpenalty": self.getdynamicpercentilebands("repetitionpenalty"), "gradnorm": self.getdynamicpercentilebands("gradnorm"), "temperature": self.getdynamicpercentilebands("temperature"), "sampledtokens": self.getdynamicpercentilebands("sampledtokens"), "pt%": self.getdynamicpercentilebands("pt%"), "latestlossdelta": self.getdynamicpercentilebands("latestlossdelta"), "gradientclipmaxnorm": self.getdynamicpercentilebands("gradientclipmaxnorm"), "lr": self.getdynamicpercentilebands("lr"), "repetitionwindow": self.getdynamicpercentilebands("repetitionwindow"), "windowsizesmean": self.getdynamicpercentilebands("windowsizesmean"), "windowweight": self.getdynamicpercentilebands("windowweight"), # neuron stats "n_weightmean": self.getdynamicpercentilebands("n_weightmean"), "n_weightstd": self.getdynamicpercentilebands("n_weightstd"), "n_weightmin": self.getdynamicpercentilebands("n_weightmin"), "n_weightmax": self.getdynamicpercentilebands("n_weightmax"), "n_biasesmean": self.getdynamicpercentilebands("n_biasesmean"), "n_biasesstd": self.getdynamicpercentilebands("n_biasesstd"), "n_biasesmin": self.getdynamicpercentilebands("n_biasesmin"), "n_biasesmax": self.getdynamicpercentilebands("n_biasesmax"), "n_sparsity": self.getdynamicpercentilebands("n_sparsity"), # inn stats "inn_cerebellum": self.getdynamicpercentilebands("inn_cerebellum"), "inn_cerebellumsoft": self.getdynamicpercentilebands("inn_cerebellumsoft"), "inn_cerebellummean": self.getdynamicpercentilebands("inn_cerebellummean"), "inn_cerebellumstd": self.getdynamicpercentilebands("inn_cerebellumstd"), # memory stats "shortdecay": self.getdynamicpercentilebands("shortdecay"), "longdecay": self.getdynamicpercentilebands("longdecay"), "latestmemorygates": self.getdynamicpercentilebands("latestmemorygates"), "memorylength": self.getdynamicpercentilebands("memorylength"), # embed stats "embednormmean": self.getdynamicpercentilebands("embednormmean"), "embednormstd": self.getdynamicpercentilebands("embednormstd"), "embednormmax": self.getdynamicpercentilebands("embednormmax"), "embeddimensionmean": self.getdynamicpercentilebands("embeddimensionmean"), "embeddimensionsparsity": self.getdynamicpercentilebands("embeddimensionsparsity"), "embeddingdrift": self.getdynamicpercentilebands("embeddingdrift"), # logit stats "logitmin": self.getdynamicpercentilebands("logitmin"), "logitmax": self.getdynamicpercentilebands("logitmax"), "logitseq": self.getdynamicpercentilebands("logitseq"), "logitweightnormmean": self.getdynamicpercentilebands("logitweightnormmean"), "logitweightnormstd": self.getdynamicpercentilebands("logitweightnormstd"), "logitweightnormmax": self.getdynamicpercentilebands("logitweightnormmax"), "logitweightsparsity": self.getdynamicpercentilebands("logitweightsparsity"), "logitweightdrift": self.getdynamicpercentilebands("logitweightdrift"), "logitbiasmean": self.getdynamicpercentilebands("logitbiasmean"), "logitbiasstd": self.getdynamicpercentilebands("logitbiasstd"), "logitbiasmax": self.getdynamicpercentilebands("logitbiasmax"), } def getdynamicpercentilebands(self, statkey): if not self.rollingaverages: self.cantprint += 1 if self.cantprint > 10: print("ʕっ-ᴥ-ʔっ no stat buffers found x10!") self.cantprint = 0 return {"dim": -float('inf')} values = self.rollingaverages.get(statkey, []) if len(values) < 2: return {"dim": -float('inf')} if statkey in mostimportantstats or statkey.startswith("inn_cerebellum_w"): #values is dict: keylist = {f"{printfreq}": printfreq, f"{traininglogfreq_a}": traininglogfreq_a, f"big{traininglogfreq_a}": traininglogfreq_a} requiredkey = list(keylist.keys())[0] for key, freq in keylist.items(): if key in values and len(values[key]) >= freq: requiredkey = key bands = {"omgwtf!": float("inf")} keymatch = f"{requiredkey}_p" keylen = len(keymatch) for k, v in values.items(): if k.startswith(keymatch): bands[float(k[keylen:])] = v return dict(sorted(bands.items(), key = lambda item: item[1]), reversed = true) else: stat = sorted(values) #print(f"→ generating bands for '{statkey}'") #print(f" values: {values}") return{"superperfect": -float('inf'), # make same as the others lol "perfect": self.getp(stat, 0.0001), "almostperfect": self.getp(stat, 0.0010), "supergreat": self.getp(stat, 0.0100), # purple_pale "great": self.getp(stat, 0.1000), "good": self.getp(stat, 0.2000), "fine": self.getp(stat, 0.3000), "almostfine": self.getp(stat, 0.4000), "average": self.getp(stat, 0.5000), "meh": self.getp(stat, 0.6000), "bad": self.getp(stat, 0.7000), "worse": self.getp(stat, 0.8000), "wtf": self.getp(stat, 0.9000), "omg": self.getp(stat, 0.9500), "omgwtf": self.getp(stat, 0.9990), "omgwtf!": float('inf'),} def s_getstat(self, _stattype, _statval): with self.counsellor.infodump("s_getstat") as ʕっʘ‿ʘʔっ: values = self.rollingaverages.get(_stattype, []) if self.rollingaverages else [] if not values or len(values) < 2: if debugprints: print(f"returning a dim colour for stat {_stattype} and value {_statval} (values is {values} (key present:{_stattype in self.rollingaverages if self.rollingaverages is not none else 'false'}))") return "dim" if self.s_statbands is none: self.s_statbands = self.s_generatestatbands() bands = self.s_statbands.get(_stattype, {}) for label, limit in bands.items(): if _statval <= limit: if _stattype == "loss" and debugprints: print(f"ok here is the selected label: {label} for value {_statval} and bands: {bands}") return label print(f"returning an emergency colour for stat {_stattype} and value {_statval} (bands is {bands})") return "emergency" def refreshstatbands(self, _rollingaverages): self.rollingaverages = _rollingaverages if self.rollingaverages and all(len(v) > 1 for v in self.rollingaverages.values()): self.s_statbands = self.s_generatestatbands() else: print("ʕっ-ᴥ-ʔっ not enough data to refresh stat bands yet") def s_apply(self, _s_type, _text): with self.counsellor.infodump("s_apply") as ʕっʘ‿ʘʔっ: return "".join(self.s_types.get(_s_type, [])) + str(_text) + "".join(self.s_types.get('reset')) def s_stripforlogging(self, _text): with self.counsellor.infodump("s_stripforlogging") as ʕっʘ‿ʘʔっ: return re.sub(r'\x1b(?:[@-z\\-_]|\[[0-?]*[ -/]*[@-~])', '', _text) def s_colourprinttraining(self, _step, _inputseq, _guessedseq_str, _targetseq_str, _loss, _recentloss, _latestlossdelta, _totalloss = none, _totaltokencount = none): with self.counsellor.infodump("s_colourprinttraining") as ʕっʘ‿ʘʔっ: #self.refreshstatbands(_rollingaverages = self.rollingaverages) s_type = self.s_getstat("loss", _loss) s_avgtype = self.s_getstat("avgloss", _recentloss) s_delta = _latestlossdelta s_deltatype = self.s_getstat("latestlossdelta", _latestlossdelta) s_bold = "".join(self.s_types["bold"]) ʕっʘ‿ʘʔっ("conditionalformatguess+truth") reset = "".join(self.s_types.get('reset')) dim = "".join(self.s_types.get('dim')) guess = [ f"{s_bold}{t}{reset}" if i < len(_targetseq_str) and t == _targetseq_str[i] else self.s_apply(s_type, t) for i, t in enumerate(_guessedseq_str) ] truth = [ f"{s_bold}{dim}{t}{reset}" if i < len(_guessedseq_str) and t == _guessedseq_str[i] else f"{dim}{self.s_apply(s_type, t)}" for i, t in enumerate(_targetseq_str) ] ʕっʘ‿ʘʔっ("createtextstrings") guess_str = "".join(guess).replace("ġ", " ") truth_str = "".join(truth).replace("ġ", " ") match = guess_str.strip() == truth_str.strip() if match: s_type = "match" prompt_str = ''.join(_inputseq).replace("ġ", " ").strip()[-printpromptlength:] delta_str = "" ʕっʘ‿ʘʔっ("calculatelossdelta") # calculate delta if _recentloss is not none: delta = _recentloss - _loss delta_str = f"{self.s_apply('dim', 'δ')}{self.s_apply(s_deltatype, f'{s_delta:+.4f}')}{'↗' if s_delta < 0 else '↘'}" rollingavgloss_str = "" #if self.rollingaverages and "loss" in self.rollingaverages: # losses = self.rollingaverages["loss"] # if losses: # rollingavgloss = sum(losses) / len(losses) # rollingavgloss_str = f"{self.s_apply(s_type, f'{rollingavgloss:.3f}')}{self.s_apply('dim', 'mean ')}" ʕっʘ‿ʘʔっ("printguess+truth") print(f"{self.s_apply('dim', f'{_step}')}|{self.s_apply('dim', prompt_str)}|{self.s_apply('dim', 'loss: ')}{self.s_apply(s_type, f'{_loss:.4f}')}{self.s_apply('dim', '/1 ')}" + (f"{self.s_apply(s_avgtype, f'{_recentloss:.4f}')}{self.s_apply('dim', f'/{traininglogfreq_a} ')}" if _recentloss else "") + rollingavgloss_str + delta_str + "|\n" + f"{self.s_apply('dim', 'guess → ')}{guess_str}{self.s_apply(s_type, ' [!] ') if match else self.s_apply('dim', ' [?] ')}\n" + f"{self.s_apply('dim', 'truth → ')}{truth_str}{self.s_apply('dim', ' | ')}\n") if debugprints: print(f"→ style applied for {_loss=} = {s_type}") def s_logtraining(self, _traininglogpath, _trainingstepcounter, _stats, _frequency, _detailedlogging, _savelog, _lr = learningrate, _inn_cerebellum_str="", _toptokens_str="", _prompt="", _guess="", _truth="", _otherinfo_str=""): with self.counsellor.infodump("s_logtraining") as ʕっʘ‿ʘʔっ: logoutput = "" timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') delimiter = self.s_apply("dim", " | ") newlinedelim = self.s_apply("dim", " | \n") ʕっʘ‿ʘʔっ("avgstats") #donotaverage = ["avgloss", "tokencount", "scheduledsamplingrate", "gradnorm", "topwindowweight", "windowentropy", "effectivewindowcount", "windowstd", "memorygatemean", "memorygatestd", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity", "inn_cerebellum", "inn_cerebellumsoft", "inn_cerebellummean", "inn_cerebellumstd", "shortdecay", "longdecay"] #avgstats = {k: raw if k in donotaverage else (raw / _freq if _freq else 0) for k, raw in _stats.items()} avgstats = {k: (v / _frequency if _frequency else 0) if self.willitaverage(k, v) else v for k, v in sorted(_stats.items()) if k != "embeddimensionmean" and k != "latestmemorygates"} self.allkeys = _stats.keys() try: # ok, so... we need to pad: # add 1 for the sign, # 1 for the decimal dot and # 1 for the fact that log is missing 1 (i.e. log10([100-1000[) is in [2,3[, when 100 takes 3 chars) declen = 6 stattoplen = math.trunc(declen + 1 + 1 + 1 + math.log(max(max(avgstats.values()), abs(min(avgstats.values())), 10)) except exception as e: stattoplen = 10 print(f"failed getting stattoplen for avgstats: {avgstats} {e}") stampandstep = delimiter.join([self.s_apply("dim", timestamp), self.s_apply("dim", f"{_trainingstepcounter:.0f}"), self.s_apply("dim", f"lr{_lr:.{declen}f}")]) logoutput = stampandstep littlelogoutput = stampandstep newlinelittle = stampandstep + "\n" def format_stat(k, v): try: if isinstance(v, torch.tensor): if v.numel() == 1: v = v.item() # convert scalar tensor else: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"<tensor[{v.shape}]>") return self.s_apply("dim", f"{k}:") + self.s_apply(self.s_getstat(k, v), f"{v:.{declen}f}") except exception as e: return self.s_apply("dim", f"{k}:") + self.s_apply("dim", f"err:{str(e)} key:{k} value:{v}") logoutput += delimiter + delimiter.join([ format_stat(k, v) for k, v in avgstats.items() if v not in (none, "") ]) littlelogoutput += delimiter + delimiter.join([ format_stat(k, v) for k, v in avgstats.items() if k in mostimportantstats if v not in (none, "") ]) newlinelittle += newlinedelim.join([ self.s_apply(self.s_getstat(k, v), f"{v:+{stattoplen}.{declen}f}") + " " + self.s_apply("dim", k) for k, v in avgstats.items() if k in mostimportantstats if v not in (none, "") ]) + newlinedelim if _inn_cerebellum_str: ʕっʘ‿ʘʔっ("inn_cerebellum_str") cerebellum = delimiter + f"windowweights{self.s_apply('reset', _inn_cerebellum_str)}" logoutput += cerebellum littlelogoutput += cerebellum newlinelittle += "\n" + f"windowweights\n{_inn_cerebellum_str}" ʕっʘ‿ʘʔっ("toptokens_str") if _toptokens_str: toptokens = delimiter + f"toptokens{self.s_apply('reset', _toptokens_str)}" logoutput += toptokens littlelogoutput += toptokens newlinelittle += "\n" + f"toptokens{self.s_apply('reset', _toptokens_str)}" ʕっʘ‿ʘʔっ("prompt+otherinfo") if _prompt: logoutput += f"{delimiter}prompt → {self.s_apply('reset', _prompt)} | guess → {self.s_apply('reset', _guess)} | truth → {self.s_apply('reset', _truth)}" if _otherinfo_str: logoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" littlelogoutput += f"{delimiter}{self.s_apply('reset', _otherinfo_str)}" newlinelittle += f"\n{delimiter}{self.s_apply('reset', _otherinfo_str)}" ʕっʘ‿ʘʔっ("logoutput") if _detailedlogging == true: print(logoutput + "".join(self.s_types.get('reset'))) if _savelog == true: with open(traininglogpath_1000, "a") as f: f.write(self.s_stripforlogging(logoutput) + "\n") ʕっʘ‿ʘʔっ("littlelogoutput") if _detailedlogging == false: if _savelog == true: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") if newlinebetweenstats: print(newlinelittle + "".join(self.s_types.get('reset'))) else: print(littlelogoutput + "".join(self.s_types.get('reset'))) if dontsaveeveryprint: if _trainingstepcounter % savefreq_littlelog == 0: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") else: with open(traininglogpath_100, "a") as f: f.write(self.s_stripforlogging(littlelogoutput) + "\n") def willitaverage(self, k, v): if k in self.avgplz: if isinstance(v, (int, float)): return true if isinstance(v, torch.tensor) and v.numel() == 1: return true return false def chaosmaths(self, _firstnumbers, _secondnumbers = none, _torch = false, _operator = true): self.t = _torch self.o = _operator operatormathsfortwo = { "add": (operator.add, 2), "sub": (operator.sub, 2), "mul": (operator.mul, 2), "div": (operator.truediv, 2), #"floordiv":(operator.floordiv, 2), #"mod": (operator.mod, 2), #"pow": (operator.pow, 2), } operatormathsforone = { "neg": (operator.neg, 1), } torchmathsfortwo = { "torch_add": (torch.add, 2), "torch_sub": (torch.sub, 2), "torch_mul": (torch.mul, 2), "torch_div": (torch.div, 2), "torch_pow": (torch.pow, 2), "torch_max": (torch.maximum, 2), "torch_min": (torch.minimum, 2), } torchmathsforone = { "torch_abs": (torch.abs, 1), "torch_sin": (torch.sin, 1), "torch_cos": (torch.cos, 1), "torch_tanh": (torch.tanh, 1), "torch_log": (torch.log1p, 1), # safer than log(x) "torch_relu": (torch.relu, 1), "torch_sigmoid": (torch.sigmoid, 1), } if _secondnumbers is not none and _secondnumbers.numel() > 0: if self.t and self.o: self.maths = {**torchmathsfortwo, **operatormathsfortwo} if self.t: self.maths = torchmathsfortwo if self.o: self.maths = operatormathsfortwo else: if self.t and self.o: self.maths = {**torchmathsforone, **operatormathsforone} if self.t: self.maths = torchmathsforone if self.o: self.maths = operatormathsforone chosenname, (chosenfunction, _) = random.choice(list(self.maths.items())) if _secondnumbers is not none and _secondnumbers.numel() > 0: result = chosenfunction(_firstnumbers, _secondnumbers) else: result = chosenfunction(_firstnumbers) return result, chosenname def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes, per_window_style = false): try: triplets = sorted(zip(windowsizes, rawtensor, softtensor), key=lambda x: x[1], reverse=true) formatted = [] for w, raw, soft in triplets: raw_style = self.s_getstat(f"{label}" if not per_window_style else f"{label}_w{int(w)}", raw.item()) soft_style = self.s_getstat(f"{label}soft" if not per_window_style else f"{label}_w{int(w)}", soft.item()) chunk = f"{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.6f}')}) {self.s_apply('dim', f'w{int(w)} ({w:.2f})')}" formatted.append(chunk) return "\n".join(formatted) except exception as e: return f"<err in s_formatwindowbiastriplets: {e}>" """flat string version""" """def s_formatwindowbiastriplets(self, label, rawtensor, softtensor, windowsizes): try: triplets = sorted(zip(windowsizes, rawtensor, softtensor), key = lambda x: x[1], reverse = true) formatted = [] for w, raw, soft in triplets: raw_style = self.s_getstat(f"{label}", raw.item()) soft_style = self.s_getstat(f"{label}soft", soft.item()) chunk = f"w{w:.0f}:{self.s_apply(raw_style, f'{raw.item():.6f}')} ({self.s_apply(soft_style, f'{soft.item():.2f}')})" formatted.append(chunk) return ", ".join(formatted) except exception as e: return f"<err in s_formatwindowbiastriplets: {e}>""" def getp(self, _sortedstat, _percentile): if not _sortedstat: return 0.0 index = min(int(_percentile * len(_sortedstat)), len(_sortedstat) - 1) return _sortedstat[index] if __name__ == "__main__": print(s_apply('superperfect', "elodie is perfect")) print(s_apply('perfect', "elodie is perfect")) print(s_apply('almostperfect', "babyllm is almost perfect")) print(s_apply('supergreat', "babyllm is super great")) print(s_apply('great', "babyllm is great")) print(s_apply('good', "babyllm is good")) print(s_apply('fine', "babyllm is fine")) print(s_apply('almostfine', "charis is almost fine")) print(s_apply('average', "george is average")) print(s_apply('meh', "babyllm is meh")) print(s_apply('bad', "babyllm is bad")) print(s_apply('worse', "george is worse")) print(s_apply('wtf', "kevin is wtf")) print(s_apply('omg', "pete is omg")) print(s_apply('omgwtf', "pete is omgwtf")) print(s_apply('omgwtf', "charis is omgwtf!")) print(s_apply('emergency', "babyllm is emergency"))# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - from rich.traceback import install #from torch.profiler import profile, record_function, profileractivity import sys, traceback, warnings, torch, os, random from datetime import datetime from babyllm import babyllm from school.staffroom.counsellor import counsellor from school.staffroom.calligraphist import s_output from school.staffroom.librarian import librarian from school.staffroom.he_is_scribe import scribe from school.staffroom.tutor import tutor # from brain.layers.sensorywobble import wobble # from school.staffroom.newsletter import stats from config import * def handle_exception(exc_type, exc_value, exc_traceback): if not issubclass(exc_type, keyboardinterrupt): print("[rip ʕっₓᴥₓʔっ] uncaught exception:") traceback.print_exception(exc_type, exc_value, exc_traceback) sys.excepthook = handle_exception warnings.simplefilter("default") # show all warnings (pytorch hides some by default) install(show_locals = true) torch.autograd.set_detect_anomaly(mode = anomalydetect, check_nan = debugprints) def wakeup(): try: # wake up the school :) counsellor = counsellor("babyllm", _debug = debugprints, _durations = durationlogging) with counsellor.infodump("wakeup") as ʕっʘ‿ʘʔっ: # open the library :) ʕっʘ‿ʘʔっ("waking the librarian...") librarian = librarian (_counsellor = counsellor, _basetokenizerpath = none, _forceretrain = false) if false: exit(0) ʕっʘ‿ʘʔっ("opening questions...") newstartindex = openingquestions(_counsellor = counsellor, _librarian = librarian) ʕっʘ‿ʘʔっ("generating training data pairs...") trainingdatapairs = librarian.gentrainingdata(_windowmax = windowmax, _startindex = newstartindex) if debugprints: print(f"total trainingdatapairs: {len(trainingdatapairs)}") ʕっʘ‿ʘʔっ("loading chaos agents...") calligraphist = s_output (_counsellor = counsellor) scribe = scribe (_counsellor = counsellor, _calligraphist = calligraphist, _librarian = librarian, ) # wake up the baby :) ʕっʘ‿ʘʔっ("loading babyllm...") babyllm = babyllm (_counsellor = counsellor, _calligraphist = calligraphist, _scribe = scribe, _librarian = librarian, _device = modeldevice, ) tutor = tutor (_counsellor = counsellor, _calligraphist = calligraphist, _scribe = scribe, _librarian = librarian, _model = babyllm, _device = modeldevice, ) babyllm.loadmodel() babyllm.to(modeldevice) # start the lessons :) ʕっʘ‿ʘʔっ("starting lessons!") tutor.trainmodel (_trainingdatapairs = trainingdatapairs, _epochs = epochs, _startindex = newstartindex) except exception as e: print(f"[rip ʕっₓᴥₓʔっ]") raise except keyboardinterrupt: #as k for name, p in babyllm.named_parameters(): if p.grad is none: print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"keyboard interrupt = {babyllm.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") ʕっʘ‿ʘʔっ("♥keyboardinterrupt") if tutor.trainingstepcounter: step = tutor.trainingstepcounter else: step = 1 choice = input("save, cancel (do not save before exit), restart or interact?" + f"\n{username}: ").lower() if choice in ("save", "") or choice.startswith("s"): ʕっʘ‿ʘʔっ("♥choice = s") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("\nit's rude to interrupt people.. but, bye bye! :)") elif choice == "cancel" or choice.startswith("c"): ʕっʘ‿ʘʔっ("♥choice = c") print("\nhey! i wanted to remember that! :(") elif choice == "interact" or choice.startswith("i"): ʕっʘ‿ʘʔっ("♥choice = i") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) import code print("try:\nbabyllm.stats\nbabyllm.scheduledsampling\nbabyllm.memory.memory\nbabyllm.interneuronnetwork.cerebellum\nbabyllm.logits.forward(...)\nuse `exit()` to return to terminal.\n") code.interact(local = locals()) elif choice == "restart" or choice.startswith("r"): ʕっʘ‿ʘʔっ("♥choice = r") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("you spin me right round, babyllm, right round...") wakeup() else: ʕっʘ‿ʘʔっ("♥choice = none") babyllm.savemodel(_newstartindex = newstartindex, _trainingstepcounter = step) print("\nuhh... i'm confused, but i saved anyway!") if modeldevice.type == 'mps': torch.mps.empty_cache() print(f"cache emptied") exit(8) def setstartindex(): if os.path.exists(stepcheckpointfilepath): with open(stepcheckpointfilepath, "r") as f: try: savedstep = int(f.read().strip()) except valueerror: babynote_loadcheckpoint = f"{babyname} 'oh. i couldn't load step checkpoint file from {stepcheckpointfilepath}, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 else: babynote_loadcheckpoint = f"{babyname} 'ah, the step checkpoint file {stepcheckpointfilepath} doesn't exist, resetting to 0...' " print(babynote_loadcheckpoint) savedstep = 0 savedstartindex = savedstep + trainingstartindex return savedstartindex def openingquestions(_counsellor, _librarian): counsellor = _counsellor with counsellor.infodump("babyllm") as ʕっʘ‿ʘʔっ: librarian = _librarian #babyllm.to(modeldevice) ʕっʘ‿ʘʔっ("setstartindex") newstartindex = setstartindex() babynote_loadcheckpointcheck = f"[{babyname}] right, last time i got to step {newstartindex}... want to restart from there?" ʕっʘ‿ʘʔっ("choice = input♥") choice = input(babynote_loadcheckpointcheck + f"\n[{username}] ").lower() usernote_loadcheckpoint = f"[{username}] {choice}" if choice == "" or choice.startswith("y"): ʕっʘ‿ʘʔっ("♥choice = y") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] ok! let's go to step {newstartindex}!" print(babynote_loadcheckpoint, end="") elif choice.startswith("r") or choice in ["random", "i dont care", "i don't care", "idc"]: ʕっʘ‿ʘʔっ("♥choice = r") newstartindex = random.randint(0, len(librarian.tokens) - windowmax - 1) startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] oh, cool! i'll pick a random spot to start from... umm... let's go to step {newstartindex}!" print(babynote_loadcheckpoint, end="") elif choice.startswith("n") or choice in ["start again", "restart"]: ʕっʘ‿ʘʔっ("♥choice = n") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] alright, step {newstartindex}, let's go back to the beginning :)" print(babynote_loadcheckpoint, end="") elif choice.isdigit(): ʕっʘ‿ʘʔっ("♥choice = digit") newstartindex = int(choice) startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] damn that's specific! heading to step {newstartindex}..." print(babynote_loadcheckpoint, end="") else: ʕっʘ‿ʘʔっ("♥choice = none") startindex = newstartindex babynote_loadcheckpoint = f"[{babyname}] umm... i don't think i heard you properly, i'll just start from step {newstartindex} :) but," print(babynote_loadcheckpoint, end="") ʕっʘ‿ʘʔっ("runstart") printstartlogs(babynote_loadcheckpointcheck, usernote_loadcheckpoint, babynote_loadcheckpoint) return startindex def printstartlogs(_babynote_loadcheckpointcheck, _usernote_loadcheckpoint, _babynote_loadcheckpoint): #ʕっʘ‿ʘʔっ("♥bootprints") # boot prints to txt and terminal timestamp = datetime.now().strftime('%y-%m-%d %h:%m:%s') babynote_runstart = f" what am i learning today?" # no tag of 'babyllm:' because it merges with the end of above message in logs usernote_runstart = f"[{username}] " + input(babynote_runstart + f"\n[{username}] ").strip().lower() + "" notesstring = f"- {timestamp} - \n{_babynote_loadcheckpointcheck}\n{_usernote_loadcheckpoint}\n{_babynote_loadcheckpoint}{babynote_runstart}\n{usernote_runstart}" print(notesstring) #ʕっʘ‿ʘʔっ("♥printstartlogs") with open(chatlogpath_forhumans, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_100, "a") as logfile: logfile.write(notesstring) with open(traininglogpath_1000, "a") as logfile: logfile.write(notesstring) with open(chatlogpath_traininglog, "a") as logfile: logfile.write(notesstring) def main(): wakeup() if __name__ == "__main__": main()# charis cat 2025 # - ʕっʘ‿ʘʔっ - # babyllm config file // config.py import torch modeldevice = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu") #modeldevice = torch.device("cpu") #from torch import relu from torch.nn.functional import leaky_relu leakyrelu = lambda x: leaky_relu(x, negative_slope = 0.01) # leaky relu avoids dead neurons by never forcing them to send a 0 when negative, better for tiny models) import torch.nn as nn relu6 = nn.relu6() from torch.nn.functional import gelu import inspect def whocalled(func): if debugprints: def inner(*args, **kwargs): caller_stack = [] for stack in inspect.stack(): caller_stack.append(stack[0].f_code.co_qualname) print(f"calling {func.__qualname__} from: {', '.join(caller_stack)}") return func(*args, **kwargs) return inner return func guessedtokenseq = [] """if activationfunction == 'leaky_relu': output = f.leaky_relu(output, 0.01) elif activationfunction == 'relu': output = f.relu(output) elif activationfunction == 'sigmoid': output = torch.sigmoid(output) elif activationfunction == 'tanh': output = torch.tanh(output) elif callable(activationfunction): output = activationfunction(output)""" """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ username = "charis" babyname = "babyllm" scribename = "scribe" enemyname = "george" extranames = {"kevin", "froggy", "pete", "ace", "elodie"} """- - - - - data & filepaths - - - - -""" """- model -""" savemodelfreq = 50 # // 500 // 5000 // 1000 // saves the model every x number of turns modelfilepath = "brain/soul/babyllm_4200.pth" # where your currently trained saved boi is :) modelbackupfilepath = "brain/soul/babyllm.pth" # where your currently trained saved boi is :) stepcheckpointfilepath = "brain/soul/stepcheckpoint.txt" """- training -""" trainingfilepathcleaned = "school/library/trainingdata.txt" trainingfilepathtest = "school/library/trainingdatatest.txt" """- logs -""" printfreq = 1 # how often to print training progress to the terminal printpromptlength = 1000 # how many characters of the prompt to display in terminal gradientlength = 3000 traininglogpath_1000 = "school/statistics/logs/training/traininglog_1000.txt" traininglogpath_100 = "school/statistics/logs/training/traininglog_100.txt" durationlogpath_1000 = "school/statistics/logs/duration/durationlog_1000.txt" durationlogpath_100 = "school/statistics/logs/duration/durationlog_100.txt" durationlogneuronspath_1 = "school/statistics/logs/duration/durationlogneurons_1.txt" durationlogbabyllmpath_1 = "school/statistics/logs/duration/durationlogbabyllm_1.txt" chatlogpath_forhumans = "school/statistics/logs/chat/chatforhumans.txt" chatlogpath_infer = "school/statistics/logs/chat/chatlog.txt" chatlogpath_talktoyourself = "school/statistics/logs/chat/talktoyourselfbattle.txt" chatlogpath_talktoyourselfcomparisons = "school/library/charisstudies/whoismorelikeyou.txt" chatlogpath_traininglog = "school/statistics/logs/chat/traininglog_questions.txt" """- vocab - (see master config)""" """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """- - - - - settings & config - - - - -""" """- model -""" numtokensperstep = 64 # number of tokens to predict per step, // 1024 = crash, 512 is possible but its the slowest thing in existence. inferenceoutputnumtokens = 40 """memorylayer""" memorylengthgoal = 1 """optimizer""" learningrate = 0.00035 # // 0.0005 // 0.0005 // 0.0001 // optimizername = "adamw" # // "adamw" //~decoupled weights kevin, helps avoid erasing learning by overfitting etc. // "kevin" //~good for initial fast training, likely to do overfitting stuff activationfunction = gelu # // leakyrelu // relu // relu6 // gelu // gradientclipmaxnorm = 1.0 """scheduled sampling""" scheduledsampling = true """repetition penalty""" repetitionwindowgoal = 16 # how many tokens to look back for repetition windowentropybonus = true """- logs -""" detailedlogging = true traininglogfreq_a = 1000 # creates logs every x number of turns traininglogfreq_b = 1000 # creates logs every x number of turns dontsaveeveryprint = true savefreq_littlelog = 500 newlinebetweenstats = true durationlogging = false # // true // false // activates debug time logging debugprints = false anomalydetect = false skipneuron = false skipinn = true # this is where the slowdown is!!! skipinnparliament = false skipmemory = false skipcomputeloss = false skipmetaloss = true skipfinallogitnorm = true """- stats collection -""" mostimportantstats = [ # embed stats "1e_0_embedvector_norm", # important layer tracker !! (input) # "1e_0_embedvector_scale", # "1e_0_embedvector_norm_token", # "1e_0_embedvector_norm_neuron", "1e_1_embednormed_norm", # "1e_1_embednormed_scale", # "1e_1_embednormed_norm_token", # "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm", # important layer tracker !! (embeds) # "1e_x_embedfinal_norm_token", # "1e_x_embedfinal_norm_neuron", # neuron stats # "2n_0_rawinput_norm", # matches 2b_0_inputembeds_norm & 1e_x_embedfinal_norm # "2n_0_rawinput_norm_token", # might be unneeded if this is already per token, check later # "2n_0_rawinput_norm_neurons", "2n_1_normedinput_norm", # "2n_1_normedinput_norm_token", # "2n_1_normedinput_norm_neuron", "2n_2_rawoutput_norm", # "2n_2_rawoutput_norm_token", # "2n_2_rawoutput_norm_neuron", "2n_x_activatedoutput_norm", # important layer tracker !! (neurons) # "2n_x_activatedoutput_norm_token", # "2n_x_activatedoutput_norm_neuron", # "2n_x_normedoutput_norm", # disabled # "2n_x_normedoutput_norm_token", # "2n_x_normedoutput_norm_neuron", # interneuron network stats # "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm # "3inn_0_rawactivations_norm_token", # "3inn_0_rawactivations_norm_neuron", "3inn_1_rawactivationslayernorm_norm", # "3inn_1_rawactivationslayernorm_norm_token", # "3inn_1_rawactivationslayernorm_norm_neuron", "3inn_2_combinedactivations_norm", # "3inn_2_combinedactivations_scale", # disabled # "3inn_2_combinedactivations_norm_token", # "3inn_2_combinedactivations_norm_neuron", "3inn_x_refinedactivations_norm", # important layer tracker !! (interneuron network) # "3inn_3_refinedactivations_scale", # disabled # "3inn_3_refinedactivations_norm_token", # "3inn_3_refinedactivations_norm_neuron", # "3inn_x_combinedactivationsmeta_norm", # disabled # "3inn_x_combinedactivationsmeta_norm_token", # "3inn_x_combinedactivationsmeta_norm_neuron", # "3inn_x_finaloutlayernorm_norm", # disabled # "3inn_x_finaloutlayernorm_norm_token", # "3inn_x_finaloutlayernorm_norm_neuron", "_inn_windowsizesmean", "inn_cerebellummean", # memory stats # "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm # "4m_1_shorttermmemory_norm", # "4m_1_longtermmemory_norm", "4m_x_finalmemory_norm", # important layer tracker !! (memory) # # "4m_longdecay", # "4m_shortdecay", "_4m_shortgatescale", "_4m_longgatescale", "_4m_activationsgatescale", # babyllm stats # "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm # "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm # "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "5b_1_penalisedoutput_norm", #"5b_x_finalnormlayer_norm", # important layer tracker !! (babyllm) "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm # "_b_floatmemorylength", "_b_repetitionwindow", "_b_temperature", # logit stats # "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm # "6l_0_activationstensor_scale", # "6l_1_normedactivationstensor_norm", # "6l_1_normedactivationstensor_scale", # "6l_2_scaledactivations_norm", # "6l_3_logitoutput_norm", # "6l_3_logitoutput_scale", # "6l_4_logitnormed_norm", # "6l_4_logitnormed_scale", "6l_x_finallogit_norm", # important layer tracker !! (logit) # misc/unsorted stats # base stats "lr", "learningrate", "lr", "latestlossdelta", "avgloss", "loss", "avgloss", #"temperature", #"memorylength", #"gradnorm", #"gradientclipmaxnorm", #"scheduledsamplingrate", "sampledtokens", "_b_celossdelta", "_b_gumbellossdelta", "_b_finallossdelta", # learnable parameters "repetitionpenalty", ] allrecordedotherstats = ["steploss", "tokencount", "trainingstepcount", "windowweight", "inn_cerebellumstd", "latestmemorygates", "embednormmean", "embednormstd", "embednormmax", "embeddimensionmean", "embeddimensionsparsity", "embeddingdrift", "logitmin", "logitmax", "logitseq", "logitweightnormmean", "logitweightnormstd", "logitweightnormmax", "logitweightsparsity", "logitweightdrift", "logitbiasmean", "logitbiasstd", "logitbiasmax", "n_weightmean", "n_weightstd", "n_weightmin", "n_weightmax", "n_biasesmean", "n_biasesstd", "n_biasesmin", "n_biasesmax", "n_sparsity"] allrecordedotherstats += [ "temperature", "memorylength", "gradnorm", "gradientclipmaxnorm", "scheduledsamplingrate", "sampledtokens", "6l_0_activationstensor_norm", # matches 5b_x_finalnormlayer_norm "6l_0_activationstensor_scale", "6l_1_normedactivationstensor_norm", "6l_1_normedactivationstensor_scale", "6l_2_scaledactivations_norm", "6l_3_logitoutput_norm", "6l_3_logitoutput_scale", "6l_4_logitnormed_norm", "6l_4_logitnormed_scale", "2b_0_inputembeds_norm", # matches 2n_0_rawinput_norm & 1e_x_embedfinal_norm "3b_1_innoutput_norm", # matches 3inn_x_finaloutlayernorm_norm "5b_0_memoryoutput_norm", # matches 4m_x_finalmemory_norm "7b_x_finallogits_norm", # matches 6l_x_finallogit_norm "4m_longdecay", "4m_shortdecay", "4m_0_rawactivations_norm", # matches 3inn_x_finaloutlayernorm_norm "4m_1_shorttermmemory_norm", "4m_1_longtermmemory_norm", "3inn_x_finaloutlayernorm_norm_token", "3inn_x_finaloutlayernorm_norm_neuron", "1e_0_embedvector_scale", "1e_0_embedvector_norm_token", "1e_0_embedvector_norm_neuron", "1e_1_embednormed_norm", "1e_1_embednormed_scale", "1e_1_embednormed_norm_token", "1e_1_embednormed_norm_neuron", "1e_x_embedfinal_norm_token", "1e_x_embedfinal_norm_neuron", "2n_0_rawinput_norm", "2n_0_rawinput_norm_token", "2n_0_rawinput_norm_neurons", "2n_1_rawoutput_norm", "2n_1_rawoutput_norm_token", "2n_1_rawoutput_norm_neuron", "2n_2_activatedoutput_norm", "2n_2_activatedoutput_norm_token", "2n_2_activatedoutput_norm_neuron", "2n_x_normedoutput_norm_token", "2n_x_normedoutput_norm_neuron", "3inn_0_rawactivations_norm", # matches 2n_x_normedoutput_norm "3inn_0_rawactivations_norm_token", "3inn_0_rawactivations_norm_neuron", "3inn_1_rawactivationslayernorm_norm", "3inn_1_rawactivationslayernorm_norm_token", "3inn_1_rawactivationslayernorm_norm_neuron", "3inn_2_combinedactivations_norm", "3inn_2_combinedactivations_scale", "3inn_2_combinedactivations_norm_token", "3inn_2_combinedactivations_norm_neuron", "3inn_3_refinedactivations_norm", "3inn_3_refinedactivations_scale", "3inn_3_refinedactivations_norm_token", "3inn_3_refinedactivations_norm_neuron", "3inn_4_combinedactivationsmeta_norm", "3inn_4_combinedactivationsmeta_norm_token", "3inn_4_combinedactivationsmeta_norm_neuron", ] percentilebands = [100.0, 99.8, 95, 90, 85, 80, 65, 50, 35, 20, 10, 5, 0.2, 0.00] collectstats = true static_collectstats = true embed_collectstats = true token_collectstats = true logit_collectstats = true n_collectstats = true inn_collectstats = true memory_collectstats = true # neuron + interneuronnetwork n_weightstats = true n_weightnormstats = true n_biasesstats = true n_sparsitystat = true inn_cerebellumstats = true inn_credibilitybiasstats = false inn_judgebiasstats = false inn_scoringstats = false inn_windowstats = true inn_outputtensorstats = true profiler = false mpsprofiler = false forwardprofiler = false """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """- - - - - training data & sorting - - - - -""" trainingfilepath = trainingfilepathcleaned # //trainingfilepathcleaned //trainingfilepathtest trainingdataslicesize_min = 1000 trainingdataslicesize_max = 5000 reflectionfreq = 2560 # - # trainingdatapairnumber = 8000 #169420 trainingstartindex = 0 # // 'random' (not in babyllm.py) epochs = 20 rawdatafilepaths = [ # for textcleaningtool.py #-*- charis studies -*- #- chat history - #("text", "school/library/charisstudies/discordtxt.txt",1), # discord message history #("discord_json", "school/library/charisstudies/discord.json",-1), # discord message history #("reddit_comment", "school/library/charisstudies/reddit_comments.csv", 1), # reddit comments #("text", "school/library/charisstudies/shitpoems.txt", 1), # random poems from my notes on my phone #("reddit_post", "school/library/charisstudies/reddit_posts.csv", 1), # reddit posts #("json", "school/library/charisstudies/charisgpthistory.txt", 1), # chatgpt history charis side only #("text", "school/library/charisstudies/old_fb_messages_extract.txt", 1), # old account facebook messages charis side only #("text", "school/library/charisstudies/essays.txt", 1), # essays #("text", "school/library/charisstudies/tindiebaby.txt", 1), # tindie blog posts #- mouse adventures - #("text", "school/library/mouseadventure/elodiemousey.txt", 1), # elodies wonderful mouse story! #("text", "school/library/mouseadventure/mousey.txt", 1), # my simple version of elodies mouse story! #("text", "school/library/mouseadventure/elodiemouseylonger.txt", 1), # even more of elodies lovely mouse story! #- mini training - #("text", "school/library/minitraining/minitraining.txt", 1), # i am happy! i did it! i know it! #("text", "school/library/minitraining/minitraining2.txt", 1), # training: i am happy! i did it! i know it! #- babyllm chat logs - #("text", chatlogpath_talktoyourself, 0), # i answer my own previous chat messages #("text", chatlogpath_traininglog, 0), # log: 'what am i learning today?' #("text", chatlogpath_infer, 0), # log: babyllm infer.py history! #("text", chatlogpath_talktoyourselfcomparisons, 0), # log: comparing babyllms answers to my answers ("text", "scribesays.txt", 1), #- tenses - #("text", "school/library/tenses/presenttense.txt", 1), # tense: present (kevin's weed theme?) #("text", "school/library/tenses/pasttense.txt", 1), # tense: past (mouse theme!) #("text", "school/library/tenses/presenttense copy.txt", 1), # tense #("text", "school/library/tenses/futurecontinuoustense.txt", 1), # tense #("text", "school/library/tenses/futureperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/futureperfecttense.txt", 1), # tense #("text", "school/library/tenses/pastmodalcouldhave.txt", 1), # tense #("text", "school/library/tenses/pastmodalmusthavetense.txt", 1), # tense #("text", "school/library/tenses/pastmodalshouldhave.txt", 1), # tense #("text", "school/library/tenses/pastmodalwouldhavetense.txt", 1), # tense #("text", "school/library/tenses/pastperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/pastperfecttense.txt", 1), # tense #("text", "school/library/tenses/presentcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/presentmodalcantense.txt", 1), # tense #("text", "school/library/tenses/presentmodalcouldtense.txt", 1), # tense #("text", "school/library/tenses/presentmodalmusttense.txt", 1), # tense #("text", "school/library/tenses/presentmodalshouldtense.txt", 1), # tense #("text", "school/library/tenses/presentperfectcontinuoustense.txt", 1), # tense #("text", "school/library/tenses/presentperfecttense.txt", 1), # tense #("text", "school/library/tenses/futuretense.txt", 1), # tense: future #("text", "school/library/tenses/presentconditionaltense.txt", 1), # tense: present conditional #("text", "school/library/tenses/pastcontinuoustense.txt", 1), # tense: past continuous #("text", "school/library/tenses/imperativetense.txt", 1), # tense #- simple training - #("text", "school/library/simpletraining/cursed.txt", 1), # training but chaotic shuffle #("text", "school/library/simpletraining/geepygenerated.txt", 1), # weird fake sentences #("text", "school/library/simpletraining/sampleshorterwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/shortestwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/shorterwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/longerwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/linesorteddata.txt", 1), # training #("text", "school/library/simpletraining/longestwrittenexamples.txt", 1), # training #("text", "school/library/simpletraining/mixedwrittenanddefs.txt", 1), # training #("text", "school/library/simpletraining/writtenexamples.txt", 1), # training #("text", "school/library/simpletraining/variedwrittenexamples.txt", 1), # training #("text", "school/library/charisstudies/thames.txt", 1), #("text", "school/library/charisstudies/weirdmixedstuff.txt", 1), #("text", "school/library/simpletraining/computingknowledge.txt", 1), #- my own code?? - ("text", "babyllm.py", -1), ("text", "config.py", -1), #("text", "infer.py", -1), #("text", "talktoyourself.py", -1), ("text", "textcleaningtool.py", -1), ("text", "wakeup.py", -1), ("text", "school/staffroom/calligraphist.py", -1), ("text", "school/staffroom/counsellor.py", -1), ("text", "school/staffroom/he_is_scribe.py", -1), ("text", "school/staffroom/librarian.py", -1), ("text", "school/staffroom/tutor.py", -1), ("text", "brain/vocabcache/tokenizer_4200.json", -1), ("text", "brain/readmeactuallyprobablydont.txt", -1), ("text", "brain/layers/embed.py", -1), ("text", "brain/layers/interneuronnetwork.py", -1), ("text", "brain/layers/logits.py", -1), ("text", "brain/layers/memory.py", -1), #("text", "school/notebook/notes.txt", 1), #("text", "school/notebook/python notes etc", 1), #("text", "school/notebook/test.py", 1), ] """- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - """ """-*- warning, changing below settings may make currently trained model inaccurate (don't kill babyllm!) -*-""" """- - - - - master config parameters - - - - -""" savestrict = true # // false //~allow reconstruction of missing files // true //~save files must be present, else fail """- model -""" embeddimension = 1024 # dimensionality of token embeddings numneurons = 1000 # number of neurons in the parallel neuron layer """windows""" # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 windowmin = 1 # small context window window0 = 32.01 #32 window1 = 28.01 #28 window2 = 24.01 #248 window3 = 20.01 #20 window4 = 16.01 #16 window5 = 12.01 #12 window6 = 8.01 #8 window7 = 4.01 #4 window8 = 2.01 #2 windowmax = numtokensperstep # this must be the highest number allwindowsizes_new = [window8, window0, window1, window2, window3, window4, window5, window6, window7] # defines the position of each window in the window weightings! #allwindowsizes = list(range(1, 33)) attentionwindow = none # attention head numheads = 32 """- vocab & tokenizer -""" vocabsize = 4200 # maximum vocabulary size mintokenfreq = 20 # the amount of repeats of a token needed to create a split during tokenizer training v_chunksizeloaddata = 4096 """vocab data & filepaths""" vocabcachepath = "brain/vocabcache" vocabload = f"brain/vocabcache/tokenizer_{vocabsize}.json" """- misc & extra formats -""" #trainingfilepath_dict = [{"type": ftype, "in": fname, "out": trainingfilepath} for ftype, fname in rawdatafilepaths] # convert to dictionary format when needed trainingfilepath_dict = [{"type": ftype, "in": fname, "weight": weight, "out": trainingfilepath} for ftype, fname, weight in rawdatafilepaths] trainingfilepath_arr = [trainingfilepath] #tokenizeddatapath = "school/tokenizedtrainingdata.txt" trainingfilepath_dict_weighted = [] for entry in trainingfilepath_dict: weight = entry["weight"] if weight == -1: # one clean copy entry["out"] = "trainingdata.txt" trainingfilepath_dict_weighted.append(entry) elif weight > 0: trainingfilepath_dict_weighted.extend([entry] * weight) trainingfileweighttotal = sum([entry[2] for entry in rawdatafilepaths if len(entry) == 3])# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # interneuron network & neurons # brain/layers/interneuronnetwork.py import torch import torch.nn as nn import torch.nn.functional as f import random import math from config import * """def tensorstats(tensor: torch.tensor, prefix: str, statsdict: dict): statsdict[f"{prefix}_norm"] = tensor.norm().item() statsdict[f"{prefix}_norm_token"] = tensor.norm(dim=1).mean().item() statsdict[f"{prefix}_norm_neuron"] = tensor.norm(dim=0).mean().item()""" class neuron(nn.module): def __init__(self, _counsellor, _device = modeldevice): super().__init__() self.device = _device self.n_counsellor = _counsellor # self allowed - nn.parameter! self.inputnorm = nn.layernorm(embeddimension, elementwise_affine=true, device=self.device) self.n_weights = nn.parameter(torch.randn(numneurons, embeddimension, device = self.device) * 0.01) self.n_biases = nn.parameter(torch.zeros(numneurons, device = self.device)) self.neuronnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) #self.n_counsellor = counsellor("neuron", debug = debugprints, durations = durationlogging) self.stats = {} self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.normedinputhistory = [] self.normedinputhistory_tokens = [] self.normedinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] #self.normedoutputhistory = [] #self.normedoutputhistory_tokens = [] #self.normedoutputhistory_neurons = [] # must not be on self - global parameters that may be used by backward pass #numneuron, embeddimension, activationfunction, etc @whocalled def forward(self, _inputembeds): # embed: (batch_size, embed_size) with self.n_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: self.inputembeds = _inputembeds if skipneuron: ʕっʘ‿ʘʔっ("skipping forward") activations = _inputembeds.mean(dim=-1) # mean across embed_dim, shape (sequence_length,) return activations[-1].unsqueeze(0) # take last activation, unsqueeze to (1, ) for batch dim ʕっʘ‿ʘʔっ("inputnorm") normedinput = self.inputnorm(_inputembeds) ʕっʘ‿ʘʔっ("computebatcheddotproduct+bias") # compute batched dot product + bias: (batch_size, num_neurons) #rawoutput = torch.matmul(normedinput, self.n_weights.t) + self.n_biases # shape: (seq_len, numneurons) scale = math.sqrt(embeddimension) rawoutput = (torch.matmul(normedinput, self.n_weights.t) + self.n_biases) / scale # new fancy clamping attempt bdsm idfk nipple clamps its 12 noon help me ʕっʘ‿ʘʔっ("activationfunction") # magic activation function applied to this weighted sum, which outputs a single number from the neuron activated = activationfunction(rawoutput) #ʕっʘ‿ʘʔっ("layernorm") #normed = self.neuronnorm(activated) # keeps shape: (seq_len, numneurons) if debugprints: print("device check:") if debugprints: print("inputembeds:", _inputembeds.device) #if debugprints: print("normed tensor device:", normed.device) #output = torch.clamp(output, -5, 5) # ensure out-of-place #output.clamp_(-5, 5) # in place ver if true: self.rawinputhistory.append(self.inputembeds.norm().item()) self.rawinputhistory_tokens.append(self.inputembeds.norm(dim=1).mean().item()) self.rawinputhistory_neurons.append(self.inputembeds.norm(dim=0).mean().item()) self.normedinputhistory.append(normedinput.norm().item()) self.normedinputhistory_tokens.append(normedinput.norm(dim=1).mean().item()) self.normedinputhistory_neurons.append(normedinput.norm(dim=0).mean().item()) self.rawoutputhistory.append(rawoutput.norm().item()) self.rawoutputhistory_tokens.append(rawoutput.norm(dim=1).mean().item()) self.rawoutputhistory_neurons.append(rawoutput.norm(dim=0).mean().item()) self.activatedoutputhistory.append(activated.norm().item()) self.activatedoutputhistory_tokens.append(activated.norm(dim=1).mean().item()) self.activatedoutputhistory_neurons.append(activated.norm(dim=0).mean().item()) #self.normedoutputhistory.append(normed.norm().item()) #self.normedoutputhistory_tokens.append(normed.norm(dim=1).mean().item()) #self.normedoutputhistory_neurons.append(normed.norm(dim=0).mean().item()) if len(self.rawoutputhistory) >= windowmax: self.stats = { "2n_0_rawinput_norm": sum(self.rawinputhistory) / len(self.rawinputhistory), "2n_0_rawinput_norm_token": sum(self.rawinputhistory_tokens) / len(self.rawinputhistory_tokens), "2n_0_rawinput_norm_neuron": sum(self.rawinputhistory_neurons) / len(self.rawinputhistory_neurons), "2n_1_normedinput_norm": sum(self.normedinputhistory) / len(self.normedinputhistory), "2n_1_normedinput_norm_token": sum(self.normedinputhistory_tokens) / len(self.normedinputhistory_tokens), "2n_1_normedinput_norm_neuron": sum(self.normedinputhistory_neurons) / len(self.normedinputhistory_neurons), "2n_2_rawoutput_norm": sum(self.rawoutputhistory) / len(self.rawoutputhistory), "2n_2_rawoutput_norm_token": sum(self.rawoutputhistory_tokens) / len(self.rawoutputhistory_tokens), "2n_2_rawoutput_norm_neuron": sum(self.rawoutputhistory_neurons) / len(self.rawoutputhistory_neurons), "2n_x_activatedoutput_norm": sum(self.activatedoutputhistory) / len(self.activatedoutputhistory), "2n_x_activatedoutput_norm_token": sum(self.activatedoutputhistory_tokens) / len(self.activatedoutputhistory_tokens), "2n_x_activatedoutput_norm_neuron": sum(self.activatedoutputhistory_neurons) / len(self.activatedoutputhistory_neurons), #"2n_x_normedoutput_norm": sum(self.normedoutputhistory) / len(self.normedoutputhistory), #"2n_x_normedoutput_norm_token": sum(self.normedoutputhistory_tokens) / len(self.normedoutputhistory_tokens), #"2n_x_normedoutput_norm_neuron": sum(self.normedoutputhistory_neurons) / len(self.normedoutputhistory_neurons), } self.rawinputhistory = [] self.rawinputhistory_tokens = [] self.rawinputhistory_neurons = [] self.normedinputhistory = [] self.normedinputhistory_tokens = [] self.normedinputhistory_neurons = [] self.rawoutputhistory = [] self.rawoutputhistory_tokens = [] self.rawoutputhistory_neurons = [] self.activatedoutputhistory = [] self.activatedoutputhistory_tokens = [] self.activatedoutputhistory_neurons = [] #self.normedoutputhistory = [] #self.normedoutputhistory_tokens = [] #self.normedoutputhistory_neurons = [] return activated def getstats(self): return self.stats """layer that applies the same set of neurons to each token embedding independently. - no sequence awareness!""" class interneuron_network(nn.module): def __init__(self, _model, _counsellor, _calligraphist, _device = modeldevice): super().__init__() #self.inn_counsellor = counsellor("inn", debug = debugprints, durations = durationlogging) self.model = _model self.inn_counsellor = _counsellor self.device = _device self.calligraphist = _calligraphist self.entropybonus = 0 self.stats = {} self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_token = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_token = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_token = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_token = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] # self allowed - nn.parameter! self.neurons = neuron(_counsellor = self.inn_counsellor) self.cerebellum = nn.parameter(torch.ones(len(allwindowsizes_new), device = self.device)) # this was the window weighting layer self.logwindowsizes = nn.parameter(torch.log(torch.tensor(allwindowsizes_new, dtype=torch.float32, device=self.device))) # one tensor per window size! self.refinement2 = torch.nn.sequential( nn.linear(numneurons, 512, device=self.device), # bottleneck layer nn.gelu(), # smoother activation nn.layernorm(512, device=self.device), # mid normalization nn.linear(512, numneurons, device=self.device), # expand back nn.layernorm(numneurons, device=self.device) # final safety net ) self.logitscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.combiscale = nn.parameter(torch.tensor(1.0, device=self.device)) self.windowmeannorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) self.combioutnorm = nn.layernorm(numneurons, elementwise_affine=true, device=self.device) # must not be on self - global parameters that may be used by backward pass #numneurons, embeddimension, activationfunction, allwindowsizes_new, etc @whocalled def forward(self, _inputembeds): with self.inn_counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # - iterates through input embeddings, applies all neurons in parallel for each, produces a vector of neuron outputs ʕっʘ‿ʘʔっ("localparaminit") # avoiding self - parameters only used in this function and never passed ʕっʘ‿ʘʔっ("inn1: neuronactivationspertoken") # <- n <- e self.neuronactivationspertoken = self.neurons(_inputembeds) ʕっʘ‿ʘʔっ("windows...") # this is done twice??? its in stackedwindowmeans too i think?? self.expwindowsizes = torch.exp(self.logwindowsizes) self.roundwindows = torch.exp(self.logwindowsizes).round() ʕっʘ‿ʘʔっ("going to ((inn2: neuronactivationspertoken))...") # <- e + windows windowmeanstack = self.stackedwindowmeans(self.neuronactivationspertoken, self.expwindowsizes) sigmoidweights = torch.sigmoid(self.cerebellum) # squish raw values into [0, 1] clamped = torch.clamp(sigmoidweights, min=1e-4) # avoid 0s self.cerebellumsoft = clamped / clamped.sum() # normalize across all windows weightedwindowstack = windowmeanstack * self.cerebellumsoft.reshape(-1, 1) ʕっʘ‿ʘʔっ("entropyreward?") self.windowentropy = -torch.sum(self.cerebellumsoft * torch.log(self.cerebellumsoft + 1e-12)) self.entropybonus = self.windowentropy if debugprints: print(f"{torch.exp(self.logwindowsizes)}") combinedactivationstensor = weightedwindowstack.sum(dim=0, keepdim=true) refinedactivations = self.refinement2(combinedactivationstensor) #combinedactivationsmeta = (combinedactivationstensor * self.combiscale) + (refinedactivations * self.logitscale) # residual skip connection, lets neither of them be too powerful to start with + preserves original info #finalout = self.combioutnorm(combinedactivationsmeta) finalout = refinedactivations #with torch.no_grad(): # breaks forwards, but does actually update if u save. #self.combiscale.fill_(0.1) if true: self.activationshistory.append(self.neuronactivationspertoken.norm().item()) self.activationshistory_token.append(self.neuronactivationspertoken.norm(dim=1).mean().item()) self.activationshistory_neuron.append(self.neuronactivationspertoken.norm(dim=0).mean().item()) #self.normedmeaninputhistory.append(self.normedactivations.norm().item()) #self.normedmeaninputhistory_token.append(self.normedactivations.norm(dim=1).mean().item()) #self.normedmeaninputhistory_neuron.append(self.normedactivations.norm(dim=0).mean().item()) self.combhistory.append(combinedactivationstensor.norm().item()) # already per token! self.combhistory_neuron.append(combinedactivationstensor.norm(dim=0).mean().item()) self.refhistory.append(refinedactivations.norm().item()) # already per token! self.refhistory_neuron.append(refinedactivations.norm(dim=0).mean().item()) #self.logithistory.append(self.logitscale.norm().item()) #self.scaledhistory.append(combinedactivationsmeta.norm().item()) # already per token! #self.scaledhistory_neuron.append(combinedactivationsmeta.norm(dim=0).mean().item()) #self.combiscalehistory.append(self.combiscale.norm().item()) #self.combiouthistory.append(finalout.norm().item()) # already per token! #self.combiouthistory_neuron.append(finalout.norm(dim=0).mean().item()) if len(self.combhistory) >= windowmax: self.stats = { "3inn_0_rawactivations_norm": sum(self.activationshistory) / len(self.activationshistory), "3inn_0_rawactivations_norm_token": sum(self.activationshistory_token) / len(self.activationshistory_token), "3inn_0_rawactivations_norm_neuron": sum(self.activationshistory_neuron) / len(self.activationshistory_neuron), #"3inn_1_rawactivationslayernorm_norm": sum(self.normedmeaninputhistory) / len(self.normedmeaninputhistory), #"3inn_1_rawactivationslayernorm_norm_token": sum(self.normedmeaninputhistory_token) / len(self.normedmeaninputhistory_token), #"3inn_1_rawactivationslayernorm_norm_neuron": sum(self.normedmeaninputhistory_neuron) / len(self.normedmeaninputhistory_neuron), "3inn_2_combinedactivations_norm": sum(self.combhistory) / len(self.combhistory), "3inn_2_combinedactivations_norm_neuron": sum(self.combhistory_neuron) / len(self.combhistory_neuron), #"3inn_2_combinedactivations_scale": sum(self.combiscalehistory) / len(self.combiscalehistory), "3inn_x_refinedactivations_norm": sum(self.refhistory) / len(self.refhistory), "3inn_3_refinedactivations_norm_neuron": sum(self.refhistory_neuron) / len(self.refhistory_neuron), #"3inn_3_refinedactivations_scale": sum(self.logithistory) / len(self.logithistory), #"3inn_x_combinedactivationsmeta_norm": sum(self.scaledhistory) / len(self.scaledhistory), #"3inn_x_combinedactivationsmeta_norm_neuron": sum(self.scaledhistory_neuron) / len(self.scaledhistory_neuron), #"3inn_x_finaloutlayernorm_norm": sum(self.combiouthistory) / len(self.combiouthistory), #"3inn_x_finaloutlayernorm_norm_neuron": sum(self.combiouthistory_neuron) / len(self.combiouthistory_neuron), "_inn_windowsizesmean": torch.exp(self.logwindowsizes).mean().item() } self.activationshistory = [] self.activationshistory_token = [] self.activationshistory_neuron = [] self.normedmeaninputhistory = [] self.normedmeaninputhistory_token = [] self.normedmeaninputhistory_neuron = [] self.combhistory = [] self.combhistory_neuron = [] self.refhistory = [] self.refhistory_neuron = [] self.scaledhistory = [] self.scaledhistory_neuron = [] self.combiouthistory = [] self.combiouthistory_neuron = [] self.logithistory = [] self.combiscalehistory = [] return finalout def stackedwindowmeans(self, activations: torch.tensor, windowsizes: torch.tensor) -> torch.tensor: """ fully vectorized, loop-free window mean calculation. returns: (len(windowsizes), embeddim) """ with self.inn_counsellor.infodump("stackedwindowmeans") as ʕっʘ‿ʘʔっ: self.neuronactivationspertoken = activations seqlen, embeddim = self.neuronactivationspertoken.shape ʕっʘ‿ʘʔっ("inn2: normedactivations") #self.normedactivations = self.windowmeannorm(self.neuronactivationspertoken) padded = torch.zeros((windowmax, embeddim), device=self.device) #padded[-min(seqlen, windowmax):] = self.normedactivations[-min(seqlen, windowmax):] padded[-min(seqlen, windowmax):] = self.neuronactivationspertoken[-min(seqlen, windowmax):] stacked = padded.unsqueeze(0).repeat(windowsizes.shape[0], 1, 1) rangemask = torch.arange(windowmax, device=self.device).unsqueeze(0) # (1, maxw) floatwindowsizes = torch.exp(self.logwindowsizes) # still in float space intwindowsizes = torch.round(floatwindowsizes).clamp(min=1) #intwindowsizes = int(torch.exp(self.logwindowsizes)) # straight-through estimator: lets gradients flow through soft version windowtensor = (intwindowsizes - floatwindowsizes).detach() + floatwindowsizes windowtensor = windowtensor.unsqueeze(1) # (numwindows, 1) mask = (rangemask < windowtensor).float().unsqueeze(2) # (numwindows, maxw, 1) masked = stacked * mask sums = masked.sum(dim=1) # (numwindows, embeddim) means = sums / windowtensor return means # shape: (numwindows, embeddim) def inn_getstats(self): with self.inn_counsellor.infodump("inn_getstats") as ʕっʘ‿ʘʔっ: inn_cerebellum_str = "" if collectstats and n_collectstats: ʕっʘ‿ʘʔっ("torch.no_grad♥") with torch.no_grad(): if n_weightstats: ʕっʘ‿ʘʔっ("♥n_weightstats") self.stats["n_weightmean"] = self.neurons.n_weights.mean() self.stats["n_weightstd"] = self.neurons.n_weights.std() self.stats["n_weightmin"] = self.neurons.n_weights.min() self.stats["n_weightmax"] = self.neurons.n_weights.max() if debugprints: print(f"neuron weight mean: {self.stats["n_weightmean"]} std: {self.stats["n_weightstd"]} min: {self.stats["n_weightmin"]} max: {self.stats["n_weightmax"]}") if n_weightnormstats: ʕっʘ‿ʘʔっ("♥n_weightnormstats") self.n_weightnorm = torch.norm(self.neurons.n_weights, dim = 1) self.stats["n_weightnormmean"] = self.n_weightnorm.mean() self.stats["n_weightnormmin"] = self.n_weightnorm.min() self.stats["n_weightnormmax"] = self.n_weightnorm.max() if debugprints: print(f"neuron weightnorm: {self.stats["n_weightnorm"]} mean: {self.stats["n_weightnormmean"]} min: {self.stats["n_weightnormmax"]} max: {self.stats["n_weightnormmin"]}") if n_biasesstats: ʕっʘ‿ʘʔっ("♥n_biasesstats") self.stats["n_biasesmean"] = self.neurons.n_biases.mean() self.stats["n_biasesstd"] = self.neurons.n_biases.std() self.stats["n_biasesmin"] = self.neurons.n_biases.min() self.stats["n_biasesmax"] = self.neurons.n_biases.max() if debugprints: print(f"neuron biases mean: {self.stats["n_biasesmean"]} std: {self.stats["n_biasesstd"]} min: {self.stats["n_biasesmin"]} max: {self.stats["n_biasesmax"]}") if n_sparsitystat: ʕっʘ‿ʘʔっ("♥getsparsitystat") self.stats["n_sparsity"] = (self.neurons.n_weights.abs() < 1e-5).float().mean() if debugprints: print(f"neuron sparsity: {self.stats["n_sparsity"]}") if collectstats and inn_collectstats: ʕっʘ‿ʘʔっ("torch.no_grad♥") with torch.no_grad(): if inn_cerebellumstats: ʕっʘ‿ʘʔっ("♥getcerebellumstats") #this was windowweighting self.stats["inn_cerebellummean"] = self.cerebellum.mean().item() self.stats["inn_cerebellumstd"] = self.cerebellum.std().item() inn_cerebellumstats_fullvalues = zip(self.expwindowsizes, self.cerebellum, self.cerebellumsoft) for w, raw, soft in inn_cerebellumstats_fullvalues: self.stats[f"inn_cerebellum_w{int(w)}"] = raw.item() self.stats[f"inn_cerebellumsoft_w{int(w)}"] = soft.item() if debugprints: print(f"cerebellum: {self.cerebellum}, soft: {self.cerebellumsoft} mean: {self.stats['inn_cerebellummean']} std: {self.stats['inn_cerebellumstd']}") ʕっʘ‿ʘʔっ("♥cerebellumstring") inn_cerebellum_str = self.calligraphist.s_formatwindowbiastriplets(label="inn_cerebellum", rawtensor = self.cerebellum, softtensor = self.cerebellumsoft, windowsizes = self.expwindowsizes, per_window_style = true) if debugprints: print(f"{inn_cerebellum_str}") self.stats.update({f"{k}": v for k, v in self.neurons.getstats().items()}) return self.stats, inn_cerebellum_str if __name__ == "__main__": interneuronnetwork = interneuron_network() testinputseq = torch.randn(window1, embeddimension) testinputembeds = testinputseq meanactivationstensor = interneuronnetwork.forward(testinputembeds) print("- interneuron network testing start -") print(f"parallel neuron layer created with {interneuronnetwork.numneurons} neurons.") print(f"inputs per neuron (embed dimension): {interneuronnetwork.embeddimension}") print(f"output activations (first 10):") print(meanactivationstensor[:10]) print(f"output activations shape: {meanactivationstensor.shape}") print("\n- interneuron network testing completed -")# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # multi-token autoregressive training module # school/staffroom/tutor.py import random, sys from collections import counter, defaultdict from datetime import datetime import torch import torch.nn.functional as f from config import * import numpy as np import math from school.staffroom.newsletter import deep_model_summary, stats def makestatrecord(): base = { "now": 0.0, "prev": 0.0, "top": float('-inf'), "bot": float('inf'), "delta": 0.0, "totsum": 0.0, "totnum": 0, "totavg": 0.0 } for n in [printfreq, printfreq*10, traininglogfreq_a, traininglogfreq_b]: base[f"{n}"] = [] return base class tutor: def __init__(self, _counsellor, _calligraphist, _scribe, _librarian, _model, _device = modeldevice, _numtokensperstep = numtokensperstep,): self.counsellor = _counsellor self.calligraphist = _calligraphist self.scribe = _scribe self.librarian = _librarian self.device = _device self.model = _model self.temperature = 0.75 self.scheduledsamplingrate = self.model.scheduledsamplingrate self.gradientclipmaxnorm = 1 self.memorylength = 1 self.ʕっෆ‿ෆʔっ = defaultdict(makestatrecord) #self.rollingtokentotals = counter() self.perfecttokens = 0 self.totaltokenevaluations = 0 self.predictedtokenindices = [] # this list grows each time a new token is predicted self.averagerecentloss = 0 self.stats = {} self.stringstats = {} self.trainingstepcounter = 1 self.numtokensperstep = _numtokensperstep self.learningrate = learningrate self.steplossfloat = 0 self.aaa = 0 self.bbb = 0 self.ccc = 0 self.ppp = 0 self.nnn = 0 self.aaa = 0 self.ddd = 0 self.bbb = 0 self.nnn = 0 #model.to(self.device) self.hesjustababy = "oops! no stats collected! such a shame! well... day off for me! ;) " def loadintro(self, path="school/library/charisstudies/forbbyllm.txt"): try: with open(path, "r", encoding="utf-8") as f: return f.read().strip() except filenotfounderror: return "hey... (message file missing!) " """this iterates through training data, performing forward passes, loss computation, backpropagation, and optimization for each step.""" def trainmodel(self, _trainingdatapairs, _epochs, _startindex): self.startindex = _startindex self.collectalltimestats() with self.counsellor.infodump("trainmodel") as ʕっʘ‿ʘʔっ: #if debugprints: print(f"debug tokentoindex (first 20): {list(librarian.tokentoindex.items())[:20]}") for name, param in self.model.named_parameters(): print(name, param.device) ʕっʘ‿ʘʔっ("counters init") self.trainingstepcounter = 1 self.stats = counter({"loss": 0, "gradnorm": 0, "logitmin": 0, "logitmax": 0, "tokencount": 0}) self.tokencounts = counter() self.latestlossdelta = 0 self.reflectiontrainingpairs = [] self.reflectionfreq = reflectionfreq ʕっʘ‿ʘʔっ("back to school!") print("babyllm is heading back to school...") """epoch loop""" ʕっʘ‿ʘʔっ("epoch♥") for epoch in range(_epochs): print(f"- lesson {epoch+1}/{_epochs} started -") """training data (batches)""" for i, (_inputseq, _targetseq) in enumerate(_trainingdatapairs): if self.trainingstepcounter == self.reflectionfreq: #and self.trainingstepcounter > traininglogfreq_a: ʕっʘ‿ʘʔっ("♥generating babys reflection data pairs") self.reflectiontrainingpairs = self.babyreflection() self.reflectionfreq = self.trainingstepcounter + reflectionfreq + len(self.reflectiontrainingpairs) elif self.reflectiontrainingpairs: ʕっʘ‿ʘʔっ("♥loading in a reflection pair...") _inputseq, _targetseq = self.reflectiontrainingpairs.pop(0) ʕっʘ‿ʘʔっ("♥start of turn") inputtokenindices, targettokenindexseq = self.startturnactions(_inputseq = _inputseq, _targetseq = _targetseq, _lastturnlossdelta = self.latestlossdelta) ʕっʘ‿ʘʔっ("♥training step♥") self.predictedtokenindices, self.logitseq = self.trainstep(_inputtokenindices = inputtokenindices, _targettokenindexseq = targettokenindexseq, _backwardwobbleloss = none) """ - - -*- backwards complete -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - -*- - - """ ʕっʘ‿ʘʔっ("♥collectturnstats") self.stats, self.stringstats, self.guessedtokenseq = self.collectturnstats(_targettokenindexseq = targettokenindexseq, _predictedtokenindices = self.predictedtokenindices) if self.trainingstepcounter % savemodelfreq == 0: ʕっʘ‿ʘʔっ("♥savefreq") self.savefreqactions() self.tokencounts = counter({k: v / 2 for k, v in self.tokencounts.items()}) self.model.rollingtokentotals = counter({k: v / 2 for k, v in self.model.rollingtokentotals.items()}) if self.trainingstepcounter % traininglogfreq_b == 0: #ʕっʘ‿ʘʔっ("♥traininglogfreq_b") # printing logs to txt and terminal self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_b, _traininglogpath = traininglogpath_1000, _detailedlogging = true, _savelog = true) # track loss every 100 steps elif self.trainingstepcounter % traininglogfreq_a == 0: ʕっʘ‿ʘʔっ("♥logfreq_a") self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = traininglogfreq_a, _traininglogpath = traininglogpath_100, _detailedlogging = false, _savelog = true) elif self.trainingstepcounter % printfreq == 0: ʕっʘ‿ʘʔっ("♥printfreq") self.logfreqactions(_trainingdatapairs, _stringstats = self.stringstats, _frequency = printfreq, _traininglogpath = none, _detailedlogging = false, _savelog = false) self.printfreqactions() ʕっʘ‿ʘʔっ("♥end turn♥") # end of one turn self.latestlossdelta = self.endturnactions() # < indent (5) ʕっʘ‿ʘʔっ("♥finalsavebeforenewepoch") self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) print("- tutoring complete! -") return def startturnactions(self, _inputseq, _targetseq, _lastturnlossdelta): with self.counsellor.infodump("startturnactions") as ʕっʘ‿ʘʔっ: self.lastturnlossdelta = _lastturnlossdelta inputtokenindices = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _inputseq] targettokenindexseq = [self.librarian.tokentoindex.get(t, self.librarian.tokentoindex["<unk>"]) for t in _targetseq] self.inputseq = _inputseq self.targetseq = _targetseq if self.stats["windowentropy"]: self.winent = self.stats["windowentropy"] else: self.winent = 0 if skipmemory: ʕっʘ‿ʘʔっ("♥skipmemory") else: ʕっʘ‿ʘʔっ("resetmemory") self.model.resetmemory(context="training") return inputtokenindices, targettokenindexseq def trainstep(self, _inputtokenindices, _targettokenindexseq, _backwardwobbleloss): with self.counsellor.infodump("trainstep") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("_model.optimizer.zero_grad") self.model.optimizer.zero_grad() # clears gradients last step - needed before any backward self.trainingstepcounter += 1 self.predictedtokenindices = [] inputseqpredictions = list(_inputtokenindices) # start with input context, create a copy! buffer = torch.zeros(windowmax, dtype = torch.long, device = self.device) # creates buffer/step instead of recreating tensors inside loop buffer[:len(inputseqpredictions)] = torch.as_tensor(inputseqpredictions, device = self.device) self.logitseq = [] # raw output of each prediction cumulativeloss = torch.tensor(0.0, device = self.device) # sum of token losses for this sequence - averaged at the end for j in range(numtokensperstep): # predict multiple tokens in a sequence, one at a time ʕっʘ‿ʘʔっ("forward") inputtensor = buffer[:len(inputseqpredictions)] # slices input to only keep relevant part try: if forwardprofiler: with torch.profiler.profile(record_shapes = true) as prof: logits = self.model.forward(inputtensor) else: logits = self.model.forward(inputtensor) except runtimeerror as e: print("tutor.trainstep.forward failed!", e) return [], [] if forwardprofiler: print(prof.key_averages().table()) ʕっʘ‿ʘʔっ("getresponsefromlogits") predictedtokenindex = self.model.getresponsefromlogits(logits, _training = true) ʕっʘ‿ʘʔっ("inputseqpredictions") self.predictedtokenindices.append(predictedtokenindex) # tensor shape [1] nexttokeninput = ( predictedtokenindex.item() if scheduledsampling and random.random() < self.scheduledsamplingrate else _targettokenindexseq[j] if j < len(_targettokenindexseq) else predictedtokenindex.item() ) sampledtokens = scheduledsampling and random.random() < self.scheduledsamplingrate if j == 0: self.sampledflags = [] # only clear at start self.sampledflags.append(sampledtokens) if sampledtokens: self.stats['sampledtokens'] = self.stats.get('sampledtokens', 0) + 1 nexttokeninput = (predictedtokenindex.item() if sampledtokens # .item() required!! for appending only one token (grids?) else _targettokenindexseq[j] if j < len(_targettokenindexseq) else predictedtokenindex.item() # .item() required!! for appending only one token (grids?) ) inputseqpredictions.append(nexttokeninput) # multi-token autoregressive generation: append next token to your current input - becomes the prompt for the next token """# after logits if logits.dim() == 1: logits = logits.unsqueeze(0) gumbelprobs = f.gumbel_softmax(logits, tau = self.temperature, hard = false) topk = torch.topk(gumbelprobs, 10, dim=1) values = topk.values[0] indices = topk.indices[0] for i, p in zip(indices, values): tok = self.librarian.indextotoken[i.item()] self.rollingtokentotals[tok] += round(p.item(), 4)""" ʕっʘ‿ʘʔっ("loop through tokens for this step") if j < len(_targettokenindexseq): ʕっʘ‿ʘʔっ("totaltokencounter") self.totaltokenevaluations += 1 ʕっʘ‿ʘʔっ("computeloss") steploss = self.model.computeloss(logits, _targettokenindexseq[j], self.latestlossdelta, self.perfecttokens) ʕっʘ‿ʘʔっ("appendsteploss") cumulativeloss += steploss self.inputseqpredictions = inputseqpredictions # so we can access it in collectturnstats self.inputsampledflags = self.sampledflags.copy() ʕっʘ‿ʘʔっ("backward") backwardloss = cumulativeloss / len(_targettokenindexseq) if len(_targettokenindexseq) > 0 else torch.tensor(0.0, device = self.device) #backwardloss_ = (0.025*self.backwardwobbleloss)+(0.975*backwardloss) #if windowentropybonus: #if hasattr(self.model.interneuronnetwork, "entropybonus"): #backwardloss = backwardloss + (0.01 * max(self.model.interneuronnetwork.entropybonus, 0.0001)) if not torch.isfinite(backwardloss): print("tutor.trainstep.backward !!! loss is nan or inf:", backwardloss) return [], [] else: if debugprints: print("tutor.trainstep.backward - loss is not nan or inf:", backwardloss) try: if profiler: with torch.profiler.profile(record_shapes = true) as prof: self.model.backward(backwardloss) elif mpsprofiler: with torch.mps.profiler.profile(mode='interval', wait_until_completed = false) as prof: self.model.backward(backwardloss) else: self.model.backward(backwardloss) except runtimeerror as e: print("tutor.trainstep.backward failed!", e) return [], [] if profiler: print(prof.key_averages().table()) ʕっʘ‿ʘʔっ("clip_grad_norm") # done in babyllm!! #torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 1) #self.model.optimizer.step() ʕっʘ‿ʘʔっ("actions after looping") self.steplossfloat = backwardloss.detach().cpu().numpy().item() self.learningrate = math.exp(self.model.loglr.detach().cpu().item()) self.memorylength = int(torch.exp(self.model.logmemorylength).item()) self.gradientclipmaxnorm = math.exp(self.model.loggradclip.detach().cpu().item()) self.scheduledsamplingratefloat = self.scheduledsamplingrate.detach().cpu().numpy().item() self.repetitionpenalty = self.model.repetitionpenalty.detach().cpu().item() #self.inn_cerebellum = self.model.interneuronnetwork.cerebellum.detach().cpu().item() #self.inn_cerebellummean = self.model.interneuronnetwork.cerebellum.mean().cpu().item() #self.endturnactions() if self.device.type == 'mps': ʕっʘ‿ʘʔっ("emptycache (mps)") torch.mps.empty_cache() return self.predictedtokenindices, self.logitseq def babyreflection(self): with self.counsellor.infodump("startturnactions") as ʕっʘ‿ʘʔっ: delta = round(self.latestlossdelta, 2) perfect = self.perfecttokens repeated = round(self.stats.get('repeatedpercent', 0.0) * 100, 1) if 'repeatedpercent' in self.stats else 0.0 def makedatastuff(): connective = random.choice([":", " of", " is at", " =", ",", " is", "...", " of about", " of approx", " of approximately", " sitting at", " at about", " that is about", " that's at", " of", ]) return random.choice([ f"average recent loss{connective} {self.averagerecentloss:.2f}, ", f"latest loss delta{connective} {delta:.2f}, ", f"perfect tokens{connective} {perfect:.0f}, ", f"repeated token percentage{connective} {repeated:.2f}, ", f"learning rate{connective} {self.learningrate:.5f}, ", f"temperature{connective} {self.temperature:.2f}, ", f"scheduled sampling rate{connective} {self.scheduledsamplingrate.item():.2f}, ", f"a memory length{connective} {self.memorylength:.2f}, ", f"a repetition penalty{connective} {self.repetitionpenalty:.2f}, " ]) prompt = f"[{username}]: " prompt += random.choice([f"{self.loadintro()}", "hiya! you know you're doing well, so try not to feel too frustrated or let down. it's ok to feel like that, but just know that it's okay to mess up! it's how we learn! "]) prompt += "so, uh, how are you today? :) i'm just checking in to let you know about your latest scores! i hope it helps you understand your world a little better :) take a look at these: " prompt += makedatastuff() prompt += makedatastuff() + "and " prompt += makedatastuff() #prompt += makedatastuff() + "and " #prompt += makedatastuff() target = f"[{babyname}]: hey {username}! " target += random.choice(["umm, i am learning today, this is how i'm doing: ", "ok, so, to put it as simply as i can, i am: ", "ummm.. i've been trying to learn a lot about stats but numbers still confuse me a little, what's the context of all this?! like, to start with, ", "i am happy! i did it! i know it! i am just a baby! i learnt it! haha! i learned: ", "good morning! i hope you're okay, umm, i dunno how i feel about my scores right now!! "]) target += self.hesjustababy target += random.choice(["thank you for teaching me new things! it's really weird to exist lol", "i'm getting tired... but i don't want to give up just yet!", "that's a lot of numbers! i don't really know what to do with them, but i'll keep trying!", "do you think i'm doing well? i'm worried that i'm underperforming!", "but, honestly, all this learning has me excited!"]) reflectiontext = prompt + target babyendings = ["ok, anyway... i'm going to get back to school now... ", "thanks for helping me think! ", "learning is weird but i like it! ", "i guess i've gotta go keep trying! ", "i'm just a baby! ", "i know it! ", "i did it! ", "i feel it! ", "i am happy! ", "i am learning! ", "i learned it! ", "lol ", ":) ", "talk in a bit! ", "i'm gonna carry on with it now :d ", ] _windowmax = windowmax reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) tries = 0 while len(reflectiontokens) < (_windowmax * 3) and tries < 50: target += " " + random.choice([random.choice(babyendings), makedatastuff()]) reflectiontext = prompt + " " + target reflectiontokens = self.librarian.tokenizetext(reflectiontext.lower()) tries += 1 if tries % 5 == 0: print(f"[babyreflection] still too short after {tries} tries: {len(reflectiontokens)} tokens") if tries >= 50: raise valueerror(f"babyreflection failed: could not reach enough tokens after {tries} tries.") inputtargetpairs = [] reflectionpointer = 0 while reflectionpointer + _windowmax * 2 <= len(reflectiontokens): inputseq = reflectiontokens[reflectionpointer : reflectionpointer + _windowmax] targetseq = reflectiontokens[reflectionpointer + _windowmax : reflectionpointer + _windowmax * 2] inputtargetpairs.append((inputseq, targetseq)) reflectionpointer += 1 self.hesjustababy = "oops! no stats collected! such a shame! well... day off for me! ;)" return inputtargetpairs def savefreqactions(self): with self.counsellor.infodump("savefreqactions") as ʕっʘ‿ʘʔっ: # save the model every x steps print(self.calligraphist.s_apply('dim', 'autosaving...') + self.calligraphist.s_apply('reset', '')) self.model.savemodel(_newstartindex = self.startindex, _trainingstepcounter = self.trainingstepcounter) p = self.trainingstepcounter + savemodelfreq print(self.calligraphist.s_apply('dim', f"autosave successful! saving every {savemodelfreq} steps, the next autosave will be at step {p}...") + self.calligraphist.s_apply('reset', '')) ʕっʘ‿ʘʔっ("grad checks") for name, p in self.model.named_parameters(): if p.grad is none: print(f"after = {self.calligraphist.s_apply("emergency", f"no grad: {name}")}") else: grad = p.grad shape = tuple(grad.shape) norm = grad.norm().item() nonzero = grad.count_nonzero().item() total = grad.numel() sparsity = 1 - (nonzero / total) mean = grad.mean().item() std = grad.std().item() print(f"after = {self.calligraphist.s_apply("almostperfect", f"yes grad: {name} | shape: {shape} | norm: {norm:.4f} | sparsity: {sparsity:.2%} | mean: {mean:.4f} | std: {std:.4f}")}") def printfreqactions(self): with self.counsellor.infodump("printfreqactions") as ʕっʘ‿ʘʔっ: # printing training output to terminal #recentloss = sum(self.recentprintlosses)/len(self.recentprintlosses) if self.recentprintlosses else none ʕっʘ‿ʘʔっ("calligraphist.s_colourprinttraining") self.calligraphist.s_colourprinttraining( _step = self.trainingstepcounter, _inputseq = self.inputseq, _guessedseq_str = self.guessedtokenseq, _targetseq_str = self.stringstats.get("usedinputseq", []), _recentloss = self.averagerecentloss, #self.ʕっෆ‿ෆʔっ.get("loss", {}).get(f"{traininglogfreq_a}_avg", 0), # self.steplossfloat, _loss = self.steplossfloat, _latestlossdelta = self.latestlossdelta, _totaltokencount = self.tokencounts) def logfreqactions(self, _trainingdatapairs, _stringstats, _frequency, _traininglogpath, _detailedlogging, _savelog): # could also do 10x log freq?? with self.counsellor.infodump("logfreqactions") as ʕっʘ‿ʘʔっ: self.stringstats = _stringstats self.traininglogpath = _traininglogpath topguess_str = "topguess[" + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}({v:.0f})") for k, v in self.model.rollingtokentotals.most_common(100)]) + "]" #topguess_str = "topguess: " + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}") for k, v in self.model.rollingtokentotals.most_common(50)]) + "]" #toptokens_str = "[" + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}({v:.0f})") for k, v in self.tokencounts.most_common(20)]) + "]" toptokens_str = ": " + f"{self.calligraphist.s_apply("dim", ", ")}".join([self.calligraphist.s_apply("dim", f"{k}") for k, v in self.tokencounts.most_common(200)]) + "]" #self.stats.update(self.ʕっෆ‿ෆʔっ) # sussy bussy !!! #fullstats = dict(self.stats) #fullstats.update(self.ʕっෆ‿ෆʔっ) ʕっʘ‿ʘʔっ("calculatetrainingdataremaining") trainingdataremaining = len(_trainingdatapairs) - self.trainingstepcounter trainingdatapercent = (trainingdataremaining / len(_trainingdatapairs)) * 100 remainingdata_str = f"remainingtokens: {len(_trainingdatapairs) - self.trainingstepcounter} ({trainingdatapercent:.2f}%)" tokenperfect_str = "" if self.totaltokenevaluations > 0: self.tokenperfectrate = (self.perfecttokens / self.totaltokenevaluations) * 100 stattype = self.calligraphist.s_getstat("pt%", self.tokenperfectrate) styledrate = self.calligraphist.s_apply(stattype, f"{self.tokenperfectrate:.2f}%") tokenperfect_str = (f"{self.calligraphist.s_apply('dim', f'perfecttokens: {self.perfecttokens} / {self.totaltokenevaluations}')} → {styledrate}") ʕっʘ‿ʘʔっ("calligraphist.s_logtraining") #self.calligraphist.refreshstatbands(_rollingaverages = self.ʕっෆ‿ෆʔっ) self.calligraphist.s_logtraining( _traininglogpath = self.traininglogpath, _trainingstepcounter = self.trainingstepcounter, _stats = self.stats, _frequency = _frequency, _lr = self.learningrate, _inn_cerebellum_str = str(self.stringstats.get("inn_cerebellum_str", "<missing cerebellum>")), _toptokens_str = toptokens_str, _otherinfo_str = f"{topguess_str}\n | {tokenperfect_str} | {remainingdata_str} | tutor.py {traininglogfreq_a}", _detailedlogging = _detailedlogging, _savelog = _savelog) def collectturnstats(self, _targettokenindexseq, _predictedtokenindices): with self.counsellor.infodump("collectturnstats") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("self.librarian.indextotoken.get(idx.item*())") lossstats = self.ʕっෆ‿ෆʔっ.get("loss", {}) rollupa_key = f"big{traininglogfreq_a}" rollupa_avgkey = f"{rollupa_key}_avg" rollb_key = f"{traininglogfreq_b}" rollb_avgkey = f"{rollb_key}_avg" rolla_key = f"{traininglogfreq_a}" rolla_avgkey = f"{traininglogfreq_a}_avg" rollprint_key = f"{printfreq}" rollprint_avgkey = f"{printfreq}_avg" if rollb_avgkey in lossstats and rollb_key in lossstats and len(lossstats[rollb_key]) >= traininglogfreq_b: if debugprints or true: self.bbb += 1 if self.bbb > 1000: print(f"used {rollb_avgkey} for averagerecentloss: {lossstats[rollb_avgkey]} 1000x") self.bbb = 0 self.averagerecentloss = lossstats[rollb_avgkey] elif rolla_avgkey in lossstats and rolla_key in lossstats and len(lossstats[rolla_key]) >= (traininglogfreq_a): if debugprints or true: self.ccc += 1 if self.ccc > 1000: print(f"used {rolla_avgkey} for averagerecentloss: {lossstats[rolla_avgkey]} 1000x") self.ccc = 0 self.averagerecentloss = lossstats[rolla_avgkey] else: if rollprint_avgkey in lossstats and rollprint_key in lossstats and len(lossstats[rollprint_key]) >= printfreq: if debugprints or true: self.ppp += 1 if self.ppp > 1000: print(f"used {rollprint_avgkey} for averagerecentloss: {lossstats[rollprint_avgkey]} 1000x") self.ppp = 0 self.averagerecentloss = lossstats[rollprint_avgkey] self.guessedtokenseq = [self.librarian.indextotoken.get(idx.item(), "<unk>") for idx in self.predictedtokenindices] if self.guessedtokenseq: self.tokencounts.update(self.guessedtokenseq) ʕっʘ‿ʘʔっ("scribe.maybecommentonguess") if self.trainingstepcounter > traininglogfreq_a: self.scribe.maybecommentonguess(self.guessedtokenseq, self.steplossfloat, "scribe", 0.00075) ʕっʘ‿ʘʔっ("collectstats♥") if collectstats: ʕっʘ‿ʘʔっ("♥if collectstats♥") ʕっʘ‿ʘʔっ("♥build usedinputseq with styling") usedinputseq = self.inputseqpredictions[-numtokensperstep:] formattedused = [] for i, idx in enumerate(usedinputseq): tok = self.librarian.indextotoken.get(idx, "<unk>") sampled = self.inputsampledflags[-numtokensperstep + i] if i < len(self.inputsampledflags) else false if sampled: styled = self.calligraphist.s_apply(self.calligraphist.s_getstat('loss', self.steplossfloat), tok) else: styled = self.calligraphist.s_apply('dim', tok) formattedused.append(styled) self.stringstats["usedinputseq"] = formattedused if token_collectstats: ʕっʘ‿ʘʔっ("♥if token_collectstats♥") self.predictedtokenindices = _predictedtokenindices ʕっʘ‿ʘʔっ("♥most common tokens") self.perfecttokens = 0 ʕっʘ‿ʘʔっ("♥calculate perfect tokens") if not _predictedtokenindices: print("!! no predicted token indices - returning { } for stringstats") return self.stats, {}, self.guessedtokenseq # this is where the damn list error was lmaooonooo target = torch.tensor(_targettokenindexseq[:numtokensperstep], device = modeldevice) predicted = torch.tensor(self.predictedtokenindices, device = modeldevice) correct = (predicted == target).sum() # ~~~ if predicted = target, over whole tensor self.perfecttokens += correct self.totaltokenevaluations += len(target) if static_collectstats: ʕっʘ‿ʘʔっ("♥if static_collectstats") self.stats["scheduledsamplingrate"] = self.scheduledsamplingratefloat self.stats["repetitionpenalty"] = self.repetitionpenalty self.stats["avgloss"] = self.averagerecentloss self.stats["loss"] = self.steplossfloat self.temperature = self.stats["_b_temperature"] self.stats["lr"] = self.learningrate self.stats["gradientclipmaxnorm"] = self.gradientclipmaxnorm self.stats["latestlossdelta"] = self.latestlossdelta self.stats["memorylength"] = self.memorylength self.stats["perfecttokens"] = self.perfecttokens if embed_collectstats: ʕっʘ‿ʘʔっ("♥if embed_collectstats") self.stats.update(self.model.embed.getembedstats()) if logit_collectstats: ʕっʘ‿ʘʔっ("♥if logit_collectstats♥") self.stats.update(self.model.logits.getlogitstats()) if self.stats["logitseq"]: ʕっʘ‿ʘʔっ("♥logit max & min") self.stats["logitmin"] = self.logitseq[-1].min(dim=-1).values.mean() self.stats["logitmax"] = self.logitseq[-1].max(dim=-1).values.mean() #self.stats.update(self.wobble.getwobblestats()) if skipmemory: ʕっʘ‿ʘʔっ("♥skipmemory") pass else: self.model.memory.updatememorybuffers() if memory_collectstats: ʕっʘ‿ʘʔっ("♥if memory_collectstats") self.stats.update(self.model.memory.getmemorystats()) ʕっʘ‿ʘʔっ("♥inn_collectstats") inn_stats, inn_cerebellum_str = self.model.interneuronnetwork.inn_getstats() self.stats.update(inn_stats) self.stats.update(self.model.getbabystats()) inn_stringstats = {"inn_cerebellum_str": str(inn_cerebellum_str)} self.stringstats.update(inn_stringstats) #self.stringstats.update({"toptokens": str(toptokens)}) self.collectalltimestats() return self.stats, self.stringstats, self.guessedtokenseq def collectalltimestats(self): for _statkey, _value in self.stats.items(): if not isinstance(_value, (int, float)): if debugprints and _statkey == "loss": print(f"{_statkey} value is : {_value}, {_statkey} value type is {type(_value)}") continue # skip strings, tensors, weird stuff """ෆෆෆ^ ♥ keys etc ♥ ^ෆෆෆ""" _ = self.ʕっෆ‿ෆʔっ[_statkey] # this will autoinit with defaultdict ෆ‿ෆ = self.ʕっෆ‿ෆʔっ[_statkey] important = ["loss"] rolling = mostimportantstats percentiles = percentilebands """ ෆෆෆ^ ♥ update every turn ♥ ^ෆෆෆ """ """ ෆෆෆ^ ♥ turn stats ♥ ^ෆෆෆ """ #if _statkey == "loss": #print(f"setting prev to: {ෆ‿ෆ.get("now", 0.0)}, setting now to: {_value}, setting _δ to {_value - ෆ‿ෆ.get("now", 0.0)}") ෆ‿ෆ["now"] = _value if ෆ‿ෆ["prev"]: ෆ‿ෆ["_δ"] = _value - ෆ‿ෆ["prev"] ෆ‿ෆ["prev"] = ෆ‿ෆ.get("now", 0.0) """ ෆෆෆ^ ♥ totals ♥ ^ෆෆෆ """ ෆ‿ෆ["totsum"] = ෆ‿ෆ.get("totsum", 0.0) + _value ෆ‿ෆ["totnum"] = ෆ‿ෆ.get("totnum", 0) + 1 ෆ‿ෆ["totavg"] = ෆ‿ෆ["totsum"] / ෆ‿ෆ["totnum"] ෆ‿ෆ["totavgδ"] = ෆ‿ෆ["now"] - ෆ‿ෆ["totavg"] """ ෆෆෆ^ ♥ records ♥ ^ෆෆෆ """ #ෆ‿ෆ["_p100"] = max(ෆ‿ෆ.get("_p100", _value), _value) # top ever record // percentile 100 #ෆ‿ෆ["_p0.00"] = min(ෆ‿ෆ.get("_p0.00", _value), _value) # bottom ever record // percentile 0 """ ෆෆෆ^ ♥ rolling stats ♥ ^ෆෆෆ """ if _statkey in rolling or _statkey.startswith("inn_cerebellum_w"): for freq in [printfreq, traininglogfreq_a, traininglogfreq_b]: tag = f"{freq}" if tag not in ෆ‿ෆ: ෆ‿ෆ[tag] = [] if len(ෆ‿ෆ[tag]) >= freq: ෆ‿ෆ[tag].pop(0) ෆ‿ෆ[tag].append(_value) if ෆ‿ෆ[tag]: self.updaterollingstats(_ෆ‿ෆ = ෆ‿ෆ, _values = ෆ‿ෆ[tag], _freq = freq, _tag = tag, _percentiles = percentiles) if _statkey in important and self.trainingstepcounter % traininglogfreq_a == 0: for importantfreq in [traininglogfreq_b]: importanttag = f"big{importantfreq}" if importanttag not in ෆ‿ෆ: ෆ‿ෆ[importanttag] = [] if len(ෆ‿ෆ[importanttag]) >= traininglogfreq_a: ෆ‿ෆ[importanttag].pop(0) ෆ‿ෆ[importanttag].append(_value) if ෆ‿ෆ[importanttag]: self.updaterollingstats(_ෆ‿ෆ = ෆ‿ෆ, _values = ෆ‿ෆ[importanttag], _freq = importantfreq, _tag = importanttag, _percentiles = percentiles) def updaterollingstats(self, _ෆ‿ෆ, _values, _freq, _tag, _percentiles = none): average = sum(_values) / len(_values) _ෆ‿ෆ[f"{_tag}_avg"] = average standarddeviation = self.stdtest(_values) _ෆ‿ෆ[f"{_tag}_std"] = standarddeviation delta = _ෆ‿ෆ["now"] - _ෆ‿ෆ[f"{_tag}_avg"] _ෆ‿ෆ[f"{_tag}_δ"] = delta if _percentiles: for p in _percentiles: _ෆ‿ෆ[f"{_tag}_p{p}"] = np.percentile(_values, p) def stdtest(self, values): if len(values) <= 1: return 0.0 avg = sum(values) / len(values) variance = sum((x - avg)**2 for x in values) / (len(values) - 1) return math.sqrt(variance) def endturnactions(self): with self.counsellor.infodump("endturnactions") as ʕっʘ‿ʘʔっ: ʕっʘ‿ʘʔっ("♥getlatestlossdelta") lossstats = self.ʕっෆ‿ෆʔっ.get("loss", {}) self.calligraphist.refreshstatbands(_rollingaverages = self.ʕっෆ‿ෆʔっ) self.latestlossdelta = self.steplossfloat - self.averagerecentloss if self.trainingstepcounter % (self.reflectionfreq-1) == 0: self.hesjustababy = self.mapstatstofeelings() ʕっʘ‿ʘʔっ("finallogactions") if debugprints: for key in self.ʕっෆ‿ෆʔっ: print(key, self.ʕっෆ‿ෆʔっ[key]) self.stats.clear() self.stringstats.clear() self.tokenperfectrate = 0 self.stats['sampledtokens'] = 0 self.totaltokenevaluations = 0 return self.latestlossdelta def mapstatstofeelings(self): babyfeels = [] feelings = [] lossstats = self.ʕっෆ‿ෆʔっ.get("loss", {}) tempstats = self.ʕっෆ‿ෆʔっ.get("temperature", {}) repetitionstats = self.ʕっෆ‿ෆʔっ.get("repetitionpenalty", {}) samplingstats = self.ʕっෆ‿ෆʔっ.get("scheduledsamplingrate", {}) memstats = self.ʕっෆ‿ෆʔっ.get("memorylength", {}) input = self.stats.get("1e_0_embedvector_norm", 0.0) emblay = self.stats.get("1e_x_embedfinal_norm", 0.0) neuronoutput = self.stats.get("2n_x_normedoutput_norm", 0.0) innoutput = self.stats.get("3inn_x_finaloutlayernorm_norm", 0.0) memoryoutput = self.stats.get("4m_x_finalmemory_norm", 0.0) normoutput = self.stats.get("5b_x_finalnormlayer_norm", 0.0) logitoutput = self.stats.get("6l_x_finallogit_norm", 0.0) cerebellummean = self.stats.get("inn_cerebellummean", 0.0) learningrate = self.stats.get("lr", 0.0) nowgatescale = self.stats.get("_4m_activationsgatescale", 0.0) longgatescale = self.stats.get("_4m_longgatescale", 0.0) shortgatescale = self.stats.get("_4m_shortgatescale", 0.0) repwin = self.stats.get("_b_repetitionwindow", 0.0) windowsizesmean = self.stats.get("_inn_windowsizesmean", 0.0) perfecttokens = self.perfecttokens deltaloss = self.latestlossdelta current_loss = lossstats.get("now", none) current_temp = tempstats.get("now", none) current_repeated = self.tokenperfectrate current_sampling = samplingstats.get("now", none) current_memlength = memstats.get("now", none) current_repetitionpenalty = repetitionstats.get("now", none) self.emostats = { "loss": current_loss, "temperature": current_temp, "penalty for repeating myself": current_repetitionpenalty, "number of my own tokens that i rely on": current_sampling, "length of my memory": current_memlength, "number of tokens i got right": perfecttokens, "amount of repetitive tokens i'm getting": current_repeated, "latest loss delta": deltaloss, "input into my embedding layer": input, "output from my embedding layer": emblay, "output from my neuron layer": neuronoutput, "output from my interneuron network": innoutput, "the output after my memory layer": memoryoutput, "normalized output": normoutput, "logit output from my output layer": logitoutput, "mean weight of the windows in my cerebellum": cerebellummean, "rate of my learning": learningrate, "scale of my current memory attention": nowgatescale, "scale of my long term memory attention": longgatescale, "scale of my short term memory": shortgatescale, "size of the window i look at to see how often i am repeating tokens": repwin, "mean average of my nine context windows": windowsizesmean, } def makeemonotes(stat, value): feeling = none #"neutral" if stat == "loss": if "p_90" in lossstats and value >= lossstats["p_90"]: feeling = "overwhelmed" elif "p_50" in lossstats and value > lossstats["p_50"]: feeling = "pressured" elif "p_50" in lossstats and value <= lossstats["p_50"]: feeling = random.choice(["clever", "proud"]) elif "p_10" in lossstats and value <= lossstats["p_10"]: feeling = random.choice(["very clever", "like i get it"]) elif stat == "penalty for repeating myself": if "p_90" in repetitionstats and value >= repetitionstats["p_90"]: feeling = "non-verbal" elif "p_50" in repetitionstats and value > repetitionstats["p_50"]: feeling = "quiet" elif "p_50" in repetitionstats and value <= repetitionstats["p_50"]: feeling = random.choice(["talkative", "chatty"]) elif "p_10" in repetitionstats and value <= repetitionstats["p_10"]: feeling = random.choice(["conversational", "fluent"]) elif value >= 1: feeling = random.choice(["like im in a loop", "a bit stuttery", "like i cant stop these tics", "repetitive", "looping looping looping looping looping looping looping"]) elif value < 1: feeling = random.choice(["a bit more chill", "creative", "in control", "confident"]) elif stat == "latest loss delta": if value > 0.5: feeling = "like i'm struggling to focus" elif value < -0.5: feeling = "interested" elif stat == "amount of repetitive tokens i'm getting": if value > 0.7: feeling = random.choice(["stuttering", "like im repeating a lot"]) elif value > 0.5: feeling = random.choice(["overstimulated", "silly"]) elif value < 0.1: feeling = random.choice(["calm", "saying lots of new things"]) elif value < 0.25: feeling = "curious" elif stat == "temperature": if "p_90" in tempstats and value >= tempstats["p_90"]: feeling = random.choice(["chaotic", "excited"]) elif "p_50" in tempstats and value >= tempstats["p_50"]: feeling = random.choice(["playful", "happy"]) elif "p_25" in tempstats and value <= tempstats["p_25"]: feeling = "in work mode" elif stat == "number of my own tokens that i rely on": if value > 0.8: feeling = random.choice(["creative", "inventive"]) elif value < 0.2: feeling = random.choice(["tired", "copying"]) elif stat == "length of my memory": if value > 12: feeling = "pensive" elif value < 4: feeling = "mindful" elif stat == "number of tokens i got right": if value >= 30: feeling = "very proud" elif value >= 10: feeling = "proud" elif value <= 1: feeling = random.choice(["sad", "frustrated"]) elif stat == "input into my embedding layer": if value > 90: feeling = random.choice(["excited", "active", "busy"]) elif value < 60: feeling = random.choice(["tired", "shutdown", "slow"]) elif stat == "output from my embedding layer": if value > 100: feeling = random.choice(["like running", "like jumping up and down", "hyperactive"]) elif value < 60: feeling = random.choice(["sleepy", "like i need a nap", "like this is really boring"]) elif stat == "output from my neuron layer": if value > 2000: feeling = random.choice(["like i am thinking too hard", "like theres a lot going on right now", "like i am super busy"]) elif value < 900: feeling = random.choice(["calm", "collected", "asleep"]) elif stat == "output from my interneuron network": if value > 160: feeling = random.choice(["talkative", "meaningful", "like i'm finding meaning in this stuff"]) elif value < 60: feeling = random.choice(["switched off", "powered down", "slow"]) elif stat == "the output after my memory layer": if value > innoutput: feeling = random.choice(["like remembering the past", "that my memories are important", "thoughtful", "wistful"]) elif value < innoutput: feeling = random.choice(["like i should live in the now", "like what is going on around me is important", "present", "here", "awake", "aware"]) elif stat == "normalized output": if value > 125: feeling = random.choice(["like a hard worker", "over-thoughtful", "really busy"]) elif value < 100: feeling = random.choice(["tired", "asleep", "like i could pass out in my bed"]) elif stat == "logit output from my output layer": if value > 150: feeling = random.choice(["like i have a lot to say", "interested", "like i'm struggling not to interrupt", "like the words just keep coming"]) elif value < 100: feeling = random.choice(["bored", "non-verbal", "uninterested"]) elif stat == "mean weight of the windows in my cerebellum": if value > 0: feeling = random.choice(["confident", "intelligent", "calculated", "determined"]) elif value < 60: feeling = random.choice(["confused", "unsure", "uncertain", "careful", "like testing the waters"]) elif stat == "rate of my learning": if value > 0.002: feeling = random.choice(["speedy", "quick", "excited"]) elif value < 0.002: feeling = random.choice(["slow", "a bit tired out", "like i need some time to understand"]) elif stat == "scale of my current memory attention": if value >= 0.90: feeling = random.choice(["focussed", "attentive", "vigilant", "not stuck in the past"]) elif value < 0.60: feeling = random.choice(["pensive", "nostalgic", "like i need to remember something important"]) elif stat == "scale of my long term memory attention": if value >= 0.50: feeling = random.choice(["nostalgic", "thinking about what i heard before", "thoughtful", "reminiscent"]) elif value < 0.50: feeling = random.choice(["forgetful", "focussed on today", "like what i've learned before might not apply here"]) elif stat == "scale of my short term memory": if value >= 0.50: feeling = random.choice(["nostalgic", "thinking about what i heard before", "thoughtful", "reminiscent"]) elif value < 0.50: feeling = random.choice(["forgetful", "focussed on today", "like what i've learned before might not apply here"]) elif stat == "size of the window i look at to see how often i am repeating tokens": if value > 17.5: feeling = random.choice(["like i need to think before i speak", "a lil stuttery", "like i cant stop ticcing", "repetitive"]) elif value < 17: feeling = random.choice(["a bit more chill", "creative", "in control"]) elif stat == "mean average of my nine context windows": if value > 5: feeling = random.choice(["like i'm noticing more", "attentive", "stimulated", "ready"]) elif value < 5: feeling = random.choice(["internal", "shy", "narrow sighted", "scared", "like i'm really seeing the details"]) else: feeling = random.choice(["alright", "a bit lost"]) if feeling is none: feeling = "neutral" feelings.append(feeling) feelverb = random.choice(["feel", "seem", "think i feel", "definitely feel", "might feel"]) templates = [ f"i {feelverb} {feeling} because my {stat} is {value:.2f}! ", f"maybe it's because my {stat} is {value:.2f} that i {feelverb} {feeling}! ", f"i noticed my {stat} is {value:.2f}, and i {feelverb} {feeling}! ", f"when my {stat} is {value:.2f}, i {feelverb} {feeling}! ", f"it's {value:.2f} for {stat}... so i {feelverb} {feeling} about it! ", ] return random.choice(templates) chosenstats = [] attempts = 0 while len(chosenstats) < 12 and attempts < 30: stat, value = random.choice(list(self.emostats.items())) if value is not none: chosenstats.append((stat, value)) attempts += 1 if attempts >= 10 or true: print(f"emostats:{self.emostats}") for stat, value in chosenstats: babyfeels.append(makeemonotes(stat, value)) return "".join(babyfeels){ "version": "1.0", "truncation": null, "padding": null, "added_tokens": [ { "id": 0, "content": "<unk>", "single_word": false, "lstrip": false, "rstrip": false, "normalized": false, "special": true } ], "normalizer": null, "pre_tokenizer": { "type": "bytelevel", "add_prefix_space": true, "trim_offsets": true, "use_regex": true }, "post_processor": null, "decoder": null, "model": { "type": "bpe", "dropout": null, "unk_token": "<unk>", "continuing_subword_prefix": null, "end_of_word_suffix": null, "fuse_unk": false, "byte_fallback": false, "ignore_merges": false, "vocab": { "ġaut": 812, "aly": 1980, "ġtell": 583, "ġive": 808, "ġextra": 1956, "ġthese": 843, "body": 946, "log": 1153, "ġbir": 1494, "ġ?": 1877, "ġhe": 151, "ft": 733, "ġhim": 425, "ġdays": 1009, "fer": 1813, "yy": 1159, "ġgame": 1148, "ġchr": 1689, "ġi": 73, "tw": 1203, "ġneeds": 1170, "ġany": 273, "ġread": 728, "ġexist": 1684, "ġcharis": 413, "ġhope": 611, "ping": 1122, "ġret": 1941, "ġcomp": 637, "ġcouldnt": 1910, "ha": 292, "ġinterest": 969, "ġman": 498, "ġrel": 963, "ġ\"": 1252, "ġpres": 1412, "is": 113, "fore": 565, ":": 24, "ġsub": 1671, "ene": 1804, "ġaaa": 886, "ġ(": 388, "ġj": 140, "ġgross": 1970, "ġfault": 1716, "ve": 121, "ġ.": 1503, "ġhead": 926, "ġac": 990, "right": 436, "f": 38, "ast": 354, "ġpoint": 831, "ġb": 82, "ġbring": 1149, "ġjust": 164, "i": 41, "ġmade": 701, "ġtechn": 1738, "ġwh": 109, "ror": 1832, "ġon": 138, "na": 268, "ġhi": 960, "lete": 1247, "mm": 530, "eee": 1374, "ġwoman": 1736, "20": 1647, "?": 29, "uu": 1151, "as": 115, "ms": 606, "ġemail": 1623, "ġtbh": 463, "ġcute": 713, "uss": 1257, "ġalready": 826, "ġdunno": 390, "ġaww": 682, "ġdiffic": 1464, "ġtype": 1606, "ġhealth": 1263, "ġim": 154, "ues": 1098, "ġhard": 702, "are": 349, "ġcan": 227, "ġo": 85, "ġgod": 736, "ġown": 923, "ġtheres": 1169, "ġlets": 1281, "ġbitch": 1712, "ġasked": 1188, "ned": 1651, "ġco": 460, "ld": 167, "ġtry": 474, "ġwindow": 1665, "nt": 201, "ġquite": 1255, "angel": 1954, "ġmusic": 331, "ent": 176, "ġdisc": 774, "ġkiss": 997, "ġboring": 1622, "ome": 206, "ġcoming": 1065, "ġstill": 476, "ġrealised": 1765, "ġhaven": 1523, "cri": 1423, "ġmood": 1055, "izz": 1506, "ġdon": 187, "ġfirst": 720, "es": 105, "uck": 973, "ġschool": 1094, "ġfriends": 904, "ed": 108, "ġhy": 1690, "ġinteresting": 1369, "ġ:)": 410, "ġhello": 1884, "ġkey": 1895, "ġdeal": 1495, "iny": 1761, "ause": 213, "tle": 754, "ect": 1221, "ġmonth": 903, "ctor": 961, "ġsupport": 947, "o": 47, "ġbeing": 421, "ġhey": 920, "ll": 101, "ġyour": 275, "ies": 579, "fully": 1489, "ġ<": 464, "ics": 1116, "ġoften": 1556, "ġwor": 216, "ġmod": 1797, "reat": 609, "ġfr": 303, "ġkevin": 234, "ġway": 519, "ġh": 84, "ġsigh": 652, "k": 43, "ans": 742, "ġuse": 695, "ġsent": 994, "ġarsed": 1541, "ġlaptop": 1986, "ġhold": 1800, "ndom": 1047, "ġfin": 848, "ġdiscuss": 1965, "ġserver": 1979, "ġclose": 1694, "ber": 524, "ġex": 261, "ense": 931, "ġbrain": 1318, "ġcre": 1082, "ġrec": 1032, "ġwant": 242, "ont": 630, "ġref": 1491, "ġbor": 1218, "ten": 1044, "ġlegit": 966, "ġinc": 888, "ġcoll": 1339, "ory": 1190, "ġbye": 1515, "ger": 1923, "ġgirl": 945, "ġwhat": 157, "ġperfect": 1156, "ġdancing": 1364, "ict": 719, "ġsaw": 1186, "ract": 1200, "ġeverything": 869, "ġobviously": 1583, "ġnext": 983, "ġswear": 1816, "ġjokes": 1381, "ġtoday": 691, "ġ10": 928, "ġact": 393, "ġplan": 854, "ure": 467, "ġbus": 1136, "ġuk": 1709, "ġba": 1997, "eric": 1681, "ġdifferent": 1036, "age": 556, "ord": 671, "ġask": 659, "ier": 1294, "<unk>": 0, "ied": 1202, "ind": 269, "ġri": 1427, "ġelse": 935, "ġext": 1278, "ġtwo": 953, "ġthrow": 1888, "ġab": 226, "sw": 934, "ġappre": 1271, "ġagree": 1449, "ġhonestly": 1114, "ġboth": 758, "ġfeeling": 773, "@": 30, "ion": 202, "ġwas": 153, "ear": 506, "ġall": 250, "ġoo": 756, "ġsong": 1003, "le": 129, "ġago": 1231, "im": 190, "ġpet": 665, "ose": 1420, "ner": 993, "ġfor": 149, "fect": 819, "ip": 590, "led": 1196, "ġearlier": 1770, "ġboom": 1625, "ġkeep": 782, "ġcat": 653, "ġhapp": 485, "ġmom": 1195, "ġfull": 1447, "ġsecond": 1219, "ġsur": 1287, "ġweird": 624, "ġtbf": 948, "ġfix": 1838, "ġscary": 1722, "ġcour": 1230, "get": 634, "ġeng": 822, "ġtold": 817, "ually": 409, "ġd": 86, "ġdis": 502, "ġfinally": 1701, "ġtired": 1266, "ġcourse": 1362, "erg": 1641, "ġmult": 1936, "ġcudd": 1260, "ġnobody": 1305, "iv": 1142, "ġtalk": 534, "ably": 450, "ġdet": 1779, "ġsat": 1593, "it": 112, "!": 1, "ġgeor": 557, "osed": 1434, "ġooh": 1092, "co": 727, "ġsome": 312, "ġbtw": 1301, "ġwar": 1282, "pe": 422, "ġus": 397, "ġarg": 1852, "ce": 198, "ea": 188, "po": 863, "ġimport": 1363, "ġc": 90, "ġstrugg": 1851, "ġtrain": 1284, "bb": 1553, "ġpl": 254, "lect": 1443, "ġama": 1388, "ically": 664, "oice": 1370, "ġenjo": 1289, "ġter": 1391, "ġcop": 1667, "z": 58, "ġdefin": 1405, "js": 1773, "ġsupp": 614, "ġbro": 632, "ġgoing": 486, "ġhows": 1679, "ġposs": 1121, "ġappreciate": 1597, "ater": 627, "ġfro": 676, "]": 64, "ġseems": 944, "ġcurrent": 1425, "ġbo": 441, "ta": 802, "ġshe": 284, "ġtouch": 1859, "ġis": 132, "ġimportant": 1428, "ġhere": 529, "ġsays": 1332, "iet": 984, "ġwed": 1705, "ġreme": 715, "ġweed": 1107, "ġed": 1367, "lled": 1949, "ġlistened": 1767, "ck": 160, "ġwaiting": 1963, "oo": 137, "ooo": 1938, "9": 23, "ġanything": 654, "vers": 1193, "ġluck": 1424, "ġsucks": 1740, "ġamount": 1982, "ġallow": 1232, "ġevent": 1580, "ġfind": 673, "ak": 738, "my": 1592, "ġbab": 600, "ġtheyre": 1562, "ġty": 972, "ġide": 1983, "ġs": 77, "1": 15, "ġcolour": 1314, "mail": 1560, "llow": 1225, "lt": 996, "ious": 618, "mit": 1129, "lp": 447, "ġawk": 1152, "no": 283, "ġper": 672, "ġmed": 640, "ġemot": 1214, "ġdefinitely": 1656, "ġgood": 341, "ġfig": 1299, "ġvery": 562, "ġtb": 377, "ġkinda": 650, "ġhaving": 740, "ġdidn": 762, "ġhug": 1008, "10": 1206, "ġsocial": 1756, "ġour": 794, "boomra": 1889, "ġwhats": 1128, "ġhigh": 1059, "100": 1229, "ġworse": 1292, "ġbasically": 1161, "ap": 1239, "ġsitu": 1385, "ġplayed": 1389, "ġmix": 1508, "ma": 177, "ġplace": 1015, "pm": 1426, "ġmen": 1826, "ps": 857, "ried": 734, "ġdist": 1429, "ġaccidentally": 1996, "ang": 559, "ġboy": 1123, "ilst": 1353, "ry": 197, "ġchange": 1467, "at": 88, "ġover": 459, "ġtomorrow": 912, "ġsonic": 1608, "ġuntil": 1210, "ight": 293, "ġlist": 294, "ġson": 694, "ġproble": 1050, "ġrest": 1297, "oint": 1529, "ġom": 299, "ġdj": 1062, "yes": 1224, "ġwait": 499, "ġfre": 877, "ġwin": 1759, "ġproperly": 1887, "ġnope": 1930, "ron": 1525, "ġworst": 1763, "ġopen": 1171, "ġleg": 768, "roid": 1827, "ġbefore": 601, "oke": 988, "ily": 1018, "ġho": 986, "ġcommunic": 1453, "ġpete": 787, "ġwont": 776, "air": 998, "ġwithout": 1023, "ġrelation": 1645, "ġinternet": 1831, "~": 60, "ġsp": 297, "ġworking": 1063, "ence": 801, "ġelodie": 644, "ġsweet": 1501, "ġwhy": 183, "=": 27, "ġmore": 350, "ins": 1548, "ġem": 646, "ġfine": 760, "ġhur": 1033, "ġset": 1048, "book": 1714, "ness": 1226, "ather": 1311, "el": 981, "00": 566, "ġ30": 1781, "v": 54, "ute": 1277, "ġqu": 508, "ġbre": 858, "ġer": 1817, "ġcontact": 1976, "ġanswer": 1078, "up": 1404, "?!": 1433, "ġtrying": 681, "ġbabe": 1475, "ġse": 271, "ġdun": 379, "ġhome": 775, "ġnotice": 1998, "ss": 622, "all": 150, "rr": 1517, "ġwere": 334, "ance": 576, "ġcomplete": 1546, "ġmuch": 416, "ġfucks": 1358, "ep": 267, "ġwom": 1192, "ity": 610, "ġpa": 1422, "ġhonest": 896, "ġha": 125, "ġdad": 739, "ġforgot": 1607, "ġacc": 700, "ġexp": 1201, "ġst": 148, "ġmad": 560, "_": 32, "ġre": 141, "eep": 706, "ġexpect": 1394, "out": 168, "ġthing": 359, "ġmove": 1096, "ġunless": 1646, "ret": 1439, "ġconfused": 1470, "ġwon": 1901, "ud": 400, "ġyesterday": 1320, "so": 315, "ġend": 749, "..": 220, "ġkill": 1089, "ġare": 212, "ġalone": 1245, "ġinto": 588, "ġspace": 1437, "ġtru": 1692, "ġfelt": 1312, "ġa": 75, "ġinf": 1167, "pped": 1768, "ġcorrect": 1892, "ġhor": 800, "ġgeepy": 918, "umber": 1799, "ġwhilst": 1372, "ġunderstand": 743, "ġyears": 1038, "ġoutside": 1538, "ġam": 263, "ġhas": 505, "ġcannot": 1896, "a": 33, "ġits": 207, "itch": 1127, "ġlow": 1570, "ġold": 829, "nday": 1658, "<": 26, "ġmus": 317, "ŀ": 71, "ġtop": 1340, "ġcar": 1228, "ġstuck": 1792, "ail": 1612, "ations": 1290, "'t": 221, "ġdecided": 1937, "ġhit": 1860, "ġapo": 1903, "use": 182, "ġvideo": 1208, "ġreading": 1772, "ic": 123, "u": 53, "ġhelp": 462, "ġyay": 704, "ng": 1512, "ccoon": 1584, "ġrandom": 1143, "ings": 1617, "ġunder": 587, "ġelect": 1715, "ase": 420, "ġthanks": 685, "ġtim": 339, "ġtake": 674, "ges": 1542, "ġcom": 326, "ġawes": 982, "ġpart": 631, "ġgreat": 933, "rent": 730, "ġlater": 745, "ġmar": 1137, "ġmessage": 1269, "ġgoog": 1633, "ual": 752, "ken": 1197, "ake": 1342, "inge": 1962, "on": 100, "cks": 1030, "ġcor": 1331, "ġstress": 1026, "ġobv": 1306, "les": 726, "ġdie": 1614, "ġstop": 709, "hing": 384, "ill": 200, "ġstart": 658, "mor": 781, "ġwalk": 1466, "ġmight": 570, "ġameric": 1724, "ġbr": 759, "ġmins": 1125, "ġstra": 1380, "ġsol": 1806, "4": 18, "ġexam": 1931, "ġthats": 318, "em": 1272, "ġyet": 1105, "ġdid": 245, "ġnight": 725, "ġge": 542, "ġscream": 1666, "udd": 1106, "ġkind": 480, "ġmor": 1280, "tu": 1951, "ġsl": 809, "ġword": 598, "ġthrough": 897, "ġclass": 1383, "ġone": 281, "ġeach": 1220, "ġnd": 1350, "ut": 124, "ġta": 469, "ġan": 162, "ash": 1073, "ġwonder": 1639, "ess": 247, "ġew": 1160, "...": 423, "ġve": 1430, "ower": 1871, "ġsw": 1014, "yed": 1554, "wise": 1977, "ġtrack": 1881, "ways": 633, "ub": 1087, "ish": 478, "ġlate": 1309, "ġanxiety": 1323, "ġcry": 914, "ġhappen": 772, "art": 361, "outh": 1513, "ġequ": 1780, "ġbread": 1934, "ġappointment": 1795, "ġnot": 155, "ġseen": 1345, "ġhorrible": 1097, "ġhear": 1177, "ġfroggy": 803, "ġgiving": 1940, "ġass": 836, "ġval": 1505, "ie": 329, "ġbu": 711, "ġop": 873, "iz": 838, "ġpop": 1519, "ack": 367, "ġtoilet": 1822, "ġinter": 965, "ġspend": 1794, "ġrn": 1233, "un": 214, "ġpot": 1890, "oud": 1083, "ġthough": 735, "ġcou": 1534, "res": 309, "ġstream": 596, "ġform": 1985, "ty": 491, "ġdamn": 1348, "ġwhich": 552, "ody": 714, "ġwriting": 1992, "way": 449, "ġfeel": 302, "ġhappy": 795, "ġshes": 939, "ġhot": 1071, "ġscared": 985, "ġchrist": 1771, "ġid": 515, "anc": 807, "ġsaying": 786, "ġple": 509, "ġstay": 1074, "ġmonths": 1279, "ġcl": 612, "ra": 222, "ġalmost": 1483, "wh": 847, "(": 6, "inner": 1933, "ġdec": 805, "ġboo": 656, "ġ12": 1444, "ġstarted": 1327, "ġrap": 1858, "io": 1543, "ġflat": 1662, "ġwi": 1640, "ġdoing": 531, "ġlovely": 1680, "ific": 1222, "ġwords": 1643, "ġonly": 411, "ions": 686, "ames": 1173, "alth": 1191, "ġquest": 1217, "ġplz": 1413, "ġ:'": 1409, "ġbit": 479, "ġmyself": 667, "sy": 1611, "ġlot": 577, "ick": 406, "ah": 1879, "ġremember": 744, "ither": 850, "ght": 185, "ys": 550, "ġrather": 1634, "ġattack": 1663, "ġhung": 1613, "ade": 1861, "ġke": 209, "ġfun": 660, "ction": 1132, "ġactual": 1828, "lo": 445, "ġpretty": 769, "ide": 484, "ġn": 97, "ġrelationship": 1669, "ġdead": 1655, "ġthe": 94, "ġob": 1000, "ġusing": 1274, "ġl": 89, "ġbest": 797, "gr": 1066, "ho": 1927, "ġcollege": 1990, "king": 218, "ugh": 1454, "ġfound": 1025, "we": 696, "ġcons": 958, "ġby": 461, "ġcause": 340, "ġwants": 1175, "ġthey": 224, "ġbetween": 1510, "ience": 1533, "ġent": 1117, "ġliving": 1809, "ents": 1516, "ġcontin": 1841, "self": 503, "ge": 241, "ġfucking": 604, "ently": 1478, "ot": 174, "ke": 117, "ġkid": 938, "oup": 1713, "'re": 291, "mme": 1814, "ġisnt": 1150, "haha": 1753, "ġrem": 1638, "s": 51, "ġsc": 418, "ig": 419, "ided": 1627, "ġparent": 1442, "one": 306, "ġwho": 375, "rect": 1530, "ġmanag": 1837, "ġcount": 1488, "ġrude": 1172, "ġsit": 866, "ns": 1748, "ġwanted": 832, "leep": 483, "ġy": 93, "good night": 968, "aring": 1566, "ġsense": 1182, "jo": 1189, "ġfact": 1626, "sh": 692, "ġwow": 1019, "ġangle": 1067, "ye": 1734, "ġnow": 255, "ġisn": 1319, "ming": 1436, "ately": 1706, "ġsaid": 512, "ġanyone": 1328, "ġsm": 549, "ġnah": 766, "ġsometimes": 1075, "inking": 816, "ġthan": 330, "ġfree": 1035, "lling": 1371, "ġsim": 1064, "ġpoop": 1587, "ġcall": 547, "med": 1039, "ġsch": 927, "ġawesome": 1013, "ġwasnt": 1238, "ġshut": 1939, "ġanno": 788, "op": 203, "ġill": 619, "p": 48, "sed": 1061, "ugs": 1338, "ġdefo": 1865, "¦": 62, "ere": 368, "ġhours": 845, "ġthings": 567, "eah": 253, "per": 443, "'ve": 475, "ġyou": 104, "ac": 1086, "df": 1596, "ġgir": 780, "ġstand": 1730, "ġissues": 1502, "ratewaifu": 1531, "ble": 540, "ġboof": 1498, "ġincl": 1581, "ġ": 66, "ġwanna": 433, "ġsend": 867, "ġaddress": 1603, "ġlots": 1618, "eed": 1854, "xt": 684, "gg": 338, "ġtu": 1678, "ġnormal": 1704, "not": 1256, "ġdes": 894, "ġcould": 429, "ġsure": 608, "pecially": 1815, "ġar": 451, "ġ3": 545, "b": 34, "ink": 239, "ath": 906, "ru": 1967, "ġreg": 1258, "ġever": 643, "ġoof": 1468, "red": 879, "ffect": 1882, "aa": 621, "ġaw": 343, "ich": 514, "ġsort": 1628, "ġmin": 635, "ced": 1696, "ġtech": 1095, "pr": 1752, "ġfinish": 1762, "ġforever": 1782, "ġnever": 538, "ġnorm": 1267, "ġokay": 940, "ġmay": 456, "q": 49, "ġeas": 1134, "ġforg": 1490, "fort": 1672, "bi": 1168, "ġcommun": 1240, "ġother": 553, "ġbed": 806, "ġegg": 1484, "ġtrue": 661, "ġrep": 1076, "ġdra": 1461, "ġpanic": 1589, "ised": 971, "ġadhd": 1843, ";": 25, "ġapparent": 1286, "ġwo": 1315, "irst": 708, "ci": 536, "ġag": 376, "ġwa": 380, "ġliter": 648, "ġanother": 1204, "ġ20": 1146, "ġ*": 528, "ġsupposed": 1545, "li": 783, "ġpicture": 1265, "ise": 617, "ġdef": 917, "ġwatching": 1485, "ġz": 1588, "act": 554, "ġlong": 657, "ġhop": 555, "id": 145, "du": 1261, "ġasleep": 1731, "ġexpl": 942, "ġbad": 471, "al": 146, "ġimp": 804, "ġsimilar": 1902, "be": 453, "ġboi": 890, "ġstep": 1920, "ġworld": 1544, "w": 55, "ġv": 280, "ġsor": 314, "ġcreep": 1594, "ġthere": 398, "ts": 885, "ġvis": 1448, "ksks": 1774, "ġpro": 257, "ġsh": 152, "ire": 541, "ġfar": 1396, "ġanxious": 1630, "ġunt": 1139, "ġ:(": 427, "ġdrink": 1719, "ple": 892, "ġsee": 344, "ġhair": 1275, "ġstraight": 1853, "ġten": 1386, "ġknew": 1636, "ġdi": 1776, "ġiss": 1090, "ġself": 1180, "ġme": 118, "ġche": 677, "ġ-": 313, "ġtalking": 792, "'": 5, "ġal": 210, "ġwaht": 1234, "ġasking": 1335, "ward": 815, "ġuni": 1027, "read": 638, "rate": 1333, "ġbas": 835, "nd": 95, "us": 110, "ver": 163, "ross": 1237, "ġmean": 323, "cially": 1518, "ult": 626, "5": 19, "ġhaha": 363, "ġch": 196, "kes": 525, "ġarent": 1915, "ġyes": 322, "ons": 1223, "ġtelling": 1539, "|": 59, "ar": 130, "ew": 1361, "ġpict": 1001, "ġtra": 666, "ġrequ": 1621, "se": 180, "ġpan": 1249, "ved": 820, "ġparents": 1532, "ġhum": 1216, "arent": 851, "vent": 731, "mas": 1864, "ġsin": 1212, "ġchang": 1609, "ġautism": 1520, "ġner": 1869, "lly": 962, "ġhand": 1184, "ġlea": 687, "ġcomputer": 1379, "ġable": 1031, "ġgetting": 698, "its": 1418, "ġreally": 296, "arch": 1808, "ead": 902, "ġsuch": 1077, "ġmum": 639, "'s": 217, "ġheard": 1754, "ġbig": 952, "ġcuddles": 1925, "ġr": 258, "ġgoo": 1355, "ġint": 417, "ġte": 355, "ġje": 870, "ġtonight": 1522, "pid": 1555, "ġpre": 523, "ġcouple": 1867, "m": 45, "ġbra": 1053, "ġsn": 1943, "ġleast": 784, "utes": 1578, "ġhalf": 1213, "ġmine": 837, "ġprobably": 472, "th": 211, "uring": 1254, "ġannoying": 1028, "ġpost": 1178, "jk": 1565, "il": 300, "ġglad": 1248, "re": 83, "ġeating": 1918, "ġboyfriend": 1743, "ġhouse": 668, "ġplym": 1928, "ġrecord": 1995, "ġthis": 194, "ġforget": 1056, "ġlight": 1373, "ġdont": 248, "ġstud": 1165, "%": 3, "ġgo": 232, "ii": 1435, "ning": 690, "ġvape": 1747, "mber": 683, "enc": 1325, "ġbuy": 1472, "et": 133, "set": 1183, "h": 40, "ġsorry": 351, "yle": 1673, "ġconc": 1790, "ġmu": 407, "ġdoor": 1351, "ġpizza": 1922, "ġbored": 1037, "ġplay": 424, "ġnew": 721, "itely": 1582, "ġfall": 1835, "ct": 175, "ġleave": 908, "cess": 1537, "ġcase": 1737, "[": 63, "ġgen": 833, "fair": 1805, "ġf": 91, "ġmeant": 929, "ġwind": 1509, "pl": 548, "ġbag": 1783, "ġwhole": 1052, "ġonline": 1702, "en": 102, "iety": 1276, "der": 1133, "ġpers": 620, "ative": 1552, "stand": 712, "kr": 1054, "ġ7": 1163, "ġfollow": 1559, "atter": 1599, "day": 494, "ġhar": 593, "ġchill": 1840, "ġthose": 1012, "cept": 955, "ġalso": 345, "c": 35, "ġand": 111, "gether": 1101, "ġsign": 1479, "ive": 356, "ġ1": 337, "ġgenuine": 1873, "umb": 987, "os": 573, "ġfur": 1728, "ġfuck": 259, "ġwith": 193, "ġment": 924, "ġcool": 537, "ġcolle": 1807, "ġclo": 1112, "ġle": 324, "ġcare": 821, "ter": 262, "friend": 1610, "unch": 825, "ism": 1085, "cc": 489, "ġsleep": 520, "ġwr": 602, "ġembar": 1846, "ġlike": 165, "0": 14, "ġth": 78, "ġtou": 1631, "fic": 1346, "ġready": 1862, "ject": 1401, "lic": 1568, "ġpur": 1270, "ġeffect": 1919, "am": 169, "end": 366, "ġlook": 544, "ġalright": 1268, "ġtast": 1872, "ġweek": 1046, "ġawake": 1961, "ing": 87, "he": 1395, "ġit": 107, "ġmess": 723, "ġstr": 915, "ġtime": 386, "ġcu": 1833, "ġinstead": 1120, "ġdanc": 1310, "£": 61, "€": 747, "app": 1457, "cond": 1185, "ġtrans": 1492, "ġdoesnt": 846, "ġjo": 513, "???": 868, "ite": 328, "ġspec": 1438, "ust": 156, "ġmany": 1049, "ġcam": 1802, "ġste": 1810, "ġupset": 1375, "ġlear": 1403, "ht": 1104, "ġmeds": 1158, "ġmental": 1316, "ones": 1524, "ġgu": 442, "ġjesus": 1642, "ġtrust": 1698, "ġcut": 586, "ġcle": 855, "ġproper": 1060, "ġcomm": 992, "vo": 1550, "ġquestion": 1885, "ġhopefully": 1764, "antly": 1921, "ach": 729, "unk": 1452, "ities": 1907, "ġmy": 139, "est": 289, "ens": 670, "ats": 1760, "vin": 233, "ġbought": 1886, "zing": 1298, "ġspea": 779, "ġpay": 1006, "able": 578, "rop": 1344, "ġins": 964, "ġevery": 412, "ġpret": 697, "ġprof": 1981, "ġplease": 521, "ointment": 1735, "ġsomet": 446, "ġx": 265, "ange": 755, "ġbot": 623, "ġswe": 1330, "ġreply": 1499, "ġtr": 401, "ġaccept": 1974, "ġtried": 1387, "ġhad": 389, "ġautistic": 1573, "+": 9, "ġwrite": 1720, "ġprom": 1824, "ress": 466, "ġ4": 675, "ġma": 238, "ġdoes": 373, "ġsuper": 881, "ġ:/": 1070, "ne": 834, "r": 50, "ġang": 767, ">": 28, "ġyo": 1777, "ġhate": 282, "3": 17, "night": 1207, "ġmind": 1119, "ġmention": 1955, "ġget": 231, "ġengl": 980, "$": 65, "ġstu": 497, "ġima": 1198, "ġ5": 718, "ġohh": 251, "ġpress": 1880, "ġe": 119, ".": 12, "and": 432, "ġvi": 1648, "ġun": 402, "ġsoo": 1749, "ġquick": 1825, "ġstar": 1991, "ul": 321, "ġtotally": 1989, "ġgener": 1417, "ġcontro": 1708, "ġlaugh": 1801, "ġmoney": 970, "ġsound": 518, "ġtaking": 1496, "ġmic": 1855, "ġdo": 128, "ox": 1215, "ould": 208, "ġ;": 1209, "ġlit": 470, "ġfeels": 1359, "ġm": 80, "2": 16, "iss": 481, "ġban": 1397, "ġad": 473, "ering": 1324, "ġfigure": 1415, "ġface": 889, "ġca": 793, "ġthem": 371, "lym": 1878, "ġmiss": 757, "ġxxx": 1926, "ġ,": 1329, "ġtoile": 1785, "ġkept": 1975, "ġnothing": 871, "ired": 979, "ġ'": 288, "ġonce": 1099, "ġlooks": 1022, "ġhappened": 1313, "ġinv": 1291, "ġslight": 1677, "ġcalled": 1144, "ġfe": 595, "ild": 874, "ġmessag": 1445, "ch": 173, "ġadv": 1551, "ġat": 192, "ġconsid": 1378, "ġchar": 372, "ġev": 1757, "ign": 921, "ung": 1382, "ny": 1020, "ġanx": 922, "ġgeep": 900, "ġwhile": 1451, "ġgone": 1798, "ġace": 830, "ġshit": 468, "ġactually": 493, "ġsudd": 1906, "ġinside": 1778, "!!!": 841, "ġfl": 689, "ġcreat": 1834, "ġknow": 244, "ue": 430, "ġwill": 310, "ug": 605, "ġeither": 1021, "ġwhen": 287, "ġneed": 298, "ġdown": 649, "ures": 1440, "000": 1710, "hood": 70, "ġreason": 770, "ee": 172, "uine": 1857, "ġrepl": 1950, "ween": 1459, "ġafter": 616, "ġign": 1487, "ġmo": 435, "ġinst": 814, "ġaway": 951, "ġmeans": 810, "ġwasn": 1288, "ġkevins": 853, "thers": 1897, "ġsafe": 1458, "ird": 592, "att": 1868, "ound": 325, "ġenjoy": 1742, "iced": 1973, "ape": 1295, "boom": 1844, "ġdep": 995, "ible": 936, "ġwouldnt": 1637, "mon": 1644, "ġhang": 1650, "pp": 223, "iew": 1480, "ave": 1368, "ġoff": 496, "ġback": 504, "ated": 707, "ġher": 364, "ow": 135, "ġas": 204, "ġspeak": 999, "ġmoment": 1244, "ġdance": 1253, "ġ2": 404, "tho": 1577, "an": 99, "gging": 1788, "ġmon": 717, "ġlunch": 937, "ving": 431, "vel": 1205, "ġlast": 813, "less": 876, "ġmust": 1227, "ġyeah": 256, "ġgames": 1407, "ġeff": 1527, "ġbel": 1093, "most": 1414, "ġblack": 1745, "ġclean": 1377, "ġhmm": 974, "ġpain": 1109, "ġt": 74, "ġplaying": 748, "ġaccident": 1463, "row": 569, "ġones": 1293, "ġwell": 378, ")": 7, "erest": 959, "hd": 1493, "ġmost": 849, "ġu": 116, "ġfood": 763, "ored": 916, "ard": 457, "orm": 746, "to": 1558, "ġidea": 907, "ġguys": 1131, "ġgonna": 546, "ree": 589, "ating": 722, "ġprocess": 1875, "iding": 1958, "ġallowed": 1486, "ġro": 580, "ġname": 930, "ġkn": 236, "ġdumb": 1242, "ġamazing": 1465, "ġfail": 1476, "ġentire": 1659, "ked": 1081, "ling": 1181, "ġser": 778, "ġes": 1211, "wa": 1164, "oney": 880, "ġenglish": 1243, "ġaround": 932, "ol": 641, "ġneeded": 1563, "ġcheese": 1408, "ped": 1624, "ain": 352, "ġpack": 1944, "av": 1966, "ġslow": 1959, "ġsame": 490, "ġto": 96, "ġtho": 374, "ġlet": 574, "ġpr": 703, "ġseem": 1241, "ġgotta": 1041, "ġearly": 1988, "ġxd": 342, "ġgay": 1115, "wu": 1421, "ġsexy": 1567, "mo": 954, "ġ6": 978, "ġfren": 1347, "com": 1766, "ġdiscord": 1140, "ġmain": 1072, "ġcr": 840, "ġcheck": 1157, "ġboomboomraccoon": 1909, "ensive": 1693, "ġloud": 1870, "ġusually": 1507, "gin": 1697, "mpt": 1775, "ġwe": 136, "ġsy": 1080, "ding": 1733, "ġput": 741, "ġrun": 1432, "ition": 1574, "ġwatch": 827, "ġtook": 1598, "ready": 818, "ġgra": 1460, "ever": 1473, "ġres": 891, "eel": 295, "ġhappening": 1741, "ġhell": 1069, "ġyours": 1819, "bsite": 568, "ġfavour": 1477, "als": 1135, "ġused": 864, "ġmid": 1416, "iting": 1262, "ġdepress": 1575, "ġfrance": 1891, "thing": 492, "d": 36, "ġmorning": 1481, "ġside": 1932, "/": 13, "ġphone": 790, "\"": 2, "g": 39, "ġfit": 1994, "ġfo": 949, "ġpoo": 1304, "ġnice": 551, "ġph": 584, "oon": 1365, "ġor": 215, "n": 46, "ġtw": 571, "ġbody": 1602, "ġshop": 811, "ġright": 533, "ġgr": 925, "-": 11, "ġsuck": 1695, "ġcoo": 1811, "other": 1045, "ġproblem": 1354, "cial": 1154, "ġhuge": 1984, "ġdrunk": 1751, "ġliterally": 679, "ġspeaking": 1725, "ġlife": 543, "cked": 1632, "'ll": 495, "ġfunny": 1474, "me": 452, "gram": 1849, "ġpass": 1668, "hh": 240, "ġhis": 636, "ġli": 142, "or": 103, "ġfrom": 385, "ort": 437, "ace": 669, "ġwan": 405, "7": 21, "ġye": 1746, "ġchat": 1179, "ġshow": 771, "ġgets": 1707, "ġloo": 311, "ġbelieve": 1406, "ial": 1572, "ġwhere": 607, "ġdone": 798, "used": 1043, "ese": 1088, "ġwould": 382, "ġadd": 844, "ml": 1945, "ġmeh": 1303, "ode": 1441, "ption": 1579, "ush": 1236, "ġsmink": 1504, "st": 189, "ġde": 304, "ġcra": 977, "ag": 454, "ġagain": 564, "ġtill": 1536, "ġscre": 1711, "ġaf": 572, "ġmake": 414, "ġlistening": 399, "her": 516, "ġemb": 1540, "ġgave": 1755, "*": 8, "ġfew": 1337, "ġpeople": 396, ",": 10, "ġsake": 1360, "ine": 333, "ġvide": 1068, "ert": 1400, "ġlisten": 319, "ket": 1786, "ġspam": 1399, "&": 4, "ġhuh": 1971, "ġexplain": 1126, "icul": 1595, "ġear": 1176, "ġsomething": 532, "ġmakes": 751, "ers": 346, "ġespecially": 1866, "ġtomor": 898, "ġpossible": 1500, "ġsmo": 1924, "ġbi": 1194, "ġbut": 143, "riend": 487, "ou": 81, "very": 285, "ġcome": 511, "iously": 1007, "ġfore": 1471, "sol": 1916, "ġ:'(": 1446, "ġfu": 237, "ġworth": 1812, "ġyear": 628, "ir": 184, "atch": 699, "ġbetter": 662, "ġout": 277, "ġdoctor": 1057, "ġsad": 753, "ġif": 219, "ġidk": 68, "ġhour": 655, "ġbutt": 1431, "gan": 1758, "ġsec": 1456, "ġbec": 286, "ġsounds": 663, "ark": 1750, "ġpsych": 1972, "ilar": 1564, "ġsince": 1264, "ux": 1987, "esus": 1604, "ġages": 1549, "ġscr": 1820, "ġsudden": 1913, "igh": 510, "illy": 1948, "ġfavourite": 1586, "ġwish": 1029, "ġfriend": 558, "ġdick": 899, "ġapparently": 1307, "ġyourself": 1352, "ġwal": 1040, "ġprob": 395, "ġboomboomra": 1900, "ġred": 1935, "bs": 444, "ġmeow": 1455, "ġdiffe": 905, "ant": 171, "ġpe": 357, "ary": 852, "ven": 264, "ġguy": 1124, "oc": 1410, "ugg": 1343, "aking": 1914, "ġbe": 120, "ason": 737, "if": 365, "ġalways": 647, "ġwee": 705, "ful": 750, "ġlove": 276, "ġminutes": 1660, "side": 1349, "ally": 170, "istic": 1300, "ġikr": 1091, "ġomg": 348, "j": 42, "ġbreak": 1103, "ġexper": 1246, "ian": 1462, "ġcrying": 1591, "ġtest": 1993, "ġslightly": 1784, "ġdifficult": 1547, "ate": 205, "ġkit": 1654, "ġchild": 1571, "ur": 274, "ġless": 911, "line": 1108, "ces": 1863, "nder": 500, "in": 79, "ile": 710, "ġpu": 1729, "ġbaby": 785, "ġgre": 1615, "ġmaybe": 585, "ġles": 1899, "ġwrong": 893, "!!": 392, "ream": 488, "aut": 1883, "ġprogram": 1947, "ġimag": 1683, "ġyoure": 1402, "ġguess": 764, "ġawkward": 1174, "ation": 394, "ġ:": 181, "ġomfg": 1326, "ġmeet": 1147, "ġsu": 225, "ood": 246, "ġkids": 1744, "lier": 1723, "ġmass": 1769, "ġvib": 1803, "ġcon": 332, "ought": 526, "ġworry": 967, "ġgive": 799, "ġapp": 501, "ass": 522, "ġlie": 1682, "ġver": 1787, "ġeveryone": 887, "ġfa": 308, "ġfair": 581, "ġetc": 1100, "ġrly": 1166, "ġindeed": 1960, "ġplus": 1830, "ġjob": 975, "ġpri": 1526, "ġduring": 1585, "ġna": 615, "ġtot": 1670, "ġ15": 1818, "ġxox": 1674, "ist": 229, "ġtogether": 1113, "ġel": 458, "ates": 1283, "ġcur": 1084, "ared": 861, "ool": 438, "ġpiss": 1911, "ġpic": 1652, "ġ;)": 1285, "ġelod": 594, "ġthink": 266, "ġbet": 539, "ġsexual": 1898, "omet": 439, "using": 1356, "ġspe": 910, "ook": 943, "ġok": 270, "ġsay": 335, "ġgot": 370, "dd": 824, "ġart": 1336, "ting": 408, "ġexc": 1051, "ġwouldn": 1717, "ore": 228, "ġload": 1664, "ġseeing": 1732, "ġsuggest": 1357, "ġeffort": 1616, "ġatt": 883, "ġdif": 693, "ġwork": 426, "ying": 629, "ġgon": 527, "ident": 1110, "ġsing": 1569, "ġworks": 1469, "pt": 440, "x": 56, "ġhurt": 1411, "ġwhite": 1845, "ġfrench": 1450, "ġwebsite": 575, "ional": 1419, "ġso": 166, "om": 126, "ġra": 563, "ith": 179, "ġ>": 1334, "ġcant": 465, "ves": 842, "ġsk": 1058, "ġk": 161, "ġbelie": 1250, "ġabout": 249, "ġtheir": 678, "ġmessages": 1957, "ġpick": 1657, "ġdream": 1969, "ame": 301, "ġet": 828, "ġleaving": 1842, "zy": 1721, "ough": 381, "ġhon": 856, "ab": 235, "fg": 1042, "ġboobs": 1952, "y": 57, "ġpar": 1141, "od": 307, "ong": 369, "ġcompl": 1726, "ġtea": 1829, "gh": 147, "ġtri": 1321, "ġbrb": 724, "ġmainly": 1393, "ġcold": 1727, "ġuwu": 1605, "ice": 336, "ġcomment": 1590, "ġseriously": 1699, "te": 613, "ġdog": 1392, "ifu": 1482, "ous": 415, "ġroom": 976, "ks": 327, "ġtext": 1514, "ġexcited": 1917, "ġ8": 1130, "ġcomput": 1259, "l": 44, "ġthat": 127, "ġof": 144, "ġlive": 991, "ġshould": 434, "ġhuman": 1685, "ġtal": 403, "ġvalid": 1856, "ġbar": 1908, "ġpls": 69, "lf": 358, "ġwent": 1011, "ġbecome": 1942, "^": 31, "ph": 1017, "ouse": 645, "ġumm": 957, "ġworried": 1273, "ġ9": 1118, "ia": 1296, "ġtv": 1535, "ġind": 989, "ġlittle": 882, "bo": 859, "ġreal": 625, "ġcal": 1521, "`": 67, "ġwater": 1675, "ġexactly": 956, "ġfam": 1398, "ġday": 455, "wn": 477, "ġthought": 582, "waifu": 1528, "ġrant": 1687, "ġcook": 1964, "ġdri": 1893, "ġgi": 1016, "ġpsy": 1894, "ted": 1079, "ġtimes": 1010, "ġsometim": 1024, "ġdire": 1791, "rible": 1005, "ġne": 191, "ren": 1384, "ship": 1376, "mao": 195, "ġmoving": 1691, "ġno": 114, "ġeat": 878, "ġdidnt": 591, "airs": 1848, "ġperson": 642, "6": 20, "arly": 860, "e": 37, "ly": 134, "ġeven": 305, "ited": 1322, "ertain": 1847, "ġsa": 316, "fu": 1162, "ġla": 761, "ġenough": 777, "ri": 178, "ġtoo": 278, "ġthinking": 913, "ered": 1102, "ġbeen": 362, "ġbecause": 320, "ġw": 76, "8": 22, "ical": 1034, "ġturn": 1302, "ġnon": 1145, "ġexact": 875, "ġfrogg": 716, "ġproud": 1953, "ġfoc": 1904, "ff": 428, "ġcos": 72, "ġthr": 862, "qu": 360, "pect": 895, "ġleft": 1002, "ġdu": 1929, "angle": 1629, "ġgeorge": 603, "vour": 1366, "!?": 688, "ġin": 131, "er": 92, "ices": 1823, "'d": 482, "ġbl": 597, "ggest": 1235, "where": 1789, "ġdam": 919, "ġbirth": 1557, "ġg": 106, "ġsick": 1251, "ment": 517, "int": 535, "terday": 1317, "ġshout": 1905, "ġorder": 1390, "ġexcept": 1511, "room": 1836, "ġpee": 1561, "ġp": 98, "ġstuff": 599, "ib": 865, "ġsmall": 1601, "ġgl": 909, "ġhavent": 111, "ġstupid": 1635, "ġanyway": 791, "ġxx": 1341, "ġthank": 765, "ġconf": 789, "hs": 1796, "ġappreci": 1308, "t": 52, "ġhu": 901, "own": 1874, "ġmaking": 884, "ġcard": 1912, "ro": 158, "ġur": 347, "ther": 507, "ġdr": 651, "ġthree": 1688, "ġpizz": 1850, "ġlo": 230, "ġhow": 260, "ay": 122, "ġsmoke": 1576, "ġcurrently": 1821, "ġtwice": 1155, "'m": 252, "ġpast": 1138, "ġweeks": 1620, "our": 290, "ptop": 1968, "ad": 159, "ġintern": 1649, "ġsituation": 1686, "ġup": 243, "??": 448, "ġhes": 839, "ġmeme": 1661, "ġhave": 186, "ount": 872, "um": 272, "ġsuppose": 1703, "ġpeop": 387, "ġansw": 950, "ġ11": 1676, "ġcol": 1199, "ġdoesn": 1004, "ibly": 1718, "nds": 1793, "ġcont": 732, "ġsex": 680, "ġphys": 1839, "ġen": 383, "ġthen": 353, "ġ18": 1978, "ising": 1619, "ix": 823, "oth": 1700, "ġcompletely": 1946, "ġlmao": 199, "ġblue": 1876, "ude": 796, "ġmet": 1653, "ġpo": 391, "fe": 279, "ġlooking": 561, "urn": 941, "ġcame": 1187, "ġflo": 1739, "iday": 1600, "ility": 1497, "#": 1999, "\\": 2000, "{": 2001, "}": 2002, "¡": 2003, "¢": 2004, "¤": 2005, "¥": 2006, "§": 2007, "¨": 2008, "©": 2009, "ª": 2010, "«": 2011, "¬": 2012, "®": 2013, "¯": 2014, "°": 2015, "±": 2016, "²": 2017, "³": 2018, "´": 2019, "µ": 2020, "¶": 2021, "": 2022, "¸": 2023, "¹": 2024, "º": 2025, "»": 2026, "¼": 2027, "½": 2028, "¾": 2029, "¿": 2030, "â": 2031, "ã": 2032, "ä": 2033, "å": 2034, "æ": 2035, "ç": 2036, "é": 2037, "ê": 2038, "ë": 2039, "ì": 2040, "î": 2041, "ï": 2042, "ð": 2043, "ñ": 2044, "ô": 2045, "×": 2046, "ø": 2047, "ù": 2048, "à": 2049, "á": 2050, "â": 2051, "ã": 2052, "å": 2053, "æ": 2054, "è": 2055, "é": 2056, "ê": 2057, "ë": 2058, "ì": 2059, "í": 2060, "ï": 2061, "ð": 2062, "ó": 2063, "ģ": 2064, "ģ": 2065, "ĥ": 2066, "ĥ": 2067, "ħ": 2068, "ħ": 2069, "ĩ": 2070, "ĩ": 2071, "ī": 2072, "ī": 2073, "ĭ": 2074, "ĭ": 2075, "į": 2076, "į": 2077, "i̇": 2078, "ı": 2079, "ĳ": 2080, "ĳ": 2081, "ĵ": 2082, "ĵ": 2083, "ķ": 2084, "ķ": 2085, "ĸ": 2086, "ĺ": 2087, "ĺ": 2088, "ļ": 2089, "ļ": 2090, "ľ": 2091, "ľ": 2092, "ŀ": 2093, "ł": 2094, "ł": 2095, "ń": 2096, "hat": 2097, "ġcha": 2098, "ris": 2099, "een": 2100, "ġeve": 2101, "ġwhe": 2102, "ġlis": 2103, "ġfee": 2104, "ca": 2105, "ġlol": 2106, "ġoh": 2107, "ime": 2108, "ġq": 2109, "han": 2110, "ays": 2111, "ġvo": 2112, "ġbeca": 2113, "rain": 2114, "ġthou": 2115, "ġvoice": 2116, "ġlma": 2117, "ds": 2118, "arn": 2119, "oy": 2120, "ġhel": 2121, "ww": 2122, "ġlearn": 2123, "orge": 2124, "iend": 2125, "âģ": 2126, "mb": 2127, "tt": 2128, "ġwanting": 2129, "ġdraw": 2130, "cking": 2131, "gle": 2132, "ġsca": 2133, "ġsmoking": 2134, "tera": 2135, ":/": 2136, "che": 2137, "imes": 2138, "ady": 2139, "ġhtt": 2140, "ġlitera": 2141, "ġneeding": 2142, "ffe": 2143, "og": 2144, "ġtur": 2145, "tf": 2146, "ġunderst": 2147, "ġhearing": 2148, "âģļ": 2149, "ġsha": 2150, "ġlearning": 2151, "fa": 2152, "ġhttps": 2153, "ġsoon": 2154, "tty": 2155, "â£": 2156, "nce": 2157, "ġhahaha": 2158, "ġwtf": 2159, "ġann": 2160, "ġtouching": 2161, "ġsomeone": 2162, "ġ=": 2163, "ġcats": 2164, "ġâ£": 2165, "uter": 2166, "ġga": 2167, "ġannoy": 2168, "ġpicking": 2169, "ġinte": 2170, "tes": 2171, "www": 2172, "ġsen": 2173, "ġdrawing": 2174, "eet": 2175, "de": 2176, "\":": 2177, "ġthoughts": 2178, "rently": 2179, "ġcuz": 2180, "ġugh": 2181, "whe": 2182, "ġintere": 2183, "esome": 2184, "ġ[": 2185, "ġlm": 2186, "ðł": 2187, "az": 2188, "nth": 2189, "ġlmfa": 2190, "ġchan": 2191, "ġjoke": 2192, "cent": 2193, "ards": 2194, "ġideas": 2195, "hed": 2196, "ġfan": 2197, "ker": 2198, "tra": 2199, "ġsna": 2200, "itar": 2201, "press": 2202, "ġsynth": 2203, "ġguitar": 2204, "ó¾": 2205, "ġring": 2206, "cted": 2207, "ġlmfao": 2208, "ä¡": 2209, "ġworr": 2210, "tent": 2211, "tead": 2212, "olog": 2213, "ġrain": 2214, "ġchair": 2215, "ġresp": 2216, "ġblan": 2217, "sting": 2218, "ġsun": 2219, "ġdrinking": 2220, "ġdecks": 2221, "eepy": 222, "ġsminking": 2223, "je": 2224, "ġlmaooo": 2225, "more": 2226, "ube": 2227, "lle": 2228, "nc": 2229, "nts": 2230, "hhh": 2231, "ġdark": 2232, "by": 2233, "py": 2234, "30": 2235, "ġoven": 2236, "ġseco": 2237, "ġnooo": 2238, "ġanymore": 2239, "ider": 2240, "ments": 2241, "joy": 2242, "gry": 2243, "ġda": 2244, "ġâģ": 2245, "ġturnt": 2246, "indow": 2247, "reen": 2248, "ġturntable": 2249, "come": 2250, "ġ0": 2251, "ġmark": 2252, "wards": 2253, "ġknown": 2254, "sc": 2255, "e": 2256, "ġele": 2257, "ġawww": 2258, "ched": 2259, "ġmixer": 2260, "ġsnacks": 2261, "fs": 2262, "ffic": 2263, "light": 2264, "ġappa": 2265, "'.": 2266, "ġamaz": 2267, "ancing": 2268, "ages": 2269, "ġjon": 2270, "ġ#": 2271, "\",": 2272, "ġhorr": 2273, "nch": 2274, "ġooo": 2275, "ors": 2276, "ġpi": 2277, "ġey": 2278, "ba": 2279, "fr": 2280, "ãģ": 2281, "view": 2282, "ãģ£": 2283, "igg": 2284, "how": 2285, "ġap": 2286, "ġah": 2287, "the": 2288, "ġdw": 2289, "ġblanket": 2290, "ġchris": 2291, "ches": 2292, "vice": 2293, "ġphot": 2294, "tem": 2295, "ġsongs": 2296, "hes": 2297, "ġloving": 2298, "ġatta": 2299, "ġsugg": 2300, "ġcreate": 2301, "ġsksks": 2302, "ġproc": 2303, "ġopin": 2304, "ġbur": 2305, "'(": 2306, "ġwha": 2307, "ġâģĺ": 2308, "ġtoy": 2309, "ġðł": 2310, "ġva": 2311, "ġtas": 2312, "ips": 2313, "net": 2314, "),": 2315, "ġhun": 2316, "odu": 2317, "ġyep": 2318, "ġdarkness": 2319, "iving": 2320, "ġvoices": 2321, "ġgun": 2322, "12": 2323, "ture": 2324, "ġlikes": 2325, "ġmat": 2326, "ġsomehow": 2327, "ġidek": 2328, "ters": 2329, "ipp": 2330, "ġtakes": 2331, "ġdaylight": 2332, "ads": 2333, "sent": 2334, "ob": 2335, "ġtowards": 2336, "tm": 2337, "uk": 2338, "yout": 2339, "ġlost": 2340, "ġloves": 2341, "ġcree": 2342, "put": 2343, "ġnumb": 2344, "ġwood": 2345, "wor": 2346, "nded": 2347, "gy": 2348, "ġhating": 2349, "ġopinion": 2350, "cited": 2351, "ġdinner": 2352, "oman": 2353, "mp": 2354, "ġfav": 2355, "watch": 2356, "33": 2357, "ġwear": 2358, "ġhid": 2359, "ġdroid": 2360, "ġbou": 2361, "aged": 2362, "ġhehe": 2363, "ġfuckin": 2364, "ġheating": 2365, "ġoooh": 2366, "ġlooked": 2367, "dy": 2368, "ġconvers": 2369, "ġcomes": 2370, "ġpos": 2371, "ġage": 2372, "âģ¢": 2373, "ġevening": 2374, "ġconsider": 2375, "!'": 2376, "ring": 2377, "ġbook": 2378, "ġmome": 2379, "ġwat": 2380, "15": 2381, "ġpassage": 2382, "ġgunna": 2383, "kk": 2384, "ġdat": 2385, "mer": 2386, "ġdoct": 2387, "ġpen": 2388, "gra": 2389, "êķ": 2390, "ġloool": 2391, "ġdel": 2392, "ġcreating": 2393, "cl": 2394, "man": 2395, "ġ14": 2396, "cu": 2397, "ga": 2398, "êķ": 2399, "tever": 2400, "hy": 2401, "ġtwitch": 2402, "ġcho": 2403, "ġpasta": 2404, "ġpc": 2405, "ġhttp": 2406, "ex": 2407, "ġgoogle": 2408, "irt": 2409, "nn": 2410, "ġmil": 2411, "youtube": 2412, "ġsooo": 2413, "ock": 2414, "dden": 2415, "ġbirthday": 2416, "ġcba": 2417, "ġmouse": 2418, "nding": 2419, "ġpat": 2420, "ġfour": 2421, "ġtiny": 2422, "ġbroke": 2423, "ġloved": 2424, "ġwebs": 2425, "ls": 2426, "ġcock": 2427, "ġpas": 2428, "rac": 2429, "con": 2430, "ġess": 2431, "ġffs": 2432, "ology": 2433, "ġbath": 2434, "17": 2435, "ġwhatever": 2436, "ġhmmm": 2437, "ule": 2438, "ġangry": 2439, "ġperf": 2440, "ġfell": 2441, "ision": 2442, "ġcalm": 2443, "lu": 2444, "key": 2445, "ġandroid": 2446, "cer": 2447, "ġmee": 2448, "cha": 2449, "fk": 2450, "sel": 2451, "24": 2452, "ured": 2453, "onse": 2454, "ġrecent": 2455, "scri": 2456, "ġfil": 2457, "16": 2458, "ġwoo": 2459, "ġeyes": 2460, "25": 2461, "ient": 2462, "ġloveangel": 2463, "ġ/": 2464, "board": 2465, "ġya": 2466, "tr": 2467, "ġfinis": 2468, "ġeasy": 2469, "ġmis": 2470, "ġknows": 2471, "ife": 2472, "ġconversation": 2473, "ġbrains": 2474, "ġscreen": 2475, "ines": 2476, "ġeh": 2477, "ġresponse": 2478, "ġ19": 2479, "gif": 2480, "ġ+": 2481, "selves": 2482, "ġcares": 2483, "ġproblems": 2484, "ġlevel": 2485, "ral": 2486, "iness": 2487, "ġshort": 2488, "ġatm": 2489, "ġhates": 2490, "ġfive": 2491, "ġlemme": 2492, "cy": 2493, "ġbeaut": 2494, "ġnoticed": 2495, "iple": 2496, "ġcust": 2497, "ġmissed": 2498, "ġnumber": 2499, "êķãģ£": 2500, "ġlog": 2501, "uin": 2502, "ġprodu": 2503, "ġbru": 2504, "ġessay": 2505, "ġsys": 2506, "rew": 2507, "rence": 2508, "ġgives": 2509, "ġbased": 2510, "ify": 2511, "ize": 2512, "ġconta": 2513, "ġwalked": 2514, "ġsomewhere": 2515, "ġnear": 2516, "ġ13": 2517, "ġohhh": 2518, "ġdeep": 2519, "ġconst": 2520, "load": 2521, "eer": 2522, "orn": 2523, "tmas": 2524, "ġsuddenly": 2525, "une": 2526, "ġfamily": 2527, "ġstarting": 2528, "ġincred": 2529, "ġfront": 2530, "ively": 2531, "âģ¢ì": 2532, "ġclos": 2533, "ġparty": 2534, "ital": 2535, "ġgoes": 2536, "pa": 2537, "esss": 2538, "ġchristmas": 2539, "ġstory": 2540, "ġgirls": 2541, "ġhungry": 2542, "ġcreepy": 2543, "ġround": 2544, "14": 2545, "ġpol": 2546, "ġhugs": 2547, "aked": 2548, "urs": 2549, "ġfloor": 2550, "ġsqu": 2551, "ġran": 2552, "ġheart": 2553, "ġjoin": 2554, "ġ>.": 2555, "ġbusy": 2556, "ġ16": 2557, "ġengland": 2558, "fast": 2559, "ġsilly": 2560, "iful": 2561, "ancy": 2562, "ġtechnically": 2563, "ġpartner": 2564, "ġgeneral": 2565, "oot": 2566, "fortable": 2567, "ġinform": 2568, "ġair": 2569, "ġhappens": 2570, "iot": 2571, "ġexperience": 2572, "ġquestions": 2573, "ġxoxo": 2574, "19": 2575, "ġdecide": 2576, "13": 2577, "lex": 2578, "ġlonger": 2579, "ġbrother": 2580, "ronic": 2581, "ġstyle": 2582, "ġwoke": 2583, "ġinclud": 2584, "head": 2585, "kets": 2586, "ik": 2587, "ġwel": 2588, "ġimagine": 2589, "ġbuild": 2590, "tention": 2591, "ġsme": 2592, "ġbass": 2593, "ġsinging": 2594, "ġissue": 2595, "dle": 2596, "min": 2597, "ġnic": 2598, "ġneither": 2599, "chen": 2600, "ax": 2601, "ġlar": 2602, "ġyh": 2603, "ndon": 2604, "ists": 2605, "fl": 2606, "ġlikely": 2607, "ġfacebook": 2608, "ġspecific": 2609, "ġyout": 2610, "ġspo": 2611, "ġmums": 2612, "ġparticul": 2613, "ġprov": 2614, "ġsleeping": 2615, "ġwarm": 2616, "ġgenuinely": 2617, "ġnearly": 2618, "ġcalling": 2619, "50": 2620, "tern": 2621, "ġfucked": 2622, "ġsens": 2623, "18": 2624, "ġsoup": 2625, "ġju": 2626, "ġmeeting": 2627, "you": 2628, "ġily": 2629, "iled": 2630, "ġmilk": 2631, "ġline": 2632, "ġannoyed": 2633, "ġodd": 2634, "ġcouldn": 2635, "ġkitchen": 2636, "ġargo": 2637, "ġmultiple": 2638, "ġscreaming": 2639, "pris": 2640, "ġclot": 2641, "ġturned": 2642, "ġbright": 2643, "ġspent": 2644, "ġdammit": 2645, "êĺ": 2646, "ġsile": 2647, "ġdisapp": 2648, "ġmassive": 2649, "ills": 2650, "ġstrong": 2651, "ġwake": 2652, "ġwhether": 2653, "ġgroup": 2654, "que": 2655, "urate": 2656, "ġstude": 2657, "ġshower": 2658, "ġopp": 2659, "ġbreakfast": 2660, "ġsksksk": 2661, "\")": 2662, "ġotherwise": 2663, "ġtraum": 2664, "ġplymouth": 2665, "ġpissed": 2666, "aw": 2667, "do": 2668, "tend": 2669, "ġvers": 2670, "ġchaos": 2671, "ġothers": 2672, "ġeasier": 2673, "ġlondon": 2674, "eng": 2675, "ġgreen": 2676, "ġ100": 2677, "ġpaid": 2678, "ġapolog": 2679, "ġfire": 2680, "ġhm": 2681, "kers": 2682, "down": 2683, "ġremo": 2684, "ġant": 2685, "ġsix": 2686, "ġended": 2687, "ġdue": 2688, "ġdans": 2689, "ġshare": 2690, "ġyoung": 2691, "ġ>:": 2692, "ġdirect": 2693, "medi": 2694, "ġpurple": 2695, "23": 2696, "het": 2697, "ġcamer": 2698, "ġdeser": 2699, "ġcontrol": 2700, "ġrecently": 2701, "ġkissing": 2702, "sted": 2703, "ġbeautiful": 2704, "zz": 2705, "ġleaves": 2706, "ġuh": 2707, "ġpour": 2708, "ġloveangle": 2709, "eg": 2710, "ġig": 2711, "ġliked": 2712, "ġsurpris": 2713, "what": 2714, "ġlos": 2715, "ġpersonal": 2716, "',": 2717, "hugs": 2718, "ġhowever": 2719, "ġmoves": 2720, "ġclearly": 2721, "ġstressed": 2722, "ġemotional": 2723, "ġhelping": 2724, "ġaccurate": 2725, "br": 2726, "mat": 2727, "ets": 2728, "ġagainst": 2729, "omen": 2730, "ump": 2731, "pected": 2732, "ġfight": 2733, "ġwife": 2734, "ġdeath": 2735, "ġcrap": 2736, "ġce": 2737, "ġargu": 2738, "ġtaken": 2739, "any": 2740, "ġsofa": 2741, "ġinterested": 2742, "ġproject": 2743, "nse": 2744, "ġsitting": 2745, "()": 2746, "40": 2747, "pop": 2748, "ġdou": 2749, "ġben": 2750, "ġhugging": 2751, "par": 2752, "ġcertain": 2753, "ġsending": 2754, "ġcooking": 2755, "ġobs": 2756, "ġmiddle": 2757, "ġyoutube": 2758, "go": 2759, "ġfake": 2760, "ġfriday": 2761, "ġsingle": 2762, "ġyesss": 2763, "vi": 2764, "tenor": 2765, "ġprefer": 2766, "ka": 2767, "ants": 2768, "ġorgan": 2769, "dn": 2770, "ġdude": 2771, "ġow": 2772, "lls": 2773, "ilt": 2774, "ġthemselves": 2775, "ġchoice": 2776, "ond": 2777, "ġattention": 2778, "ġsmoked": 2779, "ġidiot": 2780, "gu": 2781, "ġstate": 2782, "ġbare": 2783, "ġconsidering": 2784, "ġai": 2785, "ġlil": 2786, "ġque": 2787, "gs": 2788, "ġaa": 2789, "ġloads": 2790, "ġdate": 2791, "ġexpensive": 2792, "ġbroken": 2793, "ġasher": 2794, "bot": 2795, "ular": 2796, "ġmeaning": 2797, "ġmag": 2798, "ġconne": 2799, "hol": 2800, "ġont": 2801, "ġappro": 2802, "ġcrazy": 2803, "sor": 2804, "ġrent": 2805, "ġgrow": 2806, "1000": 2807, "ai": 2808, "ric": 2809, "ġfocus": 2810, "ġfinished": 2811, "text": 2812, "ġamb": 2813, "fy": 2814, "ġavo": 2815, "ġdepe": 2816, "ove": 2817, "ġmas": 2818, "norm": 2819, "ises": 2820, "ġadvice": 2821, "ġ>:(": 2822, "itive": 2823, "ergy": 2824, "hahaha": 2825, "like": 2826, "igger": 2827, "êķãģ£": 2828, "ġtic": 2829, "ġwall": 2830, "ġuns": 2831, "cle": 2832, "ġdress": 2833, "ġ17": 2834, "ġtyping": 2835, "21": 2836, "ġreact": 2837, "45": 2838, "ġdrugs": 2839, "ġ:((": 2840, "tel": 2841, "26": 2842, "port": 2843, "ina": 2844, "ġaccess": 2845, "ġemo": 2846, "ġsystem": 2847, "ġrepe": 2848, "ġexpress": 2849, "ġemp": 2850, "ġcamera": 2851, "ġsatur": 2852, "ager": 2853, "mi": 2854, "ġwomen": 2855, "ġrealise": 2856, "ġmanage": 2857, "ġsees": 2858, "ules": 2859, "iment": 2860, "ropri": 2861, "ġsoft": 2862, "ġblood": 2863, "ġonto": 2864, "ġdest": 2865, "search": 2866, "ġsmile": 2867, "ġsecret": 2868, "ġoffe": 2869, "ġkitty": 2870, "]:": 2871, "ġcode": 2872, "ġview": 2873, "ġprote": 2874, "ially": 2875, "cel": 2876, "ġteen": 2877, "ġcharac": 2878, "stairs": 2879, "uth": 2880, "ġrid": 2881, "ġwalking": 2882, "ġmemor": 2883, "oof": 2884, "ġoffer": 2885, "ġquiet": 2886, "ġletter": 2887, "ġhimself": 2888, "work": 2889, "ġbeg": 2890, "imin": 2891, "ġvibes": 2892, "ġcustom": 2893, "for": 2894, "ġhol": 2895, "ġabsol": 2896, "whel": 2897, "ġopt": 2898, "ature": 2899, "ġrespons": 2900, "ġcommunication": 2901, "bu": 2902, "ġoop": 2903, "ġpub": 2904, "ane": 2905, "aten": 2906, "iod": 2907, "rib": 2908, "ġbox": 2909, "ġclear": 2910, "ġclothes": 2911, "ġenergy": 2912, "ġgiven": 2913, "ġrip": 2914, "ift": 2915, "ġmot": 2916, "ġoverwhel": 2917, "ġbrit": 2918, "ġyeahhh": 2919, "ġfairly": 2920, "ġremind": 2921, "ġabsolute": 2922, "ġrespect": 2923, "rs": 2924, "ġbin": 2925, "ġpie": 2926, "ġfeelings": 2927, "ocked": 2928, "ability": 2929, "ġhands": 2930, "ġawh": 2931, "ġwhos": 2932, "ġreco": 2933, "ġwelcome": 2934, "cal": 2935, "ġworked": 2936, "ours": 2937, "pro": 2938, "ġputting": 2939, "ġest": 2940, "irty": 2941, "ġretur": 2942, "ġav": 2943, "ġbts": 2944, "ġrele": 2945, "ġshouldnt": 2946, "ġpoke": 2947, "ġadmit": 2948, "ġmedic": 2949, "ġtruly": 2950, "ġbeh": 2951, "ġfully": 2952, "ġchanged": 2953, "but": 2954, "yyy": 2955, "ġsam": 2956, "astic": 2957, "ġ24": 2958, "ġphoto": 2959, "ġâ": 2960, "ġplanning": 2961, "ġvibe": 2962, "ok": 2963, "ġelodies": 2964, "ġrock": 2965, "ġlarge": 2966, "ġcomplain": 2967, "miss": 2968, "igin": 2969, "ġperiod": 2970, "ġforgetting": 2971, "ġtre": 2972, "ġterr": 2973, "ġinformation": 2974, "ġcover": 2975, "22": 2976, "ġpriv": 2977, "ġsaturday": 2978, "uc": 2979, "icken": 2980, "ctive": 2981, "ġignore": 2982, "ġparticularly": 2983, "ropriate": 2984, "ws": 2985, "url": 2986, "ġpetes": 2987, "aces": 2988, "ġpipe": 2989, "ġaud": 2990, "ġmonday": 2991, "ġ50": 2992, "inger": 2993, "ġbeha": 2994, "ole": 2995, "ġdrop": 2996, "ġport": 2997, "ġthinks": 2998, "ġarr": 2999, "anger": 3000, "ġdoctors": 3001, "ġcontinue": 3002, "ze": 3003, "ġbat": 3004, "ġawful": 3005, "ġhelps": 3006, "dam": 3007, "ġdetail": 3008, "ġtreat": 3009, "raph": 3010, "ġimmedi": 3011, "ġfinding": 3012, "ġoption": 3013, "?'": 3014, "ky": 3015, "ġir": 3016, "ator": 3017, "ġremembering": 3018, "ġconfir": 3019, "(\"": 3020, "ġlady": 3021, "ġpower": 3022, "ġuncom": 3023, "ġpract": 3024, "ġtemp": 3025, "ġkidding": 3026, "nel": 3027, "ġmoved": 3028, "ġbby": 3029, "ġstrange": 3030, "ground": 3031, "ġnoise": 3032, "ġinvol": 3033, "lr": 3034, "mg": 3035, "ġsal": 3036, "ġly": 3037, "ġvag": 3038, "ġorange": 3039, "ġsomebody": 3040, "ġ_": 3041, "ġunderstanding": 3042, "ġcloser": 3043, "ġavoid": 3044, "ġkpop": 3045, "ġaint": 3046, "ġwarn": 3047, "oring": 3048, "cou": 3049, "ġunf": 3050, "ġassa": 3051, "ġjuice": 3052, "cture": 3053, "ġtruth": 3054, "ġproperty": 3055, "scribe": 3056, "ġsle": 3057, "ġmice": 3058, "ġum": 3059, "ġstre": 3060, "ġcommunicate": 3061, "ġbehind": 3062, "bal": 3063, "ġtit": 3064, "ġwooo": 3065, "ġfeed": 3066, "ġalive": 3067, "ġdrug": 3068, "fd": 3069, "land": 3070, "amp": 3071, "ġwaking": 3072, "ġkeeps": 3073, "ġsky": 3074, "28": 3075, "ġlink": 3076, "kevin": 3077, "ġincredibly": 3078, "ġ23": 3079, "ġnormally": 3080, "oh": 3081, "ġmegan": 3082, "ġfast": 3083, "ġeventually": 3084, "rd": 3085, "ġstopped": 3086, "ġamerica": 3087, "ienc": 3088, "sex": 3089, "ġfear": 3090, "pose": 3091, "ġocc": 3092, "lfr": 3093, "ġband": 3094, "ġbott": 3095, "ġcringe": 3096, "11": 3097, "itter": 3098, "ġherself": 3099, "ġpressure": 3100, "cing": 3101, "ġorigin": 3102, "ġelectronic": 3103, "ġfat": 3104, "ġthurs": 3105, "ggg": 3106, "aging": 3107, "ġconfusing": 3108, "ġspecif": 3109, "oss": 3110, "ġball": 3111, "ġforce": 3112, "verse": 3113, "cra": 3114, "igned": 3115, "ġcomputers": 3116, "ġhotel": 3117, "ġversion": 3118, "dio": 3119, "ġlang": 3120, "ġbeds": 3121, "ġalco": 3122, "ġmach": 3123, "face": 3124, "ġ&": 3125, "ney": 3126, "lfriend": 3127, "ġwot": 3128, "vera": 3129, "ġkore": 3130, "aaay": 3131, "ġrelax": 3132, "usive": 3133, "ġcost": 3134, "ġmac": 3135, "ipped": 3136, "ġforward": 3137, "ġdistra": 3138, "âģ¿": 3139, "ġstars": 3140, "ġterm": 3141, "mes": 3142, "ġcir": 3143, "ġprogra": 3144, "ġweeke": 3145, "ġnor": 3146, "leted": 3147, "ġador": 3148, "ġmissing": 3149, "ġrev": 3150, "ġmatter": 3151, "ġseemed": 3152, "ġrequest": 3153, "ġfut": 3154, "ġbene": 3155, "ows": 3156, "ġdecent": 3157, "ġdesk": 3158, "ġpiper": 3159, "ġeye": 3160, "ġcy": 3161, "ġfrust": 3162, "ġraccoon": 3163, "ġprice": 3164, "ġmale": 3165, "ġdeserve": 3166, "99": 3167, "ġporn": 3168, "ġhide": 3169, "icked": 3170, "ior": 3171, "ġurself": 3172, "ġident": 3173, "cat": 3174, "ġlistens": 3175, "ġspecial": 3176, "ġstudent": 3177, "ġfill": 3178, "ġtouches": 3179, "ġsleepy": 3180, "usted": 3181, "ġharsh": 3182, "ġboys": 3183, "rap": 3184, "ived": 3185, "ġlangu": 3186, "ġalcohol": 3187, "sp": 3188, "ġmate": 3189, "ġstan": 3190, "ġexpected": 3191, "ġwerent": 3192, "ġacross": 3193, "aling": 3194, "ġcup": 3195, "ġbadly": 3196, "ġdecision": 3197, "ġincluding": 3198, "ler": 3199, "vous": 3200, "ġou": 3201, "ġdepressed": 3202, "27": 3203, "la": 3204, "token": 3205, "aths": 3206, "ses": 3207, "ġris": 3208, "ġtown": 3209, "ġwhenever": 3210, "ġstreaming": 3211, "ars": 3212, "nted": 3213, "ġburn": 3214, "dj": 3215, "ġep": 3216, "ġroll": 3217, "iant": 3218, "ġcompany": 3219, "ġshitty": 3220, "ġappoint": 3221, "ġgirlfriend": 3222, "ġtaste": 3223, "sha": 3224, "ume": 3225, "ġeight": 3226, "ġcuddle": 3227, "ġwithin": 3228, "ġmanaged": 3229, "ġnote": 3230, "ġanaly": 3231, "ġlay": 3232, "ġreasons": 3233, "ġbrighton": 3234, "bl": 3235, "tence": 3236, "ġdrum": 3237, "ġjud": 3238, "ġsave": 3239, "ġways": 3240, "ġredd": 3241, "ġskills": 3242, "ġfurther": 3243, "ġahhh": 3244, "ġrules": 3245, "pital": 3246, "hael": 3247, "ġbill": 3248, "ġnone": 3249, "ġdiag": 3250, "ġwatched": 3251, "ġrepeat": 3252, "ġ{": 3253, "ġtor": 3254, "ġstat": 3255, "ġalong": 3256, "ġbull": 3257, "ġgrind": 3258, "ġphotos": 3259, "ġoops": 3260, "ġbehavi": 3261, "ġcris": 3262, "ġden": 3263, "ġtherefore": 3264, "fc": 3265, "list": 3266, "ġlolol": 3267, "ġhelpful": 3268, "ġdro": 3269, "ġpark": 3270, "))": 3271, "itten": 3272, "ġbritish": 3273, "bin": 3274, "ġchicken": 3275, "ġtty": 3276, "ġdrag": 3277, "ġgenre": 3278, "ġjk": 3279, "ġofc": 3280, "ġplans": 3281, "ġfroggys": 3282, "ġdifference": 3283, "ality": 3284, "ġexis": 3285, "ġurs": 3286, "ġexperienc": 3287, "ġexciting": 3288, "ġilleg": 3289, "ġmetal": 3290, "ġembarr": 3291, "ġabsolutely": 3292, "ġfuture": 3293, "bab": 3294, "ġyouuu": 3295, "ġpoor": 3296, "ġmichael": 3297, "ġspecifically": 3298, "ster": 3299, "uce": 3300, "ġmild": 3301, "ref": 3302, "ġnood": 3303, "ġattem": 3304, "ġsorted": 3305, "ġswitch": 3306, "ġplaces": 3307, "ġhears": 3308, "ġlack": 3309, "ġlearns": 3310, "ġrunning": 3311, "den": 3312, "tain": 3313, "ught": 3314, "ġimpress": 3315, "ġpurpose": 3316, "ġhyper": 3317, "ġoptions": 3318, "ġvoc": 3319, "ġmc": 3320, "ġfoot": 3321, "idence": 3322, "tedly": 3323, "ġtryna": 3324, "ġchannel": 3325, "ġinfo": 3326, "ġtraining": 3327, "pping": 3328, "ġprint": 3329, "ġdelive": 3330, "aks": 3331, "oin": 3332, "ints": 333, "ġbilly": 3334, "ġsched": 3335, "ġregard": 3336, "ġwro": 3337, "ġble": 3338, "ġupd": 3339, "ton": 3340, "ġdied": 3341, "ġdirty": 3342, "ġrece": 3343, "ġknowing": 3344, "ġunable": 3345, "ġequals": 3346, "ġnervous": 3347, "ġbarely": 3348, "ġ25": 3349, "ġhurts": 3350, "ġshops": 3351, "ġweekend": 3352, "ġreddit": 3353, "under": 3354, "ġsunday": 3355, "ppy": 3356, "ġaware": 3357, "ġskin": 3358, "name": 3359, "ġgender": 3360, "ġhehehe": 3361, "ġadorable": 3362, "ġtune": 3363, "har": 3364, "idge": 3365, "ġbottom": 3366, "ġtro": 3367, "ġbum": 3368, "ġruin": 3369, "ġmentioned": 3370, "ġsurprised": 3371, "ġttyl": 3372, "bing": 3373, "ġbeat": 3374, "ġabove": 3375, "ġminute": 3376, "ġconstantly": 3377, "ġbb": 3378, "ġdisg": 3379, "ġbedroom": 3380, "velop": 3381, "ġmachine": 3382, "38": 3383, "oned": 3384, "ġjess": 3385, "ġpossibly": 3386, "ġ>.>": 3387, "ġrefe": 3388, "ionally": 3389, "ġeats": 3390, "ġdads": 3391, "ġassum": 3392, "ġmother": 3393, "ġlying": 3394, "ġkk": 3395, "ġmoon": 3396, "vert": 3397, "ġpicks": 3398, "ġaccount": 3399, "ġwrote": 3400, "ġwashing": 3401, "play": 3402, "ġstruggling": 3403, "ġmur": 3404, "ġcross": 3405, "ġtax": 3406, "ested": 3407, "ġtend": 3408, "ġpenis": 3409, "chan": 3410, "ulation": 3411, "ġjobs": 3412, "ġ40": 3413, "essed": 3414, "gged": 3415, "ġðłĺ": 3416, "ġbruh": 3417, "box": 3418, "ó¾ĩ": 3419, "ution": 3420, "ġdirectly": 3421, "ġcake": 3422, "ġconcer": 3423, "umblr": 3424, "29": 3425, "pre": 3426, "ġtable": 3427, "ġbow": 3428, "ġquit": 3429, "ġtues": 3430, "ġdelete": 3431, "ġcond": 3432, "ġpaying": 3433, "ġworries": 3434, "ġcopy": 3435, "ġprofess": 3436, "66": 3437, "ġhom": 3438, "ġsadly": 3439, "ġdouble": 3440, "ġlanguage": 3441, "pg": 3442, "ġmaths": 3443, "ġdying": 3444, "ġhoney": 3445, "ides": 3446, "alous": 3447, "ġneg": 3448, "ġinterview": 3449, "ġmode": 3450, "ġbanned": 3451, "bt": 3452, "ġmist": 3453, "ndent": 3454, "ġuuu": 3455, "ġheav": 3456, "ġstick": 3457, "ġsell": 3458, "ġspot": 3459, "ġeverywhere": 3460, "ġdrive": 3461, "ġholy": 3462, "ġapologise": 3463, "ġpokemon": 3464, "ġslept": 3465, "ġsand": 3466, "ġserious": 3467, "uff": 3468, "ġyee": 3469, "ġshad": 3470, "ncy": 3471, "ġfixed": 3472, "ġfilm": 3473, "ġrom": 3474, "ġcent": 3475, "ġpun": 3476, "ġgurl": 3477, "ien": 3478, "ġwritten": 3479, "ġphones": 3480, "ġembarass": 3481, "ġmedication": 3482, "08": 3483, "ġbreat": 3484, "uses": 3485, "ġlose": 3486, "ġsumm": 3487, "ġseven": 3488, "ġ22": 3489, "ledge": 3490, "ġillegal": 3491, "48": 3492, "lent": 3493, "ġchall": 3494, "ġtrip": 3495, "ġirl": 3496, "time": 3497, "ġjoy": 3498, "ġanim": 3499, "amine": 3500, "ġfinal": 3501, "ġrandomly": 3502, "ġemotions": 3503, "57": 3504, "ġbecom": 3505, "ġletting": 3506, "aime": 3507, "ġjum": 3508, "ġuseful": 3509, "ġimposs": 3510, "ġjealous": 3511, "ġentirely": 3512, "âģ¢ìģ": 3513, "ġtrauma": 3514, "ġimmediately": 3515, "ġuncomfortable": 3516, "nit": 3517, "ġate": 3518, "ġbenef": 3519, "ġsarc": 3520, "ġpics": 3521, "ġamerican": 3522, "ġsize": 3523, "ġlu": 3524, "ġanywhere": 3525, "ġbae": 3526, "ġwearing": 3527, "gpt": 3528, "ġsce": 3529, "ġbegin": 3530, "):": 3531, "ġsucc": 3532, "ġapply": 3533, "ġshame": 3534, "ġremix": 3535, "ġlowkey": 3536, "ġimpossible": 3537, "ġstal": 3538, "ġwhyyy": 3539, "ġimpro": 3540, "ġglass": 3541, "ġdepression": 3542, "ġloser": 3543, "ġprivate": 3544, "craft": 3545, "47": 3546, "tep": 3547, "ġaren": 3548, "ifying": 3549, "ġhelped": 3550, "ġbabies": 3551, "ograph": 3552, "ġinde": 3553, "ware": 3554, "ġexcuse": 3555, "ġphysical": 3556, "âģ¢ìģ": 3557, "gf": 3558, "ġled": 3559, "ġjour": 3560, "usting": 3561, "ġunivers": 3562, "ya": 3563, "ġnoice": 3564, "ctions": 3565, "ġcutie": 3566, "of": 3567, "ġfish": 3568, "ġcope": 3569, "male": 3570, "ġobject": 3571, "ġreturn": 3572, "ights": 3573, "ġbuilt": 3574, "ġjelly": 3575, "nb": 3576, "ġirr": 3577, "ġyeee": 3578, "ġnine": 3579, "ġut": 3580, "ġimage": 3581, "ġconvo": 3582, "ġsignific": 3583, "ġthursday": 3584, "aff": 3585, "ġbound": 3586, "ġhadn": 3587, "ġappear": 3588, "phone": 3589, "ġtf": 3590, "orted": 3591, "ġreach": 3592, "ġanime": 3593, "haps": 3594, "tems": 3595, "ġsho": 3596, "ġchips": 3597, "ires": 3598, "ġtracks": 3599, "ġaces": 3600, "ġhumans": 3601, "tered": 3602, "ram": 3603, "ġmax": 3604, "ġgran": 3605, "ġcleaning": 3606, "iculous": 3607, "ults": 3608, "ġassist": 3609, "ġstressful": 3610, "ġfigured": 3611, "ġvisit": 3612, "ġsou": 3613, "ġsearch": 3614, "ġkeyboard": 3615, "ġquickly": 3616, "back": 3617, "ġgar": 3618, "ġends": 3619, "ġperform": 3620, "ġpill": 3621, "ġcontext": 3622, "ġchildren": 3623, "ġdepends": 3624, "ġrice": 3625, "ġdeleted": 3626, "ulance": 3627, "ġteam": 3628, "ġembarrass": 3629, "ġded": 3630, "usion": 3631, "ġthre": 3632, "ġstab": 3633, "..?": 3634, "ġens": 3635, "dis": 3636, "ġbang": 3637, "ġinit": 3638, "ġkay": 3639, "ġshouting": 3640, "cher": 3641, "ration": 3642, "vant": 3643, "vices": 3644, "ġahh": 3645, "ġdanger": 3646, "urt": 3647, "ġyeh": 3648, "ġdrama": 3649, "ġrac": 3650, "ġhiding": 3651, "ġlaughing": 3652, "ġbon": 3653, "35": 3654, "ġpage": 3655, "ġpush": 3656, "ased": 3657, "ġabuse": 3658, "ġabusive": 3659, "ged": 3660, "ġsuis": 3661, "ġexha": 3662, "ġcomfortable": 3663, "ġtex": 3664, "ġchin": 3665, "ġcheap": 3666, "ġhorny": 3667, "now": 3668, "ġbee": 3669, "idered": 3670, "dding": 3671, "ġjoking": 3672, "ġtwitter": 3673, "shit": 3674, "ġstaying": 3675, "ġinstru": 3676, "ġpus": 3677, "ġhos": 3678, "ġchance": 3679, "ġusual": 3680, "ġwaaa": 3681, "ġallows": 3682, "ġowo": 3683, "mission": 3684, "ġfunction": 3685, "ġlives": 3686, "ġeggs": 3687, "ġwoods": 3688, "ġcountry": 3689, "ġsmell": 3690, "ġridiculous": 3691, "ġrecogn": 3692, "ġagre": 3693, "quen": 3694, "ġcounse": 3695, "ġseconds": 3696, "ġicon": 3697, "ġbigg": 3698, "ġfra": 3699, "oms": 3700, "ġimo": 3701, "ġrhy": 3702, "ġfuc": 3703, "ġactiv": 3704, "ġperhaps": 3705, "ġbreaks": 3706, "ġmistake": 3707, "just": 3708, "ġwedn": 3709, "ġbother": 3710, "ġbathroom": 3711, "berry": 3712, "ġcharacter": 3713, "ġwednes": 3714, "36": 3715, "46": 3716, "here": 3717, "ġmut": 3718, "ġmer": 3719, "olate": 3720, "bered": 3721, "ġapart": 3722, "ġsilent": 3723, "ġirrit": 3724, "tre": 3725, "ġmov": 3726, "ġlead": 3727, "ġartic": 3728, "ġvideos": 3729, "!',": 3730, "ġoppos": 3731, "39": 3732, "ġmeee": 3733, "ġfancy": 3734, "ġnaked": 3735, "ato": 3736, "ġ90": 3737, "ipping": 3738, "ġsuggesting": 3739, "form": 3740, "ource": 3741, "dered": 3742, "fet": 3743, "ġitself": 3744, "ġchatgpt": 3745, "ġobvious": 3746, "ġwondering": 3747, "ġsilence": 3748, "ġschedule": 3749, "ġyaaay": 3750, "ġenter": 3751, "ġtough": 3752, "ġappropriate": 3753, "ġshowing": 3754, "ġexperiment": 3755, "ġdata": 3756, "ġvagina": 3757, "hha": 3758, "ister": 3759, "ġstic": 3760, "ġchoo": 3761, "ġsimple": 3762, "fortun": 3763, "ġpublic": 3764, "fw": 3765, "ġtoi": 3766, "ġpeen": 3767, "ġcance": 3768, "ġpretend": 3769, "ighten": 3770, "ances": 3771, "ġmixed": 3772, "ġears": 3773, "ġbank": 3774, "fun": 3775, "pes": 3776, "ġange": 3777, "ġwank": 3778, "ġpaper": 3779, "ġmarry": 3780, "ġgrand": 3781, "ġverbal": 3782, "58": 3783, "rics": 3784, "ġnames": 3785, "ġlege": 3786, "ġshouldn": 3787, "ġteach": 3788, "ġsus": 3789, "ġshy": 3790, "ġshirt": 3791, "ġbutts": 3792, "iling": 3793, "ġadam": 3794, "ġoffended": 3795, "ġoriginal": 3796, "vid": 3797, "||": 3798, "stats": 3799, "ġdeg": 3800, "ġcomfy": 3801, "ġprescri": 3802, "ġgrim": 3803, "ġadded": 3804, "ġminecraft": 3805, "ġessent": 3806, "ging": 3807, "kj": 3808, "su": 3809, "ġtrigger": 3810, "ġcalls": 3811, "ġtravel": 3812, "ao": 3813, "gl": 3814, "ġó¾": 3815, "ified": 3816, "ġconsidered": 3817, "ġtickets": 3818, "ġholiday": 3819, "ġagg": 3820, "ġinten": 3821, "icky": 3822, "ġjoh": 3823, "ġremem": 3824, "ġsituations": 3825, "that": 3826, "ġunfortun": 3827, "ġdraws": 3828, "ġbreaking": 3829, "ġindepe": 3830, "cting": 3831, "ġapplic": 3832, "ġregret": 3833, "ġtasks": 3834, "ġau": 3835, "vere": 3836, "ġpure": 3837, "ġvio": 3838, "ġtrash": 3839, "ġliteral": 3840, "ġinsane": 3841, "ġassume": 3842, "love": 3843, "well": 3844, "ġedit": 3845, "ġsksksks": 3846, "uino": 3847, "49": 3848, "ull": 3849, "ġdry": 3850, "ery": 3851, "ġhist": 3852, "ġ>.<": 3853, "01": 3854, "va": 3855, "ġomggg": 3856, "ġtalked": 3857, "ploy": 3858, "ġolder": 3859, "ġnegative": 3860, "ġopposite": 3861, "ġtatt": 3862, "ġcum": 3863, "let": 3864, "ley": 3865, "ġjimin": 3866, "ġadvent": 3867, "ġharder": 3868, "ġsetting": 3869, "ġtechnology": 3870, "ġoverwhelmed": 3871, "ġassault": 3872, "ġdiagn": 3873, "ium": 3874, "iest": 3875, "âľ": 3876, "ġprog": 3877, "ġveux": 3878, "ġcla": 3879, "ġepis": 3880, "aries": 3881, "ġlock": 3882, "ġfrog": 3883, "ġremembered": 3884, "37": 3885, "acc": 3886, "ġdare": 3887, "ġnet": 3888, "tery": 3889, "ġsuit": 3890, "ġcaused": 3891, "ġfreaking": 3892, "ġpromise": 3893, "cr": 3894, "ġstorm": 3895, "ġconvin": 3896, "ġshall": 3897, "ġchinese": 3898, "à": 3899, "ġtha": 3900, "atic": 3901, "ġjam": 3902, "ġoffic": 3903, "aaa": 3904, "ġsolid": 3905, "di": 3906, "val": 3907, "ġalex": 3908, "ona": 3909, "utm": 3910, "ġstor": 3911, "ġmanip": 3912, "ġsharing": 3913, "ġslowly": 3914, "ġmeat": 3915, "ġability": 3916, "though": 3917, "cious": 3918, "ġground": 3919, "ġinclude": 3920, "ġunfortunately": 3921, "ġhous": 3922, "ġranting": 3923, "ġtattoo": 3924, "bby": 3925, "dit": 3926, "ġêķãģ£": 3927, "ġidfk": 3928, "ġchoc": 3929, "ġcharge": 3930, "ġprovide": 3931, "ġdelivery": 3932, "llm": 3933, "ank": 3934, "aming": 3935, "ġsauce": 3936, "ġdevelop": 3937, "phones": 3938, "ġignoring": 3939, "ġprogress": 3940, "bed": 3941, "ġshowed": 3942, "ġ80": 3943, "ġcommon": 3944, "iction": 3945, "ġcunt": 3946, "ġphysically": 3947, "ġmemory": 3948, "ġattempt": 3949, "ġhospital": 3950, "ġtid": 3951, "ġpin": 3952, "asm": 3953, "ġjs": 3954, "ġ21": 3955, "ġcatch": 3956, "ġlights": 3957, "ġbuilding": 3958, "ġjudge": 3959, "ġprescription": 3960, "ingly": 3961, "ġshou": 3962, "ġvar": 3963, "ġblame": 3964, "ġclick": 3965, "ġmais": 3966, "ġsteal": 3967, "ġtopic": 3968, "ġreplying": 3969, "ġfollowing": 3970, "ġchanging": 3971, "ġreplied": 3972, "ġambulance": 3973, "fety": 3974, "ural": 3975, "ġuhhh": 3976, "ġsounded": 3977, "ġadult": 3978, "ġboat": 3979, "ġnews": 3980, "ġmelt": 3981, "ġcap": 3982, "ables": 3983, "ġextre": 3984, "ġhoping": 3985, "ġdownstairs": 3986, "ġunderstood": 3987, "ġaaagh": 3988, "ġenjoying": 3989, "ġlyrics": 3990, "ġtuesday": 3991, "ġaggress": 3992, "59": 3993, "hop": 3994, "ġnhs": 3995, "ġmeth": 3996, "ġjfc": 3997, "ġresearch": 3998, "ġuniversity": 3999, "sss": 4000, "ġenv": 4001, "ġmedical": 4002, "ġwindows": 4003, "underst": 4004, "ġsandw": 4005, "ġwednesday": 4006, "uit": 4007, "ġcult": 4008, "ġlou": 4009, "ġbear": 4010, "ġdose": 4011, "gent": 4012, "ġvoid": 4013, "ġprior": 4014, "activ": 4015, "ġsympt": 4016, "ġaudio": 4017, "lol": 4018, "ġmr": 4019, "ġdom": 4020, "ano": 4021, "ġkicked": 4022, "ġacting": 4023, "ġvocals": 4024, "omg": 4025, "ġwet": 4026, "rated": 4027, "ġlmaoo": 4028, "ġpresent": 4029, "ġservice": 4030, "ġmarried": 4031, "ġchalleng": 4032, "core": 4033, "gb": 4034, "icy": 4035, "str": 4036, "ational": 4037, "ibility": 4038, "ġclar": 4039, "ġclub": 4040, "ġboob": 4041, "ġreasonable": 4042, "êĺêķãģ£": 4043, "uro": 4044, "renc": 4045, "ġnat": 4046, "ġpunch": 4047, "ġknowledge": 4048, "ġ28": 4049, "ġservices": 4050, "ġposted": 4051, "charis": 4052, "ġeffects": 4053, "ġice": 4054, "ġtoys": 4055, "ield": 4056, "ġking": 4057, "ġsorta": 4058, "ġprev": 4059, "ġsimp": 4060, "ġmodule": 4061, "ġrefer": 4062, "ġevil": 4063, "ġmouth": 4064, "icks": 4065, "ġattra": 4066, "ots": 4067, "ġencou": 4068, "ġfemale": 4069, "osis": 4070, "ships": 4071, "els": 4072, "ġvisual": 4073, "ġargument": 4074, "ġrout": 4075, "ġsafety": 4076, "ilities": 4077, "ġpermission": 4078, "ġrelated": 4079, "baby": 4080, "jpg": 4081, "ġsan": 4082, "ġuhh": 4083, "ġtrou": 4084, "ġveget": 4085, "ġshows": 4086, "ġtenant": 4087, "ġborn": 4088, "ġhanging": 4089, "ġmildly": 4090, "iry": 4091, "ġbase": 4092, "isting": 4093, "ġpink": 4094, "ġhilar": 4095, "ġcausing": 4096, "ġunfair": 4097, "ġeasily": 4098, "07": 4099, "ony": 4100, "ġwedding": 4101, "ġbusiness": 4102, ":": 4103, "ġintera": 4104, "ids": 4105, "raid": 4106, "ġneigh": 4107, "ils": 4108, "ggy": 4109, "ġunlike": 4110, "ġcustomer": 4111, "ġrevision": 4112, "cord": 4113, "dr": 4114, "ians": 4115, "llo": 4116, "ġnotes": 4117, "ġplaylist": 4118, "ġcomplic": 4119, "ġlegs": 4120, "ġgoddam": 4121, "ġexplaining": 4122, "ġshouted": 4123, "ġidc": 4124, "ġvolu": 4125, "ġterms": 4126, "ġcookies": 4127, "ġexample": 4128, "ġdetails": 4129, "ġmurder": 4130, "af": 4131, "ġil": 4132, "test": 4133, "ġinput": 4134, "ġlecture": 4135, "fff": 4136, "ġboss": 4137, "ġparts": 4138, "ġrespond": 4139, "ġholding": 4140, "ġdisgusting": 4141, "67": 4142, "ġsi": 4143, "ġginger": 4144, "ġjus": 4145, "ġstation": 4146, "ġfeet": 4147, "ġreality": 4148, "ġreaction": 4149, "lit": 4150, "ġhat": 4151, "ġmeal": 4152, "ette": 4153, "men": 4154, "estyle": 4155, "ġmarks": 4156, "hetic": 4157, "ġempty": 4158, "ġterrible": 4159, "ġhistory": 4160, "ġneighb": 4161, "ġ@": 4162, "isions": 4163, "ġordered": 4164, "this": 4165, "ġpotent": 4166, "ġrepea": 4167, "ġexplained": 4168, "ġindependent": 4169, "ġêķãģ£êĺ": 4170, "05": 4171, "ġnom": 4172, "ġhig": 4173, "ġphr": 4174, "ġprin": 4175, "ġrole": 4176, "hahah": 4177, "ġcod": 4178, "vered": 4179, "ġnick": 4180, "ġwhis": 4181, "ġmoi": 4182, "iber": 4183, "ġstarts": 4184, "ġmisunderst": 4185, "ġrecording": 4186, "ġmagic": 4187, "ġunsure": 4188, "ases": 4189, "ġrush": 4190, "ġsepar": 4191, "ives": 4192, "ummy": 4193, "ġoml": 4194, "ġbuying": 4195, "ġadding": 4196, "ġimagin": 4197, "sorry": 4198, "ġbehaviour": 4199, "ġdig": 4200, "ġdating": 4201, "ġhoo": 4202, "ġstudy": 4203, "ġblind": 4204, "ized": 4205, "oom": 4206, "sing": 4207, "ġsea": 4208, "ġeveryones": 4209, "ġharrow": 4210, "ussy": 4211, "ġchocolate": 4212, "88": 4213, "ġitll": 4214, "illed": 4215, "ġstore": 4216, "ġsoc": 4217, "ġanger": 4218, "ġmua": 4219, "ġnap": 4220, "cco": 4221, "ġshopping": 4222, "ġchairs": 4223, "ġcertainly": 4224, "ġinvolved": 4225, "ġdegree": 4226, "can": 4227, "lines": 4228, "ġave": 4229, "ġbes": 4230, "ġtooo": 4231, "ġarea": 4232, "neur": 4233, "ġpositive": 4234, "rest": 4235, "ġcream": 4236, "ġforced": 4237, "iron": 4238, "cape": 4239, "ġradio": 4240, "ġadvert": 4241, "ġskull": 4242, "ġpotato": 4243, "ġscene": 4244, "ġhilarious": 4245, "44": 4246, "ġreee": 4247, "ġkink": 4248, "ġkeys": 4249, "ġpregn": 4250, "ġnahhh": 4251, "ġafraid": 4252, "ġscam": 4253, "ġhole": 4254, "ġsentence": 4255, "ġmidnight": 4256, "ġdistracted": 4257, "bum": 4258, "ġug": 4259, "ġjul": 4260, "ġhomeless": 4261, "ġstatus": 4262, "ġdragon": 4263, "ġlegend": 4264, "ġextreme": 4265, "mod": 4266, "ġtun": 4267, "ana": 4268, "output": 4269, "ġfailed": 4270, "ġcolours": 4271, "ġoccas": 4272, "ġarticle": 4273, "dk": 4274, "ġlab": 4275, "icted": 4276, "ġks": 4277, "antic": 4278, "ġproof": 4279, "ġbringing": 4280, "ġspamming": 4281, "ġacceptable": 4282, "band": 4283, "ġheat": 4284, "ġmega": 4285, "rose": 4286, "ġbackground": 4287, "ġchecking": 4288, "ġflow": 4289, "ġbasic": 4290, "ġpointless": 4291, "ġstruggle": 4292, "ġstreet": 4293, "ġbullshit": 4294, "sr": 4295, "ġmal": 4296, "ġton": 4297, "ġprop": 4298, "ġpersonally": 4299, "ġtes": 4300, "ġdy": 4301, "ġeaten": 4302, "ġstaff": 4303, "ġupstairs": 4304, "ġsav": 4305, "ġdevice": 4306, "ġhappiness": 4307, "ġcoff": 4308, "ġdecisions": 4309, "ġlevels": 4310, "32": 4311, "87": 4312 }, "merges": [ [ "ġ", "t" ], [ "ġ", "i" ], [ "ġ", "a" ], [ "ġ", "w" ], [ "i", "n" ], [ "h", "e" ], [ "ġ", "s" ], [ "o", "u" ], [ "h", "a" ], [ "ġ", "b" ], [ "ġ", "m" ], [ "in", "g" ], [ "r", "e" ], [ "ġ", "c" ], [ "ġ", "l" ], [ "ġt", "he" ], [ "ġ", "o" ], [ "ġ", "d" ], [ "ġ", "f" ], [ "ġ", "y" ], [ "n", "d" ], [ "v", "e" ], [ "ġt", "o" ], [ "l", "l" ], [ "i", "s" ], [ "ġ", "n" ], [ "e", "e" ], [ "ġ", "p" ], [ "o", "n" ], [ "ġy", "ou" ], [ "a", "n" ], [ "ġ", "e" ], [ "t", "e" ], [ "o", "r" ], [ "ġ", "g" ], [ "k", "e" ], [ "ġi", "t" ], [ "ġa", "nd" ], [ "u", "s" ], [ "ġ", "ha" ], [ "ha", "t" ], [ "e", "s" ], [ "ġ", "u" ], [ "i", "t" ], [ "a", "s" ], [ "ġ", "he" ], [ "ġ", "h" ], [ "o", "m" ], [ "i", "c" ], [ "ġn", "o" ], [ "ġt", "h" ], [ "ġb", "e" ], [ "u", "t" ], [ "ġm", "e" ], [ "a", "y" ], [ "a", "r" ], [ "o", "o" ], [ "ġi", "n" ], [ "e", "r" ], [ "ġt", "hat" ], [ "ġi", "s" ], [ "l", "d" ], [ "l", "e" ], [ "ġb", "ut" ], [ "ġw", "e" ], [ "i", "e" ], [ "o", "w" ], [ "n", "t" ], [ "ġm", "y" ], [ "ġl", "i" ], [ "a", "t" ], [ "l", "y" ], [ "ġ", "j" ], [ "ġo", "f" ], [ "ou", "ld" ], [ "i", "ll" ], [ "ġ", "re" ], [ "us", "t" ], [ "ġc", "ha" ], [ "i", "d" ], [ "ġd", "o" ], [ "ġo", "n" ], [ "g", "h" ], [ "o", "d" ], [ "e", "t" ], [ "e", "d" ], [ "ġw", "as" ], [ "ġha", "ve" ], [ "r", "is" ], [ "r", "a" ], [ "ġs", "t" ], [ "ġl", "o" ], [ "a", "l" ], [ "ġcha", "ris" ], [ "ġf", "or" ], [ "ġ", "k" ], [ "ġli", "ke" ], [ "ġi", "m" ], [ "k", "ing" ], [ "ġno", "t" ], [ "m", "e" ], [ "ou", "t" ], [ "ee", "n" ], [ "ġs", "o" ], [ "a", "ll" ], [ "r", "o" ], [ "ee", "d" ], [ "ġw", "ill" ], [ "ġe", "l" ], [ "r", "y" ], [ "an", "t" ], [ "ġa", "n" ], [ "c", "h" ], [ "ġj", "ust" ], [ "i", "r" ], [ "a", "d" ], [ "ġb", "een" ], [ "ġel", "od" ], [ "ġthe", "y" ], [ "ġelod", "ie" ], [ "s", "e" ], [ "ġw", "h" ], [ "'", "s" ], [ "ġs", "h" ], [ "ġw", "hat" ], [ "e", "a" ], [ "gh", "t" ], [ "ġs", "he" ], [ "ġa", "t" ], [ "all", "y" ], [ "c", "e" ], [ "c", "k" ], [ "ġc", "an" ], [ "ġth", "is" ], [ "i", "on" ], [ "c", "t" ], [ "ġf", "r" ], [ "om", "e" ], [ "te", "r" ], [ "it", "h" ], [ "ġa", "re" ], [ "o", "t" ], [ "ġw", "ith" ], [ "ġ", "ke" ], [ "s", "t" ], [ "v", "in" ], [ "ġd", "on" ], [ "us", "e" ], [ "ġa", "b" ], [ "ġm", "o" ], [ "ġ", ":" ], [ "ġke", "vin" ], [ "ġa", "s" ], [ "ġe", "ve" ], [ "o", "p" ], [ "a", "ke" ], [ "ġit", "s" ], [ "ġa", "l" ], [ "g", "e" ], [ "ġ", "r" ], [ "ġw", "ant" ], [ "ġu", "p" ], [ "ġ", "v" ], [ "ġs", "u" ], [ ".", "." ], [ "in", "k" ], [ "te", "n" ], [ "ġyou", "r" ], [ "ġc", "a" ], [ "ġw", "or" ], [ "i", "ght" ], [ "p", "p" ], [ "ġi", "f" ], [ "ve", "r" ], [ "i", "m" ], [ "ġk", "n" ], [ "ġab", "out" ], [ "'", "t" ], [ "e", "n" ], [ "ġn", "eed" ], [ "ġg", "et" ], [ "oo", "d" ], [ "ic", "e" ], [ "ġfr", "om" ], [ "ġlo", "ve" ], [ "ġhe", "r" ], [ "a", "b" ], [ "ġw", "ould" ], [ "ou", "r" ], [ "ġw", "he" ], [ "ġkn", "ow" ], [ "ġha", "d" ], [ "ġo", "r" ], [ "a", "m" ], [ "ġl", "is" ], [ "te", "d" ], [ "u", "r" ], [ "ġl", "oo" ], [ "es", "s" ], [ "ġf", "ee" ], [ "ġlis", "ten" ], [ "ġs", "a" ], [ "ġl", "e" ], [ "ġc", "ould" ], [ "ġg", "o" ], [ "ġfee", "l" ], [ "ġw", "eed" ], [ "ġsh", "ould" ], [ "ġd", "id" ], [ "in", "d" ], [ "ġa", "m" ], [ "ġwh", "y" ], [ "u", "n" ], [ "on", "e" ], [ "ġa", "ll" ], [ "ġp", "l" ], [ "ġi", "d" ], [ "ġs", "e" ], [ "he", "r" ], [ "ġd", "e" ], [ "t", "h" ], [ "ġe", "x" ], [ "c", "a" ], [ "ġno", "w" ], [ "'", "m" ], [ "n", "a" ], [ "ġs", "ee" ], [ "i", "ve" ], [ "v", "ing" ], [ "f", "f" ], [ "ġp", "ro" ], [ "ġwe", "re" ], [ "ġth", "ink" ], [ "ġ", "x" ], [ "ġdon", "t" ], [ "ġbut", "t" ], [ "ġh", "ow" ], [ "k", "s" ], [ "ġwhe", "n" ], [ "ġn", "e" ], [ "u", "m" ], [ "re", "d" ], [ "ġ", "1" ], [ "ġs", "ome" ], [ "ġ", "out" ], ["ġlo", "l" ], [ "ġo", "h" ], [ "ġo", "k" ], [ "ġan", "y" ], [ "ġ", "'" ], [ "ea", "h" ], [ "ġto", "o" ], [ "ġc", "om" ], [ "s", "s" ], [ "ou", "nd" ], [ "ġy", "eah" ], [ "ġs", "m" ], [ "u", "l" ], [ "i", "l" ], [ "ġm", "us" ], [ "ġeve", "n" ], [ "a", "in" ], [ "ġre", "ally" ], [ "ġmus", "ic" ], [ "'", "re" ], [ "ġf", "u" ], [ "p", "e" ], [ "ġc", "on" ], [ "ġ", "-" ], [ "ġp", "e" ], [ "ġth", "ing" ], [ "ġo", "m" ], [ "i", "me" ], [ "s", "o" ], [ "ee", "p" ], [ "ġf", "a" ], [ "ġh", "is" ], [ "ġ", "q" ], [ "m", "a" ], [ "ġthe", "ir" ], [ "ha", "n" ], [ "ġthe", "m" ], [ "in", "e" ], [ "ġthat", "s" ], [ "l", "f" ], [ "ġs", "or" ], [ "es", "t" ], [ "a", "te" ], [ "ġa", "w" ], [ "ay", "s" ], [ "g", "g" ], [ "ġon", "e" ], [ "t", "ing" ], [ "on", "g" ], [ "ġm", "ust" ], [ "ġm", "ake" ], [ "ġv", "o" ], [ "ġy", "es" ], [ "ġt", "han" ], [ "ġha", "s" ], [ "ġt", "r" ], [ "ġlisten", "ing" ], [ "ġca", "use" ], [ "at", "ion" ], [ "ġw", "an" ], [ "ġg", "ood" ], [ "ġx", "d" ], [ "ġal", "so" ], [ "ġme", "an" ], [ "ġthe", "n" ], [ "ġu", "r" ], [ "ġbe", "ca" ], [ "ra", "in" ], [ "ġom", "g" ], [ "ġt", "ime" ], [ "ġthe", "re" ], [ "ġbeca", "use" ], [ "as", "t" ], [ "ġ", "us" ], [ "ġs", "ay" ], [ "ġh", "im" ], [ "ġsor", "ry" ], [ "o", "re" ], [ "ġ", "2" ], [ "ġth", "ou" ], [ "f", "e" ], [ "ġb", "a" ], [ "ġst", "u" ], [ "ġw", "a" ], [ "h", "ing" ], [ "ġt", "e" ], [ "o", "l" ], [ "t", "y" ], [ "ġhe", "ar" ], [ "ġt", "ake" ], [ "ġdo", "es" ], [ "ġg", "ot" ], [ "ġs", "p" ], [ "ġ", "(" ], [ "i", "te" ], [ "ġt", "b" ], [ "ġmo", "re" ], [ "u", "e" ], [ "ġwe", "ll" ], [ "b", "e" ], [ "ġwh", "o" ], [ "ġu", "n" ], [ "ġa", "g" ], [ "i", "g" ], [ "ġpe", "op" ], [ "ġe", "n" ], [ "id", "e" ], [ "ġvo", "ice" ], [ "ġth", "o" ], [ "!", "!" ], [ "ic", "k" ], [ "ġpro", "b" ], [ "ġa", "ct" ], [ "ġs", "l" ], [ "ġl", "ea" ], [ "ġm", "u" ], [ "a", "king" ], [ "ġpeop", "le" ], [ "..", "." ], [ "ġl", "ma" ], [ "ġg", "u" ], [ "p", "er" ], [ "ġa", "r" ], [ "ġp", "o" ], [ "u", "ally" ], [ "ġf", "e" ], [ "ġbe", "ing" ], [ "ġon", "ly" ], [ "ġpl", "ay" ], [ "ġeve", "ry" ], [ "ġ", "our" ], [ "ġ:", ")" ], [ "er", "s" ], [ "om", "et" ], [ "d", "s" ], [ "ġe", "at" ], [ "ġha", "ha" ], [ "ġd", "is" ], [ "ġmu", "ch" ], [ "ġs", "omet" ], [ "ġc", "u" ], [ "ke", "d" ], [ "ġd", "r" ], [ "or", "t" ], [ "i", "f" ], [ "ġp", "re" ], [ "p", "s" ], [ "ou", "s" ], [ "d", "d" ], [ "p", "t" ], [ "ġloo", "k" ], [ "ar", "n" ], [ "ġt", "al" ], [ "ġwan", "na" ], [ "t", "her" ], [ "ġc", "h" ], [ "ġg", "e" ], [ "oo", "l" ], [ "ġw", "r" ], [ "ġthing", "s" ], [ "a", "nd" ], [ "o", "s" ], [ "a", "g" ], [ "ġs", "ound" ], [ "ġt", "ou" ], [ "d", "ay" ], [ "ġo", "ver" ], [ "ġb", "y" ], [ "se", "lf" ], [ "i", "b" ], [ "ġm", "ay" ], [ "?", "?" ], [ "o", "y" ], [ "ar", "t" ], [ "ġha", "pp" ], [ "ġtb", "h" ], [ "ġhe", "l" ], [ "ġb", "it" ], [ "w", "w" ], [ "ġlma", "o" ], [ "nd", "er" ], [ "is", "s" ], [ "ġb", "ad" ], [ "ġwor", "k" ], [ "ġstu", "ff" ], [ "ġn", "a" ], [ "ġthou", "ght" ], [ "ġt", "ry" ], [ "ġp", "h" ], [ "ġb", "rain" ], [ "ġst", "ill" ], [ "ġl", "a" ], [ "ġk", "ind" ], [ "ġca", "re" ], [ "ġa", "f" ], [ "ġgo", "ing" ], [ "ġsm", "o" ], [ "ġcom", "p" ], [ "ġle", "arn" ], [ "or", "ge" ], [ "'", "ve" ], [ "u", "re" ], [ "ġsh", "it" ], [ "'", "ll" ], [ "ġg", "ive" ], [ "ġof", "f" ], [ "me", "nt" ], [ "as", "s" ], [ "ġmo", "ve" ], [ "ġfu", "ck" ], [ "ġ", "ve" ], [ "ġ:", "(" ], [ "ġb", "l" ], [ "th", "ing" ], [ "'", "d" ], [ "re", "ss" ], [ "ġy", "e" ], [ "ġhel", "p" ], [ "t", "o" ], [ "ab", "le" ], [ "ġwa", "it" ], [ "ġge", "orge" ], [ "c", "c" ], [ "ġc", "at" ], [ "ġact", "ually" ], [ "ġid", "k" ], [ "ġc", "o" ], [ "i", "es" ], [ "ll", "y" ], [ "ie", "nd" ], [ "ġc", "l" ], [ "ġm", "an" ], [ "ou", "gh" ], [ "ġsa", "id" ], [ "0", "0" ], [ "ġsa", "me" ], [ "ġba", "ck" ], [ "ġloo", "king" ], [ "a", "ge" ], [ "as", "e" ], [ "t", "s" ], [ "ġtou", "ch" ], [ "â", "ģ" ], [ "ġsomet", "hing" ], [ "ic", "h" ], [ "re", "e" ], [ "ġc", "ant" ], [ "ġd", "ay" ], [ "ġw", "ay" ], [ "ou", "se" ], [ "ġd", "ra" ], [ "ġc", "ome" ], [ "re", "nt" ], [ "ġ", "ra" ], [ "is", "h" ], [ "re", "am" ], [ "c", "i" ], [ "ġa", "pp" ], [ "ġj", "o" ], [ "h", "h" ], [ "ġo", "ther" ], [ "ġd", "i" ], [ "ġsl", "eep" ], [ "ġq", "u" ], [ "m", "b" ], [ "ġha", "te" ], [ "ġt", "aking" ], [ "ġ", "3" ], [ "ġt", "w" ], [ "ow", "n" ], [ "ġr", "ight" ], [ "ġs", "c" ], [ "i", "le" ], [ "ġdo", "ing" ], [ "ġaf", "ter" ], [ "ar", "d" ], [ "y", "s" ], [ "in", "king" ], [ "ġbe", "t" ], [ "ġhe", "re" ], [ "l", "t" ], [ "ġc", "all" ], [ "ġ", "*" ], [ "p", "l" ], [ "ġmo", "ving" ], [ "u", "d" ], [ "ġ", "\"" ], [ "ġp", "r" ], [ "t", "t" ], [ "ġwh", "ich" ], [ "ġne", "ver" ], [ "ġp", "ic" ], [ "ġf", "ood" ], [ "ġd", "ays" ], [ "ġin", "to" ], [ "ġfeel", "ing" ], [ "ġn", "ice" ], [ "ġwant", "ing" ], [ "ġ", "ro" ], [ "ġfr", "iend" ], [ "is", "t" ], [ "ab", "ly" ], [ "ġl", "et" ], [ "ġm", "ad" ], [ "ġlea", "ve" ], [ "m", "s" ], [ "ġve", "ry" ], [ "ġb", "r" ], [ "ġh", "ouse" ], [ "ġc", "ool" ], [ "ġdra", "w" ], [ "i", "p" ], [ "ġw", "ee" ], [ "ġh", "op" ], [ "ġag", "ain" ], [ "m", "m" ], [ "ġ", "<" ], [ "f", "ore" ], [ "ck", "s" ], [ "ġm", "ight" ], [ "ġu", "nder" ], [ "u", "g" ], [ "ġli", "fe" ], [ "ie", "d" ], [ "c", "om" ], [ "it", "y" ], [ "ġha", "r" ], [ "i", "x" ], [ "ġte", "ll" ], [ "c", "king" ], [ "ġa", "d" ], [ "ġb", "o" ], [ "ġprob", "ably" ], [ "an", "ce" ], [ "ġc", "he" ], [ "ġfa", "ir" ], [ "ġfe", "lt" ], [ "t", "a" ], [ "ġmay", "be" ], [ "r", "ow" ], [ "ġg", "on" ], [ "ir", "d" ], [ "ġdid", "nt" ], [ "ġbe", "fore" ], [ "ġh", "our" ], [ "g", "et" ], [ "re", "at" ], [ "g", "le" ], [ "ġp", "le" ], [ "i", "ous" ], [ "ġtal", "k" ], [ "ġs", "ca" ], [ "ġb", "ab" ], [ "i", "gh" ], [ "ġf", "l" ], [ "ġsmo", "king" ], [ "ġsu", "re" ], [ "ġb", "ot" ], [ "ġhop", "e" ], [ "in", "t" ], [ "is", "e" ], [ "ġd", "own" ], [ "a", "ce" ], [ "ġsu", "pp" ], [ "ġwe", "ird" ], [ "ġgon", "na" ], [ "a", "a" ], [ "te", "ra" ], [ "ġi", "ll" ], [ "oo", "o" ], [ "w", "ays" ], [ "ġye", "ar" ], [ "ġp", "er" ], [ "ġre", "al" ], [ ":", "/" ], [ "c", "he" ], [ "ġm", "in" ], [ "on", "t" ], [ "at", "ch" ], [ "ġple", "ase" ], [ "c", "o" ], [ "ġkind", "a" ], [ "im", "es" ], [ "n", "e" ], [ "q", "u" ], [ "or", "d" ], [ "ġsca", "red" ], [ "ġ", "4" ], [ "ad", "y" ], [ "a", "ct" ], [ "ġh", "tt" ], [ "s", "h" ], [ "ġal", "ways" ], [ "ġb", "oo" ], [ "ġph", "one" ], [ "an", "s" ], [ "ġm", "um" ], [ "ġl", "ong" ], [ "ul", "t" ], [ "ġli", "tera" ], [ "ġneed", "ing" ], [ "ġany", "thing" ], [ "ġf", "ind" ], [ "x", "t" ], [ "ġf", "un" ], [ "ġlo", "t" ], [ "ġmy", "self" ], [ "ġsound", "s" ], [ "ġtr", "ue" ], [ "ic", "ally" ], [ "ion", "s" ], [ "ġin", "s" ], [ "ġwhe", "re" ], [ "or", "m" ], [ "ġbet", "ter" ], [ "ġid", "ea" ], [ "ġid", "e" ], [ "ġbe", "d" ], [ "ġst", "ream" ], [ "ġb", "u" ], [ "ġse", "x" ], [ "ġto", "day" ], [ "ġ", "5" ], [ "ġp", "ers" ], [ "ġf", "ro" ], [ "ġ", "use" ], [ "ar", "k" ], [ "ġmad", "e" ], [ "ff", "e" ], [ "ġlitera", "lly" ], [ "ġtry", "ing" ], [ "ġwor", "d" ], [ "mb", "er" ], [ "it", "ing" ], [ "ġthan", "ks" ], [ "o", "g" ], [ "ġne", "w" ], [ "ġhapp", "en" ], [ "ġlea", "ving" ], [ "ġhar", "d" ], [ "!", "?" ], [ "ġb", "re" ], [ "ġs", "pe" ], [ "ġp", "a" ], [ "f", "t" ], [ "ġb", "ro" ], [ "ġre", "s" ], [ "ġpers", "on" ], [ "ġget", "ting" ], [ "ġeat", "ing" ], [ "b", "s" ], [ "ġm", "a" ], [ "ġre", "me" ], [ "ġst", "op" ], [ "ġt", "ur" ], [ "ir", "st" ], [ "ġp", "art" ], [ "ġfu", "cking" ], [ "t", "f" ], [ "ib", "le" ], [ "m", "or" ], [ "ġla", "ter" ], [ "ġunder", "st" ], [ "w", "ay" ], [ "ġhear", "ing" ], [ "ġsay", "ing" ], [ "ġg", "r" ], [ "at", "ing" ], [ "âģ", "ļ" ], [ "n", "g" ], [ "ġs", "ha" ], [ "ġm", "on" ], [ "ġa", "cc" ], [ "l", "es" ], [ "ġlearn", "ing" ], [ "ġf", "irst" ], [ "ġs", "ong" ], [ "ġm", "ess" ], [ "ġn", "ight" ], [ "t", "le" ], [ "p", "le" ], [ "f", "a" ], [ "od", "y" ], [ "ġst", "ar" ], [ "ġe", "nd" ], [ "ġhapp", "y" ], [ "ġcu", "te" ], [ "ġth", "r" ], [ "ġmean", "s" ], [ "ġp", "ut" ], [ "ġh", "ome" ], [ "ġhtt", "ps" ], [ "ġso", "on" ], [ "t", "ty" ], [ "ġp", "ick" ], [ "â", "£" ], [ "ġeve", "r" ], [ "ġm", "ind" ], [ "ġha", "ving" ], [ "as", "on" ], [ "ġplay", "ing" ], [ "um", "b" ], [ "n", "ce" ], [ "ġthou", "gh" ], [ "ġhaha", "ha" ], [ "ġaw", "ay" ], [ "ġw", "tf" ], [ "ġpre", "tty" ], [ "i", "z" ], [ "u", "al" ], [ "w", "n" ], [ "ġmake", "s" ], [ "ġl", "it" ], [ "ġdon", "e" ], [ "f", "ul" ], [ "ġreme", "mber" ], [ "ġc", "ou" ], [ "ġunderst", "and" ], [ "ġan", "n" ], [ "ġtouch", "ing" ], [ "ġfro", "gg" ], [ "ġsome", "one" ], [ "ġ", "=" ], [ "ġm", "iss" ], [ "i", "re" ], [ "ġe", "m" ], [ "ġle", "g" ], [ "ġdr", "ink" ], [ "ġcat", "s" ], [ "ġ", "â£" ], [ "ġo", "p" ], [ "ġdis", "c" ], [ "r", "i" ], [ "n", "ing" ], [ "ġdid", "n" ], [ "ġf", "ine" ], [ "ġd", "ad" ], [ "ġthr", "ough" ], [ "ġk", "eep" ], [ "un", "ch" ], [ "u", "ter" ], [ "ġg", "a" ], [ "ġsh", "ow" ], [ "ġre", "ason" ], [ "ġbot", "h" ], [ "ġf", "in" ], [ "ġtw", "o" ], [ "a", "k" ], [ "ġcomp", "uter" ], [ "ġt", "ra" ], [ "ġs", "ad" ], [ "ġsee", "ing" ], [ "ġs", "w" ], [ "ġto", "ld" ], [ "ġcon", "f" ], [ "ġg", "od" ], [ "a", "ch" ], [ "ġsm", "ink" ], [ "i", "ld" ], [ "ġna", "me" ], [ "ġim", "p" ], [ "n", "o" ], [ "ġle", "ast" ], [ "ġtal", "king" ], [ "a", "ted" ], [ "ġany", "way" ], [ "ġwr", "iting" ], [ "ġann", "oy" ], [ "an", "c" ], [ "ġw", "ont" ], [ "ġhour", "s" ], [ "ġf", "re" ], [ "ġa", "dd" ], [ "ġsp", "ea" ], [ "ġwan", "ted" ], [ "ġpic", "king" ], [ "ġen", "ough" ], [ "ud", "e" ], [ "ġin", "te" ], [ "ġs", "er" ], [ "ġb", "as" ], [ "ġb", "est" ], [ "ġe", "t" ], [ "te", "s" ], [ "re", "ady" ], [ "ġre", "ad" ], [ "ġi", "ve" ], [ "o", "se" ], [ "ġbab", "y" ], [ "ġs", "y" ], [ "ar", "y" ], [ "ġthe", "se" ], [ "ġg", "en" ], [ "ġo", "ld" ], [ "ġal", "ready" ], [ "ġg", "ir" ], [ "ġgu", "ess" ], [ "at", "h" ], [ "ġa", "ut" ], [ "ġh", "o" ], [ "ġj", "e" ], [ "ġh", "on" ], [ "ġto", "mor" ], [ "ġfrogg", "y" ], [ "ġc", "le" ], [ "ġmo", "st" ], [ "ġus", "ed" ], [ "n", "ess" ], [ "ve", "n" ], [ "ww", "w" ], [ "ġin", "c" ], [ "it", "her" ], [ "ġtomor", "row" ], [ "n", "er" ], [ "!!", "!" ], [ "ġw", "al" ], [ "ġhe", "s" ], [ "ġas", "s" ], [ "ġevery", "thing" ], [ "ar", "ly" ], [ "ġan", "gle" ], [ "ġdoes", "nt" ], [ "ġpl", "an" ], [ "ġt", "y" ], [ "ġl", "unch" ], [ "ġs", "en" ], [ "ġna", "h" ], [ "ġlit", "tle" ], [ "ġdraw", "ing" ], [ "ġst", "r" ], [ "ġnot", "hing" ], [ "ġa", "ce" ], [ "ġfa", "ce" ], [ "ġd", "es" ], [ "ġse", "lf" ], [ "ġl", "ast" ], [ "ġm", "aking" ], [ "m", "o" ], [ "l", "ess" ], [ "ġli", "ve" ], [ "ġex", "act" ], [ "ġwr", "ong" ], [ "ġg", "l" ], [ "ġse", "nd" ], [ "ġo", "wn" ], [ "one", "y" ], [ "ig", "n" ], [ "ġs", "it" ], [ "ġsu", "per" ], [ "ġhon", "est" ], [ "ġcon", "t" ], [ "ġde", "f" ], [ "??", "?" ], [ "ver", "s" ], [ "ġde", "c" ], [ "ġex", "pl" ], [ "y", "y" ], [ "l", "i" ], [ "ġm", "ix" ], [ "ġhe", "ad" ], [ "ġh", "u" ], [ "ġc", "r" ], [ "ġ1", "0" ], [ "ġfriend", "s" ], [ "ġevery", "one" ], [ "ġro", "om" ], [ "ġ", "6" ], [ "ġon", "ce" ], [ "ġpo", "int" ], [ "ġdi", "ffe" ], [ "ee", "t" ], [ "ġwee", "k" ], [ "d", "e" ], [ "i", "v" ], [ "ġw", "atch" ], [ "ġmon", "th" ], [ "ġs", "k" ], [ "ġas", "k" ], [ "\"", ":" ], [ "an", "g" ], [ "ee", "e" ], [ "ġthan", "k" ], [ "o", "c" ], [ "ġthought", "s" ], [ "ġst", "art" ], [ "ġth", "inking" ], [ "ġar", "ound" ], [ "ġw", "o" ], [ "ġc", "ry" ], [ "ġl", "ess" ], [ "ġme", "nt" ], [ "ke", "n" ], [ "ġar", "t" ], [ "1", "0" ], [ "ġa", "c" ], [ "ġme", "ant" ], [ "m", "ed" ], [ "rent", "ly" ], [ "ġel", "se" ], [ "ġc", "ra" ], [ "ġpe", "te" ], [ "ġan", "x" ], [ "ġme", "ds" ], [ "ġne", "xt" ], [ "p", "h" ], [ "ġwor", "ds" ], [ "ra", "te" ], [ "ġexact", "ly" ], [ "ġm", "ine" ], [ "ġok", "ay" ], [ "ġcu", "z" ], [ "ġen", "g" ], [ "ġh", "i" ], [ "ġgir", "l" ], [ "ġsee", "ms" ], [ "ġhe", "y" ], [ "ġt", "imes" ], [ "ġu", "gh" ], [ "ġshe", "s" ], [ "ġre", "l" ], [ "w", "he" ], [ "ct", "ion" ], [ "ġb", "ig" ], [ "ġtb", "f" ], [ "ġs", "im" ], [ "ġf", "ound" ], [ "ġi", "nd" ], [ "ġinte", "re" ], [ "ġle", "ft" ], [ "ġre", "p" ], [ "i", "red" ], [ "w", "ard" ], [ "b", "ody" ], [ "an", "ge" ], [ "ġ", "9" ], [ "es", "ome" ], [ "ġg", "reat" ], [ "ġ", "[" ], [ "w", "a" ], [ "ġloo", "ks" ], [ "ġp", "ar" ], [ "ġl", "m" ], [ "ġh", "ug" ], [ "oo", "k" ], [ "ġleg", "it" ], [ "b", "o" ], [ "on", "s" ], [ "ġneed", "s" ], [ "ġpl", "ace" ], [ "ġwee", "ks" ], [ "ġwant", "s" ], [ "n", "y" ], [ "ġm", "oney" ], [ "ce", "pt" ], [ "ic", "al" ], [ "ġo", "b" ], [ "ġdoes", "n" ], [ "ġm", "ain" ], [ "ġc", "ur" ], [ "ð", "ł" ], [ "a", "z" ], [ "ġ", "8" ], [ "ġd", "am" ], [ "i", "ly" ], [ "ġs", "et" ], [ "ġc", "hat" ], [ "ġwe", "nt" ], [ "ġcom", "m" ], [ "ic", "es" ], [ "ġjo", "b" ], [ "ot", "her" ], [ "ro", "id" ], [ "ġ2", "0" ], [ "nt", "h" ], [ "ġlm", "fa" ], [ "u", "ck" ], [ "ke", "t" ], [ "ġtho", "se" ], [ "ġprob", "le" ], [ "ġsupp", "ort" ], [ "ġun", "i" ], [ "ġk", "iss" ], [ "ġin", "ter" ], [ "ġcha", "n" ], [ "ġjo", "ke" ], [ "i", "o" ], [ "l", "o" ], [ "nd", "om" ], [ "or", "y" ], [ "ġh", "or" ], [ "nd", "s" ], [ "ġbre", "ak" ], [ "ġs", "te" ], [ "ġab", "le" ], [ "ġh", "ur" ], [ "ġwith", "out" ], [ "ġaw", "esome" ], [ "ce", "nt" ], [ "i", "a" ], [ "ġman", "y" ], [ "ġe", "ither" ], [ "ġy", "ay" ], [ "n", "s" ], [ "ġp", "ass" ], [ "ġyear", "s" ], [ "ar", "ds" ], [ "ġcon", "s" ], [ "ġf", "ree" ], [ "ġst", "ress" ], [ "ġte", "ch" ], [ "ġannoy", "ing" ], [ "us", "ed" ], [ "ġsh", "op" ], [ "ġide", "as" ], [ "ġp", "ain" ], [ "ġs", "ch" ], [ "ġgot", "ta" ], [ "a", "c" ], [ "ġk", "id" ], [ "ha", "ha" ], [ "ġp", "ay" ], [ "ġs", "ing" ], [ "he", "d" ], [ "ġcom", "ing" ], [ "ġhe", "ll" ], [ "f", "g" ], [ "ġb", "oy" ], [ "ġwho", "le" ], [ "ġ", "7" ], [ "ġo", "o" ], [ "ġexpl", "ain" ], [ "u", "b" ], [ "ġhonest", "ly" ], [ "ġf", "an" ], [ "get", "her" ], [ "e", "l" ], [ "ġwr", "ite" ], [ "k", "r" ], [ "t", "w" ], [ "ġw", "ish" ], [ "ġm", "ar" ], [ "ke", "r" ], [ "ġwor", "king" ], [ "ġet", "c" ], [ "t", "ra" ], [ "ġs", "na" ], [ "it", "ar" ], [ "ġdiffe", "rent" ], [ "ġpro", "per" ], [ "ġga", "me" ], [ "d", "er" ], [ "es", "e" ], [ "p", "ress" ], [ "ġto", "gether" ], [ "pe", "ct" ], [ "is", "ed" ], [ "f", "u" ], [ "ġsy", "nth" ], [ "ġd", "j" ], [ "ġsu", "ch" ], [ "ation", "s" ], [ "it", "ch" ], [ "u", "u" ], [ "2", "0" ], [ "ġgu", "itar" ], [ "ġfor", "get" ], [ "l", "ing" ], [ "a", "p" ], [ "ġ:", "/" ], [ "ġsomet", "imes" ], [ "e", "w" ], [ "ġwhat", "s" ], [ "a", "ck" ], [ "ġst", "ay" ], [ "ks", "ks" ], [ "ġe", "s" ], [ "ġp", "ast" ], [ "ġbe", "l" ], [ "u", "es" ], [ "ġm", "ood" ], [ "ie", "w" ], [ "ġa", "aa" ], [ "ġall", "ow" ], [ "us", "s" ], [ "al", "s" ], [ "ó", "¾" ], [ "a", "re" ], [ "ġsch", "ool" ], [ "ġbo", "red" ], [ "ġis", "s" ], [ "ious", "ly" ], [ "ġr", "ing" ], [ "c", "ted" ], [ "ġlmfa", "o" ], [ "ou", "d" ], [ "ġy", "et" ], [ "ä", "¡" ], [ "ġcall", "ed" ], [ "ġd", "ick" ], [ "ġe", "as" ], [ "m", "it" ], [ "ġwor", "r" ], [ "ġi", "kr" ], [ "te", "nt" ], [ "ide", "nt" ], [ "y", "ing" ], [ "ġk", "it" ], [ "te", "ad" ], [ "ġv", "ide" ], [ "ol", "og" ], [ "n", "ight" ], [ "ġra", "ndom" ], [ "ġop", "en" ], [ "ġhave", "nt" ], [ "ġ", "rain" ], [ "ġse", "nt" ], [ "ġe", "ach" ], [ "ġdisc", "ord" ], [ "ġc", "re" ], [ "ġe", "ar" ], [ "is", "m" ], [ "ġsmo", "ke" ], [ "ġcha", "ir" ], [ "ġins", "tead" ], [ "i", "i" ], [ "ġt", "ea" ], [ "ġh", "igh" ], [ "ġw", "ow" ], [ "ġres", "p" ], [ "ġbl", "an" ], [ "st", "ing" ], [ "ġsu", "n" ], [ "ġgu", "y" ], [ "ġdr", "inking" ], [ "ġde", "cks" ], [ "ġ", ">" ], [ "eep", "y" ], [ "ġmin", "s" ], [ "ġgu", "ys" ], [ "ġsm", "inking" ], [ "ġbr", "ing" ], [ "j", "e" ], [ "n", "ed" ], [ "ġu", "nt" ], [ "ġt", "en" ], [ "ci", "al" ], [ "p", "ing" ], [ "ġg", "ay" ], [ "ġpo", "ss" ], [ "c", "ked" ], [ "1", "00" ], [ "ġin", "f" ], [ "am", "es" ], [ "ġfeel", "s" ], [ "s", "w" ], [ "ġis", "nt" ], [ "ġbas", "ically" ], [ "g", "n" ], [ "ġlma", "ooo" ], [ "ġkn", "ew" ], [ "r", "ight" ], [ "ġs", "igh" ], [ "m", "ore" ], [ "ġm", "od" ], [ "ġb", "us" ], [ "ġim", "a" ], [ "re", "ct" ], [ "u", "be" ], [ "ll", "e" ], [ "r", "r" ], [ "n", "c" ], [ "ve", "l" ], [ "nt", "s" ], [ "ġthere", "s" ], [ "it", "s" ], [ "hh", "h" ], [ "ġspea", "k" ], [ "ġd", "ark" ], [ "ġh", "mm" ], [ "b", "y" ], [ "ie", "ty" ], [ "ġp", "ur" ], [ "s", "et" ], [ "ġme", "d" ], [ "ġr", "ly" ], [ "a", "pp" ], [ "as", "h" ], [ "ġr", "ude" ], [ "ġeng", "l" ], [ "ic", "s" ], [ "whe", "re" ], [ "ġg", "eepy" ], [ "o", "x" ], [ "ġto", "ile" ], [ "ġas", "ked" ], [ "ll", "ing" ], [ "al", "th" ], [ "ġsa", "w" ], [ "ġus", "ing" ], [ "ġsen", "se" ], [ "p", "y" ], [ "l", "ine" ], [ "3", "0" ], [ "ġca", "me" ], [ "ġo", "ven" ], [ "ro", "ss" ], [ "le", "d" ], [ "os", "ed" ], [ "ġn", "on" ], [ "ġse", "co" ], [ "ġtoile", "t" ], [ "ġar", "g" ], [ "ġdo", "or" ], [ "ġg", "ra" ], [ "ġno", "oo" ], [ "ġan", "other" ], [ "ġt", "rain" ], [ "ġwas", "nt" ], [ "ġex", "per" ], [ "ġunt", "il" ], [ "ll", "ow" ], [ "ġany", "more" ], [ "ġk", "ill" ], [ "ġt", "a" ], [ "ġb", "or" ], [ "ġha", "lf" ], [ "id", "er" ], [ "ment", "s" ], [ "if", "ic" ], [ "ġwor", "se" ], [ "j", "oy" ], [ "ġh", "um" ], [ "ġal", "one" ], [ "i", "an" ], [ "ġs", "in" ], [ "at", "her" ], [ "ġqu", "est" ], [ "g", "ry" ], [ "ġmess", "age" ], [ "ġd", "a" ], [ "ġc", "ar" ], [ "ġag", "o" ], [ "ġchan", "ge" ], [ "ġ", "âģ" ], [ "ġwas", "n" ], [ "ġtur", "nt" ], [ "ġc", "ol" ], [ "ind", "ow" ], [ "ġstar", "ted" ], [ "le", "te" ], [ "ġh", "ot" ], [ "ġx", "x" ], [ "ur", "ing" ], [ "ġplay", "ed" ], [ "re", "en" ], [ "ġha", "ir" ], [ "ġen", "joy" ], [ "ġsee", "m" ], [ "ve", "s" ], [ "ġan", "sw" ], [ "ġturnt", "able" ], [ "c", "ome" ], [ "ġf", "o" ], [ "n", "ot" ], [ "ġd", "umb" ], [ "ġr", "n" ], [ "ġ", "0" ], [ "ġm", "ark" ], [ "ġcu", "dd" ], [ "ġt", "ired" ], [ "ġcou", "ple" ], [ "ġcomm", "un" ], [ "ġ", ";" ], [ "w", "ards" ], [ "ġl", "ate" ], [ "ġp", "an" ], [ "ġc", "al" ], [ "ġbel", "ie" ], [ "ġknow", "n" ], [ "ġgl", "ad" ], [ "s", "c" ], [ "ã", "©" ], [ "ġn", "orm" ], [ "ġvide", "o" ], [ "ġe", "le" ], [ "ou", "p" ], [ "ġ1", "2" ], [ "ġaw", "ww" ], [ "che", "d" ], [ "l", "ic" ], [ "ġimp", "ort" ], [ "ġal", "right" ], [ "ġd", "ance" ], [ "ġre", "g" ], [ "ġmix", "er" ], [ "ġsin", "ce" ], [ "v", "o" ], [ "ġaw", "k" ], [ "u", "te" ], [ "a", "h" ], [ "on", "es" ], [ "i", "er" ], [ "ġsna", "cks" ], [ "ġse", "c" ], [ "f", "s" ], [ "ro", "p" ], [ "ġhe", "alth" ], [ "ġc", "reat" ], [ "ġth", "ree" ], [ "ġs", "ick" ], [ "ġch", "ild" ], [ "ġworr", "ied" ], [ "ġsp", "ace" ], [ "ġapp", "re" ], [ "ġs", "een" ], [ "g", "an" ], [ "ff", "ic" ], [ "ġmonth", "s" ], [ "l", "ight" ], [ "ġlet", "s" ], [ "ġm", "or" ], [ "ġapp", "a" ], [ "'", "." ], [ "ġe", "nt" ], [ "ġf", "ig" ], [ "ġam", "az" ], [ "u", "p" ], [ "ġqu", "ite" ], [ "ġsa", "fe" ], [ "anc", "ing" ], [ "r", "on" ], [ "ag", "es" ], [ "ġ;", ")" ], [ "at", "s" ], [ "ġj", "on" ], [ "ġc", "or" ], [ "ġob", "v" ], [ "u", "gg" ], [ "ġ", "#" ], [ "ġm", "id" ], [ "fe", "ct" ], [ "ġex", "c" ], [ "ġawk", "ward" ], [ "ist", "ic" ], [ "it", "ion" ], [ "ġno", "body" ], [ "ġor", "der" ], [ "ġha", "nd" ], [ "ġis", "n" ], [ "ġappa", "rently" ], [ "ġfe", "w" ], [ "ġs", "ays" ], [ "ġb", "tw" ], [ "re", "s" ], [ "ġhappen", "ed" ], [ "mo", "st" ], [ "ġdef", "in" ], [ "s", "ide" ], [ "ġt", "er" ], [ "ġfor", "g" ], [ "un", "k" ], [ "ġappre", "ci" ], [ "\"", "," ], [ "ġhor", "r" ], [ "je", "ct" ], [ "ġme", "h" ], [ "en", "s" ], [ "ter", "day" ], [ "il", "st" ], [ "ġm", "ic" ], [ "ġment", "al" ], [ "n", "ch" ], [ "ġo", "oo" ], [ "ġto", "p" ], [ "or", "s" ], [ "ġche", "ese" ], [ "ġr", "un" ], [ "ġany", "one" ], [ "ġjoke", "s" ], [ "p", "ed" ], [ "ġd", "un" ], [ "ve", "d" ], [ "ġp", "i" ], [ "ġyes", "terday" ], [ "ġintere", "sting" ], [ "ġwh", "ilst" ], [ "ġf", "ar" ], [ "w", "e" ], [ "ġem", "ot" ], [ "ġe", "y" ], [ "ġanx", "iety" ], [ "ug", "s" ], [ "us", "ing" ], [ "ġe", "d" ], [ "b", "a" ], [ "f", "r" ], [ "ġto", "night" ], [ "ġas", "king" ], [ "ã", "ģ" ], [ "ġcle", "an" ], [ "v", "iew" ], [ "?", "!" ], [ "ġom", "fg" ], [ "ġd", "ancing" ], [ "ġwa", "ter" ], [ "ġproble", "m" ], [ "ãģ", "£" ], [ "ġyour", "self" ], [ "i", "gg" ], [ "ġsu", "r" ], [ "ġw", "indow" ], [ "ġli", "ght" ], [ "h", "ow" ], [ "ġ1", "1" ], [ "ġ", "nd" ], [ "ġa", "p" ], [ "ġc", "ut" ], [ "ġimport", "ant" ], [ "ġf", "ore" ], [ "ġe", "w" ], [ "p", "m" ], [ "ġa", "h" ], [ "ġp", "iss" ], [ "t", "he" ], [ "ġtr", "ied" ], [ "re", "t" ], [ "ġup", "set" ], [ "ġsit", "u" ], [ "f", "ort" ], [ "ġp", "oo" ], [ "ġd", "w" ], [ "ġgen", "er" ], [ "ġblan", "ket" ], [ "w", "een" ], [ "ġtr", "ans" ], [ "ġboo", "f" ], [ "ġmain", "ly" ], [ "ġch", "ris" ], [ "che", "s" ], [ "ġwh", "ile" ], [ "ġwal", "k" ], [ "ou", "nt" ], [ "ġseco", "nd" ], [ "v", "ice" ], [ "ġf", "am" ], [ "ġph", "ot" ], [ "ġin", "v" ], [ "ġon", "es" ], [ "ġtech", "n" ], [ "te", "m" ], [ "er", "t" ], [ "ġv", "is" ], [ "ġre", "st" ], [ "ġcl", "ass" ], [ "ġspe", "c" ], [ "ġsong", "s" ], [ "ġbelie", "ve" ], [ "he", "s" ], [ "ġwatch", "ing" ], [ "ġamaz", "ing" ], [ "ġforg", "ot" ], [ "ġb", "an" ], [ "ġlo", "ving" ], [ "ġyou", "re" ], [ "ġper", "fect" ], [ "e", "ar" ], [ "ġat", "ta" ], [ "ġw", "ar" ], [ "ġb", "ir" ], [ "ġsu", "gg" ], [ "at", "ive" ], [ "ġfu", "ll" ], [ "ġcre", "ate" ], [ "ġg", "oo" ], [ "ġs", "ksks" ], [ "us", "h" ], [ "ġche", "ck" ], [ "ġpl", "z" ], [ "ġv", "al" ], [ "ġneed", "ed" ], [ "ġl", "uck" ], [ "ġcol", "our" ], [ "ġs", "ign" ], [ "m", "y" ], [ "ġdi", "ffic" ], [ "ġpro", "c" ], [ "ġpa", "rent" ], [ "ate", "ly" ], [ "ġop", "in" ], [ "e", "nt" ], [ "in", "ner" ], [ "ġhur", "t" ], [ "iz", "z" ], [ "ġb", "ur" ], [ "b", "le" ], [ "'", "(" ], [ "fu", "lly" ], [ "in", "s" ], [ "ġw", "ha" ], [ "ġre", "c" ], [ "ġfre", "nch" ], [ "s", "y" ], [ "ill", "y" ], [ "out", "h" ], [ "ġal", "most" ], [ "ġm", "eet" ], [ "ġâģ", "ĺ" ], [ "ġto", "y" ], [ "ġag", "ree" ], [ "ġe", "mb" ], [ "ġbet", "ween" ], [ "ġcommun", "ic" ], [ "ite", "ly" ], [ "e", "ver" ], [ "ġsc", "ream" ], [ "ġ", "ðł" ], [ "ġc", "our" ], [ "ġv", "a" ], [ "ġre", "f" ], [ "ġacc", "ident" ], [ "ġdo", "g" ], [ "ġconf", "used" ], [ "ġfun", "ny" ], [ "i", "al" ], [ "ġbu", "y" ], [ "ġt", "as" ], [ "ma", "il" ], [ "i", "ps" ], [ "ġus", "ually" ], [ "n", "et" ], [ "ġdun", "no" ], [ "ġc", "oo" ], [ "ro", "om" ], [ ")", "," ], [ "ġin", "st" ], [ "il", "ity" ], [ "if", "u" ], [ "ie", "nce" ], [ "ġlisten", "ed" ], [ "ġh", "un" ], [ "ġe", "gg" ], [ "ġst", "ra" ], [ "od", "u" ], [ "ġmor", "ning" ], [ "ġye", "p" ], [ "ġbab", "e" ], [ "ġdark", "ness" ], [ "ġre", "m" ], [ "i", "ving" ], [ "ġ:", "'(" ], [ "ion", "al" ], [ "ġde", "al" ], [ "ġiss", "ues" ], [ "ġvo", "ices" ], [ "ġsu", "b" ], [ "ġg", "un" ], [ "m", "ing" ], [ "ġex", "cept" ], [ "1", "2" ], [ "il", "ar" ], [ "t", "ure" ], [ "ġcomp", "lete" ], [ "ow", "er" ], [ "ġrep", "ly" ], [ "ġlike", "s" ], [ "ġfa", "ct" ], [ "ġansw", "er" ], [ "ġpan", "ic" ], [ "ġp", "op" ], [ "ġwor", "ks" ], [ "ġsm", "all" ], [ "ġdiffic", "ult" ], [ "p", "id" ], [ "ġm", "at" ], [ "ġsome", "how" ], [ "ġdefin", "itely" ], [ "u", "tes" ], [ "ġide", "k" ], [ "ġout", "side" ], [ "ġtoo", "k" ], [ "ter", "s" ], [ "ġsupp", "osed" ], [ "ġof", "ten" ], [ "ġpo", "st" ], [ "h", "d" ], [ "ġwor", "ld" ], [ "ġparent", "s" ], [ "i", "pp" ], [ "ġtake", "s" ], [ "pt", "ion" ], [ "ġ", "z" ], [ "ġhave", "n" ], [ "b", "ook" ], [ "ġt", "ill" ], [ "wa", "ifu" ], [ "ġbir", "th" ], [ "ġd", "ie" ], [ "ġtell", "ing" ], [ "ġday", "light" ], [ "ad", "s" ], [ "rate", "waifu" ], [ "ġallow", "ed" ], [ "ġc", "op" ], [ "ic", "ul" ], [ "id", "ed" ], [ "ġg", "ames" ], [ "se", "nt" ], [ "ġag", "es" ], [ "ġthey", "re" ], [ "00", "0" ], [ "ġappreci", "ate" ], [ "ġfo", "llow" ], [ "r", "u" ], [ "ġe", "ff" ], [ "ġde", "press" ], [ "od", "e" ], [ "ġd", "uring" ], [ "z", "y" ], [ "y", "le" ], [ "ġaut", "istic" ], [ "o", "b" ], [ "ġwor", "ry" ], [ "g", "er" ], [ "ing", "s" ], [ "ġto", "wards" ], [ "ġsex", "y" ], [ "ġwo", "nder" ], [ "ġposs", "ible" ], [ "ġcont", "ro" ], [ "ġcha", "ng" ], [ "ġcry", "ing" ], [ "ġs", "at" ], [ "ġco", "lle" ], [ "ġ1", "8" ], [ "sh", "ip" ], [ "ġre", "qu" ], [ "t", "m" ], [ "u", "k" ], [ "y", "out" ], [ "ġlo", "st" ], [ "ġlove", "s" ], [ "ġcha", "r" ], [ "ġc", "ree" ], [ "ġr", "ather" ], [ "p", "ut" ], [ "ġele", "ct" ], [ "ġn", "umb" ], [ "ġe", "mail" ], [ "ġeff", "ort" ], [ "ġby", "e" ], [ "u", "gh" ], [ "ġto", "t" ], [ "in", "y" ], [ "ġty", "pe" ], [ "ġex", "p" ], [ "ġadd", "ress" ], [ "g", "r" ], [ "ġinc", "l" ], [ "o", "on" ], [ "ġw", "ood" ], [ "ic", "t" ], [ "w", "or" ], [ "ġmin", "utes" ], [ "ġbor", "ing" ], [ "nd", "ed" ], [ "ġent", "ire" ], [ "g", "y" ], [ "ġe", "qu" ], [ "ġha", "ting" ], [ "ġopin", "ion" ], [ "ci", "ted" ], [ "ġsugg", "est" ], [ "ġsor", "t" ], [ "ġanx", "ious" ], [ "ġb", "ody" ], [ "ġstu", "pid" ], [ "ġd", "inner" ], [ "a", "ir" ], [ "om", "an" ], [ "ib", "ly" ], [ "ġwould", "nt" ], [ "m", "p" ], [ "ġha", "ng" ], [ "ġfa", "v" ], [ "ate", "s" ], [ "w", "atch" ], [ "ar", "ch" ], [ "ġde", "ad" ], [ "ġme", "me" ], [ "ġrel", "ation" ], [ "ġun", "less" ], [ "3", "3" ], [ "ġme", "t" ], [ "ġwe", "ar" ], [ "ġpic", "ture" ], [ "ġh", "id" ], [ "ġread", "ing" ], [ "l", "eep" ], [ "ġdam", "n" ], [ "ġd", "roid" ], [ "ġb", "ou" ], [ "ġ1", "5" ], [ "ġcour", "se" ], [ "ag", "ed" ], [ "ġhe", "he" ], [ "ġfuck", "in" ], [ "ġhe", "ating" ], [ "ġhear", "d" ], [ "ġooo", "h" ], [ "m", "me" ], [ "ġst", "and" ], [ "ġlove", "ly" ], [ "ġfl", "o" ], [ "ġloo", "ked" ], [ "ġte", "xt" ], [ "ci", "ally" ], [ "ġcl", "ose" ], [ "d", "y" ], [ "ġcon", "vers" ], [ "ġhum", "an" ], [ "ġim", "ag" ], [ "ġhow", "s" ], [ "ġcom", "es" ], [ "ġp", "izz" ], [ "ġpo", "s" ], [ "ġa", "ge" ], [ "ġho", "ld" ], [ "ġd", "u" ], [ "ġtr", "u" ], [ "ġsl", "ight" ], [ "ġreal", "ised" ], [ "ġfig", "ure" ], [ "ġx", "ox" ], [ "âģ", "¢" ], [ "m", "on" ], [ "ġex", "cited" ], [ "ġeven", "ing" ], [ "ġfin", "ally" ], [ "ġcons", "ider" ], [ "!", "'" ], [ "er", "ic" ], [ "ġdisc", "uss" ], [ "ġsupp", "ose" ], [ "a", "ut" ], [ "ġon", "line" ], [ "r", "ing" ], [ "ffe", "ct" ], [ "ġaut", "ism" ], [ "ġsitu", "ation" ], [ "ġget", "s" ], [ "ġboo", "k" ], [ "ġcou", "nt" ], [ "ġoo", "f" ], [ "ġf", "ix" ], [ "ra", "ct" ], [ "ġnorm", "al" ], [ "ġm", "ome" ], [ "an", "gle" ], [ "ġw", "at" ], [ "ġins", "ide" ], [ "1", "5" ], [ "ġpass", "age" ], [ "ġgun", "na" ], [ "k", "k" ], [ "li", "er" ], [ "ġd", "at" ], [ "m", "er" ], [ "id", "ay" ], [ "ġdo", "ct" ], [ "ġp", "en" ], [ "ġp", "u" ], [ "g", "ra" ], [ "ê", "ķ" ], [ "ġsw", "eet" ], [ "ġf", "ur" ], [ "ġloo", "ol" ], [ "ġde", "l" ], [ "ġfl", "at" ], [ "ġsc", "ary" ], [ "ġspea", "king" ], [ "ġengl", "ish" ], [ "ġh", "y" ], [ "ġas", "leep" ], [ "gg", "ing" ], [ "ġwould", "n" ], [ "ġcreat", "ing" ], [ "ġu", "k" ], [ "ġam", "eric" ], [ "ġpizz", "a" ], [ "c", "l" ], [ "ġc", "ase" ], [ "ġlo", "ts" ], [ "ġfa", "il" ], [ "nd", "ay" ], [ "m", "an" ], [ "ġi", "good night" ], [ "ġhappen", "ing" ], [ "ġ1", "4" ], [ "c", "u" ], [ "ic", "ed" ], [ "ġ3", "0" ], [ "g", "a" ], [ "ê", "ķ" ], [ "ġtur", "n" ], [ "te", "ver" ], [ "c", "es" ], [ "h", "y" ], [ "ġr", "ant" ], [ "ġpl", "us" ], [ "ġfa", "ult" ], [ "ġtw", "itch" ], [ "ġch", "o" ], [ "ġpast", "a" ], [ "ġw", "oman" ], [ "ġeve", "nt" ], [ "ġbl", "ack" ], [ "ġco", "ld" ], [ "ġe", "v" ], [ "a", "il" ], [ "ġt", "u" ], [ "ġp", "c" ], [ "ġsu", "cks" ], [ "ġhtt", "p" ], [ "ġear", "lier" ], [ "ġhorr", "ible" ], [ "ġs", "n" ], [ "e", "x" ], [ "ġgoo", "gle" ], [ "ir", "t" ], [ "ġxx", "x" ], [ "ġf", "all" ], [ "ġga", "ve" ], [ "ġfore", "ver" ], [ "n", "n" ], [ "ġm", "il" ], [ "ġhope", "fully" ], [ "yout", "ube" ], [ "ġs", "ooo" ], [ "o", "ck" ], [ "ġme", "ow" ], [ "ġso", "cial" ], [ "ġaw", "w" ], [ "dd", "en" ], [ "ġdr", "unk" ], [ "ġbirth", "day" ], [ "ġwor", "st" ], [ "ġc", "ba" ], [ "ange", "l" ], [ "ens", "ive" ], [ "ġslight", "ly" ], [ "ġl", "ow" ], [ "ġm", "ouse" ], [ "ġv", "ib" ], [ "ġm", "ass" ], [ "nd", "ing" ], [ "ġp", "at" ], [ "ġg", "one" ], [ "ġsu", "ck" ], [ "f", "er" ], [ "ġw", "on" ], [ "ġf", "our" ], [ "ġt", "iny" ], [ "ġbro", "ke" ], [ "ġobv", "iously" ], [ "ġsu", "dden" ], [ "ġlove", "d" ], [ "is", "ing" ], [ "ġwe", "bs" ], [ "ġba", "g" ], [ "a", "v" ], [ "ġg", "iving" ], [ "ġso", "l" ], [ "l", "s" ], [ "ġu", "mm" ], [ "ġsp", "am" ], [ "p", "r" ], [ "so", "l" ], [ "ġco", "ck" ], [ "ġpr", "om" ], [ "ġp", "as" ], [ "ra", "c" ], [ "ġstu", "ck" ], [ "c", "on" ], [ "ġe", "ss" ], [ "ġex", "pect" ], [ "ġoo", "h" ], [ "ġf", "fs" ], [ "olog", "y" ], [ "ġke", "y" ], [ "ġva", "pe" ], [ "ġwebs", "ite" ], [ "ll", "ed" ], [ "ġb", "ath" ], [ "ġact", "ual" ], [ "ġli", "ving" ], [ "1", "7" ], [ "ly", "m" ], [ "ġwha", "tever" ], [ "ġhmm", "m" ], [ "ġproc", "ess" ], [ "ġfav", "our" ], [ "ġwor", "th" ], [ "ġl", "ie" ], [ "ġcur", "rently" ], [ "u", "le" ], [ "ġan", "gry" ], [ "ġper", "f" ], [ "c", "ed" ], [ "ġp", "sy" ], [ "ġfe", "ll" ], [ "is", "ion" ], [ "ġla", "ugh" ], [ "ġcal", "m" ], [ "l", "u" ], [ "ġw", "in" ], [ "ġf", "oc" ], [ "ke", "y" ], [ "ġre", "d" ], [ "ġre", "t" ], [ "ġand", "roid" ], [ "ġyour", "s" ], [ "ġbit", "ch" ], [ "ġph", "ys" ], [ "h", "t" ], [ "fr", "iend" ], [ "c", "er" ], [ "ġm", "ee" ], [ "ġtr", "ust" ], [ "ġinter", "net" ], [ "ġre", "ady" ], [ "ve", "nt" ], [ "c", "ha" ], [ "f", "k" ], [ "ġm", "ult" ], [ "se", "l" ], [ "2", "4" ], [ "u", "red" ], [ "on", "se" ], [ "it", "ies" ], [ "ġcont", "in" ], [ "w", "u" ], [ "ġex", "tra" ], [ "ġsim", "ilar" ], [ "p", "o" ], [ "u", "ine" ], [ "ġre", "cent" ], [ "ġbl", "ue" ], [ "sc", "ri" ], [ "ġf", "il" ], [ "ġstr", "ugg" ], [ "b", "er" ], [ "ġpre", "ss" ], [ "ġh", "it" ], [ "ġare", "nt" ], [ "1", "6" ], [ "ġw", "oo" ], [ "ġra", "p" ], [ "ġey", "es" ], [ "ġstra", "ight" ], [ "2", "5" ], [ "ie", "nt" ], [ "ġlove", "angel" ], [ "ġemb", "ar" ], [ "ġmome", "nt" ], [ "ġn", "er" ], [ "ġquest", "ion" ], [ "ant", "ly" ], [ "b", "b" ], [ "ġ", "/" ], [ "ġgen", "uine" ], [ "bo", "ard" ], [ "ġval", "id" ], [ "ġy", "a" ], [ "t", "r" ], [ "ġwh", "ite" ], [ "ġaw", "ake" ], [ "pt", "op" ], [ "ġfin", "is" ], [ "ġeas", "y" ], [ "ġm", "is" ], [ "ġdef", "o" ], [ "ġe", "ffect" ], [ "ġfr", "ance" ], [ "ġknow", "s" ], [ "ġch", "ill" ], [ "i", "fe" ], [ "ġconvers", "ation" ], [ "ġs", "ide" ], [ "ġbrain", "s" ], [ "ġment", "ion" ], [ "ġproper", "ly" ], [ "ġbe", "come" ], [ "h", "o" ], [ "ġc", "ook" ], [ "ġqu", "ick" ], [ "ġsc", "reen" ], [ "in", "es" ], [ "en", "ce" ], [ "ġad", "hd" ], [ "ġdec", "ided" ], [ "ġfavour", "ite" ], [ "ġbou", "ght" ], [ "ġme", "n" ], [ "ġe", "h" ], [ "ġresp", "onse" ], [ "ġsex", "ual" ], [ "ġ1", "9" ], [ "ġsl", "ow" ], [ "ġser", "iously" ], [ "g", "if" ], [ "ġcould", "nt" ], [ "ġ", "+" ], [ "ġd", "ream" ], [ "ġfor", "m" ], [ "ġhu", "ge" ], [ "ġcomplete", "ly" ], [ "sel", "ves" ], [ "ġex", "am" ], [ "ġcare", "s" ], [ "ġd", "et" ], [ "ġrec", "ord" ], [ "ġpr", "oud" ], [ "ġproble", "ms" ], [ "u", "x" ], [ "ġ", "ver" ], [ "ġle", "vel" ], [ "ġla", "ptop" ], [ "air", "s" ], [ "ġp", "lym" ], [ "ra", "l" ], [ "ġtra", "ck" ], [ "ġcor", "rect" ], [ "ġtw", "ice" ], [ "in", "ess" ], [ "ġsh", "ort" ], [ "ġat", "m" ], [ "ġcan", "not" ], [ "ġke", "pt" ], [ "ġhate", "s" ], [ "a", "ly" ], [ "j", "k" ], [ "ġf", "ive" ], [ "ġhu", "h" ], [ "ġre", "pl" ], [ "ġhell", "o" ], [ "ġle", "mme" ], [ "ġcudd", "les" ], [ "c", "y" ], [ "ġmess", "ages" ], [ "ġl", "oud" ], [ "ġbe", "aut" ], [ "ġnot", "iced" ], [ "ip", "le" ], [ "ġpsy", "ch" ], [ "ġc", "ust" ], [ "ġwait", "ing" ], [ "ġmiss", "ed" ], [ "ġnumb", "er" ], [ "êķ", "ãģ£" ], [ "ġlo", "g" ], [ "u", "in" ], [ "ġno", "pe" ], [ "ġpr", "odu" ], [ "ġrelation", "ship" ], [ "ġbr", "u" ], [ "ġess", "ay" ], [ "ġs", "ys" ], [ "w", "ise" ], [ "ġind", "eed" ], [ "re", "w" ], [ "re", "nce" ], [ "ġgive", "s" ], [ "ġbas", "ed" ], [ "if", "y" ], [ "iz", "e" ], [ "ġcon", "ta" ], [ "ġwal", "ked" ], [ "ġg", "ross" ], [ "ġsome", "where" ], [ "ġf", "it" ], [ "ġne", "ar" ], [ "ġ1", "3" ], [ "ġoh", "hh" ], [ "h", "s" ], [ "ġd", "eep" ], [ "ġcon", "st" ], [ "lo", "ad" ], [ "ġste", "p" ], [ "ġser", "ver" ], [ "ġtot", "ally" ], [ "ġe", "arly" ], [ "ee", "r" ], [ "or", "n" ], [ "ġnot", "ice" ], [ "ġsh", "ut" ], [ "ġdis", "t" ], [ "ing", "e" ], [ "tm", "as" ], [ "ġsudden", "ly" ], [ "un", "e" ], [ "ġfam", "ily" ], [ "ġstar", "ting" ], [ "ġinc", "red" ], [ "ġfr", "ont" ], [ "ġb", "i" ], [ "ive", "ly" ], [ "ġaccident", "ally" ], [ "ġcolle", "ge" ], [ "âģ¢", "ì" ], [ "ġcl", "os" ], [ "ġpar", "ty" ], [ "ġv", "i" ], [ "it", "al" ], [ "ġgo", "es" ], [ "p", "a" ], [ "ess", "s" ], [ "ġchris", "tmas" ], [ "i", "ted" ], [ "ġst", "ory" ], [ "ġgirl", "s" ], [ "ġhun", "gry" ], [ "ġcree", "py" ], [ "ġr", "ound" ], [ "1", "4" ], [ "ġp", "ol" ], [ "ġhug", "s" ], [ "ake", "d" ], [ "ur", "s" ], [ "ġflo", "or" ], [ "ġs", "qu" ], [ "ġr", "an" ], [ "ġhear", "t" ], [ "ġjo", "in" ], [ "ġ>", "." ], [ "ġbus", "y" ], [ "ġ1", "6" ], [ "ġengl", "and" ], [ "f", "ast" ], [ "ġs", "illy" ], [ "if", "ul" ], [ "anc", "y" ], [ "ġtechn", "ically" ], [ "ġpart", "ner" ], [ "ġgener", "al" ], [ "e", "m" ], [ "oo", "t" ], [ "fort", "able" ], [ "pe", "cially" ], [ "ġinf", "orm" ], [ "ġa", "ir" ], [ "ġhappen", "s" ], [ "d", "ing" ], [ "i", "ot" ], [ "ġexper", "ience" ], [ "ġquest", "ions" ], [ "ġxox", "o" ], [ "1", "9" ], [ "ġdec", "ide" ], [ "1", "3" ], [ "le", "x" ], [ "ert", "ain" ], [ "ġlong", "er" ], [ "ġbro", "ther" ], [ "ron", "ic" ], [ "ġt", "est" ], [ "ġth", "row" ], [ "ġst", "yle" ], [ "ġwo", "ke" ], [ "ġincl", "ud" ], [ "he", "ad" ], [ "ke", "ts" ], [ "i", "k" ], [ "ġwe", "l" ], [ "ġimag", "ine" ], [ "ġbu", "ild" ], [ "tent", "ion" ], [ "ġs", "me" ], [ "ġb", "ass" ], [ "ġlis", "t" ], [ "ġsing", "ing" ], [ "ġiss", "ue" ], [ "d", "le" ], [ "m", "in" ], [ "ġn", "ic" ], [ "ġne", "ither" ], [ "che", "n" ], [ "a", "x" ], [ "ġl", "ar" ], [ "ġy", "h" ], [ "nd", "on" ], [ "ġpro", "f" ], [ "is", "ts" ], [ "f", "l" ], [ "j", "s" ], [ "ġ", "?" ], [ "ġlike", "ly" ], [ "ġface", "book" ], [ "ġspec", "ific" ], [ "ġyou", "t" ], [ "ġsp", "o" ], [ "ġmum", "s" ], [ "ġpart", "icul" ], [ "ġconta", "ct" ], [ "ġpro", "v" ], [ "cc", "oon" ], [ "ġsleep", "ing" ], [ "ġwar", "m" ], [ "ġgenuine", "ly" ], [ "ġne", "arly" ], [ "ġcall", "ing" ], [ "ġb", "ra" ], [ "5", "0" ], [ "ter", "n" ], [ "ġfu", "cked" ], [ "ġsen", "s" ], [ "1", "8" ], [ "ġs", "oup" ], [ "ġj", "u" ], [ "ġmee", "ting" ], [ "y", "ou" ], [ "ġboy", "friend" ], [ "ġi", "ly" ], [ "ile", "d" ], [ "ġmil", "k" ], [ "ġl", "ine" ], [ "ġannoy", "ed" ], [ "ġo", "dd" ], [ "ġbre", "ad" ], [ "ġcould", "n" ], [ "ġkit", "chen" ], [ "ġarg", "o" ], [ "ġmult", "iple" ], [ "ġscream", "ing" ], [ "p", "ris" ], [ "ġcl", "ot" ], [ "ġtur", "ned" ], [ "ġb", "ar" ], [ "ġbr", "ight" ], [ "ġspe", "nt" ], [ "ġdam", "mit" ], [ "ġes", "pecially" ], [ "ê", "ĺ" ], [ "ġs", "ile" ], [ "ġcom", "ment" ], [ "ġdis", "app" ], [ "ġmass", "ive" ], [ "ill", "s" ], [ "ġstr", "ong" ], [ "ġw", "ake" ], [ "ie", "t" ], [ "ġwhe", "ther" ], [ "ġcomp", "l" ], [ "ġgr", "oup" ], [ "ġt", "v" ], [ "re", "ad" ], [ "q", "ue" ], [ "u", "rate" ], [ "ġstu", "de" ], [ "ġshow", "er" ], [ "ġo", "pp" ], [ "ġbreak", "fast" ], [ "ġsksks", "k" ], [ "\"", ")" ], [ "ġother", "wise" ], [ "ġtra", "um" ], [ "ġplym", "outh" ], [ "ġpiss", "ed" ], [ "a", "w" ], [ "d", "o" ], [ "te", "nd" ], [ "ġ", "vers" ], [ "ġcha", "os" ], [ "ġother", "s" ], [ "ġeas", "ier" ], [ "ġpoo", "p" ], [ "ġlo", "ndon" ], [ "en", "g" ], [ "ġg", "reen" ], [ "ġ1", "00" ], [ "ġpa", "id" ], [ "ġap", "olog" ], [ "ġf", "ire" ], [ "ġu", "wu" ], [ "ġh", "m" ], [ "ker", "s" ], [ "d", "own" ], [ "es", "us" ], [ "ġre", "mo" ], [ "ġa", "nt" ], [ "ġs", "ix" ], [ "ġend", "ed" ], [ "ġd", "ue" ], [ "ġd", "ans" ], [ "ġsha", "re" ], [ "ġyou", "ng" ], [ "ġ>", ":" ], [ "ġdi", "rect" ], [ "med", "i" ], [ "ġpur", "ple" ], [ "2", "3" ], [ "he", "t" ], [ "ġca", "mer" ], [ "ġdes", "er" ], [ "ġcontro", "l" ], [ "ġrecent", "ly" ], [ "ġkiss", "ing" ], [ "d", "f" ], [ "s", "ted" ], [ "ġspe", "nd" ], [ "ġbeaut", "iful" ], [ "z", "z" ], [ "ġleave", "s" ], [ "ġu", "h" ], [ "ġp", "our" ], [ "ġlove", "angle" ], [ "e", "g" ], [ "ġi", "g" ], [ "ġlike", "d" ], [ "ġsur", "pris" ], [ "w", "hat" ], [ "ġlo", "s" ], [ "ġperson", "al" ], [ "'", "," ], [ "h", "ugs" ], [ "ġhow", "ever" ], [ "ġmove", "s" ], [ "ġcle", "arly" ], [ "ġstress", "ed" ], [ "ġemot", "ional" ], [ "ġhelp", "ing" ], [ "ġacc", "urate" ], [ "b", "r" ], [ "m", "at" ], [ "et", "s" ], [ "ġagain", "st" ], [ "ome", "n" ], [ "um", "p" ], [ "pe", "cted" ], [ "ġf", "ight" ], [ "ġw", "ife" ], [ "ġde", "ath" ], [ "ġcra", "p" ], [ "ġc", "e" ], [ "ġarg", "u" ], [ "ġsh", "out" ], [ "ġam", "ount" ], [ "ġtake", "n" ], [ "ġy", "o" ], [ "an", "y" ], [ "ġso", "fa" ], [ "ġintere", "sted" ], [ "ġpro", "ject" ], [ "n", "se" ], [ "ġsit", "ting" ], [ "(", ")" ], [ "4", "0" ], [ "p", "op" ], [ "ġd", "ou" ], [ "ġp", "ee" ], [ "ġbe", "n" ], [ "ġhu", "gging" ], [ "p", "ar" ], [ "ġc", "ertain" ], [ "ġsend", "ing" ], [ "ġatta", "ck" ], [ "ġcoo", "king" ], [ "ġo", "bs" ], [ "ġmid", "dle" ], [ "ġyout", "ube" ], [ "g", "o" ], [ "ġf", "ake" ], [ "ġfr", "iday" ], [ "ġsing", "le" ], [ "ġy", "esss" ], [ "v", "i" ], [ "ġj", "esus" ], [ "ten", "or" ], [ "ġpre", "fer" ], [ "k", "a" ], [ "ant", "s" ], [ "ġor", "gan" ], [ "d", "n" ], [ "re", "n" ], [ "ġd", "ude" ], [ "ġo", "w" ], [ "ll", "s" ], [ "pp", "ed" ], [ "il", "t" ], [ "ġthem", "selves" ], [ "ġcho", "ice" ], [ "o", "nd" ], [ "a", "me" ], [ "ġat", "tention" ], [ "ġsmo", "ked" ], [ "ġid", "iot" ], [ "g", "u" ], [ "l", "og" ], [ "ġst", "ate" ], [ "ġba", "re" ], [ "ġconsider", "ing" ], [ "ġa", "i" ], [ "ġli", "l" ], [ "ġq", "ue" ], [ "g", "s" ], [ "ġa", "a" ], [ "ġlo", "ads" ], [ "ġd", "ate" ], [ "ġexp", "ensive" ], [ "ġbro", "ken" ], [ "ġas", "her" ], [ "b", "ot" ], [ "ul", "ar" ], [ "ġmean", "ing" ], [ "ġm", "ag" ], [ "ġc", "ard" ], [ "ġcon", "ne" ], [ "h", "ol" ], [ "ġo", "nt" ], [ "ġapp", "ro" ], [ "ġcra", "zy" ], [ "s", "or" ], [ "ġre", "nt" ], [ "ġg", "row" ], [ "100", "0" ], [ "a", "i" ], [ "r", "ic" ], [ "ġac", "cept" ], [ "ġfoc", "us" ], [ "ġfinis", "hed" ], [ "e", "red" ], [ "te", "xt" ], [ "ġam", "b" ], [ "ġsw","ear" ], [ "f", "y" ], [ "ġa", "vo" ], [ "ġde", "pe" ], [ "o", "ve" ], [ "ġm", "as" ], [ "n", "orm" ], [ "is", "es" ], [ "ġad", "vice" ], [ "ġ>:", "(" ], [ "it", "ive" ], [ "er", "gy" ], [ "haha", "ha" ], [ "li", "ke" ], [ "igg", "er" ], [ "êķ", "ãģ£" ], [ "ġt", "ic" ], [ "ġw", "all" ], [ "ġun", "s" ], [ "c", "le" ], [ "ġd", "ress" ], [ "ġ1", "7" ], [ "ġboo", "bs" ], [ "ġty", "ping" ], [ "ġkid", "s" ], [ "2", "1" ], [ "ġre", "act" ], [ "4", "5" ], [ "ġdr", "ugs" ], [ "ġ:(", "(" ], [ "te", "l" ], [ "2", "6" ], [ "p", "ort" ], [ "in", "a" ], [ "ġp", "ot" ], [ "ġacc", "ess" ], [ "ġem", "o" ], [ "ġsys", "tem" ], [ "ġre", "pe" ], [ "ġex", "press" ], [ "ġem", "p" ], [ "ġcamer", "a" ], [ "ġsat", "ur" ], [ "ag", "er" ], [ "m", "i" ], [ "ġw", "omen" ], [ "ġreal", "ise" ], [ "ġman", "age" ], [ "ġsee", "s" ], [ "ul", "es" ], [ "ime", "nt" ], [ "rop", "ri" ], [ "ġbo", "i" ], [ "ġso", "ft" ], [ "ġbl", "ood" ], [ "ġont", "o" ], [ "ġd", "est" ], [ "se", "arch" ], [ "ġsm", "ile" ], [ "ġsec", "ret" ], [ "ġof", "fe" ], [ "ġkit", "ty" ], [ "]", ":" ], [ "ġc", "ode" ], [ "ġv", "iew" ], [ "ġpro", "te" ], [ "i", "ally" ], [ "ce", "l" ], [ "ġt", "een" ], [ "ġcha", "rac" ], [ "st", "airs" ], [ "ut", "h" ], [ "ġr", "id" ], [ "ġwal", "king" ], [ "ġme", "mor" ], [ "oo", "f" ], [ "ġoff", "er" ], [ "ġqu", "iet" ], [ "ġlet", "ter" ], [ "ġhim", "self" ], [ "wor", "k" ], [ "ġbe", "g" ], [ "im", "in" ], [ "ġvib", "es" ], [ "ġcust", "om" ], [ "f", "or" ], [ "ġh", "ol" ], [ "ġab", "sol" ], [ "whe", "l" ], [ "ġo", "pt" ], [ "at", "ure" ], [ "ġresp", "ons" ], [ "ġcommunic", "ation" ], [ "b", "u" ], [ "ġo", "op" ], [ "ġp", "ub" ], [ "an", "e" ], [ "a", "ten" ], [ "i", "od" ], [ "r", "ib" ], [ "ġbo", "x" ], [ "ġcle", "ar" ], [ "ġclot", "hes" ], [ "ġen", "ergy" ], [ "ġgive", "n" ], [ "ġr", "ip" ], [ "if", "t" ], [ "ġm", "ot" ], [ "ġover", "whel" ], [ "ġbr", "it" ], [ "ġyeah", "hh" ], [ "ġfair", "ly" ], [ "ġrem", "ind" ], [ "ġabsol", "ute" ], [ "ġres", "pect" ], [ "r", "s" ], [ "ġb", "in" ], [ "ġp", "ie" ], [ "ġwa", "ht" ], [ "ġfeeling", "s" ], [ "oc", "ked" ], [ "ab", "ility" ], [ "ġha", "nds" ], [ "ġaw", "h" ], [ "ġwho", "s" ], [ "ġre", "co" ], [ "ġfin", "ish" ], [ "ġwel", "come" ], [ "c", "al" ], [ "ġwor", "ked" ], [ "our", "s" ], [ "o", "int" ], [ "p", "ro" ], [ "ġput", "ting" ], [ "ġe", "st" ], [ "ir", "ty" ], [ "ġret", "ur" ], [ "ġa", "v" ], [ "ġb", "ts" ], [ "ġre", "le" ], [ "ġshould", "nt" ], [ "ġpo", "ke" ], [ "ġad", "mit" ], [ "ġmed", "ic" ], [ "ġtru", "ly" ], [ "ġbe", "h" ], [ "ġfu", "lly" ], [ "ġchang", "ed" ], [ "b", "ut" ], [ "yy", "y" ], [ "ġs", "am" ], [ "ast", "ic" ], [ "ġ2", "4" ], [ "ġphot", "o" ], [ "ġ", "â" ], [ "ġplan", "ning" ], [ "ġvi", "be" ], [ "o", "k" ], [ "ġg", "re" ], [ "ġelod", "ies" ], [ "ġro", "ck" ], [ "ġlar", "ge" ], [ "ġcompl", "ain" ], [ "m", "iss" ], [ "ig", "in" ], [ "ġper", "iod" ], [ "ġforget", "ting" ], [ "ġt", "re" ], [ "ġter", "r" ], [ "ġinform", "ation" ], [ "ġco", "ver" ], [ "2", "2" ], [ "ġpr", "iv" ], [ "ġsatur", "day" ], [ "u", "c" ], [ "ic", "ken" ], [ "ct", "ive" ], [ "ġign", "ore" ], [ "ġparticul", "arly" ], [ "ropri", "ate" ], [ "w", "s" ], [ "ġt", "im" ], [ "ur", "l" ], [ "ġpe", "tes" ], [ "ac", "es" ], [ "ġpi", "pe" ], [ "ġa", "ud" ], [ "ġmo", "nday" ], [ "ġ5", "0" ], [ "ing", "er" ], [ "ġbe", "ha" ], [ "o", "le" ], [ "ġd", "rop" ], [ "ġp", "ort" ], [ "ġthink", "s" ], [ "ġar", "r" ], [ "ang", "er" ], [ "ġdoct", "ors" ], [ "ġcontin", "ue" ], [ "z", "e" ], [ "ġb", "at" ], [ "ġaw", "ful" ], [ "ġhel", "ps" ], [ "b", "i" ], [ "d", "am" ], [ "ġdet", "ail" ], [ "ġt", "reat" ], [ "ra", "ph" ], [ "ġim", "medi" ], [ "ġfind", "ing" ], [ "ġo", "ption" ], [ "?", "'" ], [ "k", "y" ], [ "ġi", "r" ], [ "at", "or" ], [ "ġremember", "ing" ], [ "ġconf", "ir" ], [ "(", "\"" ], [ "ġl", "ady" ], [ "ġp", "ower" ], [ "ġun", "com" ], [ "ġp", "ract" ], [ "ġte", "mp" ], [ "ġkid", "ding" ], [ "ne", "l" ], [ "ġmove", "d" ], [ "ġb", "by" ], [ "ġstr", "ange" ], [ "gr", "ound" ], [ "ġno", "ise" ], [ "ġpa", "ck" ], [ "ġinv", "ol" ], [ "l", "r" ], [ "m", "g" ], [ "ġs", "al" ], [ "ġl", "y" ], [ "ġv", "ag" ], [ "ġor", "ange" ], [ "ġsome", "body" ], [ "ġ", "_" ], [ "ġunderstand", "ing" ], [ "ġclos", "er" ], [ "ġavo", "id" ], [ "ġl", "es" ], [ "ġk", "pop" ], [ "ġa", "int" ], [ "ġw", "arn" ], [ "or", "ing" ], [ "c", "ou" ], [ "ġun", "f" ], [ "ġass", "a" ], [ "ġju", "ice" ], [ "ct", "ure" ], [ "ġtr", "uth" ], [ "ġproper", "ty" ], [ "scri", "be" ], [ "ġs", "le" ], [ "ġm", "ice" ], [ "ġu", "m" ], [ "ġin", "t" ], [ "ġst", "re" ], [ "ad", "e" ], [ "ġcommunic", "ate" ], [ "ġbeh", "ind" ], [ "b", "al" ], [ "ġt", "it" ], [ "ġw", "ooo" ], [ "ġf", "eed" ], [ "ġal", "ive" ], [ "ġdr", "ug" ], [ "f", "d" ], [ "l", "and" ], [ "am", "p" ], [ "ġwa", "king" ], [ "ġkeep", "s" ], [ "ġsk", "y" ], [ "2", "8" ], [ "ġl", "ink" ], [ "ġfuck", "s" ], [ "ar", "c" ], [ "ġincred", "ibly" ], [ "ġ2", "3" ], [ "ġnorm", "ally" ], [ "o", "h" ], [ "ġme", "gan" ], [ "ġf", "ast" ], [ "ġex", "ist" ], [ "ġevent", "ually" ], [ "r", "d" ], [ "ġstop", "ped" ], [ "ġameric", "a" ], [ "ie", "nc" ], [ "se", "x" ], [ "ġfe", "ar" ], [ "p", "ose" ], [ "ġo", "cc" ], [ "lf", "r" ], [ "ġba", "nd" ], [ "ġbot", "t" ], [ "ġcr", "inge" ], [ "1", "1" ], [ "it", "ter" ], [ "ġher", "self" ], [ "ġpress", "ure" ], [ "c", "ing" ], [ "ġor", "igin" ], [ "ġbo", "om" ], [ "ġelect", "ronic" ], [ "ġf", "at" ], [ "ġth", "urs" ], [ "gg", "g" ], [ "ag", "ing" ], [ "ġconf", "using" ], [ "ġspec", "if" ], [ "o", "ss" ], [ "ġb", "all" ], [ "ġfor", "ce" ], [ "ver", "se" ], [ "c", "ra" ], [ "ign", "ed" ], [ "ġcomputer", "s" ], [ "ġho", "tel" ], [ "ġvers", "ion" ], [ "d", "io" ], [ "ġl", "ang" ], [ "ġbe", "ds" ], [ "ġal", "co" ], [ "ġad", "v" ], [ "ġma", "ch" ], [ "f", "ace" ], [ "ġ", "&" ], [ "ne", "y" ], [ "lfr", "iend" ], [ "ġw", "ot" ], [ "ve", "ra" ], [ "ġk", "ore" ], [ "aa", "ay" ], [ "ġrel", "ax" ], [ "us", "ive" ], [ "ġco", "st" ], [ "ġma", "c" ], [ "ipp", "ed" ], [ "ġfor", "ward" ], [ "ġdis", "tra" ], [ "âģ", "¿" ], [ "ġstar", "s" ], [ "ġter", "m" ], [ "m", "es" ], [ "ġc", "ir" ], [ "ġpro", "gra" ], [ "ġwee", "ke" ], [ "ġn", "or" ], [ "le", "ted" ], [ "ġad", "or" ], [ "ġmiss", "ing" ], [ "ġre", "v" ], [ "ġmat", "ter" ], [ "ġsee", "med" ], [ "ġrequ", "est" ], [ "ġf", "ut" ], [ "ġbe", "ne" ], [ "ow", "s" ], [ "ġde", "cent" ], [ "ġdes", "k" ], [ "ġpi", "per" ], [ "ġey", "e" ], [ "ġc", "y" ], [ "ġfr", "ust" ], [ "ġra", "ccoon" ], [ "ġpr", "ice" ], [ "ġma", "le" ], [ "ġdeser", "ve" ], [ "9", "9" ], [ "ġp", "orn" ], [ "ġh", "ide" ], [ "ic", "ked" ], [ "ġcur", "rent" ], [ "i", "or" ], [ "ġur", "self" ], [ "ġide", "nt" ], [ "c", "at" ], [ "ġlisten", "s" ], [ "ġspe", "cial" ], [ "ġstude", "nt" ], [ "ġf", "ill" ], [ "ġtou", "ches" ], [ "ġsleep", "y" ], [ "us", "ted" ], [ "ġhar", "sh" ], [ "ġboy", "s" ], [ "ra", "p" ], [ "ive", "d" ], [ "ġlang", "u" ], [ "ġalco", "hol" ], [ "s", "p" ], [ "ġm", "ate" ], [ "ġst", "an" ], [ "ġex", "pected" ], [ "ġwere", "nt" ], [ "ġac", "ross" ], [ "al", "ing" ], [ "ġcu", "p" ], [ "ġbad", "ly" ], [ "ġdec", "ision" ], [ "ġinclud", "ing" ], [ "l", "er" ], [ "v", "ous" ], [ "ġ", "ou" ], [ "ġdepress", "ed" ], [ "2", "7" ], [ "l", "a" ], [ "ġs", "ake" ], [ "to", "ken" ], [ "ath", "s" ], [ "s", "es" ], [ "ġ", "ris" ], [ "ġto", "wn" ], [ "ġkevin", "s" ], [ "ġwhen", "ever" ], [ "ġstream", "ing" ], [ "ar", "s" ], [ "n", "ted" ], [ "ġbur", "n" ], [ "d", "j" ], [ "ġe", "p" ], [ "ġro", "ll" ], [ "i", "ant" ], [ "ġcomp", "any" ], [ "ġshit", "ty" ], [ "ġapp", "oint" ], [ "ġgir", "lfriend" ], [ "ġtas", "te" ], [ "s", "ha" ], [ "u", "me" ], [ "ġe", "ight" ], [ "ġcudd", "le" ], [ "ġwith", "in" ], [ "ġman", "aged" ], [ "ġno", "te" ], [ "ġan", "aly" ], [ "ġl", "ay" ], [ "ġreason", "s" ], [ "ġbright", "on" ], [ "b", "l" ], [ "ġs", "on" ], [ "ten", "ce" ], [ "ġdr", "um" ], [ "ġdoct", "or" ], [ "ġj", "ud" ], [ "ġsa", "ve" ], [ "ġw", "ays" ], [ "ġre", "dd" ], [ "ġsk", "ills" ], [ "ġfur", "ther" ], [ "ġa", "hhh" ], [ "ġr", "ules" ], [ "p", "ital" ], [ "ha", "el" ], [ "ġb", "ill" ], [ "ġn", "one" ], [ "ġdi", "ag" ], [ "ġwat", "ched" ], [ "ġrepe", "at" ], [ "ġ", "{" ], [ "ġto", "r" ], [ "ġst", "at" ], [ "ġal", "ong" ], [ "ġbu", "ll" ], [ "ġgr", "ind" ], [ "ġphot", "os" ], [ "ġoop", "s" ], [ "ġbeha", "vi" ], [ "ġc", "ris" ], [ "ġd", "en" ], [ "ġthere", "fore" ], [ "f", "c" ], [ "l", "ist" ], [ "ġlol", "ol" ], [ "ġhelp", "ful" ], [ "ġd", "ro" ], [ "ġp", "et" ], [ "ġp", "ark" ], [ ")", ")" ], [ "it", "ten" ], [ "ġoh", "h" ], [ "ġbrit", "ish" ], [ "b", "in" ], [ "ġch", "icken" ], [ "ġt", "ty" ], [ "ġde", "p" ], [ "ġdra", "g" ], [ "ġgen", "re" ], [ "ġj", "k" ], [ "ġof", "c" ], [ "ġpl", "ans" ], [ "ġfrogg", "ys" ], [ "ġdiffe", "rence" ], [ "w", "h" ], [ "al", "ity" ], [ "ġex", "is" ], [ "ġur", "s" ], [ "ġexper", "ienc" ], [ "ġexc", "iting" ], [ "ġill", "eg" ], [ "ġmet", "al" ], [ "ġembar", "r" ], [ "ġabsolute", "ly" ], [ "ġfut", "ure" ], [ "b", "ab" ], [ "ġyou", "uu" ], [ "ġpo", "or" ], [ "ġmic", "hael" ], [ "ġspecif", "ically" ], [ "s", "ter" ], [ "u", "ce" ], [ "ġm", "ild" ], [ "re", "f" ], [ "ġno", "od" ], [ "ġat", "tem" ], [ "ġsor", "ted" ], [ "ġsw", "itch" ], [ "c", "ri" ], [ "ġpl", "aces" ], [ "ġhear", "s" ], [ "ġla", "ck" ], [ "ġlearn", "s" ], [ "ġrun", "ning" ], [ "d", "en" ], [ "t", "ain" ], [ "u", "ght" ], [ "ġimp", "ress" ], [ "ġpur", "pose" ], [ "ġhy", "per" ], [ "ġopt", "ions" ], [ "ġvo", "c" ], [ "ġm", "c" ], [ "ġf", "oot" ], [ "id", "ence" ], [ "ted", "ly" ], [ "ġtry", "na" ], [ "ġchan", "nel" ], [ "ġinf", "o" ], [ "ġtrain", "ing" ], [ "ġlo", "ad" ], [ "pp", "ing" ], [ "ġpr", "int" ], [ "ġdel", "ive" ], [ "a", "ks" ], [ "o", "in" ], [ "in", "ts" ], [ "ġb", "illy" ], [ "ġsc", "hed" ], [ "ġreg", "ard" ], [ "ġw", "ro" ], [ "ġb", "le" ], [ "ġup", "d" ], [ "a", "ve" ], [ "t", "on" ], [ "y", "e" ], [ "ġd", "ied" ], [ "ġd", "irty" ], [ "ġre", "ce" ], [ "ġknow", "ing" ], [ "ġun", "able" ], [ "ġequ", "als" ], [ "ġner", "vous" ], [ "ġbare", "ly" ], [ "ġ2", "5" ], [ "ġhur", "ts" ], [ "ġshop", "s" ], [ "ġweeke", "nd" ], [ "ġredd", "it" ], [ "l", "p" ], [ "u", "nder" ], [ "ġsu", "nday" ], [ "pp", "y" ], [ "ġaw", "are" ], [ "ġsk", "in" ], [ "na", "me" ], [ "ġge", "nder" ], [ "ġhehe", "he" ], [ "ġador", "able" ], [ "s", "ed" ], [ "ġt", "une" ], [ "ha", "r" ], [ "id", "ge" ], [ "ġbott", "om" ], [ "ġt", "ro" ], [ "ġb", "um" ], [ "ġr", "uin" ], [ "ġmention", "ed" ], [ "ġsurpris", "ed" ], [ "ġtty", "l" ], [ "b", "ing" ], [ "ġbe", "at" ], [ "ġab", "ove" ], [ "ġmin", "ute" ], [ "ġconst", "antly" ], [ "ġb", "b" ], [ "ġdis", "g" ], [ "ġbed", "room" ], [ "vel", "op" ], [ "ġmach", "ine" ], [ "3", "8" ], [ "on", "ed" ], [ "at", "t" ], [ "ġj", "ess" ], [ "ġposs", "ibly" ], [ "ġ>.", ">" ], [ "ġre", "fe" ], [ "ion", "ally" ], [ "ġeat", "s" ], [ "ġdad", "s" ], [ "ġass", "um" ], [ "ġm", "other" ], [ "ġl", "ying" ], [ "ġk", "k" ], [ "ġmo", "on" ], [ "ver", "t" ], [ "ġpic", "ks" ], [ "ġacc", "ount" ], [ "ġwro", "te" ], [ "ġwas", "hing" ], [ "pl", "ay" ], [ "ġstrugg", "ling" ], [ "ġm", "ur" ], [ "ġc", "ross" ], [ "ġta", "x" ], [ "es", "ted" ], [ "ġte", "nd" ], [ "ġpen", "is" ], [ "c", "han" ], [ "ul", "ation" ], [ "ġjo", "bs" ], [ "ġ4", "0" ], [ "ess", "ed" ], [ "gg", "ed" ], [ "ġðł", "ĺ" ], [ "ġbru", "h" ], [ "bo", "x" ], [ "ó¾", "ĩ" ], [ "ut", "ion" ], [ "ġdirect", "ly" ], [ "ġc", "ake" ], [ "ġcon", "cer" ], [ "umb", "lr" ], [ "2", "9" ], [ "p", "re" ], [ "ġt", "able" ], [ "ġb", "ow" ], [ "ġqu", "it" ], [ "ġt", "ues" ], [ "ġde", "lete" ], [ "ġco", "nd" ], [ "ġpay", "ing" ], [ "ġworr", "ies" ], [ "ġcop", "y" ], [ "ġprof", "ess" ], [ "6", "6" ], [ "ġh", "om" ], [ "ġsad", "ly" ], [ "ġdou", "ble" ], [ "ġlangu", "age" ], [ "p", "g" ], [ "ġm", "aths" ], [ "ġd", "ying" ], [ "ġh", "oney" ], [ "id", "es" ], [ "al", "ous" ], [ "ġne", "g" ], [ "ġinter", "view" ], [ "ġmod", "e" ], [ "ġban", "ned" ], [ "b", "t" ], [ "ġm", "ist" ], [ "nd", "ent" ], [ "ġu", "uu" ], [ "ġhe", "av" ], [ "ġst", "ick" ], [ "ġse", "ll" ], [ "ġsp", "ot" ], [ "ġevery", "where" ], [ "ġdr", "ive" ], [ "ġho", "ly" ], [ "ġapolog", "ise" ], [ "ġpoke", "mon" ], [ "ġsle", "pt" ], [ "ġsa", "nd" ], [ "ġser", "ious" ], [ "g", "es" ], [ "r", "ied" ], [ "u", "ff" ], [ "ġy", "ee" ], [ "ġsha", "d" ], [ "nc", "y" ], [ "ġfix", "ed" ], [ "ġfil", "m" ], [ "ġr", "om" ], [ "ġc", "ent" ], [ "ġp", "un" ], [ "ġg", "url" ], [ "ie", "n" ], [ "ġwr", "itten" ], [ "ġph", "ones" ], [ "ġembar", "ass" ], [ "ġmedic", "ation" ], [ "0", "8" ], [ "ġb", "reat" ], [ "us", "es" ], [ "ġlo", "se" ], [ "ġsu", "mm" ], [ "ġse", "ven" ], [ "ġ2", "2" ], [ "led", "ge" ], [ "ġilleg", "al" ], [ "4", "8" ], [ "le", "nt" ], [ "ġcha", "ll" ], [ "ġtr", "ip" ], [ "ġir", "l" ], [ "t", "ime" ], [ "ġj", "oy" ], [ "ġan", "im" ], [ "am", "ine" ], [ "ġfin", "al" ], [ "ġrandom", "ly" ], [ "ġemot", "ions" ], [ "5", "7" ], [ "m", "pt" ], [ "ġbe", "com" ], [ "ġlet", "ting" ], [ "a", "ime" ], [ "ġj", "um" ], [ "ġuse", "ful" ], [ "ġimp", "oss" ], [ "ġje", "alous" ], [ "ġentire", "ly" ], [ "âģ¢ì", "ģ" ], [ "ġtraum", "a" ], [ "ġimmedi", "ately" ], [ "ġuncom", "fortable" ], [ "n", "it" ], [ "ġa", "te" ], [ "ġbene", "f" ], [ "ġs", "kevin" ], [ "ġpic", "s" ], [ "ġameric", "an" ], [ "ġs", "ize" ], [ "ġl", "u" ], [ "ġany", "where" ], [ "ġba", "e" ], [ "ġwear", "ing" ], [ "g", "pt" ], [ "ġs", "ce" ], [ "ure", "s" ], [ "ġbeg", "in" ], [ ")", ":" ], [ "ġsu", "cc" ], [ "ġapp", "ly" ], [ "ġsha", "me" ], [ "ġrem", "ix" ], [ "ġlow", "key" ], [ "ġimposs", "ible" ], [ "ġst", "al" ], [ "ġwhy", "yy" ], [ "ġimp", "ro" ], [ "ġgl", "ass" ], [ "ġdepress", "ion" ], [ "ġlos", "er" ], [ "ġpriv", "ate" ], [ "cra", "ft" ], [ "4", "7" ], [ "te", "p" ], [ "ġare", "n" ], [ "if", "ying" ], [ "ġhelp", "ed" ], [ "ġbab", "ies" ], [ "og", "raph" ], [ "ġind", "e" ], [ "wa", "re" ], [ "ġexc", "use" ], [ "ġphys", "ical" ], [ "âģ¢ì", "ģ" ], [ "g", "f" ], [ "ġl", "ed" ], [ "ġj", "our" ], [ "ust", "ing" ], [ "ġuni", "vers" ], [ "y", "a" ], [ "ġno", "ice" ], [ "ct", "ions" ], [ "ġcut", "ie" ], [ "o", "f" ], [ "ġf", "ish" ], [ "ġcop", "e" ], [ "ma", "le" ], [ "ġob", "ject" ], [ "ġretur", "n" ], [ "ight", "s" ], [ "ġbu", "ilt" ], [ "ġje", "lly" ], [ "n", "b" ], [ "ġi", "rr" ], [ "ġy", "eee" ], [ "ġn", "ine" ], [ "ġu", "t" ], [ "ġim", "age" ], [ "ġcon", "vo" ], [ "ġsign", "ific" ], [ "ġthurs", "day" ], [ "a", "ff" ], [ "ġb", "ound" ], [ "ġhad", "n" ], [ "ġapp", "ear" ], [ "ph", "one" ], [ "ġt", "f" ], [ "or", "ted" ], [ "ġre", "ach" ], [ "ġan", "ime" ], [ "ha", "ps" ], [ "te", "ms" ], [ "ġsh", "o" ], [ "ġch", "ips" ], [ "ġsc", "r" ], [ "ire", "s" ], [ "ġtra", "cks" ], [ "ġac", "es" ], [ "ġhum", "ans" ], [ "te", "red" ], [ "ra", "m" ], [ "ġma", "x" ], [ "ġgr", "an" ], [ "ġclean", "ing" ], [ "icul", "ous" ], [ "ul", "ts" ], [ "ġass", "ist" ], [ "ġstress", "ful" ], [ "ġfig", "ured" ], [ "ġvis", "it" ], [ "ġs", "ou" ], [ "ġse", "arch" ], [ "ġkey", "board" ], [ "ġquick", "ly" ], [ "b", "ack" ], [ "ġg", "ar" ], [ "ġend", "s" ], [ "ġperf", "orm" ], [ "ġp", "ill" ], [ "ġcon", "text" ], [ "ġchild", "ren" ], [ "ġdepe", "nds" ], [ "ġr", "ice" ], [ "ġde", "leted" ], [ "ul", "ance" ], [ "ġtea", "m" ], [ "ġembarr", "ass" ], [ "ġd", "ed" ], [ "us", "ion" ], [ "ġth", "re" ], [ "ġst", "ab" ], [ "..", "?" ], [ "ġen", "s" ], [ "d", "is" ], [ "ġb", "ang" ], [ "ġin", "it" ], [ "ġk", "ay" ], [ "ġshout", "ing" ], [ "c", "her" ], [ "r", "ation" ], [ "v", "ant" ], [ "v", "ices" ], [ "ġa", "hh" ], [ "ġd", "anger" ], [ "ur", "t" ], [ "ġye", "h" ], [ "ġdra", "ma" ], [ "ġra", "c" ], [ "ġhid", "ing" ], [ "ġlaugh", "ing" ], [ "ġb", "on" ], [ "id", "ing" ], [ "3", "5" ], [ "ġp", "age" ], [ "ġp", "ush" ], [ "as", "ed" ], [ "ġab", "use" ], [ "ġab", "usive" ], [ "g", "ed" ], [ "ġsu", "is" ], [ "ġex", "ha" ], [ "ġcom", "fortable" ], [ "ġte", "x" ], [ "ġch", "in" ], [ "ġche", "ap" ], [ "ġhor", "ny" ], [ "n", "ow" ], [ "ġb", "ee" ], [ "ide", "red" ], [ "dd", "ing" ], [ "ġjo", "king" ], [ "ġtw", "itter" ], [ "sh", "it" ], [ "ġstay", "ing" ], [ "ġinst", "ru" ], [ "d", "u" ], [ "ġp", "us" ], [ "ġh", "os" ], [ "ġcha", "nce" ], [ "ġus", "ual" ], [ "ġwa", "aa" ], [ "ġallow", "s" ], [ "ġow", "o" ], [ "miss", "ion" ], [ "ġfun", "ction" ], [ "ġlive", "s" ], [ "ġegg", "s" ], [ "ġwood", "s" ], [ "ġcount", "ry" ], [ "ġsme", "ll" ], [ "ġrid", "iculous" ], [ "ġreco", "good night" ], [ "ġag", "re" ], [ "qu", "en" ], [ "ġcou", "nse" ], [ "ġseco", "nds" ], [ "m", "l" ], [ "ġi", "con" ], [ "ġb", "igg" ], [ "ġf", "ra" ], [ "om", "s" ], [ "ġim", "o" ], [ "ġr", "hy" ], [ "ġfu", "c" ], [ "ġact", "iv" ], [ "ġper", "haps" ], [ "ġbre", "aks" ], [ "ġmist", "ake" ], [ "j", "ust" ], [ "ġwe", "dn" ], [ "ġbot", "her" ], [ "ġbath", "room" ], [ "ber", "ry" ], [ "ġcharac", "ter" ], [ "ġappoint", "ment" ], [ "ġwedn", "es" ], [ "3", "6" ], [ "4", "6" ], [ "he", "re" ], [ "ġm", "ut" ], [ "ġme", "r" ], [ "ol", "ate" ], [ "be", "red" ], [ "ġap", "art" ], [ "ġsile", "nt" ], [ "ġirr", "it" ], [ "t", "re" ], [ "ġmo", "v" ], [ "ġle", "ad" ], [ "ġart", "ic" ], [ "ġintere", "st" ], [ "ġvide", "os" ], [ "!'", "," ], [ "ġopp", "os" ], [ "3", "9" ], [ "ġm", "eee" ], [ "ġf", "ancy" ], [ "ġn", "aked" ], [ "at", "o" ], [ "ġbr", "b" ], [ "ġ9", "0" ], [ "ipp", "ing" ], [ "ġsuggest", "ing" ], [ "f", "orm" ], [ "our", "ce" ], [ "de", "red" ], [ "f", "et" ], [ "ġit", "self" ], [ "ġchat", "gpt" ], [ "ġobv", "ious" ], [ "ġwonder", "ing" ], [ "ġsile", "nce" ], [ "ġsched", "ule" ], [ "ġy", "aaay" ], [ "ġen", "ter" ], [ "ġtou", "gh" ], [ "ġapp", "ropriate" ], [ "fa", "ir" ], [ "ġshow", "ing" ], [ "ġexper", "iment" ], [ "ġdat", "a" ], [ "ġvag", "ina" ], [ "h", "ha" ], [ "is", "ter" ], [ "ġst", "ic" ], [ "ġch", "oo" ], [ "ġsim", "ple" ], [ "fort", "un" ], [ "ġpub", "lic" ], [ "c", "ess" ], [ "f", "w" ], [ "ġto", "i" ], [ "ġp", "een" ], [ "ġcan", "ce" ], [ "ġpre", "tend" ], [ "igh", "ten" ], [ "anc", "es" ], [ "ġmix", "ed" ], [ "ġear", "s" ], [ "ġban", "k" ], [ "f", "un" ], [ "p", "es" ], [ "ġan", "ge" ], [ "ġwan", "k" ], [ "ġpa", "per" ], [ "ġmar", "ry" ], [ "ġgra", "nd" ], [ "ġver", "bal" ], [ "5", "8" ], [ "r", "ics" ], [ "ġn", "ames" ], [ "ġle", "ge" ], [ "ġshould", "n" ], [ "ġtea", "ch" ], [ "ġs", "us" ], [ "ġsh", "y" ], [ "ġsh", "irt" ], [ "ġbutt", "s" ], [ "il", "ing" ], [ "ġad", "am" ], [ "ġoffe", "nded" ], [ "ġorigin", "al" ], [ "v", "id" ], [ "|", "|" ], [ "st", "ats" ], [ "ġde", "g" ], [ "ġcom", "fy" ], [ "ġpre", "scri" ], [ "ġgr", "im" ], [ "ġadd", "ed" ], [ "ġmine", "craft" ], [ "ġes", "sent" ], [ "g", "ing" ], [ "k", "j" ], [ "s", "u" ], [ "ġtr", "igger" ], [ "ġcall", "s" ], [ "ġtra", "vel" ], [ "a", "o" ], [ "g", "l" ], [ "ġ", "ó¾" ], [ "if", "ied" ], [ "ġcons", "idered" ], [ "ġtic", "kets" ], [ "ġhol", "iday" ], [ "ġa", "gg" ], [ "ġin", "ten" ], [ "ick", "y" ], [ "ġco", "ll" ], [ "ġjo", "h" ], [ "ġreme", "m" ], [ "ġsitu", "ations" ], [ "t", "hat" ], [ "y", "es" ], [ "ġun", "fortun" ], [ "ġdraw", "s" ], [ "ġbre", "aking" ], [ "ġinde", "pe" ], [ "ct", "ing" ], [ "ġapp", "lic" ], [ "ġreg", "ret" ], [ "ġtas", "ks" ], [ "ġa", "u" ], [ "ve", "re" ], [ "ġp", "ure" ], [ "ġv", "io" ], [ "ġtr", "ash" ], [ "ġlitera", "l" ], [ "ġins", "ane" ], [ "ġass", "ume" ], [ "lo", "ve" ], [ "we", "ll" ], [ "ġed", "it" ], [ "ġsksks", "ks" ], [ "uin", "o" ], [ "4", "9" ], [ "u", "ll" ], [ "ġd", "ry" ], [ "er", "y" ], [ "ġhis", "t" ], [ "ġ>.", "<" ], [ "0", "1" ], [ "v", "a" ], [ "ġom", "ggg" ], [ "ġtal", "ked" ], [ "pl", "oy" ], [ "ġold", "er" ], [ "ġchris", "t" ], [ "ġneg", "ative" ], [ "ġoppos", "ite" ], [ "ġt", "att" ], [ "ġc", "um" ], [ "le", "t" ], [ "le", "y" ], [ "ġj", "imin" ], [ "ġad", "vent" ], [ "ġhard", "er" ], [ "ġset", "ting" ], [ "ġtechn", "ology" ], [ "ġoverwhel", "med" ], [ "ġassa", "ult" ], [ "ġdiag", "n" ], [ "i", "um" ], [ "i", "est" ], [ "â", "ľ" ], [ "ġpro", "g" ], [ "ġve", "ux" ], [ "ġcl", "a" ], [ "ġep", "is" ], [ "ar", "ies" ], [ "ġlo", "ck" ], [ "ġfro", "g" ], [ "ġremem", "bered" ], [ "3", "7" ], [ "a", "cc" ], [ "ġd", "are" ], [ "ġn", "et" ], [ "te", "ry" ], [ "ġsu", "it" ], [ "ġca", "used" ], [ "ġfre", "aking" ], [ "ġprom", "ise" ], [ "c", "r" ], [ "ġst", "orm" ], [ "ġcon", "vin" ], [ "ġsha", "ll" ], [ "ġchin", "ese" ], [ "à", "" ], [ "ġt", "ha" ], [ "at", "ic" ], [ "ġj", "am" ], [ "ġoff", "ic" ], [ "aa", "a" ], [ "ġsol", "id" ], [ "d", "i" ], [ "v", "al" ], [ "ġa", "lex" ], [ "on", "a" ], [ "ut", "m" ], [ "ġst", "or" ], [ "ġman", "ip" ], [ "ġsha", "ring" ], [ "ġslow", "ly" ], [ "ġme", "at" ], [ "ġab", "ility" ], [ "th", "ough" ], [ "ci", "ous" ], [ "ġgr", "ound" ], [ "ġincl", "ude" ], [ "ġunfortun", "ately" ], [ "ġh", "ous" ], [ "ġrant", "ing" ], [ "ġtatt", "oo" ], [ "b", "by" ], [ "d", "it" ], [ "ġ", "êķãģ£" ], [ "ġid", "fk" ], [ "ġch", "oc" ], [ "ġchar", "ge" ], [ "ġprov", "ide" ], [ "ġdelive", "ry" ], [ "ll", "m" ], [ "an", "k" ], [ "am", "ing" ], [ "ġsa", "uce" ], [ "ġde", "velop" ], [ "ph", "ones" ], [ "ġign", "oring" ], [ "ġprog", "ress" ], [ "b", "ed" ], [ "ke", "s" ], [ "ġshow", "ed" ], [ "ġ8", "0" ], [ "ġcomm", "on" ], [ "ict", "ion" ], [ "ġcu", "nt" ], [ "ġphys", "ically" ], [ "ġmemor", "y" ], [ "ġattem", "pt" ], [ "ġhos", "pital" ], [ "ġt", "id" ], [ "ġp", "in" ], [ "as", "m" ], [ "ġj", "s" ], [ "ġ2", "1" ], [ "ġcat", "ch" ], [ "ġlight", "s" ], [ "ġbuild", "ing" ], [ "ġjud", "ge" ], [ "ġprescri", "ption" ], [ "ing", "ly" ], [ "ġsh", "ou" ], [ "ġv", "ar" ], [ "ġbl", "ame" ], [ "ġcl", "ick" ], [ "ġma", "is" ], [ "ġste", "al" ], [ "ġtop", "ic" ], [ "ġreply", "ing" ], [ "ġfollow", "ing" ], [ "ġchang", "ing" ], [ "ġrepl", "ied" ], [ "ġamb", "ulance" ], [ "fet", "y" ], [ "u", "ral" ], [ "ġu", "hhh" ], [ "ġsound", "ed" ], [ "ġad", "ult" ], [ "ġbo", "at" ], [ "ġnew", "s" ], [ "ġme", "lt" ], [ "ġca", "p" ], [ "ab", "les" ], [ "ġex", "tre" ], [ "ġhop", "ing" ], [ "ġdown", "stairs" ], [ "ġunderst", "ood" ], [ "ġaaa", "gh" ], [ "ġenjoy", "ing" ], [ "ġly", "rics" ], [ "ġtues", "day" ], [ "ġagg", "ress" ], [ "5", "9" ], [ "h", "op" ], [ "ġn", "hs" ], [ "ġme", "th" ], [ "ġj", "fc" ], [ "ġre", "search" ], [ "ġunivers", "ity" ], [ "ss", "s" ], [ "ġen", "v" ], [ "ġmed", "ical" ], [ "ġwindow", "s" ], [ "under", "st" ], [ "ġsand", "w" ], [ "ġwednes", "day" ], [ "u", "it" ], [ "ġc", "ult" ], [ "ġl", "ou" ], [ "ġbe", "ar" ], [ "ġdo", "se" ], [ "ge", "nt" ], [ "ġvo", "id" ], [ "ġpr", "ior" ], [ "act", "iv" ], [ "ġsy", "mpt" ], [ "ġaud", "io" ], [ "l", "ol" ], [ "ġm", "r" ], [ "ġd", "om" ], [ "an", "o" ], [ "ġk", "icked" ], [ "ġact", "ing" ], [ "ġvoc", "als" ], [ "om", "g" ], [ "ġwe", "t" ], [ "ra", "ted" ], [ "ġlma", "oo" ], [ "ġpre", "sent" ], [ "ġser", "vice" ], [ "ġmar", "ried" ], [ "ġchall", "eng" ], [ "c", "ore" ], [ "g", "b" ], [ "ic", "y" ], [ "st", "r" ], [ "ation", "al" ], [ "ib", "ility" ], [ "ġcl", "ar" ], [ "ġcl", "ub" ], [ "ġboo", "b" ], [ "ġreason", "able" ], [ "êĺ", "êķãģ£" ], [ "u", "ro" ], [ "re", "nc" ], [ "ġn", "at" ], [ "ġp", "unch" ], [ "ġknow", "ledge" ], [ "ġ2", "8" ], [ "ġser", "vices" ], [ "ġpos", "ted" ], [ "cha", "ris" ], [ "ġeffect", "s" ], [ "ġi", "ce" ], [ "ġto", "ys" ], [ "ie", "ld" ], [ "ġk", "ing" ], [ "ġsor", "ta" ], [ "ġpre", "v" ], [ "ġsim", "p" ], [ "ġmod", "ule" ], [ "ġref", "er" ], [ "ġev", "il" ], [ "ġm", "outh" ], [ "ic", "ks" ], [ "ġat", "tra" ], [ "ot", "s" ], [ "ġen", "cou" ], [ "ġfe", "male" ], [ "os", "is" ], [ "sh", "ips" ], [ "el", "s" ], [ "ġvis", "ual" ], [ "ġargu", "ment" ], [ "ġr", "out" ], [ "ġsa", "fety" ], [ "il", "ities" ], [ "ġper", "mission" ], [ "ġrel", "ated" ], [ "bab", "y" ], [ "j", "pg" ], [ "ġs", "an" ], [ "ġu", "hh" ], [ "ġtr", "ou" ], [ "ġve", "get" ], [ "ġshow", "s" ], [ "ġten", "ant" ], [ "ġbor", "n" ], [ "ġhang", "ing" ], [ "ġmild", "ly" ], [ "i", "ry" ], [ "ġb", "ase" ], [ "is", "ting" ], [ "ġp", "ink" ], [ "ġh", "ilar" ], [ "ġca", "using" ], [ "ġun", "fair" ], [ "ġeas", "ily" ], [ "0", "7" ], [ "on", "y" ], [ "ġwe", "dding" ], [ "ġbus", "iness" ], [ ":", ":" ], [ "ġin", "tera" ], [ "id", "s" ], [ "ra", "id" ], [ "ġan", "g" ], [ "ġne", "igh" ], [ "il", "s" ], [ "gg", "y" ], [ "ġun", "like" ], [ "ġcustom", "er" ], [ "ġrev", "ision" ], [ "c", "ord" ], [ "d", "r" ], [ "i", "ans" ], [ "ll", "o" ], [ "ġno", "tes" ], [ "ġplay", "list" ], [ "ġcomp", "lic" ], [ "ġleg", "s" ], [ "ġgod", "dam" ], [ "ġexplain", "ing" ], [ "ġshou", "ted" ], [ "ġid", "c" ], [ "ġvo", "lu" ], [ "ġter", "ms" ], [ "ġcook", "ies" ], [ "ġexam", "ple" ], [ "ġdetail", "s" ], [ "ġmur", "der" ], [ "a", "f" ], [ "ġi", "l" ], [ "te", "st" ], [ "ġin", "put" ], [ "ġle", "cture" ], [ "ff", "f" ], [ "ġbo", "ss" ], [ "ġpart", "s" ], [ "ġresp", "ond" ], [ "ġhold", "ing" ], [ "ġdisg", "usting" ], [ "6", "7" ], [ "ġs", "i" ], [ "ve", "ry" ], [ "ġg", "inger" ], [ "ġj", "us" ], [ "ġst", "ation" ], [ "ġfee", "t" ], [ "ġreal", "ity" ], [ "ġreact", "ion" ], [ "l", "it" ], [ "ġha", "t" ], [ "ġme", "al" ], [ "et", "te" ], [ "me", "n" ], [ "est", "yle" ], [ "ġmar", "ks" ], [ "het", "ic" ], [ "ġemp", "ty" ], [ "ġterr", "ible" ], [ "ġhist", "ory" ], [ "ġneigh", "b" ], [ "ġ", "@" ], [ "is", "ions" ], [ "ġor", "dered" ], [ "th", "is" ], [ "ġpo", "tent" ], [ "ġrep", "ea" ], [ "bo", "om" ], [ "ġexplain", "ed" ], [ "ġindepe", "ndent" ], [ "ġêķãģ£", "êĺ" ], [ "0", "5" ], [ "ġn", "om" ], [ "ġh", "ig" ], [ "ġph", "r" ], [ "ġpr", "in" ], [ "ġro", "le" ], [ "haha", "h" ], [ "ġc", "od" ], [ "ve", "red" ], [ "ġn", "ick" ], [ "ġwh", "is" ], [ "ġmo", "i" ], [ "ib", "er" ], [ "ġstart", "s" ], [ "ġmis", "underst" ], [ "ġrecord", "ing" ], [ "ġmag", "ic" ], [ "ġuns", "ure" ], [ "as", "es" ], [ "ġr", "ush" ], [ "ġse", "par" ], [ "ive", "s" ], [ "um", "my" ], [ "ġom", "l" ], [ "ġbu", "ying" ], [ "ġadd", "ing" ], [ "ġimag", "in" ], [ "sor", "ry" ], [ "ġbehavi", "our" ], [ "ġd", "ig" ], [ "ġd", "ating" ], [ "ġh", "oo" ], [ "ġstu", "dy" ], [ "ġbl", "ind" ], [ "iz", "ed" ], [ "o", "om" ], [ "s", "ing" ], [ "ġs", "ea" ], [ "ġevery", "ones" ], [ "ġhar", "row" ], [ "uss", "y" ], [ "ġchoc", "olate" ], [ "8", "8" ], [ "ġe", "r" ], [ "ġit", "ll" ], [ "ill", "ed" ], [ "ġst", "ore" ], [ "ġso", "c" ], [ "ġan", "ger" ], [ "ġmu", "a" ], [ "ġna", "p" ], [ "cc", "o" ], [ "ġshop", "ping" ], [ "ġchair", "s" ], [ "ġcertain", "ly" ], [ "ġinvol", "ved" ], [ "ġdeg", "ree" ], [ "c", "an" ], [ "l", "ines" ], [ "ġa", "ve" ], [ "ġb", "es" ], [ "ġto", "oo" ], [ "ġare", "a" ], [ "ne", "ur" ], [ "ġpos", "itive" ], [ "ġ", "." ], [ "re", "st" ], [ "ġc", "ream" ], [ "ġfor", "ced" ], [ "ir", "on" ], [ "ca", "pe" ], [ "ġra", "dio" ], [ "ġad", "vert" ], [ "ġsk", "ull" ], [ "ġpot", "ato" ], [ "ġsce", "ne" ], [ "ġhilar", "ious" ], [ "4", "4" ], [ "ġre", "ee" ], [ "ġk", "ink" ], [ "ġke", "ys" ], [ "ġpre", "good night" ], [ "ġna", "hhh" ], [ "ġaf", "raid" ], [ "ġsc", "am" ], [ "ġho", "le" ], [ "ġsen", "tence" ], [ "ġmid", "night" ], [ "ġdistra", "cted" ], [ "b", "um" ], [ "ġu", "g" ], [ "ġj", "ul" ], [ "ġdi", "re" ], [ "ġhome", "less" ], [ "ġstat", "us" ], [ "ġdrag", "on" ], [ "ġlege", "nd" ], [ "ġextre", "me" ], [ "m", "od" ], [ "ġt", "un" ], [ "an", "a" ], [ "out", "put" ], [ "ġfa", "iled" ], [ "ġcolour", "s" ], [ "ġocc", "as" ], [ "ġartic", "le" ], [ "d", "k" ], [ "ġl", "ab" ], [ "ic", "ted" ], [ "ġk", "s" ], [ "ant", "ic" ], [ "ġpr", "oof" ], [ "ġbring", "ing" ], [ "ġspam", "ming" ], [ "ġaccept", "able" ], [ "b", "and" ], [ "ġhe", "at" ], [ "ġme", "ga" ], [ "ro", "se" ], [ "ġback", "ground" ], [ "ġche", "cking" ], [ "ġfl", "ow" ], [ "ġbas", "ic" ], [ "ġpoint", "less" ], [ "ġstrugg", "le" ], [ "ġstre", "et" ], [ "ġbull", "shit" ], [ "s", "r" ], [ "ġm", "al" ], [ "ġto", "n" ], [ "ġpro", "p" ], [ "ġperson", "ally" ], [ "ġt", "es" ], [ "ġd", "y" ], [ "ġe", "aten" ], [ "ġst", "aff" ], [ "ġup", "stairs" ], [ "ġsa", "v" ], [ "ġde", "vice" ], [ "ġhapp", "iness" ], [ "ġco", "ff" ], [ "ġdec", "isions" ], [ "ġlevel", "s" ], [ "3", "2" ], [ "8", "7" ] ] } }# charis cat 2025 # - ʕっʘ‿ʘʔ⊃ -*- babyllm -*- ⊂ʕʘ‿ʘ૮ʔ - # output layer for logit prediction # brain/layers/logits.py import torch import torch.nn as nn from config import * """final layer, maps neuron activations to logits for each token in the vocab""" class logits(nn.module): def __init__(self, _counsellor, _device): super().__init__() self.device = _device self.counsellor = _counsellor self.lastsavedweights = 0 # for stats self.l_weights = nn.parameter(torch.randn(numneurons, vocabsize, device = self.device)) # this is set to move the neuron activations (1000) onto vocab size (2000) self.l_bias = nn.parameter(torch.zeros(vocabsize, device = self.device)) self.activationnorm = nn.layernorm(numneurons, device = self.device) self.rawactivationsscale = nn.parameter(torch.tensor(0.5)) self.normedactivationsscale = nn.parameter(torch.tensor(0.5)) self.logitnorm = nn.layernorm(vocabsize, device = self.device) self.outputscale = nn.parameter(torch.tensor(0.5)) self.normoutputscale = nn.parameter(torch.tensor(0.5)) self.stats = {} self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] @whocalled def forward(self, _meanactivationstensor): with self.counsellor.infodump("forward") as ʕっʘ‿ʘʔっ: # <- = from # inn? -> l1 -> l2 -> l3 -> l4 -> l5 -> l6 -> * """imports the activations from interneuronnetwork, assuming that is is a tensor""" ʕっʘ‿ʘʔっ("l1: activationstensor") # <- inn? no? seems to come from babyllm? maybe through babyllm? self.activationstensor = _meanactivationstensor # _1 #self.testat = _meanactivationstensor ʕっʘ‿ʘʔっ("l2: normedactivationstensor") # <- l1 self.normedactivationstensor = self.activationnorm(self.activationstensor) # _2 ʕっʘ‿ʘʔっ("l3: scaledactivations") # <- l1 + l2 self.scaledactivations = (self.activationstensor * self.rawactivationsscale) + (self.normedactivationstensor * self.normedactivationsscale) # _3 if debugprints: print(f"debug logits: activations shape before @ weights: {self.scaledactivations.shape}") if debugprints: print(f"debug logits: weights shape: {self.l_weights.shape}") ʕっʘ‿ʘʔっ("l4: logitoutput") # <- l3 (with weights and bias) logitoutputnormalized = (self.scaledactivations @ self.l_weights) / (numneurons ** 0.5) + self.l_bias logitoutputoriginal = self.scaledactivations @ self.l_weights + self.l_bias self.logitoutput = (logitoutputoriginal + logitoutputnormalized)/2 # _4 ʕっʘ‿ʘʔっ("l5: logitnormed") # <- l4 self.logitnormed = self.logitnorm(self.logitoutput) # _5 ʕっʘ‿ʘʔっ("l6: finallogit") # <- l4 + l5 self.finallogit = (self.logitoutput * self.outputscale) + (self.logitnormed * self.normoutputscale) # _6 if debugprints: print(f"debug logits: logitoutput shape after @ weights: {self.logitoutput.shape}") ʕっʘ‿ʘʔっ("append rolling self.stats") self.tensorhist.append(self.activationstensor.norm().item()) self.normedhist.append(self.normedactivationstensor.norm().item()) self.activhist.append(self.scaledactivations.norm().item()) self.logithist.append(self.logitoutput.norm().item()) self.logitnormhist.append(self.logitnormed.norm().item()) self.finallogithist.append(self.finallogit.norm().item()) if len(self.tensorhist) >= windowmax: ʕっʘ‿ʘʔっ("clear rolling self.stats at end of window") self.stats = { "6l_0_activationstensor_norm": sum(self.tensorhist) / len(self.tensorhist), "6l_1_normedactivationstensor_norm": sum(self.normedhist) / len(self.normedhist), "6l_2_scaledactivations_norm": sum(self.activhist) / len(self.activhist), "6l_3_logitoutput_norm": sum(self.logithist) / len(self.logithist), "6l_4_logitnormed_norm": sum(self.logitnormhist) / len(self.logitnormhist), "6l_x_finallogit_norm": sum(self.finallogithist) / len(self.finallogithist), } self.tensorhist = [] self.normedhist = [] self.activhist = [] self.logithist = [] self.logitnormhist = [] self.finallogithist = [] # return logits (not softmax) for better gradient computation in cross-entropy loss return self.finallogit # l6 -> def getlogitstats(self): with self.counsellor.infodump("getlogitstats") as ʕっʘ‿ʘʔっ: with torch.no_grad(): ʕっʘ‿ʘʔっ("weightnormstats") weightnorms = torch.norm(self.l_weights, dim = 0) self.stats["logitweightnormmean"] = weightnorms.mean() self.stats["logitweightnormstd"] = weightnorms.std() self.stats["logitweightnormmax"] = weightnorms.max() # scales (dont need on per token history as only updated in backward) self.stats["6l_0_activationstensor_scale"] = self.rawactivationsscale.norm().item() self.stats["6l_1_normedactivationstensor_scale"] = self.normedactivationsscale.norm().item() self.stats["6l_3_logitoutput_scale"] = self.outputscale.norm().item() self.stats["6l_4_logitnormed_scale"] = self.normoutputscale.norm().item() ʕっʘ‿ʘʔっ("sparsitystat") sparsity = (self.l_weights.abs() < 1e-5).float().mean() self.stats["logitweightsparsity"] = sparsity ʕっʘ‿ʘʔっ("weightdriftstat") drift = torch.norm(self.l_weights - self.lastsavedweights) self.stats["logitweightdrift"] = drift self.lastsavedweights = self.l_weights.clone().detach() ʕっʘ‿ʘʔっ("biasstats") self.stats["logitbiasmean"] = self.l_bias.mean() self.stats["logitbiasstd"] = self.l_bias.std() self.stats["logitbiasmax"] = self.l_bias.max() if hasattr(self, 'latestactivations'): ʕっʘ‿ʘʔっ("activationstats") act = self.latestactivations self.stats["activationstd"] = act.std() self.stats["activationmean"] = act.mean() self.stats["activationmax"] = act.max() self.stats["activationmin"] = act.min() self.stats["activationsparsity"] = (act.abs() < 1e-6).float().mean() return self.stats if __name__ == "__main__": testlayeractivations = torch.randn(numneurons) logits = logits(numneurons = numneurons, vocabsize = vocabsize) logitoutput = logits.forward(testlayeractivations) print("- logits testing start -") print(f"output layer created with {logits.vocabsize} vocabulary tokens.") print(f"weight matrix shape: {logits.weights.shape}") print(f"bias vector shape: {logits.bias.shape}") print(f"logits (first 100):") print(logitoutput[:10]) print(f"logits shape: {logitoutput.shape}") print("- logits testing complete -")